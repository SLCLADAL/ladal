knitr::include_graphics("https://slcladal.github.io/images/uq1.jpg")
knitr::include_graphics("https://slcladal.github.io/images/gy_chili.jpg")
# activate packages
library(pdftools)
library(tesseract)
library(tidyverse)
library(here)
library(hunspell)
# set tesseract engine
eng <- tesseract("eng")
# activate klippy for copy-to-clipboard button
klippy::klippy()
knitr::include_graphics("https://slcladal.github.io/images/pdf0.png")
# you can use an url or a path that leads to a pdf document
pdf_path <- "https://slcladal.github.io/data/PDFs/pdf0.pdf"
# extract text
txt_output <- pdftools::pdf_text(pdf_path) %>%
paste0(collapse = " ") %>%
paste0(collapse = " ") %>%
stringr::str_squish()
# show data
txt_output %>%
substr(start = 1, stop = 1000) %>%
as.data.frame() %>%
flextable::flextable() %>%
flextable::set_table_properties(width = .95, layout = "autofit") %>%
flextable::theme_zebra() %>%
flextable::fontsize(size = 12) %>%
flextable::fontsize(size = 12, part = "header") %>%
flextable::align_text_col(align = "center") %>%
flextable::set_caption(caption = "First 1000 characters of the extracted text from a pdf of the wikipedia article on corpus linguistics.") %>%
flextable::border_outer()
convertpdf2txt <- function(dirpath) {
files <- list.files(dirpath, full.names = T)
x <- sapply(files, function(x) {
x <- pdftools::pdf_text(x) %>%
paste0(collapse = " ") %>%
stringr::str_squish()
return(x)
})
}
# apply function
txts <- convertpdf2txt(here::here("data", "PDFs/"))
# show data
txts %>%
substr(start = 1, stop = 1000) %>%
as.data.frame() %>%
flextable::flextable() %>%
flextable::set_table_properties(width = .95, layout = "autofit") %>%
flextable::theme_zebra() %>%
flextable::fontsize(size = 12) %>%
flextable::fontsize(size = 12, part = "header") %>%
flextable::align_text_col(align = "center") %>%
flextable::set_caption(caption = "First 1000 characters of the extracted texts from pdfs of selected wikipedia articles.") %>%
flextable::border_outer()
fls <- list.files(here::here("data/PDFs"), full.names = T)
# load
ocrs <- sapply(fls, function(x) {
# store name
nm <- stringr::str_replace_all(x, ".*/(.*?).pdf", "\\1")
# perform ocr
x <- tesseract::ocr(x, engine = eng) %>%
paste0(collapse = " ")
})
# show data
ocrs %>%
substr(start = 1, stop = 1000) %>%
as.data.frame() %>%
flextable::flextable() %>%
flextable::set_table_properties(width = .95, layout = "autofit") %>%
flextable::theme_zebra() %>%
flextable::fontsize(size = 12) %>%
flextable::fontsize(size = 12, part = "header") %>%
flextable::align_text_col(align = "center") %>%
flextable::set_caption(caption = "First 1000 characters of the extracted text from the  wikipedia article on corpus linguistics.") %>%
flextable::border_outer()
# create token list
tokens_ocr <- sapply(ocrs, function(x) {
x <- hunspell::hunspell_parse(x)
})
# show data
tokens_ocr %>%
substr(start = 1, stop = 1000) %>%
as.data.frame() %>%
flextable::flextable() %>%
flextable::set_table_properties(width = .95, layout = "autofit") %>%
flextable::theme_zebra() %>%
flextable::fontsize(size = 12) %>%
flextable::fontsize(size = 12, part = "header") %>%
flextable::align_text_col(align = "center") %>%
flextable::set_caption(caption = "First 1000 characters of the spell-checked  wikipedia article on corpus linguistics.") %>%
flextable::border_outer()
# clean
clean_ocrtext <- sapply(tokens_ocr, function(x) {
correct <- hunspell::hunspell_check(x)
x <- ifelse(correct == F,
x[hunspell::hunspell_check(x)],
x
)
x <- paste0(x, collapse = " ")
})
# show data
clean_ocrtext %>%
substr(start = 1, stop = 1000) %>%
as.data.frame() %>%
flextable::flextable() %>%
flextable::set_table_properties(width = .95, layout = "autofit") %>%
flextable::theme_zebra() %>%
flextable::fontsize(size = 12) %>%
flextable::fontsize(size = 12, part = "header") %>%
flextable::align_text_col(align = "center") %>%
flextable::set_caption(caption = "First 1000 characters of the processed text from a pdf of the wikipedia article on corpus linguistics.") %>%
flextable::border_outer()
sessionInfo()
install.packages("rgdal")
