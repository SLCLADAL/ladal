{
  "hash": "fac399e32cb3eb6fe3c845fc5187dac6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Dimension Reduction Methods with R - AcqVA Aurora workshop\"\nauthor: \"Martin Schweinberger\"\n---\n\n\n\n![](/images/uq1.jpg){ width=100% }\n\n# Introduction{-}\n\nThis tutorial introduces dimension reduction methods with R with the aim of showcasing how these methods work, how to prepare data and how to implement selected dimension reduction methods (Principal Component Analysis, Factor Analysis, and Multidimensional Scaling). Unfortunately, we cannot deal with other, more complex dimension reduction methods such as Uniform Manifold Approximation and Projection (UMAP) or t-Distributed Stochastic Neighbor Embedding (t-SNE) in this tutorial (but we plan on adding a separate tutorial for these methods in the future). The reduction of multiple variables is useful for many things, e.g., for visualization, noise reduction, and simplifying complex data sets. However, it's essential to note that interpretability of the principal components may not always be straightforward, as they are linear combinations of the original features.\n\n![](/images/gy_chili.png){ width=15% style=\"float:right; padding:10px\" }\n\nThis tutorial is aimed at beginners and intermediate users of R. The aim is not to provide a fully-fledged guide but rather to show and exemplify some common dimension reduction methods with R.\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nThe entire R Notebook for the tutorial can be downloaded [**here**](https://slcladal.github.io/content/dimred.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rproj file. <br><br>\n**[Here](https://colab.research.google.com/drive/16VNNnRXAC6CFU9oBwLlQsoG_Xpub-CRl?usp=sharing)** is a **link to an interactive version of this tutorial on Binder**. The interactive tutorial is based on a Jupyter notebook of this tutorial. This interactive Jupyter notebook allows you to execute code yourself and - if you copy the Jupyter notebook - you can also change and edit the notebook, e.g. you can change code and upload your own data.<br></p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\nTo be able to follow this tutorial, we suggest you check out and familiarize yourself with the content of the following **R Basics** tutorials:<br>\n</p>\n<p style='margin-top:1em; text-align:left'>\n<ul>\n  <li>[Introduction to Quantitative Reasoning](https://ladal.edu.au/introquant.html) </li>\n  <li>[Basic Concepts in Quantitative Research](https://ladal.edu.au/basicquant.html) </li>\n  <li>[Getting started with R](https://ladal.edu.au/intror.html) </li>\n  <li>[Loading, saving, and generating data in R](https://ladal.edu.au/load.html) </li>\n</ul>\n</p>\n<p style='margin-top:1em; text-align:center'>\nClick [**here**](https://ladal.edu.au/content/kwics.Rmd)^[If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://slcladal.github.io/content/bibliography.bib) and store it in the same folder where you store the Rmd file.] to download the **entire R Notebook** for this tutorial.<br><br>\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252Fdimred_cb.ipynb%26branch%3Dmain)<br>\nClick [**here**](https://mybinder.org/v2/gh/SLCLADAL/interactive-notebooks-environment/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252FSLCLADAL%252Finteractive-notebooks%26urlpath%3Dlab%252Ftree%252Finteractive-notebooks%252Fnotebooks%252Fdimred_cb.ipynb%26branch%3Dmain) to open an interactive Jupyter notebook that allows you to execute, change, and edit the code as well as to upload your own data. <br>\n</p>\n<p style='margin-left:1em;'>\n</p></span>\n</div>\n\n<br>\n\n**Preparation and session set up**\n\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R [here](https://slcladal.github.io/intror.html). For this tutorials, we need to install certain *packages* from an R *library* so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"writexl\")\ninstall.packages(\"here\")\ninstall.packages(\"flextable\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"MASS\")\ninstall.packages(\"factoextra\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"report\")\ninstall.packages(\"psych\")\ninstall.packages(\"tufte\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n```\n:::\n\n\n\n\nNow that we have installed the packages, we activate them as shown below.\n\n\nAfter installing these packages, we fetch them from the library to activate the packages.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(scipen = 999) # supress math. notation\nlibrary(dplyr)\nlibrary(here)\nlibrary(flextable)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(MASS)\nlibrary(factoextra)\nlibrary(ggplot2)\nlibrary(report)\nlibrary(psych)\nlibrary(tufte)\n```\n:::\n\n\n\n\n# What are dimension reduction methods? {-}\n\nDimension reduction methods such as Principal Component Analysis (PCA), Multidimensional Scaling (MDS), and Factor Analysis are techniques used to *reduce the number of variables or dimensions in a data set while preserving as much relevant information as possible*. \n\nThe choice of method depends on the specific goals of the analysis and the nature of the data being analyzed. Each method addresses different aspects of dimension reduction:\n\n+ PCA emphasizes variance\n\n+ MDS emphasizes pairwise distances\n\n+ Factor Analysis emphasizes the underlying latent factors.\n\nBelow are brief explanations of the three commonly used dimension reduction methods.\n\n**Principal Component Analysis (PCA)**\n\nPrincipal Component Analysis is a statistical technique that transforms a data set into a new coordinate system where the variables (features) are linearly uncorrelated.\n\nIt aims to find a set of orthogonal axes (principal components) in such a way that the first principal component accounts for the maximum variance in the data, the second principal component for the second maximum variance, and so on. By selecting a subset of these principal components, you can achieve dimension reduction while retaining the most significant information in the data.\n\nUnfortunately, PCA only works really well when the number of features (or variables) isn't too big and when the majority of the variance is explained by 2 or 3 principal components. If you are dealing with a very large data set with many features, UMAP is a better alternative.\n\n**Multidimensional Scaling (MDS)**\n\nMultidimensional Scaling is a method used to represent high-dimensional data in a lower-dimensional space (usually 2D or 3D) while maintaining pairwise distances or similarities between data points. MDS attempts to preserve the relationships among data points as much as possible. It's often used in visualization to represent complex data in a way that makes it easier to interpret or analyze.\n\nLike PCA, MDS only works really well when the number of variables isn't too big and when the majority of the variance is explained by 2 or 3 dimensions.\n\n**Factor Analysis**\n\nFactor Analysis is a statistical technique that aims to uncover underlying latent factors that contribute to the observed variables in a data set. It's particularly useful when dealing with data where variables may be correlated, and you want to identify common factors that explain the shared variance. Factor Analysis assumes that observed variables are influenced by both the common factors and unique factors specific to each variable.\n\n# Principal Component Analysis{-}\n\nPrincipal Component Analysis (PCA) [see, e.g., @clark2011introduction] is  used for dimensionality reduction and data compression while preserving as much variance as possible in the data. It achieves this by transforming the original data into a new coordinate system defined by a set of orthogonal axes called *principal components*. The first principal component captures the maximum variance in the data, the second captures the second maximum variance, and so on.\n\n>\"Imagine you have just opened a cider shop. You have 50 varieties of cider and you want to work out how to allocate them onto shelves, so that similar-tasting ciders are put on the same shelf. There are lots of different tastes and textures in cider - sweetness, tartness, bitterness, yeastiness, fruitiness, clarity, fizziness etc etc. So what you need to do to put the bottles into categories is answer two questions:\n>\n>1) What qualities are most important for identifying groups of ciders? e.g. does classifying based on sweetness make it easier to cluster your ciders into similar-tasting groups than classifying based on fruitiness?\n>\n>2) Can we reduce our list of variables by combining some of them? e.g. is there actually a variable that is some combination of \"yeastiness and clarity and fizziness\" and which makes a really good scale for classifying varieties?\n>\n>This is essentially what PCA does. Principal components are variables that usefully explain variation in a data set - in this case, that usefully differentiate between groups. Each principal component is one of your original explanatory variables, or a combination of some of your original explanatory variables.\" \n><footer>Freya Harrison</footer>\n\n\nLet's delve into an easy example to get a better understanding of the theoretical underpinnings of PCA and to see how it works [this section is based on @stackpca3] and a very brief, nice, and easy to understand explanation of PCA is available [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/2700#2700) [@stackpca4]... \n\n**Primer: Why should I use a PCA and what does PCA tell us?**\n\nImagine we have a data set representing measurements of 4 features across a sample of 6 languages. We now want to see if the languages from groups based on the frequencies of these features.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-d4b1ce04{}.cl-d4ae4dce{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d4afb966{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d4afb970{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d4afc88e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4afc88f{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4afc890{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4afc891{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4afc898{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4afc899{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-d4b1ce04'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-d4afc88e\"><p class=\"cl-d4afb966\"><span class=\"cl-d4ae4dce\">Language</span></p></th><th class=\"cl-d4afc88f\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">Feat1</span></p></th><th class=\"cl-d4afc88f\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">Feat2</span></p></th><th class=\"cl-d4afc88f\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">Feat3</span></p></th><th class=\"cl-d4afc88f\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">Feat4</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d4afc890\"><p class=\"cl-d4afb966\"><span class=\"cl-d4ae4dce\">Lang1</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">10</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">6.0</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">12.0</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d4afc890\"><p class=\"cl-d4afb966\"><span class=\"cl-d4ae4dce\">Lang2</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">11</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">4.0</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">9.0</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">7</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d4afc890\"><p class=\"cl-d4afb966\"><span class=\"cl-d4ae4dce\">Lang3</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">8</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">5.0</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">10.0</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">6</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d4afc890\"><p class=\"cl-d4afb966\"><span class=\"cl-d4ae4dce\">Lang4</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">3</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">3.0</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">2.5</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">2</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d4afc890\"><p class=\"cl-d4afb966\"><span class=\"cl-d4ae4dce\">Lang5</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">1</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">2.8</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">1.3</span></p></td><td class=\"cl-d4afc891\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d4afc898\"><p class=\"cl-d4afb966\"><span class=\"cl-d4ae4dce\">Lang6</span></p></td><td class=\"cl-d4afc899\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">2</span></p></td><td class=\"cl-d4afc899\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">1.0</span></p></td><td class=\"cl-d4afc899\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">2.0</span></p></td><td class=\"cl-d4afc899\"><p class=\"cl-d4afb970\"><span class=\"cl-d4ae4dce\">7</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\nTo understand how PCA works, let us start by plotting the first feature (Feat1).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nWhen we check the visualization of the first feature, we see that there appear to be two groups in our data!\n\nWe now add the second feature (Feat2) to check if the second feature adds information and supports our hypothesis that there are two types of languages in our data based on the features we collected.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nWhile adding the second feature has not added much information (in that sense, Feat2 not as distinctive), there still appear to be two groups. However, we continue by adding the third feature (Feat3) by adding size as a way to show the third feature.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nFeat3 supports the two group hypothesis as small dots have values below 5 and big dots have values above 5 on the first feature scale. Finally, we add the fourth feature (Feat4) and use color to visualize this 4^th^ dimension.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nLike the second feature, Feat4 has not added much information.\n\nNow, what would we use a PCA for in this context?\n\n> **PCA can tell us if and how many groups there are in our data and it can tell use what features are responsible for the division into groups!**\n\n\nLet's now go through a PCA step-by-step. Also, we only consider Feat1 and Feat2 to keep things very simple.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-d51267b4{}.cl-d50fa0ce{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d510c38c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d510c38d{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d510d052{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d510d053{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d510d054{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d510d055{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d510d05c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d510d05d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-d51267b4'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-d510d052\"><p class=\"cl-d510c38c\"><span class=\"cl-d50fa0ce\">Language</span></p></th><th class=\"cl-d510d053\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">Feat1</span></p></th><th class=\"cl-d510d053\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">Feat2</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d510d054\"><p class=\"cl-d510c38c\"><span class=\"cl-d50fa0ce\">Lang1</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">10</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">6.0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d510d054\"><p class=\"cl-d510c38c\"><span class=\"cl-d50fa0ce\">Lang2</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">11</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">4.0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d510d054\"><p class=\"cl-d510c38c\"><span class=\"cl-d50fa0ce\">Lang3</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">8</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">5.0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d510d054\"><p class=\"cl-d510c38c\"><span class=\"cl-d50fa0ce\">Lang4</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">3</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">3.0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d510d054\"><p class=\"cl-d510c38c\"><span class=\"cl-d50fa0ce\">Lang5</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">1</span></p></td><td class=\"cl-d510d055\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">2.8</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d510d05c\"><p class=\"cl-d510c38c\"><span class=\"cl-d50fa0ce\">Lang6</span></p></td><td class=\"cl-d510d05d\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">2</span></p></td><td class=\"cl-d510d05d\"><p class=\"cl-d510c38d\"><span class=\"cl-d50fa0ce\">1.0</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\nWe start by calculating the center of the data.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nNext we scale and center the data so that the center is at (0,0).\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nThe relationship between data points is unchanged!\n\n> Important: **always center and scale your data** as the scales will impact the magnitude of variance!\n> In other words, if you do not center and scale, components can be deemed important simply because the scale of the variables is bigger (not because they are more important)!\n\nNow, we fit a line through the data.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n<div class=\"warning\" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>\n<span>\n<p style='margin-top:1em; text-align:center'>\n<b>IMPORTANT!</b></p>\n<p style='margin-left:1em;'>\n\nBut how do we arrive at this line and what is there always talk about \"orthogonal\" when talking about PCA?\n\nLet's have a look at simple the Ordinary Least Squared (OLS) procedure that underlies regression:\n</p>\n<p style='margin-top:1em; text-align:center'>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n</p>\n<p style='margin-left:1em;'>\n\nIn OLS, the lines that represent variance (or residuals) are perpendicular to the x-axis.\n\nHowever, in PCA, the line are perpendicular to the regression line as shown below!\n</p>\n<p style='margin-top:1em; text-align:center'>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n</p>\n</span>\n</div>\n\n\n\nThe line that minimizes variance is called **Principal Component 1** (PC1) and it is different from a regression line. The PC1 can be found by minimizing the orthogonal distances (blue lines above) between data points and their projections (green points above)!. \n\nAn alternative way to find PC1 is actually not to try and minimize the distance between the data point and its projection but to maximize the distance between the projection (the green points on the line) and the origin (the center of the coordinate system). We can then square the distances between projections and the origin and add these squared distances, which gives us the *sum of squared distances* or *SS(distances)*.\n\n\n\n\nPC1 has a slope of 0.9411239 - this already shows us that the data is spread much more along PC1 that along PC2!\n\nWe go about 1.0625593 units (1 / 0.9411) to the right and 1 unit up.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\nNow, we find the value for a (square root of b^2^ + c^2^) =  1.4591204. \n\nNext, we scale b and c so that a is 1.\n\na^2^ = b^2^ + c^2^\n\nb = 1.0625593 / 1.4591204 = 0.7282191\n\nc = 1 / 1.4591204 = 0.6853444\n\n\nTo make PC1, we need 0.738 of Feat1 and 0.685 of Feat2.\n\nThis 1 unit long vector (a) is called the Eigenvector for PC1.\n\n> Eigenvector and Eigenvalue are not the same! \n> Eigenvector represent the factor loadings for each observed variable on a specific factor.\n> Eigenvalue represents the amount of variance explained by an entire factor.\n\nThe proportions (0.738 and 0.685) are called the loading scores.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\nPC2 is simply the perpendicular line to PC1 that goes through 0,0.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nAs PC1 and PC2 are perpendicular, the Eigenvector of PC2 is -0.685 of Feat1 and 0.728 of Feat2.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\nWe can (but have to) rotate the PC1 and make the points smaller (so that our plot looks more like a proper PCA plot).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\nTo plot the results of a PCA, we simply rotate the plot so that the PCs reflect the axes and we project the points onto the PCs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate data\ndf <- data.frame(\n  pcadat$Feat1,\n  pcadat$Feat2\n)\ncolnames(df) <- c(\"Feat1\", \"Feat2\")\n# perform PCA\npcaex <- prcomp(df, center = T, scale = T)\n# visualize PCA results\nfviz_pca_ind(pcaex, label = \"\") +\n  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) +\n  ggtitle(\"PCA plot\") +\n  theme(aspect.ratio = 1)\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\nNow we have an understanding of how we arrive at the visual representation of a PCA. But how do we arrive at the explained variance?\n\nIn the context of PCA, variance refers to summative variance or overall/total variability. \n\nTo get a better grasp of what this means, let's consider a covariance matrix of some 3 variables (rather than just 2 variables as above). Their variances are on the diagonal, and the sum of the 3 values (3.248) is the overall variability.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-d5d84128{}.cl-d5d587da{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d5d6a728{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d5d6b36c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d5d6b36d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d5d6b36e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-d5d84128'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-d5d6b36c\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">V1</span></p></th><th class=\"cl-d5d6b36c\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">V2</span></p></th><th class=\"cl-d5d6b36c\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">V3</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d5d6b36d\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">1.3437305</span></p></td><td class=\"cl-d5d6b36d\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">-0.1601523</span></p></td><td class=\"cl-d5d6b36d\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">0.1864702</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d5d6b36d\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">-0.1601523</span></p></td><td class=\"cl-d5d6b36d\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">0.6192056</span></p></td><td class=\"cl-d5d6b36d\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">-0.1266843</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d5d6b36e\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">0.1864702</span></p></td><td class=\"cl-d5d6b36e\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">-0.1266843</span></p></td><td class=\"cl-d5d6b36e\"><p class=\"cl-d5d6a728\"><span class=\"cl-d5d587da\">1.4855496</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\n\nNow, PCA replaces original variables with new variables, called principal components, which are orthogonal (i.e. they have zero covariation!) and have variances (called eigenvalues) in decreasing order. So, the covariance matrix between the principal components extracted from the above data is this:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-d5e0e7a6{}.cl-d5ddfafa{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d5df3e56{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d5df49fa{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d5df4a04{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d5df4a22{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-d5e0e7a6'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-d5df49fa\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">V1</span></p></th><th class=\"cl-d5df49fa\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">V2</span></p></th><th class=\"cl-d5df49fa\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">V3</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d5df4a04\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">1.651354</span></p></td><td class=\"cl-d5df4a04\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">0.000000</span></p></td><td class=\"cl-d5df4a04\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">0.0000000</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d5df4a04\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">0.000000</span></p></td><td class=\"cl-d5df4a04\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">1.220288</span></p></td><td class=\"cl-d5df4a04\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">0.0000000</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d5df4a22\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">0.000000</span></p></td><td class=\"cl-d5df4a22\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">0.000000</span></p></td><td class=\"cl-d5df4a22\"><p class=\"cl-d5df3e56\"><span class=\"cl-d5ddfafa\">0.5768431</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\nNote that the diagonal sum is still 3.448, which says that all 3 components account for all the multivariate variability. The PC1 accounts for or \"explains\" 1.651/3.448 = 47.9% of the overall variability; the PC2 explains 1.220/3.448 = 35.4% of it; the 3rd one explains .577/3.448 = 16.7% of it.\n\nSo, what do they mean when they say that PCA explains maximal variance? That is not, of course, that it finds the largest variance among three values 1.343730519 .619205620 1.485549631. PCA finds, in the data space, the dimension (direction) with the largest variance out of the overall variance 1.343730519+.619205620+1.485549631 = 3.448. That largest variance would be 1.651354285. Then it finds the dimension of the second largest variance, orthogonal to the first one, out of the remaining 3.448-1.651354285 overall variance. That 2nd dimension would be 1.220288343 variance. And so on. The last remaining dimension is .576843142 variance. \n\nFor our example, we can extract the amount of variance accounted for as shown below:\n\nStep 1: get loading scores\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npcaex$x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            PC1        PC2\n[1,] -1.6229306  0.2698980\n[2,] -0.9855078 -0.6922526\n[3,] -0.8983094  0.1947325\n[4,]  0.7132969  0.2067653\n[5,]  1.1180034  0.4515144\n[6,]  1.6754475 -0.4306576\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n\nStep 2: calculate standard deviations of PC1 and PC2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdpc1 <- sd(pcaex$x[, 1])\nsdpc2 <- sd(pcaex$x[, 2])\nsdpc1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.339995\n```\n\n\n:::\n\n```{.r .cell-code}\nsdpc2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.45212\n```\n\n\n:::\n:::\n\n\n\nStep 3: use standard deviations to calculate the amount of variance explained by each component \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Proportion of Variance PC1 = sd PC1 squared / (sd PC1 squared + sd PC2 squared)\nsdpc1^2 / (sdpc1^2 + sdpc2^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8977938\n```\n\n\n:::\n\n```{.r .cell-code}\n# Proportion of Variance PC2 = sd PC2 squared / (sd PC1 squared + sd PC2 squared)\nsdpc2^2 / (sdpc1^2 + sdpc2^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1022062\n```\n\n\n:::\n:::\n\n\n\nAccording to our manual calculation, PC1 explains 0.8978 % of the variation while PC2 explains 0.1022 % of the variation. To check if this is correct, we check our results against the results provided by the `prcomp` function.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2\nStandard deviation     1.3400 0.4521\nProportion of Variance 0.8978 0.1022\nCumulative Proportion  0.8978 1.0000\n```\n\n\n:::\n:::\n\n\n\nBut how do we arrive at the standard deviations of the PCs? The standard deviations of the PCs are the covariance matrix values for the respective PCs! So let's have a look at the covariance matrices of the raw data, the scales data, and  the rotated, final data. \n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n          Feat1    Feat2\nFeat1 18.966667 6.126667\nFeat2  6.126667 3.126667\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Feat1     Feat2\nFeat1 1.0000000 0.7955875\nFeat2 0.7955875 1.0000000\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          PC1                       PC2\nPC1  1.7955875066756541436064 -0.0000000000000004884981\nPC2 -0.0000000000000004884981  0.2044124933243467445720\n```\n\n\n:::\n:::\n\n\n\nWe see that the standard deviation of PC1 is equal to the sum of the first row (or column) of the scaled covariance matrix. But more importantly, we see that the **squared standard deviations of the PCs are equal to the variances (eigenvalues) of the PCs in the covariance matrix**. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdpc1^2 # sd of PC1 squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.795588\n```\n\n\n:::\n\n```{.r .cell-code}\n# which is equal to\nsum(cov(pcaex$x)[1, ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.795588\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsdpc2^2 # sd of PC2 squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2044125\n```\n\n\n:::\n\n```{.r .cell-code}\n# which is equal to\nsum(cov(pcaex$x)[2, ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2044125\n```\n\n\n:::\n:::\n\n\n\nNow that we have worked through one example of a PCA manually, we turn to several examples to show how one can implement PCA in R to showcase what PCA can be used for.\n\n### Example 1 {-} \n\nHere, we will show\n\n+ how to use the `prcomp()` function to perform PCA  \n\n+ how to draw a PCA plot using ggplot2  \n\n+ how to determine how much variation each component accounts for  \n\n+ how to examine the loading scores to determine what variables have the largest effect on the graph\n\nIn this example, the data is in a matrix called `pcadat` where columns are individual samples (i.e. languages) and rows are measurements taken for all the samples (i.e. features).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npcadat <- matrix(nrow = 100, ncol = 10)\ncolnames(pcadat) <- c(\n  paste(\"indoeu\", 1:5, sep = \"\"), # Indo-European languages\n  paste(\"austro\", 1:5, sep = \"\")\n) # Austronesian languages\nrownames(pcadat) <- paste(\"feature\", 1:100, sep = \"\")\nfor (i in 1:100) {\n  indoeu.values <- rpois(5, lambda = sample(x = 10:1000, size = 1))\n  austro.values <- rpois(5, lambda = sample(x = 10:1000, size = 1))\n\n  pcadat[i, ] <- c(indoeu.values, austro.values)\n}\n# inspect data\nhead(pcadat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         indoeu1 indoeu2 indoeu3 indoeu4 indoeu5 austro1 austro2 austro3\nfeature1     777     753     798     751     788     656     730     689\nfeature2     992     994    1026     974    1059     632     675     682\nfeature3      42      39      44      45      45      92      90      79\nfeature4     803     805     744     765     792     353     325     352\nfeature5     612     612     667     619     661     753     788     780\nfeature6     281     284     280     331     301     699     726     677\n         austro4 austro5\nfeature1     649     734\nfeature2     634     670\nfeature3     102      81\nfeature4     333     306\nfeature5     742     745\nfeature6     665     682\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(pcadat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 100  10\n```\n\n\n:::\n:::\n\n\n\nWe now implement the PCA using the `prcomp` function from the `stats` package. In our case, we specify that we want to transpose the data (using the `t` function) because **we need features to represent columns (not rows!)** and we set the argument `scale` to TRUE as the data has to be normalised for a PCA to provide reliable results. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca <- prcomp(t(pcadat), scale = TRUE)\n```\n:::\n\n\n\nNow, we generate a scree plot to show how much variance is accounted for by each component. We start by preparing the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prepare data\npca.var <- pca$sdev^2\n# extract percentage of contribution\npca.var.per <- round(pca.var / sum(pca.var) * 100, 1)\n# generate data frame for visualization\npcascreedat <- data.frame(\n  Component = paste0(\"PC\", 1:10),\n  Percent = round(pca.var / sum(pca.var) * 100, 2)\n) %>%\n  dplyr::mutate(\n    Text = paste0(Percent, \"%\"),\n    Component = factor(Component,\n      levels = c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\")\n    )\n  )\n# inspect\npcascreedat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Component Percent   Text\n1        PC1   91.83 91.83%\n2        PC2    2.84  2.84%\n3        PC3    1.26  1.26%\n4        PC4    1.08  1.08%\n5        PC5    0.94  0.94%\n6        PC6    0.61  0.61%\n7        PC7    0.56  0.56%\n8        PC8    0.55  0.55%\n9        PC9    0.33  0.33%\n10      PC10    0.00     0%\n```\n\n\n:::\n:::\n\n\n\nNow that the dtaa is formatted appropriately, we generate the scree plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pcascreedat, aes(x = Component, y = Percent, label = Text)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(vjust = -1.6, size = 3) +\n  ggtitle(\"Scree Plot\") +\n  labs(x = \"Principal Components\", y = \"Percent Variation\") +\n  theme_bw() +\n  coord_cartesian(ylim = c(0, 100))\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\nThe scree plots shows that the PC1 accounts for 87.69% of the variability while the PC2 only accounts for 3.52% of the variability. This means that one component suffices to differentiate the data. \n\nNow we make a fancy looking plot that shows the PCs and the variation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  Sample = rownames(pca$x),\n  X = pca$x[, 1],\n  Y = pca$x[, 2]\n) %>%\n  ggplot(aes(x = X, y = Y, label = Sample)) +\n  geom_text() +\n  xlab(paste(\"PC1 (\", pca.var.per[1], \"%)\", sep = \"\")) +\n  ylab(paste(\"PC2 (\", pca.var.per[2], \"%)\", sep = \"\")) +\n  theme_bw() +\n  ggtitle(\"PCA Graph\") +\n  coord_cartesian(xlim = c(-11, 11))\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\nNext, we get the names of the most important 5 features that contribute most to PC1.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloading_scores <- pca$rotation[, 1]\n# extract the magnitudes of the scores\nfeature_scores <- abs(loading_scores)\n# extract the 5 highest scores\nfeature_score_ranked <- sort(feature_scores, decreasing = TRUE)\ntop_5_features <- names(feature_score_ranked[1:5])\n# show the names of the top 5 features\ntop_5_features\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"feature38\" \"feature15\" \"feature43\" \"feature47\" \"feature53\"\n```\n\n\n:::\n:::\n\n\n\nNow, we show the scores (and +/- sign)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca$rotation[top_5_features, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n feature38  feature15  feature43  feature47  feature53 \n-0.1042985  0.1042909 -0.1042813 -0.1042760  0.1042701 \n```\n\n\n:::\n:::\n\n\n\n### Example 2{-} \n\nIn this example, we explore the use of PCA for survey data. Specifically, we want to use PCA to check if 5 items reflect the underlying factor appropriately or if one of the items needs to be replaced.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveydata <- base::readRDS(url(\"https://slcladal.github.io/data/sud.rda\", \"rb\"))\n# inspect\nflextable::flextable(head(surveydata, 10))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-d68648f4{}.cl-d6824c36{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d68396fe{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d68396ff{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d683a4c8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d683a4c9{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d683a4d2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d683a4dc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d683a4dd{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d683a4e6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-d68648f4'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-d683a4c8\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q01_Outgoing</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q02_Outgoing</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q03_Outgoing</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q04_Outgoing</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q05_Outgoing</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q06_Intelligence</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q07_Intelligence</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q08_Intelligence</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q09_Intelligence</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q10_Intelligence</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q11_Attitude</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q12_Attitude</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q13_Attitude</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q14_Attitude</span></p></th><th class=\"cl-d683a4c9\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">Q15_Attitude</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_01</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">3</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">3</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">3</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">3</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">3</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">3</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_02</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_03</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_04</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_05</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_06</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_07</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_08</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4d2\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_09</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4dc\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d683a4dd\"><p class=\"cl-d68396fe\"><span class=\"cl-d6824c36\">Respondent_10</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">4</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">5</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">2</span></p></td><td class=\"cl-d683a4e6\"><p class=\"cl-d68396ff\"><span class=\"cl-d6824c36\">1</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\nWe now implement the PCA.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# entering raw data and extracting PCs  from the correlation matrix\npca_res2 <- princomp(surveydata[c(\n  \"Q01_Outgoing\",\n  \"Q02_Outgoing\",\n  \"Q03_Outgoing\",\n  \"Q04_Outgoing\",\n  \"Q05_Outgoing\"\n)])\n# print variance accounted for\nsummary(pca_res2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          Comp.1     Comp.2     Comp.3     Comp.4      Comp.5\nStandard deviation     3.2317383 0.62656643 0.50633614 0.44585671 0.332592305\nProportion of Variance 0.9159511 0.03442977 0.02248422 0.01743374 0.009701174\nCumulative Proportion  0.9159511 0.95038087 0.97286509 0.99029883 1.000000000\n```\n\n\n:::\n:::\n\n\n\nThe cumulative proportion of variance already shows that the five items represent a single underlying factor as 91.5% of the variance is explained by PC1 alone!\n\nThe loading scores for PC1 are very similar for all items and thus show that none of the items under performs. In such a case, it might make sense to remove items, not because they do not capture the underlying concept but to reduce the items for participants.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prepare data\npca2.var <- pca_res2$sdev^2\n# extract percentage of contribution\npca2.var.per <- round(pca.var / sum(pca.var) * 100, 1)\n# generate data frame for visualization\ndata.frame(\n  Component = paste0(\"PC\", 1:5),\n  Percent = round(pca2.var / sum(pca2.var) * 100, 2)\n) %>%\n  dplyr::mutate(\n    Text = paste0(Percent, \"%\"),\n    Component = factor(Component,\n      levels = c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\")\n    )\n  ) %>%\n  ggplot(aes(x = Component, y = Percent, label = Text)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(vjust = -1.6, size = 3) +\n  ggtitle(\"Scree Plot\") +\n  labs(x = \"Principal Components\", y = \"Percent Variation\") +\n  theme_bw() +\n  coord_cartesian(ylim = c(0, 100))\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\nWe now plot the loading scores for each participant to see if we can see groups among the participants based on outgoingness.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_res2$scores %>%\n  as.data.frame() %>%\n  dplyr::rename(\n    PC1 = 1,\n    PC2 = 2,\n    PC3 = 3,\n    PC4 = 4,\n    PC5 = 5\n  ) %>%\n  dplyr::mutate(\n    Participants = paste0(\"P\", 1:20),\n    clr = ifelse(PC1 > 0, 1, 0)\n  ) %>%\n  ggplot(aes(x = PC1, y = PC2, color = clr, label = Participants)) +\n  geom_point(size = 2) +\n  geom_text(size = 3, nudge_x = -.2, check_overlap = T) +\n  theme_bw() +\n  scale_color_viridis_b() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\nThe figure shows that based on PC1, we can distinguish between subjects who are very outgoign and subjects that are not very outgoing.\n\n### Example 3 {-}\n\n\nIn this example, we use the `biopsy` data to see what variables can be used to predict malignant melanomas.\n\nIn a first step, we remove any data points with missing values and add meaningful variable names.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(biopsy)\nbiopsy_nona <- biopsy %>%\n  tidyr::drop_na() %>%\n  # add meaningful variable names\n  dplyr::rename(\n    ClumpSize = V1,\n    CellSize = V2,\n    CellShape = V3,\n    Adhesion = V4,\n    EpithelialSize = V5,\n    Nuclei = V6,\n    Chromatin = V7,\n    Nucleoli = V8,\n    Mitoses = V9\n  )\n# inspect data\nflextable::flextable(head(biopsy_nona, 10))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-d6b882ce{}.cl-d6b53362{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d6b67f60{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d6b67f6a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d6b68c58{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6b68c59{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6b68c62{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6b68c63{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6b68c6c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6b68c6d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-d6b882ce'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-d6b68c58\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">ID</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">ClumpSize</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">CellSize</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">CellShape</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">Adhesion</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">EpithelialSize</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">Nuclei</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">Chromatin</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">Nucleoli</span></p></th><th class=\"cl-d6b68c59\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">Mitoses</span></p></th><th class=\"cl-d6b68c58\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">class</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1000025</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">5</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1002945</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">5</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">4</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">4</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">5</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">7</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">10</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1015425</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1016277</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">6</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">8</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">8</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">4</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">7</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1017023</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">4</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1017122</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">8</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">10</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">10</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">8</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">7</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">10</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">9</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">7</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">malignant</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1018099</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">10</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1018561</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">3</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1033078</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c63\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">5</span></p></td><td class=\"cl-d6b68c62\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6b68c6c\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">1033078</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">4</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">2</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c6d\"><p class=\"cl-d6b67f6a\"><span class=\"cl-d6b53362\">1</span></p></td><td class=\"cl-d6b68c6c\"><p class=\"cl-d6b67f60\"><span class=\"cl-d6b53362\">benign</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\nNext, we remove non-numeric variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiopsy_num <- biopsy_nona %>%\n  dplyr::select_if(is.numeric)\n# inspect data\nflextable::flextable(head(biopsy_num, 10))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-d6c78bb6{}.cl-d6c36fcc{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d6c4b9f4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d6c4c5f2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6c4c5fc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6c4c5fd{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-d6c78bb6'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">ClumpSize</span></p></th><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">CellSize</span></p></th><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">CellShape</span></p></th><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">Adhesion</span></p></th><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">EpithelialSize</span></p></th><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">Nuclei</span></p></th><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">Chromatin</span></p></th><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">Nucleoli</span></p></th><th class=\"cl-d6c4c5f2\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">Mitoses</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">5</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">5</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">4</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">4</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">5</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">7</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">10</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">6</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">8</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">8</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">4</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">7</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">4</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">8</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">10</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">10</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">8</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">7</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">10</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">9</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">7</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">10</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">3</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fc\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">4</span></p></td><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">2</span></p></td><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td><td class=\"cl-d6c4c5fd\"><p class=\"cl-d6c4b9f4\"><span class=\"cl-d6c36fcc\">1</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\nWe can  perform the PCA.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiopsy_pca <- prcomp(biopsy_num,\n  scale = T\n)\n# summary\nsummary(biopsy_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.4289 0.88088 0.73434 0.67796 0.61667 0.54943 0.54259\nProportion of Variance 0.6555 0.08622 0.05992 0.05107 0.04225 0.03354 0.03271\nCumulative Proportion  0.6555 0.74172 0.80163 0.85270 0.89496 0.92850 0.96121\n                           PC8     PC9\nStandard deviation     0.51062 0.29729\nProportion of Variance 0.02897 0.00982\nCumulative Proportion  0.99018 1.00000\n```\n\n\n:::\n:::\n\n\n\n\nFirst, we create a scree plot of the variance captured by each component.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactoextra::fviz_eig(biopsy_pca,\n  addlabels = T,\n  ylim = c(0, 80),\n  barfill = \"gray50\",\n  barcolor = \"gray20\"\n)\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\n\nWe now create a pretty biplot to show the results of the PCA. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_pca_biplot(biopsy_pca,\n  geom = \"point\",\n  # no data point labels\n  label = \"var\",\n  # color by class\n  habillage = biopsy_nona$class,\n  # change variable color\n  col.var = \"black\"\n) +\n  # adapt data point color\n  ggplot2::scale_color_manual(values = c(\"orange\", \"purple\")) +\n  # add title\n  ggtitle(\"A pretty biplot\")\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n\nThe plot above is also known as variable correlation plot or biplot. A biplot overlays a score plot with a loading plot and thus shows individual data points as well as the relationships between all variables (in the form of arrows). It can be interpreted as follow:\n\n+ Positively correlated variables are grouped together.  \n+ Negatively correlated variables would be positioned on opposite sides of the plot origin (opposed quadrants).  \n+ The distance between variables and the origin measures the quality of the variables on the factor map.  \n+ Variables that are away from the origin are well represented on the factor map.\n\nBut why are all variables cluster together?\n\nLet's have a look at the correlations between variables.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-d6ffe6f0{}.cl-d6fcaf26{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d6fdf25a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d6fe002e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6fe002f{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d6fe0038{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-d6ffe6f0'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">ClumpSize</span></p></th><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">CellSize</span></p></th><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">CellShape</span></p></th><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">Adhesion</span></p></th><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">EpithelialSize</span></p></th><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">Nuclei</span></p></th><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">Chromatin</span></p></th><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">Nucleoli</span></p></th><th class=\"cl-d6fe002e\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">Mitoses</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6424815</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6534700</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4878287</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5235960</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5930914</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5537424</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5340659</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.3509572</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6424815</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.9072282</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7069770</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7535440</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6917088</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7555592</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7193460</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4607547</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6534700</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.9072282</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6859481</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7224624</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7138775</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7353435</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7179634</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4412576</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4878287</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7069770</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6859481</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5945478</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6706483</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6685671</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6031211</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4188983</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5235960</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7535440</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7224624</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5945478</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5857161</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6181279</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6289264</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4805833</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5930914</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6917088</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7138775</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6706483</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5857161</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6806149</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5842802</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.3392104</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5537424</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7555592</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7353435</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6685671</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6181279</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6806149</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6656015</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.3460109</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5340659</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7193460</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.7179634</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6031211</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6289264</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.5842802</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.6656015</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td><td class=\"cl-d6fe002f\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4337573</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.3509572</span></p></td><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4607547</span></p></td><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4412576</span></p></td><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4188983</span></p></td><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4805833</span></p></td><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.3392104</span></p></td><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.3460109</span></p></td><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">0.4337573</span></p></td><td class=\"cl-d6fe0038\"><p class=\"cl-d6fdf25a\"><span class=\"cl-d6fcaf26\">1.0000000</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\nThe correlation matrix shows that all variables are positively correlated which explains why all variables cluster together. The low correlations between the other variables and *Mitosis* explain why it stands out.\n\n# Multidimensional Scaling {-}\n\nMultidimensional Scaling (MDS, see @davison1983introduction or @jacoby2018multidimensional) is used for visualizing high-dimensional data in a lower-dimensional space **while preserving pairwise distances** or similarities between data points. MDS aims to represent the data in a way that maintains the relationships among data points as much as possible. \n\nThere are two types of MDS:\n\n+ **metric or classical MDS**: metric MDS aims to represent the distances between objects in a way that preserves the original dissimilarity/similarity relationships as accurately as possible. It assumes that the input data reflects actual distances or dissimilarities and it uses algorithms such as *Principal Coordinate Analysis* (PCoA) to transform the data into a lower-dimensional space while trying to maintain the original pairwise distances.\n\n+ **non-metric MDS**: non-metric MDS aims to represent the rank order of the distances or dissimilarities between objects, rather than the actual distances. It is more suitable when the input data does not have a direct metric interpretation or when the distances are only known in terms of ordinal rankings.\n\nMDS works exactly like PCA with one important difference: **while PCA starts off with correlations between variables (the covariance matrix), MDS start of with distances (and thus relies on a distance matrix)!**\n\n### Example 1 {-}\n\nWe begin by loading data. This fictitious data set represents responses to 15 items by 20 participants. The 15 items aim to assess 3 different psychological constructs:\n\n+ outgoingness (extroversion)  \n\n+ intelligence  \n\n+ attitude\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveydata <- base::readRDS(url(\"https://slcladal.github.io/data/sud.rda\", \"rb\"))\n# inspect\nreport(surveydata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe data contains 20 observations of the following 16 variables:\n\n  - Respondent: 20 entries, such as Respondent_01 (n = 1); Respondent_02 (n = 1);\nRespondent_03 (n = 1) and 17 others (0 missing)\n  - Q01_Outgoing: n = 20, Mean = 3.15, SD = 1.53, Median = 3.50, MAD = 2.22,\nrange: [1, 5], Skewness = -0.18, Kurtosis = -1.52, 0 missing\n  - Q02_Outgoing: n = 20, Mean = 3.25, SD = 1.59, Median = 3.50, MAD = 2.22,\nrange: [1, 5], Skewness = -0.10, Kurtosis = -1.73, 0 missing\n  - Q03_Outgoing: n = 20, Mean = 3.15, SD = 1.50, Median = 3.50, MAD = 2.22,\nrange: [1, 5], Skewness = -0.07, Kurtosis = -1.58, 0 missing\n  - Q04_Outgoing: n = 20, Mean = 3.00, SD = 1.56, Median = 3.50, MAD = 2.22,\nrange: [1, 5], Skewness = -0.09, Kurtosis = -1.64, 0 missing\n  - Q05_Outgoing: n = 20, Mean = 3.20, SD = 1.58, Median = 3.50, MAD = 2.22,\nrange: [1, 5], Skewness = -0.19, Kurtosis = -1.59, 0 missing\n  - Q06_Intelligence: n = 20, Mean = 2.90, SD = 1.45, Median = 2.00, MAD = 1.48,\nrange: [1, 5], Skewness = 0.31, Kurtosis = -1.43, 0 missing\n  - Q07_Intelligence: n = 20, Mean = 3.05, SD = 1.39, Median = 3.00, MAD = 1.48,\nrange: [1, 5], Skewness = 0.03, Kurtosis = -1.24, 0 missing\n  - Q08_Intelligence: n = 20, Mean = 2.95, SD = 1.54, Median = 2.50, MAD = 2.22,\nrange: [1, 5], Skewness = 0.19, Kurtosis = -1.54, 0 missing\n  - Q09_Intelligence: n = 20, Mean = 2.80, SD = 1.51, Median = 2.00, MAD = 1.48,\nrange: [1, 5], Skewness = 0.38, Kurtosis = -1.48, 0 missing\n  - Q10_Intelligence: n = 20, Mean = 3.60, SD = 1.50, Median = 4.00, MAD = 1.48,\nrange: [1, 5], Skewness = -0.58, Kurtosis = -1.33, 0 missing\n  - Q11_Attitude: n = 20, Mean = 2.75, SD = 1.62, Median = 2.50, MAD = 2.22,\nrange: [1, 5], Skewness = 0.20, Kurtosis = -1.65, 0 missing\n  - Q12_Attitude: n = 20, Mean = 2.95, SD = 1.43, Median = 2.50, MAD = 2.22,\nrange: [1, 5], Skewness = 0.22, Kurtosis = -1.40, 0 missing\n  - Q13_Attitude: n = 20, Mean = 2.75, SD = 1.65, Median = 2.00, MAD = 1.48,\nrange: [1, 5], Skewness = 0.21, Kurtosis = -1.77, 0 missing\n  - Q14_Attitude: n = 20, Mean = 2.95, SD = 1.54, Median = 2.50, MAD = 2.22,\nrange: [1, 5], Skewness = 0.19, Kurtosis = -1.54, 0 missing\n  - Q15_Attitude: n = 20, Mean = 2.95, SD = 1.70, Median = 2.50, MAD = 2.22,\nrange: [1, 5], Skewness = 0.09, Kurtosis = -1.82, 0 missing\n```\n\n\n:::\n:::\n\n\n\nAs MDS, like PCA and FA, works only on numeric data, we remove non-numeric variables from the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveydata <- surveydata %>%\n  dplyr::select(-Respondent)\n# inspect\nstr(surveydata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t20 obs. of  15 variables:\n $ Q01_Outgoing    : int  4 5 5 5 4 5 4 4 5 4 ...\n $ Q02_Outgoing    : int  5 4 4 5 5 5 5 4 5 5 ...\n $ Q03_Outgoing    : int  4 5 4 5 4 5 4 5 4 5 ...\n $ Q04_Outgoing    : int  4 4 5 4 5 5 5 4 4 4 ...\n $ Q05_Outgoing    : int  5 4 5 5 5 4 5 5 4 4 ...\n $ Q06_Intelligence: int  2 2 2 1 2 5 4 5 5 4 ...\n $ Q07_Intelligence: int  3 2 1 1 2 4 5 4 5 4 ...\n $ Q08_Intelligence: int  3 2 1 1 1 5 4 5 4 5 ...\n $ Q09_Intelligence: int  2 1 2 1 2 2 4 4 4 4 ...\n $ Q10_Intelligence: int  2 2 2 1 1 2 5 5 5 5 ...\n $ Q11_Attitude    : int  3 4 5 5 4 1 2 1 1 2 ...\n $ Q12_Attitude    : int  3 4 4 4 5 2 1 2 2 1 ...\n $ Q13_Attitude    : int  2 4 4 5 4 1 1 1 2 2 ...\n $ Q14_Attitude    : int  3 5 4 5 5 2 2 1 1 2 ...\n $ Q15_Attitude    : int  3 4 4 5 5 1 1 2 2 1 ...\n```\n\n\n:::\n:::\n\n\n\nFor MDS, we first calculate the distance matrix using the Euclidian distance. Note that we are transposing, scaling and centering the data just like for PCA.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvey_dist <- dist(scale(t(surveydata), center = TRUE, scale = TRUE),\n  method = \"euclidean\"\n)\n```\n:::\n\n\n\nNow, we generate an MDS object (this is basically eigenvalue decomposition)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmds.obj <- cmdscale(survey_dist,\n  eig = TRUE, # adds a matrix of eignevalues to the mds object\n  x.ret = TRUE\n) # adds a symmetric distance matrix to the mds object\n```\n:::\n\n\n\nNow, we calculate the percentage of variation that each MDS axis accounts for...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmds.var.per <- round(mds.obj$eig / sum(mds.obj$eig) * 100, 1)\n# inspect\nmds.var.per\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 54.8 29.0  4.8  2.7  2.1  1.7  1.5  1.0  0.9  0.6  0.4  0.3  0.1  0.0  0.0\n```\n\n\n:::\n:::\n\n\n\nWe  make a fancy looking plot that shows the MDS axes and the variation:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmds.values <- mds.obj$points\nmds.data <- data.frame(\n  Sample = rownames(mds.values),\n  X = mds.values[, 1],\n  Y = mds.values[, 2]\n)\n# inspect\nmds.data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                           Sample          X          Y\nQ01_Outgoing         Q01_Outgoing  1.2441258 -2.9521948\nQ02_Outgoing         Q02_Outgoing  1.3832771 -3.2215742\nQ03_Outgoing         Q03_Outgoing  0.8464687 -2.8364303\nQ04_Outgoing         Q04_Outgoing  0.6884322 -3.1352690\nQ05_Outgoing         Q05_Outgoing  0.3742058 -3.2123840\nQ06_Intelligence Q06_Intelligence  3.7001253  1.1236968\nQ07_Intelligence Q07_Intelligence  3.4292730  1.8623706\nQ08_Intelligence Q08_Intelligence  4.0132660  1.5106590\nQ09_Intelligence Q09_Intelligence  3.8680562  2.5632774\nQ10_Intelligence Q10_Intelligence  1.5665476  3.9359300\nQ11_Attitude         Q11_Attitude -4.4171807  0.6011738\nQ12_Attitude         Q12_Attitude -3.8486419  1.2134235\nQ13_Attitude         Q13_Attitude -4.6650593  1.4927673\nQ14_Attitude         Q14_Attitude -3.8067797  0.1881540\nQ15_Attitude         Q15_Attitude -4.3761161  0.8663999\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = mds.data, aes(x = X, y = Y, label = Sample)) +\n  geom_text() +\n  theme_bw() +\n  xlab(paste(\"MDS1 - \", mds.var.per[1], \"%\", sep = \"\")) +\n  ylab(paste(\"MDS2 - \", mds.var.per[2], \"%\", sep = \"\")) +\n  ggtitle(\"MDS plot using Euclidean distance\") +\n  coord_cartesian(xlim = c(-7, 7), ylim = c(-4, 4))\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\n\nThe plot shows that we are dealing with 3 groups that are neatly separated along 2 dimensions. We also see that Q10 behaves different from the other items aiming to assess IQ. We could use this to optimize the item set of our survey. \n\n# Factor Analysis {-}\n\nFactor Analysis is used to identify underlying latent factors that explain the observed correlations among variables in a data set [@kim1978introduction; @field2012discovering, 749-811]. It aims to reduce the complexity of high-dimensional data by identifying a smaller number of factors (latent variables) that contribute to the variance and covariance among (observed) variables. Ideally those observed variables that represent an underlying factor should be highly correlated with each other but not correlated with (observed) variables that represent other factors (underlying variables). Like PCA and MDS, we find factors by aiming to reduce variability in the data but unlike PCA, which is based on a co-variance matrix, or MDS, which is based on a distance matrix, EFA is based on a correlation matrix. \n\nThere are two types of factor analysis:  \n\n+ exploratory factor analysis (EFA)  \n\n+ confirmatory factor analysis (CFA)\n\n\nHere, we are only going to look at EFA which is used to identify how many latent variables (factors) are present in a data set. In contrast to CFA, EFA allows factors to be correlated with each other and it uses a rotation step to transform the initial factor structure into a more interpretable form. The rotation aims to maximize the loadings of variables on only a single factor  and it thereby maximizes the separation between factors. Common rotation methods include *Varimax*, *Oblimin*, and *Promax*. The *Varimax* rotation is an orthogonal rotation and it is the most common rotation method. It assumes that the factors are not correlated; another type of rotation are *oblique* rotations, such as *Oblimin* and *Promax*, which do not assume that the factors are not correlated [@field2012discovering, 755].\n\nCFA is used for hypothesis testing to confirm structures assumed to be present in the data. In R, we can use the `sem` or `lavaan` packages that implement Structural Equation Models (SEM)) to implement CFA and test specific hypothesis about the existence and interactions of latent factors.  \n\n\n### Example 1 {-}\n\nWe continue with the survey data set and we can now implement the EFA using the `factanal` function from the base `stats` package. Although `factanal` implements a EFA, we need to provide it with the number of factors (latent variables) it should look for (commonly, you need to vary this to find the optimal number of factors).\n\nAs a first step, we generate a scree plot to assess how many factors are in our data. There are different approaches to interpret the output: (a) we choose the number of factors where there is a noticeable bend in the eigenvalues of the factors or, if there is no noticeable bend (the so-called *point of inflexion* [@field2012discovering, 763]), (b) select the last value above 1 (following @kaiser1960application - however, @jolliffe1987rotation suggests using .7 as 1 is too strict) [see @field2012discovering, 763 for more details on this discussion]. Alternatively, we perform the analysis with different numbers of factors and then check if the `SS loadings` (the sum-of-squared loadings) of each factor has a value greater than 1.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract eigenvalues\neigenvalues <- eigen(cor(surveydata))\n# inspect\n# eigenvalues\n\n# create scree plot\neigenvalues$values %>%\n  as.data.frame() %>%\n  dplyr::rename(Eigenvalues = 1) %>%\n  dplyr::mutate(Factors = 1:nrow(.)) %>%\n  ggplot(aes(x = Factors, y = Eigenvalues, label = round(Eigenvalues, 2))) +\n  geom_bar(stat = \"identity\", fill = \"lightgray\") +\n  geom_line(color = \"red\", size = 1) +\n  geom_point() +\n  geom_text(vjust = -1.2) +\n  geom_hline(yintercept = 1, color = \"blue\", linetype = \"dotted\") +\n  theme_bw() +\n  coord_cartesian(ylim = c(0, 10)) +\n  ggtitle(\"Scree plot\")\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n\n\nAs there is a noticeable bend in the Eigenvalues of the factors at 3 factors, we are justified by assuming that there are 3 factors (latent or hidden variables) in our data. We can now perform the factor analysis and set the number of expected factors to 3.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactoranalysis <- factanal(\n  surveydata, # data\n  3\n) # number of factors\nprint(factoranalysis,\n  digits = 2, # round to x decimals\n  cutoff = .2, # do not show loadings smaller than .2\n  sort = TRUE\n) # show items sorted\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nfactanal(x = surveydata, factors = 3)\n\nUniquenesses:\n    Q01_Outgoing     Q02_Outgoing     Q03_Outgoing     Q04_Outgoing \n            0.09             0.06             0.12             0.07 \n    Q05_Outgoing Q06_Intelligence Q07_Intelligence Q08_Intelligence \n            0.14             0.10             0.13             0.10 \nQ09_Intelligence Q10_Intelligence     Q11_Attitude     Q12_Attitude \n            0.28             0.41             0.08             0.14 \n    Q13_Attitude     Q14_Attitude     Q15_Attitude \n            0.04             0.09             0.06 \n\nLoadings:\n                 Factor1 Factor2 Factor3\nQ06_Intelligence -0.82    0.25    0.41  \nQ07_Intelligence -0.80            0.47  \nQ08_Intelligence -0.85            0.42  \nQ09_Intelligence -0.79            0.29  \nQ11_Attitude      0.96                  \nQ12_Attitude      0.92                  \nQ13_Attitude      0.97                  \nQ14_Attitude      0.95                  \nQ15_Attitude      0.96                  \nQ01_Outgoing              0.94          \nQ02_Outgoing              0.96          \nQ03_Outgoing              0.93          \nQ04_Outgoing              0.96          \nQ05_Outgoing              0.92          \nQ10_Intelligence -0.22   -0.46    0.57  \n\n               Factor1 Factor2 Factor3\nSS loadings       7.29    4.78    1.02\nProportion Var    0.49    0.32    0.07\nCumulative Var    0.49    0.80    0.87\n\nTest of the hypothesis that 3 factors are sufficient.\nThe chi square statistic is 62.79 on 63 degrees of freedom.\nThe p-value is 0.484 \n```\n\n\n:::\n:::\n\n\n\nLet us examine the output.\n\nWe start with the *function call*, followed by *uniqueness* scores. Uniqueness refers to the portion of the variance in an observed variable that is not explained by the underlying factors extracted from the data. It represents the variability in a variable that is unique to that variable and cannot be accounted for by the common factors identified in the analysis (uniqueness is the variance that is unique to that variable whereas commonness is the variance it shares with other variables or factors) [see @field2012discovering, 759]. This means that items that do not capture or represent a latent variable well, will have high uniqueness scores (see, e.g., *Q10_Intelligence*).\n\nNext, the output shows a table with factor loadings. *Loadings* are correlations between an observed variable and a latent factor. Loadings indicate how strongly each observed variable is associated with each extracted factor. They provide insights into which factors influence the variation in each observed variable. Factor loadings can be categorized as low (<0.4), acceptable (0.4–0.6), or high [>0.6  @stevens1992applied; @tabachnick2007using; @kline2014easy]. The fact that Q10_Intelligence loads across factors shows that it does not represent its intended factor (IQ) not very well.  \n\n\nWe then get a table showing the sum-of-squares loadings with the amount of variance explained by each latent variable which can help identify the optimal or necessary number of latent variables (factors) that we should or need to look for. As stated above, all factors should have a value greater than 1 - if there is a value below 1 for the `SS loadings`, then  this could mean that at least one factor could be removed.\n\nThe p-value should not be significant (if it is significant, then you have missed factors). In our case,  2 factors would also report a non significant result due to the low number of data points (but p is highest for 3 factors). If this test is significant, then this suggests that you may need more factors than you currently have in your PA (but be careful: more factors can also lead to situations where you get factors that are more difficult to interpret. So make sure that the factors you include *make sense*).\n\nWe can now plot the results to see if and how the different items group together. We start by created a data frame from the FA results containing the information we need for the visualization.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfadat <- data.frame(\n  Factor1 = factoranalysis$loadings[, 1],\n  Factor2 = factoranalysis$loadings[, 2],\n  Items = names(surveydata)\n)\n# inspect\nhead(fadat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     Factor1   Factor2            Items\nQ01_Outgoing     -0.13793094 0.9411952     Q01_Outgoing\nQ02_Outgoing     -0.15957196 0.9560680     Q02_Outgoing\nQ03_Outgoing     -0.06826263 0.9334254     Q03_Outgoing\nQ04_Outgoing     -0.05828165 0.9619694     Q04_Outgoing\nQ05_Outgoing      0.07146568 0.9228791     Q05_Outgoing\nQ06_Intelligence -0.82137535 0.2535663 Q06_Intelligence\n```\n\n\n:::\n:::\n\n\n\nNow, that the data is formatted appropriately, we can plot it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfadat %>%\n  ggplot(aes(x = Factor1, y = Factor2, label = Items, color = Factor1)) +\n  geom_text() +\n  theme_bw() +\n  coord_cartesian(xlim = c(-1.5, 1.5), ylim = c(-1, 1.5)) +\n  scale_color_viridis_b() +\n  labs(x = \"Factor 1 (49 %)\", y = \"Factor 2 (32 %)\")\n```\n\n::: {.cell-output-display}\n![](dimred_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\n\nAs we can see, there are 3 groups (factors) with *Q10_Intelligence* not aligning well with the other elements in that group. This could suggest that *Q10_Intelligence* is not an optimal item / variable for reflecting (or capturing) the latent intelligence factor.\n\n\nThat's all folks!\n\n# Citation & Session Info {-}\n\nSchweinberger, Martin. 2023. *Introduction to Dimension Reduction Methods with R*. The University of Queensland, Australia. School of Languages and Culture. url: www.ladal.edu.au/dimred.html  (Version 2023.09.09).\n\n```\n@manual{schweinberger2023dimred,\n  author = {Schweinberger, Martin},\n  title = {Introduction to Dimension Reduction Methods with R},\n  note = {www.ladal.edu.au/dimred.html},\n    year = {2023},\n  organization = {The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2023.09.09}\n}\n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tufte_0.13       psych_2.4.6.26   report_0.5.9     factoextra_1.0.7\n [5] ggplot2_3.5.1    MASS_7.3-61      tidyr_1.3.1      stringr_1.5.1   \n [9] flextable_0.9.7  here_1.0.1       dplyr_1.1.4     \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6            xfun_0.49               htmlwidgets_1.6.4      \n [4] rstatix_0.7.2           insight_0.20.5          ggrepel_0.9.6          \n [7] lattice_0.22-6          vctrs_0.6.5             tools_4.4.1            \n[10] generics_0.1.3          datawizard_0.13.0       parallel_4.4.1         \n[13] tibble_3.2.1            fansi_1.0.6             pkgconfig_2.0.3        \n[16] data.table_1.16.2       uuid_1.2-1              lifecycle_1.0.4        \n[19] farver_2.1.2            compiler_4.4.1          textshaping_0.4.0      \n[22] munsell_0.5.1           mnormt_2.1.1            codetools_0.2-20       \n[25] carData_3.0-5           fontquiver_0.2.1        fontLiberation_0.1.0   \n[28] htmltools_0.5.8.1       Formula_1.2-5           car_3.1-3              \n[31] ggpubr_0.6.0            pillar_1.9.0            openssl_2.2.2          \n[34] abind_1.4-8             nlme_3.1-166            fontBitstreamVera_0.1.1\n[37] tidyselect_1.2.1        zip_2.3.1               digest_0.6.37          \n[40] stringi_1.8.4           purrr_1.0.2             labeling_0.4.3         \n[43] rprojroot_2.0.4         fastmap_1.2.0           grid_4.4.1             \n[46] colorspace_2.1-1        cli_3.6.3               magrittr_2.0.3         \n[49] utf8_1.2.4              broom_1.0.7             withr_3.0.2            \n[52] backports_1.5.0         gdtools_0.4.0           scales_1.3.0           \n[55] rmarkdown_2.28          officer_0.6.7           ggsignif_0.6.4         \n[58] askpass_1.2.1           ragg_1.3.3              evaluate_1.0.1         \n[61] knitr_1.48              viridisLite_0.4.2       rlang_1.1.4            \n[64] Rcpp_1.0.13             glue_1.8.0              xml2_1.3.6             \n[67] jsonlite_1.8.9          R6_2.5.1                systemfonts_1.1.0      \n```\n\n\n:::\n:::\n\n\n\n***\n\n[Back to top](#introduction)\n\n\n***\n\n# References {-}\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/tabwid-1.1.3/tabwid.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/tabwid-1.1.3/tabwid.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}