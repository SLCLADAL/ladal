[
  {
    "objectID": "resources.html#atap",
    "href": "resources.html#atap",
    "title": "RESOURCES",
    "section": "ATAP",
    "text": "ATAP\nLADAL is part of the Australian Text Analytics Platform (ATAP). The aim of ATAP is to provide researchers with a Notebook environment – in other words a tool set - that is more powerful and customisable than standard packages, while being accessible to a large number of researchers who do not have strong coding skills."
  },
  {
    "objectID": "resources.html#tools",
    "href": "resources.html#tools",
    "title": "RESOURCES",
    "section": "Tools",
    "text": "Tools\n\n\nAntConc (and other Ant tools)\nAntConc is a freeware corpus analysis toolkit for concordancing and text analysis developed by Laurence Anthony. In addition to AntConc, Laurence Anthony’s AntLab contains a conglomeration of extremely useful and very user-friendly software tools, that help and facilitate the analysis of textual data. Laurence has really developed an impressive, very user-friendly selection of tools that assist anyone interested in working with language data.\n\n\n\nSMARTool\nSMARTool is a corpus-based language learning and analysis tool for for English-speaking learners of Russian. It is linguist-built and thus informed by modern linguistic theory. SMARTool assists learners with coping with the rich Russian morphology and has user-friendly, corpus-based information and help for learning the most frequent forms of 3,000 basic vocabulary items."
  },
  {
    "objectID": "resources.html#courses",
    "href": "resources.html#courses",
    "title": "RESOURCES",
    "section": "Courses",
    "text": "Courses\n\n\nApplied Language Technology\nApplied Language Technology is a website hosting learning materials for two courses taught at the University of Helsinki: Working with Text in Python and Natural Language Processing for Linguists. Together, these two courses provide an introduction to applied language technology for audiences who are unfamiliar with language technology and programming. The learning materials assume no previous knowledge of the Python programming language.\n\n\n\nCultural Analytics with Python\nIntroduction to Cultural Analytics & Python is a website established by Melanie Walsh that hosts an online textbook which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences.\n\n\nGLAM Workbench\nThe GLAM Workbench is a collection of tools, tutorials, examples, and hacks to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time.\nThe resources in the GLAM Workbench are created and shared as Jupyter notebooks. Jupyter lets you combine narrative text and live code in an environment that encourages you to learn and explore. Jupyter notebooks run in your browser, and you can get started without installing any software!\nOne really great advantage of the GLAM workbench is that it is interactive: if you click on one of the links that says Run live on Binder, this will open the notebook, ready to use, in a customised computing environment using the Binder service.\n\n\nProgramming Historian\nThe Programming Historian is a collaborative blog that contains lessons on various topics associated with computational humanities. The blog was founded in 2008 by William J. Turkel and Alan MacEachern. It focused heavily on the Python programming language and was published open access as a Network in Canadian History & Environment (NiCHE) Digital Infrastructure project. In 2012, Programming Historian expanded its editorial team and launched as an open access peer reviewed scholarly journal of methodology for digital historians.\n\n\nTAPoR 3\nTAPoR 3, the Text Analysis Portal for Research contains a unique, well-curated list of a wide variety of tools commonly used or widely respected groups of tools used by leading scholars in the various fields of Digital Humanities. TAPoR 3 was developed with support from the Text Mining the Novel Project. The tools mentioned in the list provided by TAPoR 3 represent both tried and trusted tools used by DH scholars, or new advancements that offer exciting new possibilities in their field. Curated lists are excellent places to start exploring TAPoR from, and can help with deciding what to include in your own lists.\n\n\nQuick-R\nQuick-R by DataCamp and maintained by Rob Kabacoff that contains tutorial and R code on R, data management, statistics, and data visualization. The site is extremely helpful and recommendable for everyone that looks for code snippets that can be adapted for your own use.\n\n\nSTHDA\nStatistical Tools For High-Throughput Data Analysis (STHDA) is a website containing various tutorials on the implementation of statistical methods in R. It is particularly useful due to the wide range of topics and procedures it covers."
  },
  {
    "objectID": "resources.html#centers-labs",
    "href": "resources.html#centers-labs",
    "title": "RESOURCES",
    "section": "Centers | Labs",
    "text": "Centers | Labs\n\n\nText Crunching Centre\nAt the Text Crunching Centre, a team of experts in Natural Language Processing supports your text technology needs. The TCC is part of the Department of Computational Linguistics at the University of Zurich and it is a service offered to all departments of the University of Zurich as well as to external partners or customers.\n\n\n\nVARIENG\nVARIENG stands for the Research Unit for the Study of Variation, Contacts and Change in English. It also stands for innovative thinking and team work in English corpus linguistics and the study of language variation and change. VARIENG members study the English language, its uses and users, both today and in the past.\n\n\n\nAcqVA Aurora Lab\nThe AcqVA Aurora Lab, is the lab of the UiT Aurora Center for Language Acquisition, Variation & Attrition at The Arctic University of Norway in Tromsø. Together with the Psycholinguistics of Language Representation (PoLaR) lab, the AcqVA Lab provides methodological support for researchers at the AcqVA Aurora Center.\n\n\n\nSydney Corpus Lab\nThe Sydney Corpus Lab aims to promote corpus linguistics in Australia. It’s a virtual, rather than a physical lab, and is an online platform for connecting computer-based linguists across the University of Sydney and beyond. Its mission is to build research capacity in corpus linguistics at the University of Sydney, to connect Australian corpus linguists, and to promote the method in Australia, both in linguistics and in other disciplines. We have strong links with the Sydney Centre for Language Research (Computational Approaches to Language node) and the Sydney Digital Humanities Research Group.\n\n\n\nAntLab\nLaurence Anthony’s AntLab contains a conglomeration of extremely useful and very user-friendly software tools, such as AntConc that help and facilitate the analysis of textual data.\n\n\n\n\nMedia Research Methods Lab\nThe Media Research Methods Lab (MRML) at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI) is designed as a method-oriented lab, which focuses on linking established social science methods (surveys, observations, content analysis, experiments) with new digital methods from the field of computational social science (e.g. automated content analysis, network analysis, log data analysis, experience sampling) across topics and disciplines.\n\n\nNational Centre for Text Mining\nThe National Centre for Text Mining (NaCTeM), located in the UK, is the first publicly-funded text mining centre in the world. We provide text mining services in response to the requirements of the UK academic community. On the NaCTeM website, you can find pointers to sources of information about text mining such as links to text mining services provided by NaCTeM, software tools (both those developed by the NaCTeM team and by other text mining groups), seminars, general events, conferences and workshops, tutorials and demonstrations, and text mining publications."
  },
  {
    "objectID": "resources.html#blogs",
    "href": "resources.html#blogs",
    "title": "RESOURCES",
    "section": "Blogs",
    "text": "Blogs\n\nHypotheses\nHypotheses is a research blog by [Guillaume Desagulier](http://www2.univ-paris8.fr/desagulier/home/. In this blog, entitled Around the word, A corpus linguist’s notebook, Guillaume has recorded reflections and experiments on his practice as a usage-based corpus linguist and code for analyzing language using R.\n\n\nAneesha Bakharia\nAneesha Bakharia is a blog by Aneesha Bakharia about Data Science, Topic Modeling, Deep Learning, Algorithm Usability and Interpretation, Learning Analytics, and Electronics. Aneesha began her career as an electronics engineer but quickly transitioned to an educational software developer. Aneesha has worked in the higher education and vocational educational sectors in a variety of technical, innovation and project management roles. In her most recent role before commencing at UQ, Aneesha was a project manager for a large OLT Teaching and Learning grant on Learning Analytics at QUT. Aneesha’s primary responsibilities at ITaLI include directing the design, development and implementation of learning analytics initiatives (including the Course Insights teacher facing dashboard) at UQ.\n\n\nLinguistics with a Corpus\nLinguistics with a Corpus is a companion blog to the Cambridge Element book Doing Linguistics with a Corpus. Methodological Considerations for the Everyday User (Egbert, Larsson, and Biber, n.d.). The blog is intended to be a forum for methodological discussion in corpus linguistics, and Jesse, Tove, and Doug very much welcome comments and thoughts by visitors and readers!\n\n\nDigital Observatory Blog\nThe blog of the Digital Observatory at the Queensland University of Technology (QUT) contains updates on resources, meetings, (open) office hours, workshops, and developments at the Digital Observatory which provides services to researchers such as retrieving and processing social media data or offering workshops on data processing, data visualization, and anything related to gathering and working with social media data."
  },
  {
    "objectID": "resources.html#other-resources",
    "href": "resources.html#other-resources",
    "title": "RESOURCES",
    "section": "Other Resources",
    "text": "Other Resources\n\nBookNLP\nBookNLP is a natural language processing pipeline that scales to books and other long documents (in English), including:\n\nPart-of-speech tagging\n\nDependency parsing\n\nEntity recognition\n\nCharacter name clustering (e.g., “Tom”, “Tom Sawyer”, “Mr. Sawyer”, “Thomas Sawyer” -&gt; TOM_SAWYER) and conference resolution\n\nQuotation speaker identification\n\nSupersense tagging (e.g., “animal”, “artifact”, “body”, “cognition”, etc.)\n\nEvent tagging\n\nReferential gender inference (TOM_SAWYER -&gt; he/him/his)\n\nBookNLP ships with two models, both with identical architectures but different underlying BERT sizes. The larger and more accurate big model is fit for GPUs and multi-core computers; the faster small model is more appropriate for personal computers.\n\n\nDigital Humanities Awards\nThe Digital Humanities Awards page contains a list of great DH resources for the following categories:\n\nBest Exploration Of DH Failure/Limitations\n\nBest DH Data Visualization\n\nBest Use Of DH For Fun\n\nBest DH Dataset\n\nBest DH Short Publication\n\nBest DH Tool Or Suite Of Tools\n\nBest DH Training Materials\n\nSpecial Category: Best DH Response To COVID-19\n\n\n\nText Analysis in Python for Social Scientists\nThe book Text Analysis in Python for Social Scientists. Prediction and Classification contains a wealth of information about about a wide variety of sociocultural constructs. Automated prediction methods can infer these quantities (sentiment analysis is probably the most well-known application). However, there is virtually no limit to the kinds of things we can predict from text: power, trust, misogyny, are all signaled in language. These algorithms easily scale to corpus sizes unfeasible for manual analysis. Prediction algorithms have become steadily more powerful, especially with the advent of neural network methods. However, applying these techniques usually requires profound programming knowledge and machine learning expertise. As a result, many social scientists do not apply them. This Element provides the working social scientist with an overview of the most common methods for text classification, an intuition of their applicability, and Python code to execute them. It covers both the ethical foundations of such work as well as the emerging potential of neural network methods.\n\n\nData Measurements Tool\nThis blog entry introduces the Data Measurements Tool. The Data Measurements Tool is a Interactive Tool for Looking at Datasets. The Data Measurements Tool is a open-source Python library and no-code interface called the Data Measurements Tool, using our Dataset and Spaces Hubs paired with the great Streamlit tool. This can be used to help understand, build, curate, and compare datasets.\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "resources.html#introduction",
    "href": "resources.html#introduction",
    "title": "RESOURCES",
    "section": "Introduction",
    "text": "Introduction\nIf you want to contribute to LADAL and become an author, you can use this stylesheet for your LADAL tutorial. The sections below provide additional information on the style and format of your LADAL tutorial. The R Markdown document that you can use as a template can be downloaded here.\nYou will also have to download the bibliography file from https://slcladal.github.io/content/bibliography.bib for the tutorial to be knitted correctly. Although the knitted (or rendered) html file will look different from the LADAL design (because we have modified the theme for the LADAL page), it will be just like a proper LADAL tutorial once we have knitted the Rmd file on our machines and integrated your tutorial into the LADAL website.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# install packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNext, we activate the packages. Please include klippy in the installation and loading chunks to allow easy copy&pasting of code.\n\n# activate packages\nlibrary(tidyverse)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R, RStudio, and have also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "resources.html#tutorial-content",
    "href": "resources.html#tutorial-content",
    "title": "RESOURCES",
    "section": "Tutorial content",
    "text": "Tutorial content\nLoad some data and show what you want to show.\n\n# load data\ndata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/sld.rda\", \"rb\"))\n\nUse flextable for displaying tabular data as shown below.\n\ndata %&gt;%\n    as.data.frame() %&gt;%\n    head(10) %&gt;%\n    flextable() %&gt;%\n    flextable::set_table_properties(width = .5, layout = \"autofit\") %&gt;%\n    flextable::theme_zebra() %&gt;%\n    flextable::fontsize(size = 12) %&gt;%\n    flextable::fontsize(size = 12, part = \"header\") %&gt;%\n    flextable::align_text_col(align = \"center\") %&gt;%\n    flextable::set_caption(caption = \"\") %&gt;%\n    flextable::border_outer()\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North\n\n\nBelow is the code chunk for exercises.\n\n\n\nEXERCISE TIME!\n\n\n\n\nThis is an example question.\n\n\n\nAnswer\n\n\n# this is some code\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "resources.html#remarks-on-type-setting",
    "href": "resources.html#remarks-on-type-setting",
    "title": "RESOURCES",
    "section": "Remarks on type setting",
    "text": "Remarks on type setting\nLevel 1 headers with numbers,, lower level headers without numbering (add {-} at the end of the header to suppress numbering).\nFunction and package names in package style (different from normal text).\nUse italics for emphasis rather than bold type."
  },
  {
    "objectID": "resources.html#citation-session-info",
    "href": "resources.html#citation-session-info",
    "title": "RESOURCES",
    "section": "Citation & Session Info",
    "text": "Citation & Session Info\nYour last name, your first name. 2024. The title of your tutorial. Your location: your affiliation (in case you have one). url: https://slcladal.github.io/shorttitleofyourtutorial.html (Version 2024.12.20).\n@manual{yourlastname2024net,\n  author = {YourLastName, YourFirstName},\n  title = {The title of your tutorials},\n  note = {https://slcladal.github.io/shorttitleofyourtutorial.html},\n  year = {2024},\n  organization = {Your affiliation},\n  address = {Your location},\n  edition = {2024.12.20}\n}\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] flextable_0.9.7 lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1  \n [5] dplyr_1.1.4     purrr_1.0.2     readr_2.1.5     tidyr_1.3.1    \n [9] tibble_3.2.1    ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4              generics_0.1.3          fontLiberation_0.1.0   \n [4] xml2_1.3.6              stringi_1.8.4           hms_1.1.3              \n [7] digest_0.6.37           magrittr_2.0.3          evaluate_1.0.1         \n[10] grid_4.4.1              timechange_0.3.0        fastmap_1.2.0          \n[13] jsonlite_1.8.9          zip_2.3.1               fansi_1.0.6            \n[16] scales_1.3.0            fontBitstreamVera_0.1.1 codetools_0.2-20       \n[19] klippy_0.0.0.9500       textshaping_0.4.0       cli_3.6.3              \n[22] rlang_1.1.4             fontquiver_0.2.1        munsell_0.5.1          \n[25] withr_3.0.2             yaml_2.3.10             gdtools_0.4.0          \n[28] tools_4.4.1             officer_0.6.7           uuid_1.2-1             \n[31] tzdb_0.4.0              colorspace_2.1-1        assertthat_0.2.1       \n[34] vctrs_0.6.5             R6_2.5.1                lifecycle_1.0.4        \n[37] htmlwidgets_1.6.4       ragg_1.3.3              pkgconfig_2.0.3        \n[40] pillar_1.9.0            gtable_0.3.6            glue_1.8.0             \n[43] data.table_1.16.2       Rcpp_1.0.13             systemfonts_1.1.0      \n[46] xfun_0.49               tidyselect_1.2.1        rstudioapi_0.17.1      \n[49] knitr_1.48              htmltools_0.5.8.1       rmarkdown_2.28         \n[52] compiler_4.4.1          askpass_1.2.1           openssl_2.2.2          \n\n\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "tutorials/reinfnlp/reinfnlp.html#preparation-and-session-set-up",
    "href": "tutorials/reinfnlp/reinfnlp.html#preparation-and-session-set-up",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts in this tutorial are executed without errors. Before continuing, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\nFor this tutorial we will be primarily requiring four packages: tidytext for text manipulations, tidyverse for general tasks, textrank for the implementation of the TextRank algorithm and rvest to scrape through an article to use as an example. For this analysis an article for Time has been selected.\n\n# set options\noptions(stringsAsFactors = F)\n# install libraries\ninstall.packages(c(\"tidytext\", \"tidyverse\", \"textrank\", \"rvest\", \"ggplot2\"))\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(textrank)\nlibrary(rvest)\nlibrary(ggplot2)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/reinfnlp/reinfnlp.html#text-summarisation",
    "href": "tutorials/reinfnlp/reinfnlp.html#text-summarisation",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Text Summarisation",
    "text": "Text Summarisation\nA deep reinforced model for text summarisation involves sequence of input tokens x={x1,x2,…,xn} and produces a sequence of output (summary) tokens. A schematic presentation of the process is shown below:\n\nFor the article summarisation objective the deep RL has the following components:\n\nAction which involves a function ut which copies and generates summary output yt\nState it encapsulates the hidden states of encoder and previous outputs\nReward which generates a rough score determining the performance of the summarisation\n\nText summarisation (see Mihalcea and Tarau) is highly critical in extracting important information from large texts.\nIn case of text summarisation there are broadly two categories:\n\nExtractive Summarisation\nAbstractive Summarisation\n\nIn case of Extractive Summarisation words and sentences are scored according to a specific metric and then utilizing that information for summarizing based copying or pasting the most informative parts of the text.\nOn the other hand Abstractive Summarisation involves building a semantic representation of the text and then incorporating natural language generation techniques to generate text highlighting the informative parts of the parent text document.\nHere, we will be focusing on an extractive summarisation method called TextRank which is hinged upon the PageRank algorithm which was developed by Google to rank websites based on their importance.\nThe TextRank Algorithm\nTextRank is a graph-based ranking algorithm for NLP. Graph-based ranking algorithms evaluate the importance of a vertex within a graph, based on global information extracted recursively from the entire graph. When one vertex is associated with another it is actually casting a vote for that vertex. The higher the number of votes cast for a vertex, the higher importance of that vertex.\nIn the NLP case it is necessary to define vertices and edges. In this tutorial we will be using sentences as vertices and words as edges. Thus sentences with words present in many other sentences will have higher priority\n\n# define url\nurl &lt;- \"http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\"\n# read in data\nsite &lt;- read_html(url)\narticle &lt;- html_text(html_nodes(site, \"p,h1,h2,h3\"))\n# inspect\narticle\n\n[1] \"Fitbit’s Newest Fitness Tracker Is Just for Kids\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n[2] \"Fitbit is launching a new fitness tracker designed for children called the Fitbit Ace, which will go on sale for $99.95 in the second quarter of this year.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n[3] \"The [tempo-ecommerce src=”http://www.amazon.com/Fitbit-Activity-Tracker-Purple-Stainless/dp/B07B9DX4WB&#8221; title=”Fitbit Ace” context=”body”] looks a lot like the company’s Alta tracker, but with a few child-friendly tweaks. The most important of which is Fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the Children’s Online Privacy Protection Act, or COPPA. Parents must approve who their child can connect with via the Fitbit app and can view their kid’s activity progress and sleep trends, the latter of which can help them manage their children’s bedtimes.\"                                                              \n[4] \"Like many of Fitbit’s other products, the Fitbit Ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long. But while Fitbit’s default move goal is 30 minutes for adult users, the Ace’s will be 60 minutes, in line with the World Health Organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day. Fitbit says the tracker is designed for children eight years old and up.\"                                                                                                                                                                                                                  \n[5] \"Fitbit will also be introducing a Family Faceoff feature that lets kids compete in a five-day step challenge against the other members of their family account. The app also will reward children with in-app badges for achieving their health goals. Fitbit’s new child-friendly fitness band will be available in blue and purple, is showerproof, and should last for five days on a single charge.\"                                                                                                                                                                                                                                                                                                                         \n[6] \"The Ace launch is part of Fitbit’s broader goal of branching out to new audiences. The company also announced a new smartwatch on Tuesday called the Versa, which is being positioned as an everyday smartwatch rather than a fitness-only device or sports watch, like some of the company’s other products.\"                                                                                                                                                                                                                                                                                                                                                                                                                   \n[7] \"Above all else, the Ace is an effort to get children up and moving. The Centers for Disease Control and Prevention report that the percentage of children and adolescents affected by obesity has more than tripled since the 1970’s. But parents who want to encourage their children to move already have several less expensive options to choose from. Garmin’s $79.99 Vivofit Jr. 2, for example, comes in themed skins like these Minnie Mouse and Star Wars versions, while the wristband entices kids to move by reflecting their fitness achievements in an accompanying smartphone game. The $39.99 Nabi Compete, meanwhile, is sold in pairs so that family members can work together to achieve movement milestones.\"\n[8] \"More Must-Reads from TIME\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n[9] \"Contact us at letters@time.com\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n\n\nNext the article is loaded into a tibble. Then tokenisation is implemented according to sentences. Although this tokenisation is fully perfect it has a lower number of dependencies and is suitable for this case. Finally we add column for sentence number and switch the order of the columns.\n\ntibble::tibble(text = article) %&gt;%\n    tidytext::unnest_tokens(sentence, text, token = \"sentences\") %&gt;%\n    dplyr::mutate(sentence_id = row_number()) %&gt;%\n    dplyr::select(sentence_id, sentence) -&gt; article_sentences\n# inspect\narticle_sentences\n\n# A tibble: 20 × 2\n   sentence_id sentence                                                         \n         &lt;int&gt; &lt;chr&gt;                                                            \n 1           1 fitbit’s newest fitness tracker is just for kids                 \n 2           2 fitbit is launching a new fitness tracker designed for children …\n 3           3 the [tempo-ecommerce src=”http://www.amazon.com/fitbit-activity-…\n 4           4 the most important of which is fitbit’s new family account optio…\n 5           5 parents must approve who their child can connect with via the fi…\n 6           6 like many of fitbit’s other products, the fitbit ace can automat…\n 7           7 but while fitbit’s default move goal is 30 minutes for adult use…\n 8           8 fitbit says the tracker is designed for children eight years old…\n 9           9 fitbit will also be introducing a family faceoff feature that le…\n10          10 the app also will reward children with in-app badges for achievi…\n11          11 fitbit’s new child-friendly fitness band will be available in bl…\n12          12 the ace launch is part of fitbit’s broader goal of branching out…\n13          13 the company also announced a new smartwatch on tuesday called th…\n14          14 above all else, the ace is an effort to get children up and movi…\n15          15 the centers for disease control and prevention report that the p…\n16          16 but parents who want to encourage their children to move already…\n17          17 garmin’s $79.99 vivofit jr. 2, for example, comes in themed skin…\n18          18 the $39.99 nabi compete, meanwhile, is sold in pairs so that fam…\n19          19 more must-reads from time                                        \n20          20 contact us at letters@time.com                                   \n\n\nNext we will tokenize based on words.\n\narticle_words &lt;- article_sentences %&gt;%\n    tidytext::unnest_tokens(word, sentence)\narticle_words\n\n# A tibble: 466 × 2\n   sentence_id word    \n         &lt;int&gt; &lt;chr&gt;   \n 1           1 fitbit’s\n 2           1 newest  \n 3           1 fitness \n 4           1 tracker \n 5           1 is      \n 6           1 just    \n 7           1 for     \n 8           1 kids    \n 9           2 fitbit  \n10           2 is      \n# ℹ 456 more rows\n\n\nWe have one last step left is to remove the stop words in article_words as they are prone to result in redundancy.\n\narticle_words &lt;- article_words %&gt;%\n    dplyr::anti_join(stop_words, by = \"word\")\narticle_words\n\n# A tibble: 225 × 2\n   sentence_id word     \n         &lt;int&gt; &lt;chr&gt;    \n 1           1 fitbit’s \n 2           1 fitness  \n 3           1 tracker  \n 4           1 kids     \n 5           2 fitbit   \n 6           2 launching\n 7           2 fitness  \n 8           2 tracker  \n 9           2 designed \n10           2 children \n# ℹ 215 more rows\n\n\nUsing the textrank package it is really easy to implement the TextRank algorithm. The textrank_sentences function requires only 2 inputs:\n\nA data frame with sentences\nA data frame with tokens which are part of each sentence\n\n\narticle_summary &lt;- textrank_sentences(\n    data = article_sentences,\n    terminology = article_words\n)\n# inspect the summary\narticle_summary\n\nTextrank on sentences, showing top 5 most important sentences found:\n  1. fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.\n  2. fitbit says the tracker is designed for children eight years old and up.\n  3. fitbit’s newest fitness tracker is just for kids\n  4. like many of fitbit’s other products, the fitbit ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long.\n  5. the most important of which is fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the children’s online privacy protection act, or coppa.\n\n\nLets have a look where these important sentences appear in the article:\n\nlibrary(ggplot2)\narticle_summary[[\"sentences\"]] %&gt;%\n    ggplot(aes(textrank_id, textrank, fill = textrank_id)) +\n    geom_col() +\n    theme_minimal() +\n    scale_fill_viridis_c() +\n    guides(fill = \"none\") +\n    labs(\n        x = \"Sentence\",\n        y = \"TextRank score\",\n        title = \"Most informative sentences appear within first half of sentences\",\n        subtitle = 'In article \"Fitbits Newest Fitness Tracker Is Just for Kids\"',\n        caption = \"Source: http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\"\n    )\n\n\n\n\nPosition of Important Sentences in the Article"
  },
  {
    "objectID": "tutorials/reinfnlp/reinfnlp.html#other-applications-of-rl",
    "href": "tutorials/reinfnlp/reinfnlp.html#other-applications-of-rl",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Other Applications of RL",
    "text": "Other Applications of RL\n\nDialogue Generation\nIn today’s digital world dialogue generation is a widely used application especially in chatbots. One widely used model in this regard is the Long Short Term Memory (LSTM) sequence-to-sequence (SEQ2SEQ) model. It is a neural generative model that maximizes the probability of generating a response given the previous dialogue. However SEQ2SEQ model has some constraints:\n\nThey tend to generate highly generic responses\nOften they are stuck in an infinite loop of repetitive responses\n\nThis is where deep RL is much more efficient as it can integrate developer-defined rewards which efficiently mimics the true goal of chatbot development. In case of dialogue generation the component:\n\nAction which involves a function that generates sequences of arbitrary lengths\nState it comprises of previous 2 dialogue turns [pi,qi]\nReward which determines the ease of answering, information flow and semantic coherence\n\nThe schematic diagram highlighting the dialogue simulation between 2 agents using deep RL is shown below:\n\n\n\nNeural Machine Translation\nMost of Neural Machine Translation (NMT) models are based encoder-decoder framework with attention mechanism. The encoder initially maps a source sentence x={x1,x2,…,xn} to a set of continuous representations z={z1,z2,…,zn} . Given z the decoder then generates a target sentence y={y1,y2,…,ym} of word tokens one by one. RL is used to bridge the gap between training and inference of of NMT by directly optimizing the loss function at training time. In this scenario the NMT model acts as the agent which interacts with the environment which in this case are the previous words and the context vector z available at each step t. This is a a policy based RL and in place of a state a policy will be assigned in every iteration. The critical components of the RL for NMT are discussed below:\n\nPolicy which is a conditional probability defined by the parameters of the agent\nAction is decided by the agent based on the policy and it will pick up a candidate word from the vocabulary\nReward is evaluated once the agent generates a complete sequence which in case of machine translation is Bilingual Evaluation Understudy (BLEU).BLEU is defined by comparing the generated sequence with the ground truth sequence.\n\nThe schematic of the overall process is depicted below:"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html",
    "href": "tutorials/atap_docclass/atap_docclass.html",
    "title": "Classifying American Speeches",
    "section": "",
    "text": "This tutorial shows how to perform document classification using R. The entire R markdown document for the tutorial can be downloaded here.\nThe tutorial requires you to install and load several packages to analyze linguistic data. To help you, we directly include the commands to do so in the script and walk you through it step by step. Let’s get on it!"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#preparation-and-session-setup",
    "href": "tutorials/atap_docclass/atap_docclass.html#preparation-and-session-setup",
    "title": "Classifying American Speeches",
    "section": "Preparation and Session Setup",
    "text": "Preparation and Session Setup\nAs mentioned at the outset, this is an introduction to document classification based on R. A rudimentary familiarity with R and RStudio are helpful for getting the most out of this. If you have yet to install R or are new to it, we can recommend this tutorial to R, which walks you through the installation and shows a range of its functionalities. In the following, we assume that you have downloaded and installed both R and RStudio and take it from there.\nAlthough R in its basic version already contains a lot of functionalities, we do need to install some packages for the code below to run without exploding into a bouquet of error messages. If you already have the packages quanteda, quanteda.textmodels, stopwords and ggplot2 installed, just skip to the next heading. If you don’t have them yet, the first thing you want to do is run these first lines of code:\n\n# install.packages(\"quanteda\")\n# install.packages(\"quanteda.textmodels\")\n# install.packages(\"stopwords\")\n# install.packages(\"ggplot2\")\n\nThis may take a minute or three. In most cases, this will work without any hiccups. If you should get an error message, we recommend taking a moment to read what it says, and, if it does not make any sense to you, to google it. If an issue comes up for you, chances are that this has already happened to someone else - and, fortunately, the R community has a pretty good track record of responding to questions about technical issues. Generally, it is also a good idea to use a relatively new version of R. If you have last used R two years ago, do update it.\nOnce you have installed the packages, you’ll need to load them in the current session. This is done with the following lines of code:\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(stopwords)\nlibrary(ggplot2)\n\nWith the packages loaded, the stage is set. Now we need to dress up the actors."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#from-data-to-useable-objects",
    "href": "tutorials/atap_docclass/atap_docclass.html#from-data-to-useable-objects",
    "title": "Classifying American Speeches",
    "section": "From Data to Useable Objects",
    "text": "From Data to Useable Objects\nBefore we can do anything further, we need to load the data into R. As mentioned earlier, we are working with the CORPS II corpus compiled by Guerini et al. (2010). There are many ways to get the corpus into R, but we are going to download the data from the data directory of the LADAL GitHub repository. By executing the following line of code, you tell R to download the file containing the corpus from github and store it in the session under the variable name rt:\n\nrt &lt;- base::readRDS(url(\"https://ladal.edu.au/data/SEL_perparty_v2.rda\", \"rb\"))\n\nIn the meantime, we want to draw your attention to the second parameter of the readtext command, namely the specification defining the text field. These specifications are called flags. These are often useful and sometimes necessary. In this case, the flag does exactly what it says: it tells R in which column the text is stored.\nLet’s take a look at rt, the variable in which we stored the corpus. We can do this simply by str which returns the structure of the object:\n\nstr(rt)\n\nClasses 'readtext' and 'data.frame':    3247 obs. of  5 variables:\n $ doc_id: chr  \"SEL_perparty_v2.csv.1\" \"SEL_perparty_v2.csv.2\" \"SEL_perparty_v2.csv.3\" \"SEL_perparty_v2.csv.4\" ...\n $ text  : chr  \"We who are Christians usually think about Christ in terms of obligations of charity , and faith in God , and so\"| __truncated__ \"It is a very great honor and pleasure for me to be here and participate in a dinner this evening that aims at s\"| __truncated__ \"Thank you . Praise God . Thank you very much . Tonight , I would like you first of all to join me in rememberin\"| __truncated__ \"Thank you . Thank you . Praise God . Thank you very much . I want to tell you I 'm very happy to have a few mom\"| __truncated__ ...\n $ party : chr  \"rep\" \"rep\" \"rep\" \"rep\" ...\n $ fileid: chr  \"akeyes-95\" \"akeyes-98\" \"akeyes1-2-00\" \"akeyes1-6-96\" ...\n $ name  : chr  \"Alan_Keyes\" \"Alan_Keyes\" \"Alan_Keyes\" \"Alan_Keyes\" ...\n\n\nThe output tells us what we are dealing with: a object consisting of 3247 documents and 3 document variables. We also see the first six rows of the object, which shows that we are dealing with a two dimensional matrix or a table, which in the R context is also called a data frame. What may be surprising is that, although the object is described as having three document variables, the data frame has five columns.\nIf we look at the structure of the corpus, we can see that the file contains four columns, labeled party, fileid, name and text, respectively. The object rt to which we assigned the data created an additional column, labeled doc_id. In this variable, the row number of each row in the file is stored. If we were to load in a second document and append it to the rt object, it would be clear which row of data comes from which corpus. So, while it’s great to have this doc_id, the output also makes it clear that it is not meaningful information relevant to the text we are interested in.\nThe text source files come from the textual component of the files, and the document-level metadata (“docvars”) come from either the file contents or filenames.\nSo the three document variables are the document-level metadata, which pertain to the texts we are interested in. Depending on the type of analysis or operation you are interested in performing on the data, this is not essential information. For document classification, however, as well as many other analyses, this metadata is essential."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#tokenization",
    "href": "tutorials/atap_docclass/atap_docclass.html#tokenization",
    "title": "Classifying American Speeches",
    "section": "Tokenization",
    "text": "Tokenization\nThe first step here is to transform the text in the corpus object into individual tokens. Right now, each text cell in the fulltext object contains one whole presidential campaign speech. By tokenizing the corpus, each text entry in fulltext is transformed into a list of words. In order to do this, we define a new variable, toks, and use the perfectly descriptive command tokens() to tokenize the corpus and assign it to the new variable toks:\n\ntoks &lt;- tokens(fulltext, remove_punct = TRUE)\n\nSneakily, we also use a flag here to remove punctuation. Let’s look at how this step changed the content of the corpus. Having removed the punctuation, it makes sense to pick a speech which contained punctuation in the beginning, so we choose speech four. We can look at this specific speech by entering the name of the object, toks, and adding the row number of choice in square brackets, thus:\n\ntoks[4]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.4 :\n [1] \"Thank\"  \"you\"    \"Thank\"  \"you\"    \"Praise\" \"God\"    \"Thank\"  \"you\"   \n [9] \"very\"   \"much\"   \"I\"      \"want\"  \n[ ... and 2,530 more ]"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#lowercasing",
    "href": "tutorials/atap_docclass/atap_docclass.html#lowercasing",
    "title": "Classifying American Speeches",
    "section": "Lowercasing",
    "text": "Lowercasing\nThis next step is fairly obvious: the heading says it, the command says it, and it’s precisely what we are going to do. We’re going to replace all capital letters by their lowercase equivalent in the corpus. This is mainly to reduce the variation that arises from words that stand at the beginning of a sentence. To do it, we use the command tokens_tolower(), as demonstrated here:\n\ntoks &lt;- tokens_tolower(toks)\n\nThe command is self explanatory, but we should add: we define the object toks as input, replace capital letters with lowercase ones and assign the outcome again to the object toks, thus overwriting what was there before. If we now look at row four again, we see the result of this transformation:\n\ntoks[4]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.4 :\n [1] \"thank\"  \"you\"    \"thank\"  \"you\"    \"praise\" \"god\"    \"thank\"  \"you\"   \n [9] \"very\"   \"much\"   \"i\"      \"want\"  \n[ ... and 2,530 more ]\n\n\nWhere before there was Thank and I, we now have thank and i. Not much more to see here, so let’s keep moving."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#stemming",
    "href": "tutorials/atap_docclass/atap_docclass.html#stemming",
    "title": "Classifying American Speeches",
    "section": "Stemming",
    "text": "Stemming\nThis time round, the transformation is a bit more drastic: we are going to reduce each word in the corpus to its word stem. Take a quick look at the first item of the toks object:\n\ntoks[1]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.1 :\n [1] \"we\"          \"who\"         \"are\"         \"christians\"  \"usually\"    \n [6] \"think\"       \"about\"       \"christ\"      \"in\"          \"terms\"      \n[11] \"of\"          \"obligations\"\n[ ... and 6,212 more ]\n\n\nWe have here, among other words, christians, usually and obligations. Keep those in mind as you reduce the corpus to wordstems, again using a fairly intuitive command:\n\ntoks &lt;- tokens_wordstem(toks)\n\nUsing the same procedure as with lowercasing, we overwrite the contents of the toks object with its latest transformation. We can see how this plays out by again looking at the first item.\n\ntoks[1]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.1 :\n [1] \"we\"        \"who\"       \"are\"       \"christian\" \"usual\"     \"think\"    \n [7] \"about\"     \"christ\"    \"in\"        \"term\"      \"of\"        \"oblig\"    \n[ ... and 6,212 more ]\n\n\nEverything is shortened compared to before: christian, usually, and oblig. Compared to the preceding steps, a bit more informational value is lost, but this is often a trade-off worth taking, seeing as it typically enhances the performance of the document classification quite substantially."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#remove-stopwords",
    "href": "tutorials/atap_docclass/atap_docclass.html#remove-stopwords",
    "title": "Classifying American Speeches",
    "section": "Remove Stopwords",
    "text": "Remove Stopwords\nThe final intrusion into the linguistic source material is the removal of stopwords. These are words which are very common, to a degree that one can assume that they will, on average, be equally distributed between the two classes. They are typically also words that have no meaning on their own, for example the, in, etc. As they rather regulate the relations between words, they are also called function words (as opposed to content words, which we want to keep). There is no universal list of stopwords - and some methods make do without removing stopwords - but for our purposes, it makes sense to just work with a list that is readily available, which is the one specified in the flag. The command is as follows:\n\ntoks &lt;- tokens_remove(toks, pattern = stopwords::stopwords(language = \"en\", source = \"snowball\"))\n\nAgain, the command is very explicit in its function, the input is the tokenized, punctuation-less, lowercased and stemmed version of the toks object, which we overwrite to create a version that is all of the above and additionally does not contain stopwords. We see in row one how that looks:\n\ntoks[1]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.1 :\n [1] \"christian\" \"usual\"     \"think\"     \"christ\"    \"term\"      \"oblig\"    \n [7] \"chariti\"   \"faith\"     \"god\"       \"forth\"     \"don\"       \"t\"        \n[ ... and 2,751 more ]\n\n\nThis looks quite a bit different: a lot of words are gone, such as we, who and are. For the most part, what remains are the stems of nouns and verbs, words which carry semantic meaning."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#imperfections",
    "href": "tutorials/atap_docclass/atap_docclass.html#imperfections",
    "title": "Classifying American Speeches",
    "section": "Imperfections",
    "text": "Imperfections\nClearly, this approach to pre-processing is not without flaws. We still see, for instance, in the last two positions of the output, that don’t was split into two tokens, don and t. Don would probably also appear as the appropriate word for various garments a lot in a Victorian fashion weekly, which is probably not what Alan Keyes was referring to in his presidential campaign speech, while not should probably be part of the stopword list. Or, to return to the purged stopwards mentioned above, who could have been accidentally removed: once the World Health Organization’s abbreviation is lowercased, it is indistinguishable from the pronoun. So you can see that there are some pitfalls to pre-processing, which are usually quite harmless compared to the benefits (and improved model). However, it can lead to problematic omissions in certain contexts. Ultimately, you need to be aware of the linguistic context you’re working in and make the trade-off on whether finer-grained pre-processing is worth the effort."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#document-feature-matrix",
    "href": "tutorials/atap_docclass/atap_docclass.html#document-feature-matrix",
    "title": "Classifying American Speeches",
    "section": "Document-Feature Matrix",
    "text": "Document-Feature Matrix\nWith the linguistic pre-processing done, we continue to the next step: creating a document-feature matrix – simply a table which says how often which word occurs in each document. This is the format we need to run the document classification. We’ll get into its structure in a second. But first, we turn the pre-processed corpus into a document-feature matrix. Once more, the command, dfm, is fairly self-explanatory:\n\ndtm &lt;- dfm(toks)\n\nThis time round, we create a new object with the name dtm, and the input to this operation is the latest version of the toks object. In case you are not familiar with this particular type of matrix, you might get a better sense for what we are dealing with is by looking at the object directly:\n\ndtm\n\nDocument-feature matrix of: 3,247 documents, 31,939 features (98.41% sparse) and 3 docvars.\n                       features\ndocs                    christian usual think christ term oblig chariti faith\n  SEL_perparty_v2.csv.1        17     3    28     12    4     1       1    22\n  SEL_perparty_v2.csv.2         1     1    14      3    0     1       0     4\n  SEL_perparty_v2.csv.3         0     0    12      0    0     0       0     7\n  SEL_perparty_v2.csv.4         1     0    15      0    0     0       0     0\n  SEL_perparty_v2.csv.5         6     1    16     11    1     0       0     3\n  SEL_perparty_v2.csv.6         3     0    26     10    4     3       0     7\n                       features\ndocs                    god forth\n  SEL_perparty_v2.csv.1  82     1\n  SEL_perparty_v2.csv.2  21     2\n  SEL_perparty_v2.csv.3   8     0\n  SEL_perparty_v2.csv.4   4     1\n  SEL_perparty_v2.csv.5  48     5\n  SEL_perparty_v2.csv.6   6     5\n[ reached max_ndoc ... 3,241 more documents, reached max_nfeat ... 31,929 more features ]\n\n\nAt first glance, you can see that our variable dtm is an object similar to the earlier fulltext, in that only the first six rows of the matrix are pasted in the output, and that there is a summary of the object. And now we can also see the structure of this document-feature matrix. Turns out, the name is quite the give-away: the document feature matrix allows us to see how often each feature appears in each document.\nIn this matrix, we can access both the columns and the rows, the way we usually do with data frames in R. To do this, we append a square bracket to the object. We can access rows thusly:\n\ndtm[3, ]\n\nDocument-feature matrix of: 1 document, 31,939 features (98.61% sparse) and 3 docvars.\n                       features\ndocs                    christian usual think christ term oblig chariti faith\n  SEL_perparty_v2.csv.3         0     0    12      0    0     0       0     7\n                       features\ndocs                    god forth\n  SEL_perparty_v2.csv.3   8     0\n[ reached max_nfeat ... 31,929 more features ]\n\n\nHere, we see that think features twelve times in the third speech in the corpus. Or, accounting for what the pre-processing did to the text, we can say that some version of think appears in this speech.\nAccessing the columns works analogously: we simply place the number after the comma:\n\ndtm[, 3]\n\nDocument-feature matrix of: 3,247 documents, 1 feature (19.00% sparse) and 3 docvars.\n                       features\ndocs                    think\n  SEL_perparty_v2.csv.1    28\n  SEL_perparty_v2.csv.2    14\n  SEL_perparty_v2.csv.3    12\n  SEL_perparty_v2.csv.4    15\n  SEL_perparty_v2.csv.5    16\n  SEL_perparty_v2.csv.6    26\n[ reached max_ndoc ... 3,241 more documents ]\n\n\nWe see here that think is a frequent feature, at least in the first six speeches in the corpus. By looking at the metadata, or, as we call them here, the document variables, we can see that the first six speeches were all given by the same speaker. To access the document variables and solve the riveting riddle of who this famous thinker is, we can use another one of these intuitively named commands:\n\nhead(docvars(dtm))\n\n  party       fileid       name\n1   rep    akeyes-95 Alan_Keyes\n2   rep    akeyes-98 Alan_Keyes\n3   rep akeyes1-2-00 Alan_Keyes\n4   rep akeyes1-6-96 Alan_Keyes\n5   rep akeyes1-6-98 Alan_Keyes\n6   rep  akeyes10-96 Alan_Keyes\n\n\nWith the docvars command, we get a list of the document variables for each document, that is each row, in the dtm object. However, we nest this inside of the command head() - which is useful for taking a first look at any variable, since it only displays the first six rows of a variable. We do this here because the docvars are not subject to the same behavior protocol that the corpus object is. Your computer is unlikely to overload if you were to accidentally view the whole docvars without taking this head precaution. So let’s try it:\n\ndocvars(dtm)[1:10, 1:3]\n\n   party         fileid       name\n1    rep      akeyes-95 Alan_Keyes\n2    rep      akeyes-98 Alan_Keyes\n3    rep   akeyes1-2-00 Alan_Keyes\n4    rep   akeyes1-6-96 Alan_Keyes\n5    rep   akeyes1-6-98 Alan_Keyes\n6    rep    akeyes10-96 Alan_Keyes\n7    rep  akeyes11-5-00 Alan_Keyes\n8    rep  akeyes11-9-95 Alan_Keyes\n9    rep akeyes12-10-96 Alan_Keyes\n10   rep  akeyes12-5-00 Alan_Keyes\n\n\nEven though it becomes clear enough which document variable stands in which column, it requires a lot of scrolling to get to the top where the precise name of each document variable is pasted at the top of each column. The very top of the list is also where we find the answer to the mystery of who uses think so gratuitously. If the name Alan Keyes does not ring a bell, you are in the fortunate position to only learn today of a person whose claims to fame include having run for president three times, being appointed ambassador to the UN’s Economic and Social Council by Ronald Reagan and filing a lawsuit against Barack Obama - requesting documentation proving that Barack Obama is a natural born citizen of the US. The things you learn in computational linguistics never cease to amaze, eh?"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#document-frequency-of-the-features",
    "href": "tutorials/atap_docclass/atap_docclass.html#document-frequency-of-the-features",
    "title": "Classifying American Speeches",
    "section": "Document Frequency of the Features",
    "text": "Document Frequency of the Features\nThe next step in our journey toward document classification requires us to count how often each feature appears in the corpus, for reasons which will become apparent shortly. Our trusty friend, the Quanteda package, anticipates this need and furnishes us with the appropriate command, named docfreq. We use it to create a new variable, doc_freq:\n\ndoc_freq &lt;- docfreq(dtm)\n\nThe docfreq command takes our pre-processed corpus as an argument, in the form of the dtm variable. The new variable, doc_freq is now a long list of all the features and their frequencies. We can confirm this by looking at the first twenty entries, i.e. words:\n\nhead(doc_freq, n = 20)\n\n  christian       usual       think      christ        term       oblig \n        143         278        2630          30         954         559 \n    chariti       faith         god       forth         don           t \n        158         827        1739         294        2341        2831 \n      alway       jesus       great      wisdom citizenship  understood \n       1825          32        2812         285         137         277 \n       said        fact \n       2572        1558 \n\n\nThe output shows us each feature and the number of occurrences. It also shows us here that the features are not ranked by frequency yet, but by order of appearance in the corpus. The first few features are the ones we encounter in the first speech in the corpus.\nTo see which features occur most and least frequently in the corpus, we can sort the variable doc_freq. To do this, we use the intuitively named sort() function. In order to save ourselves the trouble of scrolling, we nest the sort() function inside the head function, which then looks thusly:\n\nhead(sort(doc_freq, decreasing = T), n = 15)\n\n       s    thank    peopl      can       us      one     want     year \n    3165     3121     3091     3076     3041     3040     3030     3009 \n countri     time     make     know     work      now american \n    3009     2978     2977     2975     2955     2925     2908 \n\n\nThe outermost layer of the line is the head() command. Inside of this, we nest the sort() function, which takes as an argument the variable of interest, doc_freq and the logical argument decreasing = TRUE. After closing the brackets on the sort function, we add the specification that we want to see the first fifteen entries of the head.\nIn the output, we see many features that we would expect to appear in presidential campaign speeches. The contracted ’s is the most common feature, which makes sense considering that it can be used both in the sense of let’s build a better future together or as a possessive. Contraction are also generally used frequently in spoken language. We are clearly reminded that we are dealing with a corpus of political speeches, in because many words are associated with shaping something: can, want, make, work indicating the activities and countri pointing to the object to be shaped. There are also flavors of urgency coming through, with year, time and now referring to the temporality of the campaign. Finally, we are reminded that we are dealing with American politicians. So, we have a pretty good interpretive turn-out already here.1\nFor some insightful contrast, let’s look at the last fifteen features in this ranking. To do this, we use almost exactly the same command as before, but instead of nesting everything under head(), we do it under the tail() function, like so:\n\ntail(sort(doc_freq, decreasing = T), n = 15)\n\n           umphf           brasim t-worry-be-happi        5-percent \n               1                1                1                1 \nafrica-caribbean              koz            4x400         meteorit \n               1                1                1                1 \n     micro-organ        somebody-            nowak          pantoja \n               1                1                1                1 \n          aspira          rosello        hillcrest \n               1                1                1 \n\n\nIt would be possible to go and look at the contexts of these features, but in the interest of your sanity and ours, we graciously skip this step.\nInstead, we return the earlier promise and explain why we created the doc_freq variable. As we can still see in the output, the features in the tail are on the one hand extremely rare, and on the other hand not very useful. The latter reason would not be a criteria for exclusion, but the fact that they are extremely rare could muddle up the document classification model we are interested in, the model could learn corpus coincidences, which are often referred to as noise, thus blurring the signal of frequent words. So, we remove all features from the corpus which occur less frequently than five times. We do this using the following command:\n\ndtm &lt;- dtm[, doc_freq &gt;= 5]\n\nAs you can see, we are once again overwriting the dtm variable with a specific iteration of the variable. This time, we choose to retain all the features which occur more than five times in the corpus, with the result that the features which occur less frequently drop out. Let’s see how well this worked, using the same set of commands as above. First, we overwrite the doc_freq variable with the document frequencies of the latest version, then we check out the tail:\n\ndoc_freq &lt;- docfreq(dtm)\ntail(sort(doc_freq, decreasing = T), n = 15)\n\n     salina        soto   job-train  oldfashion    kennelli   underwood \n          5           5           5           5           5           5 \n    telecom        huey   gutenberg      hilari kaleidoscop  multi-raci \n          5           5           5           5           5           5 \n  sweepstak      bifida        1910 \n          5           5           5 \n\n\nWe now find a new set of random seeming words at the tail of the doc_freq variable, but at least they all occur at least five times, which means they won’t distort the model too badly. And just to make sure we didn’t accidentally mess up the frequent features, we can again take a look at the head:\n\nhead(sort(doc_freq, decreasing = T), n = 15)\n\n       s    thank    peopl      can       us      one     want     year \n    3165     3121     3091     3076     3041     3040     3030     3009 \n countri     time     make     know     work      now american \n    3009     2978     2977     2975     2955     2925     2908 \n\n\nOnly to find, lo and behold, that they are identical to what we had above. Awesome! But it makes sense to check if our data develops in the expected way. Such checks, also called sanity checking, detect early on if one is on the right track – instead of getting error messages in one’s last processing step. On to a final step of preparation."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#tfidf",
    "href": "tutorials/atap_docclass/atap_docclass.html#tfidf",
    "title": "Classifying American Speeches",
    "section": "TFIDF",
    "text": "TFIDF\nBefore we continue to the actual document classification (never fear: we are actually going to get there), we need to weight the features in the corpus object dtm by their tfidf. This beauty of an abbreviation stands for term frequency-inverse document frequency, and does exactly what it says: the number of occurrences of the word in the current document is divided by the number of documents in which the feature occurs. By this division, we account for the fact that some words are generally very frequent, and thus would receive an overproporational amount of significance despite being rather pedestrian and carrying little specific semantic content if we relied on frequency alone. An example we’ve already discussed above is the contracted ’s, which is the most frequent feature in the corpus, on account of just being a very common word in the English language.\nBefore we apply the tfidf weights, let’s load the first document in the dtm object:\n\ndtm[1, ]\n\nDocument-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.\n                       features\ndocs                    christian usual think christ term oblig chariti faith\n  SEL_perparty_v2.csv.1        17     3    28     12    4     1       1    22\n                       features\ndocs                    god forth\n  SEL_perparty_v2.csv.1  82     1\n[ reached max_nfeat ... 11,770 more features ]\n\n\nThis will save us a lot of scrolling time when we compare the pre- and post-tfidf weights.\nOnce again, quanteda actually furnishes us with a command to easily transform our dtm variable to adapt to the tfidf weights. The command is self-explanatorily labeled dfm_tfidf(), and we use it to overwrite the current version of the dtm variable:\n\ndtm &lt;- dfm_tfidf(dtm, force = TRUE)\n\nYou can see that the command takes as its argument first the object to be transformed. The second argument, the flag force=TRUE is usually not needed. As a protection to makes sure that one does not tfidf-weight twice, the dfm_tfidf function refuses to accept matrices that are already weighted. Let’s take a look at the first entry of the dtm object now, we can see that the weights of the features have changed substantially:\n\ndtm[1, ]\n\nDocument-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.\n                       features\ndocs                    christian    usual    think   christ     term     oblig\n  SEL_perparty_v2.csv.1  23.05449 3.202312 2.562743 24.41233 2.127736 0.7640705\n                       features\ndocs                     chariti    faith      god    forth\n  SEL_perparty_v2.csv.1 1.312825 13.06749 22.23698 1.043135\n[ reached max_nfeat ... 11,770 more features ]\n\n\nWhere before we had a simple count of occurrences, we now have the tfidf scores. Some of the salient changes can be seen in the features christian and think. In terms of raw counts, Alan Keyes’ speech contains christian 17 times. With the tfidf weights, christian actually becomes more important, with a weight of 23.06. This indicates that christian is used comparatively rarely in other speeches, making Keyes’ usage here more impactful. By contrast, although think occurs 28 times in Keyes’ speech, it only receives a tfidf score of 2.56, indicating that it also features heavily in many other speeches in the corpus. Turns out, Keyes is not a lone thinker among these politicians. What a shocker.\nLet’s stick with the feature think for a second, and look at its tfidf score in the first few speeches. Remember that, since dtm is an object containing data and a behavior, we can simply enter the third column of dtm and receive six manageable lines of output:\n\ndtm[, 3]\n\nDocument-feature matrix of: 3,247 documents, 1 feature (19.00% sparse) and 3 docvars.\n                       features\ndocs                       think\n  SEL_perparty_v2.csv.1 2.562743\n  SEL_perparty_v2.csv.2 1.281372\n  SEL_perparty_v2.csv.3 1.098318\n  SEL_perparty_v2.csv.4 1.372898\n  SEL_perparty_v2.csv.5 1.464425\n  SEL_perparty_v2.csv.6 2.379690\n[ reached max_ndoc ... 3,241 more documents ]\n\n\nWe see that think generally has a relatively low score, but the scores differ from speech to speech. The reason for this is that each speech contains a different number of think features, but quite a high number of speeches contain a relatively high number. Thus, the count in each of these first six speeches is divided by the total number of speeches containing think, yielding a different but consistently rather low tfidf weight in each row.\nAs an aside, tfidf can be used directly to see which words are most characteristic of a document, i.e. its keywords. We could see the keywords of a document, for example document 1, by considering the words with the highest tfidf values. The simplest way to do so is to sort the tfidf values in decreasing order, from most to least frequent, and the inspect the top of the list.\n\nsdtm_tfidf &lt;- dfm_sort(dtm[1, ], decreasing = T, margin = \"features\")\nhead(sdtm_tfidf)\n\nDocument-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.\n                       features\ndocs                      caesar   christ christian      god    roman    fetus\n  SEL_perparty_v2.csv.1 35.48031 24.41233  23.05449 22.23698 18.86858 18.66469\n                       features\ndocs                    citizenship     coin     imag    faith\n  SEL_perparty_v2.csv.1     17.8719 15.84003 15.52798 13.06749\n[ reached max_nfeat ... 11,770 more features ]"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#the-training-data",
    "href": "tutorials/atap_docclass/atap_docclass.html#the-training-data",
    "title": "Classifying American Speeches",
    "section": "The Training Data",
    "text": "The Training Data\nIn broad brushstrokes, it works the way it usually does: if you want to generate an output, you need an input. This is what we spent the last ten pages setting up. From the removal of punctuation, via lowercasing and the removal of the features with less than five occurrences to the weighting using the tfidf score, each step is a little boost that helps our model achieve better results. The final (promise) step before creating the actual model is choosing the input.\nThe situation, of course, is as follows: we have a corpus with 3,247 speeches, all of which have already been classified as being either Republican or Democrat. If we were to simply throw the full corpus into the model, we would run into a problem: we couldn’t test how good the model is generally, because every scrap of data available to us right now would be informing the model. Our results would not be robust. Instead, what we want to do is sample a large chunk of the speeches available to us, use that to train the model, and then test how good our model is on the remaining speeches.\nOnce again, quanteda holds ready for us a command with the descriptive name dfm_sample(). We use it as follows:\n\ntrain_dtm &lt;- dfm_sample(dtm, size = 2800)\n\nOr, to put it into words: we create a new object called train_dtm, since we are going to train the model with this. The train_dtm object contains a sample of 2,800 speeches from the pre-processed and weighted dtm corpus. For this tutorial, we have just taken a random sample with a size corresponding to roughly 85% of the corpus. There are no hard and fast rules as to which percentage of the data should be used for training and which for testing, but if you’re somewhere in the 90% ballpark for training data you should be doing fine.\nLet’s take a superficial look at our training data:\n\ntrain_dtm\n\nDocument-feature matrix of: 2,800 documents, 11,780 features (95.78% sparse) and 3 docvars.\n                          features\ndocs                       christian    usual      think christ      term oblig\n  SEL_perparty_v2.csv.1936  0        0        0.64068578      0 0             0\n  SEL_perparty_v2.csv.2384  0        0        0.27457962      0 0             0\n  SEL_perparty_v2.csv.3232  1.356146 0        2.28816350      0 0             0\n  SEL_perparty_v2.csv.1058  0        0        0.18305308      0 0.5319339     0\n  SEL_perparty_v2.csv.971   0        1.067437 0.09152654      0 0             0\n  SEL_perparty_v2.csv.3120  0        0        0.54915924      0 1.5958017     0\n                          features\ndocs                       chariti     faith       god forth\n  SEL_perparty_v2.csv.1936       0 1.1879536 0             0\n  SEL_perparty_v2.csv.2384       0 0         0.2711827     0\n  SEL_perparty_v2.csv.3232       0 6.5337446 0.5423654     0\n  SEL_perparty_v2.csv.1058       0 0         0.2711827     0\n  SEL_perparty_v2.csv.971        0 0.5939768 0.5423654     0\n  SEL_perparty_v2.csv.3120       0 0         0.8135481     0\n[ reached max_ndoc ... 2,794 more documents, reached max_nfeat ... 11,770 more features ]\n\n\nWe see in the first row of the output that the object is a document-feature matrix consisting of 2,800 documents, 11,785 features and 3 document variables. We also see in the first column that the speeches included do not form an interpretable sequence, indicating that they are random. All is as it should be."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#the-testing-data",
    "href": "tutorials/atap_docclass/atap_docclass.html#the-testing-data",
    "title": "Classifying American Speeches",
    "section": "The Testing Data",
    "text": "The Testing Data\nNow, let’s create the necessary counterpart, the testing data. We create a new variable, which we’ll call test_dtm, and which contains those speeches which are not part of the training data. The way to do this is by taking the dtm object and choosing those documents which constitute the difference between the full dtm corpus and the train_dtm object. The appropriate function for this is setdiff(), which takes as arguments the documents which constitute the full dtm corpus and removes the documents named in the train_dtm variable. The function setdiff is actually used here as an index of the dtm. Indices cannot only contain numbers, but also variables and entire functions – Python programmers refer to this as list comprehension. Observe that dtm is a two-dimensional object. We constrain the first dimension (the rows, i.e. the documents) to those that are not part of the training set, but the second dimension (the column, i.e. the word features) is just kept. This explains the last letters of the following code, the comma followed by the square bracket.\n\ntest_dtm &lt;- dtm[setdiff(docnames(dtm), docnames(train_dtm)), ]\ntest_dtm\n\nDocument-feature matrix of: 447 documents, 11,780 features (95.77% sparse) and 3 docvars.\n                        features\ndocs                     christian    usual     think christ     term     oblig\n  SEL_perparty_v2.csv.7   1.356146 0        2.7457962      0 3.191603 0        \n  SEL_perparty_v2.csv.10  0        0        2.4712166      0 0        0.7640705\n  SEL_perparty_v2.csv.20  0        0        1.0983185      0 2.659670 3.8203524\n  SEL_perparty_v2.csv.29  0        0        1.4644246      0 3.723537 0        \n  SEL_perparty_v2.csv.34  0        1.067437 0.8237389      0 1.063868 0        \n  SEL_perparty_v2.csv.46  0        0        3.9356412      0 1.595802 3.0562819\n                        features\ndocs                      chariti     faith      god    forth\n  SEL_perparty_v2.csv.7  0        0         1.898279 3.129405\n  SEL_perparty_v2.csv.10 1.312825 0         1.627096 5.215675\n  SEL_perparty_v2.csv.20 0        0.5939768 2.169462 3.129405\n  SEL_perparty_v2.csv.29 0        0.5939768 1.084731 4.172540\n  SEL_perparty_v2.csv.34 0        0         3.525375 1.043135\n  SEL_perparty_v2.csv.46 0        0.5939768 3.254192 4.172540\n[ reached max_ndoc ... 441 more documents, reached max_nfeat ... 11,770 more features ]\n\n\nLooking at the new test_dtm object, we see that it contains the 447 speeches not included in the training data, but with the same number of features and document variables. We can also see that, this time round, the object is not a random sample: the documents included are ordered numerically. In other words: all is as it should be. And with that, onwards."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#naive-bayes",
    "href": "tutorials/atap_docclass/atap_docclass.html#naive-bayes",
    "title": "Classifying American Speeches",
    "section": "Naive Bayes",
    "text": "Naive Bayes\nThe first document classification model we are going to train is of a type called Naive Bayes. We will not get into the statistical theory behind Naive Bayes too much, only pointing out the distinctive feature of this model. The name derives from the fact that the model applies Bayes’ theorem with a strong, i.e. naive, independence assumption. Basically, the model assumes that the class of any feature is independent of the class of every other feature in the set. While this is pretty simplistic, or even naive, it works well enough in practice. Let’s have a look.\nTraining the model is simple enough, requiring just a single command, aptly titled textmodel_nb():\n\nnb_model &lt;- textmodel_nb(train_dtm, y = docvars(train_dtm, \"party\"))\n\nYou see that we once again created a new variable, this time one called nb_model. We do so with the aforementioned textmodel_nb command, which takes as first argument, or input, the train_dtm object which we sampled earlier. The second argument might be a bit more confusing.\nOne way to think about it is this: we have a two dimensional object. On the x-axis, we have the training data. On the y-axis we have the variable we want to predict, in this case the party of the speaker. With Naive Bayes, training the model is basically a question of one simple calculation: given that we see a feature on the x-axis, what is the probability that the corresponding position on the y-axis is Republican. Each feature is thus assigned a probability between 0 and 1, meaning it is associated either more with the Democratic or more with the Republican party. The dividing line for the result suggested is 0.5. Simple as that, one might think. However, looking at the output of the model, you may feel a bit underwhelmed:\n\nnb_model\n\n\nCall:\ntextmodel_nb.dfm(x = train_dtm, y = docvars(train_dtm, \"party\"))\n\n Distribution: multinomial ; priors: 0.5 0.5 ; smoothing value: 1 ; 2800 training documents;  fitted features. \n\n\nWe get some information regarding the model’s distribution, priors, smoothing value, training documents and fitted features, but nothing about how well it performs."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#prediction",
    "href": "tutorials/atap_docclass/atap_docclass.html#prediction",
    "title": "Classifying American Speeches",
    "section": "Prediction",
    "text": "Prediction\nIn order to see how good our model is, we draw on the data we set aside earlier, the data for testing. In this step, we use the nb_model we trained a minute ago to predict the party to which the politicians delivering the speeches in the test data belong. The command for this is the predict function, which works as follows:\n\npred_nb &lt;- predict(nb_model, newdata = test_dtm)\n\nThe first argument is the model, on the basis of which the prediction is to be made. In this instance, we are obviously using our Naive Bayes model, stored in the nb_model object. The second argument is where we give the new data, which we set aside earlier in the test_dtm variable. With that we define a new variable, pred_nb which contains the predictions our model generated. Let’s have a look at the first twenty positions:\n\nhead(pred_nb, n = 20)\n\n  SEL_perparty_v2.csv.7  SEL_perparty_v2.csv.10  SEL_perparty_v2.csv.20 \n                    rep                     rep                     rep \n SEL_perparty_v2.csv.29  SEL_perparty_v2.csv.34  SEL_perparty_v2.csv.46 \n                    rep                     rep                     rep \n SEL_perparty_v2.csv.52  SEL_perparty_v2.csv.64  SEL_perparty_v2.csv.88 \n                    dem                     dem                     dem \n SEL_perparty_v2.csv.90  SEL_perparty_v2.csv.95 SEL_perparty_v2.csv.110 \n                    dem                     dem                     rep \nSEL_perparty_v2.csv.113 SEL_perparty_v2.csv.124 SEL_perparty_v2.csv.131 \n                    dem                     dem                     dem \nSEL_perparty_v2.csv.136 SEL_perparty_v2.csv.153 SEL_perparty_v2.csv.158 \n                    dem                     dem                     dem \nSEL_perparty_v2.csv.159 SEL_perparty_v2.csv.163 \n                    dem                     dem \nLevels: dem rep\n\n\nWhat we get in the output here is exactly what we asked for: the document title and the corresponding prediction. This is great, insofar that we can essentially check the box on classify documents. But, and this is where we’re really getting into the thick of it: how can we figure out how well our model performs?"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#confusion-matrix",
    "href": "tutorials/atap_docclass/atap_docclass.html#confusion-matrix",
    "title": "Classifying American Speeches",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe first step for this is constructing the so-called confusion matrix. The confusion matrix is a two-dimensional table which shows the actual party associated with each speech on one axis, and the predictions on the other. We can construct this using the table() function:\n\nconfmat_nb &lt;- table(prediction = pred_nb, party = docvars(test_dtm, \"party\"))\n\nCreating a new object called confmat_nb, our confusion matrix for the Naive Bayes model, we assign to it a table. The table() function takes as its arguments first the content of the rows, here the values we predicted for the pred_nb variable. Then it takes the content of the columns, here the document variable party from the test data. As you will see as soon as you look at the confmat_nb object, the labels prediction and party to which we assign the data are exactly that: the labels for the vertical and the horizontal axis. Let’s take a look:\n\nconfmat_nb\n\n          party\nprediction dem rep\n       dem 245  20\n       rep  14 168\n\n\nIf everything worked out alright, you should see two pretty high numbers in the top left and the bottom right. This is the diagonal on which the actual label and the prediction correspond. As I’m running this, I have 232 speeches by Democrats that were correctly predicted to be by Democrats, and I have 187 speeches by Republicans that were correctly predicted to be by Republicans (due to the fact that we used a random sample, your numbers may be a bit different).\nOn the other diagonal, from bottom left to top right, we have the wrong predictions. In my case here, that is 10 Democratic speeches that were classified as Republican and 18 Republican speeches that were classified as Democratic. With this representation of correct and incorrect predictions, we can already see that the model isn’t too bad. But to get an even better sense for how well it does, let’s put a number on it."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#accuracy",
    "href": "tutorials/atap_docclass/atap_docclass.html#accuracy",
    "title": "Classifying American Speeches",
    "section": "Accuracy",
    "text": "Accuracy\nGetting from the confusion matrix to a numeric representation of the accuracy is pretty straightforward. First, we take the sum of all correct predictions, then we divide it through the total sum of predictions. One line of code to save this percentage value, then we look at it, easy-peasy:\n\naccuracy_nb &lt;- (confmat_nb[1, 1] + confmat_nb[2, 2]) / sum(confmat_nb)\naccuracy_nb\n\n[1] 0.9239374\n\n\nIn my case, I get an accuracy of 94%, which is not at all shabby. You will have received a slightly different number, which is due to the fact that you have, in all conceivable likelihood, sampled a different set of speeches to train your model. More on which in a minute. But first, let’s dive a bit deeper into the Naive Bayes model and take a look at the features which are representative for each party."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#feature-table",
    "href": "tutorials/atap_docclass/atap_docclass.html#feature-table",
    "title": "Classifying American Speeches",
    "section": "Feature Table",
    "text": "Feature Table\nLet’s start with an obvious one: God. At least, it’s an obvious feature in the American political context. One party is known for heavily pandering to their god-fearing subjects, the other is known for mildly pandering to their more or less god-fearing subjects. You get the idea. Let’s see, then, how God features in our model as a predictor for the two parties. We can see that by accessing the parameters of the nb_model and specifying the parameter god:\n\nnb_model$param[, \"god\"]\n\n         dem          rep \n0.0003779813 0.0007297060 \n\n\nAgain, there will be slight differences in the output on account of us using different samples, but your output should also indicate that the feature god has roughly twice the weight for Republicans than it does for Democrats. With this example, we are obviously being facetious, but we think it’s useful to see\n\nwhat the quote-unquote insides of the model look like, and\nhow the same feature is differently weighted for the two parties.\n\nBut let’s shift our perspective somewhat: instead of assuming that we know how a feature will be differently weighted for the different parties, let’s look at the features which have the strongest weights, to find out what our data says is most typical for the Republican or the Democratic party. To do so, we are again going to use the sort() function, this time on the model parameters for the Republican party, and nest it inside the head() function to show the top thirty Republican features:\n\nhead(sort(nb_model$param[\"rep\", ], decreasing = T), n = 30)\n\n   terrorist          tax         bush         iraq        kerri       terror \n0.0027263357 0.0025833085 0.0024335081 0.0022634099 0.0021382798 0.0018931958 \n       enemi        iraqi        senat      freedom         vote  afghanistan \n0.0014407140 0.0014318273 0.0014307967 0.0014120269 0.0013972301 0.0013688277 \n         war       attack     militari           re         must         peac \n0.0013649638 0.0013375806 0.0012460776 0.0011354051 0.0010909518 0.0010831288 \n      saddam       weapon        secur       govern        women        georg \n0.0010679771 0.0010587423 0.0010486988 0.0010331179 0.0010324453 0.0010277067 \n          11        oppon         forc           ve            9          men \n0.0010202837 0.0009950418 0.0009903109 0.0009785223 0.0009727465 0.0009671968 \n\n\nThis is where it becomes interesting on a content level: we can see that many of the strongest Republican features directly reflect aspects of Republican policy priorities. There are strong flavors of national security in the top thirty features, among them terrorist, enemi, war, attack, militari, and several more. There are features pointing to fiscal policy, with tax and money; former Republican presidents, with georg and bush; and we can be pretty certain that the feature 11 refers to 9/11. Clearly, there is more to unpack here, but we’ll leave that to you, and carry on by looking at the top features in Democratic speeches:\n\nhead(sort(nb_model$param[\"dem\", ], decreasing = T), n = 30)\n\n      health        insur          kid       school       colleg        crime \n0.0017384387 0.0014966996 0.0014830241 0.0013742897 0.0012697023 0.0012523090 \n        educ         care           re          got        thing        parti \n0.0011907901 0.0011545364 0.0011449606 0.0011446589 0.0011396034 0.0010852208 \n   communiti         bill      compani      economi     children          tax \n0.0010817212 0.0010578212 0.0010378343 0.0010255794 0.0010237001 0.0010198765 \n      invest          lot      student    everybodi           mr      tonight \n0.0010085849 0.0010046681 0.0009972725 0.0009866790 0.0009757271 0.0009701784 \n       money        chang      problem      deficit         rate      centuri \n0.0009654147 0.0009642819 0.0009617746 0.0009580582 0.0009533844 0.0009505423 \n\n\nThe output again is not surprising, in the sense that the policy focus of the Democratic party also becomes visible here. There are features pointing to Medicare, with health, insur, care, likely also bill (although it will probably also be a reference to Bill Clinton, which would look exactly the same as a result of our pre-processing); education is a big theme as well, with school, colleg, educ and student featuring prominently; and there is also a sense of inclusion, with features like communiti, everybodi and tonight.\nWe should state here clearly that these feature tables are great to get a high-level view, firstly on whether the model managed to roughly identify the key features, and secondly on what the content of a corpus is, like the party programs here, but obviously no conclusions should be drawn merely on the basis of these keywords. Just in case anyone needed reminding.\nThat being said, there are many ways to mine these feature tables for insight, and we’ll discuss some of them right now."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#weighted-feature-tables",
    "href": "tutorials/atap_docclass/atap_docclass.html#weighted-feature-tables",
    "title": "Classifying American Speeches",
    "section": "Weighted Feature Tables",
    "text": "Weighted Feature Tables\nThe first weighting we do is very simple: we look at features that are relatively more important for Republicans than for Democrats. They way we do this is by dividing the the weight each feature has for Republicans by the weight the same feature has for the Democrats, like this:\n\nrel_weight_rep &lt;- (nb_model$param[\"rep\", ] / nb_model$param[\"dem\", ])\n\nBefore looking at the results, a quick sanity check. We saw above that all of the weights are rather small decimal numbers. What happens when you divide a fraction by an even smaller fraction? You get a higher number. So, if a feature has a high weight for Republicans, but a small weight for Democrats, the result of the operation above yields a relatively high number, one that is probably bigger than 1. We save the results of the operation in a new variable, rel_weight_rep, a step you’re very familiar with by now. Let’s see how this plays out, by looking at the top 30 entries:\n\nhead(sort(rel_weight_rep, decreasing = T), n = 30)\n\n   baldrig       womb        sdi sandinista     riyadh     allawi    nuisanc \n 240.90668  201.61104  198.51968  178.78209  167.58407  141.95236  135.99654 \n      bork        lsu     embryo      frist     frivol    zarqawi  decontrol \n 131.51267  128.52548  125.41913  123.88258  117.32835  114.19406  112.63911 \n murkowski    ghadafi     malais     casper        inf  bioshield nicaraguan \n 112.61713  112.04156  105.07997  103.54245  102.94571  100.40961   97.19428 \n    khobar     gipper      saxbi      cruis   deadlier    midland       goos \n  97.11587   96.92068   93.52018   92.36902   91.18261   89.39415   88.85977 \n   moammar up-or-down \n  88.21562   87.86447 \n\n\nThis looks pretty different than the top 30 unweighted Republican features, right? Most obviously, there are a lot of rare words in there - sandinista or bork, anyone? - which indicates that there is a sparse data problem. This is to say that, because some features only occur a handful of times (in this case, thanks to our pre-processing, a minimum of five times), they can end up overly prominent. Our results will differ slightly from yours again, but we’ll just take this as read in the following discussion.\nNot everything we get in our output is readily interpretable, but there are some features that are insightful and point to certain hot topics. For instance, we have womb and embryo which are plausibly connected with the Republican’s stance and debates on abortion; we also find moammar and ghadafi, who was the dictator of Libya at the time these speeches were made; another feature of interest during these plague times is bioshield, which refers to a US government project aimed at setting up an early warning system for anthrax and other pathogens.\nLet’s use the same procedure to generate the equivalent list for the Democrats:\n\nrel_weight_dem &lt;- (nb_model$param[\"dem\", ] / nb_model$param[\"rep\", ])\nhead(sort(rel_weight_dem, decreasing = T), n = 30)\n\n        obes       tipper     preexist     kathleen          wto          arn \n   211.09946    171.62596    158.24533    109.89303    106.98350    106.20290 \nsuperhighway    sotomayor       slater      downsiz      aristid       calori \n   102.03083     96.79835     95.53900     91.53139     88.13759     87.05692 \n      turbin        felon   work-studi         jare      tobacco         1835 \n    86.02117     85.42189     81.75980     80.96393     77.97834     77.91716 \n     shalala           bp          dnc      bailout           gm        romer \n    77.58837     77.51820     75.63924     74.73056     74.64072     73.02344 \n  youngstown    underserv     refinanc        sonia          g20         2020 \n    72.50484     71.95238     70.09749     69.85221     67.26647     66.95123 \n\n\nAs above, we get a bit of a mixed bag of features here, with some features being somewhat unplaceable, but others tying in to relevant policy topics. We get, for instance, obes, calori and nutriti, which all relate to diet - which ties in with public health policy; we also see the features refinanc and bailout, which are reasonably important in the context of the global financial crisis of the late 2000s.\nOkay, so while there are some insights to be gleaned here that point us to single events or referents, we are also left with a bunch of features from which we cannot readily make heads nor tails. But we can get more insightful feature tables: In order to detect broader trends, we can add biases that favor features that occur with a high frequency.\nFor this next step, take almost exactly the same measure as above, except this time round, we take the Republican weights for the features to the power of two, like this:\n\nrel_weight_rep2 &lt;- (nb_model$param[\"rep\", ]^2 / nb_model$param[\"dem\", ])\n\nSo, what are we doing here? Essentially, we are entering the exponential realm, where the changes to numbers are in reference to their original value. What this means is that the effect of an operation is bigger on the weights which had higher value starting out, than to those which had lower values.\nPerhaps this becomes clearer with a numeric example. If we take the second power of 0.1, the result is further away from the original number than if we take the second power of 0.01. We can see this by subtracting the second power:\n\n0.1 - (0.1^2)\n\n[1] 0.09\n\n0.01 - (0.01^2)\n\n[1] 0.0099\n\n\nWhy does this matter? Because the change to the rare words we saw above, which came out on top of the list because they had higher weights, is bigger than the change to the more frequent words, which did not make the top pick because they had lower weights. Since we are keeping the Democrat weights the way they were, we are basically evening the playing field in our variable rel_weight_rep2. Let’s see how that affects the top 30 feature table:\n\nhead(sort(rel_weight_rep2, decreasing = T), n = 30)\n\n       baldrig        nuisanc         frivol           womb            sdi \n    0.06209630     0.05779213     0.04905146     0.04349072     0.04216724 \n    sandinista          kerri         riyadh      terrorist         liabil \n    0.03419921     0.03365541     0.03004924     0.02990812     0.02811661 \n        casper         allawi     up-or-down        lawsuit           bush \n    0.02769779     0.02156022     0.02145746     0.02037906     0.01857149 \n          bork    bush-cheney            lsu          regim         embryo \n    0.01850560     0.01769796     0.01767447     0.01709440     0.01683045 \n         frist          iraqi two-and-a-half         terror        zarqawi \n    0.01642058     0.01602498     0.01496621     0.01464207     0.01395260 \n         index      decontrol      murkowski        ghadafi          enemi \n    0.01374865     0.01357521     0.01356991     0.01343156     0.01316258 \n\n\nAfter all the conceptual trouble we went through for these results, you may be a bit underwhelmed to find a lot of the same words cropping up. In our example, we find some shifts that indicate broader trends, and you’ll likely see one or two more mainstream words in your top 30 feature table us well, if you look closely.\nIn our case, the relevant new features are regim, iraqi and libya. As mentioned earlier, Libya and former dictator Muammar Gahadafi were big topics at the time, and we think it’s telling that libya and regim only shows up in the feature table after this weighting step, since it’s quite likely that the Democratic candidates would also have discussed the Libyan situation frequently during this period. The same, of course, goes for the iraqi feature.\nOkay, now let’s see how things stand for the Democrats:\n\nrel_weight_dem2 &lt;- (nb_model$param[\"dem\", ]^2 / nb_model$param[\"dem\", ])\nhead(sort(rel_weight_dem2, decreasing = T), n = 30)\n\n      health        insur          kid       school       colleg        crime \n0.0017384387 0.0014966996 0.0014830241 0.0013742897 0.0012697023 0.0012523090 \n        educ         care           re          got        thing        parti \n0.0011907901 0.0011545364 0.0011449606 0.0011446589 0.0011396034 0.0010852208 \n   communiti         bill      compani      economi     children          tax \n0.0010817212 0.0010578212 0.0010378343 0.0010255794 0.0010237001 0.0010198765 \n      invest          lot      student    everybodi           mr      tonight \n0.0010085849 0.0010046681 0.0009972725 0.0009866790 0.0009757271 0.0009701784 \n       money        chang      problem      deficit         rate      centuri \n0.0009654147 0.0009642819 0.0009617746 0.0009580582 0.0009533844 0.0009505423 \n\n\nIn our results, we see a rather pronounced change in the output once we apply this weighting to the Democrat features. The features showing up in the top thirty here are fairly similar to the unweighted features, and point to broadly recognizable policy issues: health, insur, care and bill are likely in reference to Medicare for All; kid, school, educ, student and college point to the education platform which a lot of Democratic candidates highlight.\nLet’s look at two more ways of biasing the feature tables in favor of frequent words. The next one is a lot more intuitive: we take the weights, and multiply them by the document frequencies for each feature. The more frequent a feature, the higher the result, and words mentioned particularly frequently, for instance because they were hot topics for longer periods of time, are boosted. Easy peasy:\n\nrel_weight_rep3 &lt;- (nb_model$param[\"rep\", ] * doc_freq)\nhead(sort(rel_weight_rep3, decreasing = T), n = 30)\n\n      tax        re     senat        ve   freedom    govern    presid       war \n 3.763880  2.988386  2.748560  2.583299  2.451279  2.274926  2.244644  2.211241 \n  america    nation      vote      must     secur         t       got terrorist \n 2.189457  2.070588  2.053928  2.049898  2.029232  1.997125  1.971281  1.924793 \n     bush     world      unit       job         m        ll    terror     women \n 1.917604  1.797263  1.774218  1.762006  1.724214  1.701073  1.694410  1.623004 \n     need     state      busi      forc      iraq       get \n 1.590850  1.574099  1.572305  1.562711  1.557226  1.550382 \n\n\nWe are really entering mainstream topics now, and the features start to tie in with the Republican policy themes on a major level: we get tax, freedom, secur, job and busi, pointing strongly to Republican domestic policy focal points; we get foreign policy for a turbulent world with war, terror, forc, iraq; and we get aspects of US gouvernance, with state, presid, nation, vote and bush.\nLet’s also take a look at the top Democratic features under this weighting scheme:\n\nrel_weight_dem3 &lt;- (nb_model$param[\"dem\", ] * doc_freq)\nhead(sort(rel_weight_dem3, decreasing = T), n = 30)\n\n       re     thing         t    school    health       got        ve     think \n 3.013536  2.697441  2.659516  2.417376  2.352108  2.344261  2.315918  2.185339 \n       go      care       get       job       don  children      educ     world \n 2.174433  2.146283  2.132836  2.075324  2.068395  2.032045  1.996955  1.970602 \n      say   economi       lot communiti    becaus      just    famili      give \n 1.901712  1.894245  1.890785  1.860560  1.766426  1.760574  1.756249  1.753537 \n    chang       let       new    believ      need     right \n 1.723172  1.718571  1.711858  1.691188  1.688258  1.687635 \n\n\nHere, we also get a rather distinct set of new words in the table: we still have aspects of care and health, as well as educ and school, but on top of that we get some value-based features like right and just. This weighting also shows a bit of how Democrats argue: there is a flavor of causality with becaus and need, and there are calls for the new and chang.\nWhat cropped up in the top features for both parties are contractions: re, ve, don, t, and m. These are words that are generally frequent, and our frequency weighting now has the side effect that entries for frequent words overlap.\nOne could imagine a research question here: how well can document classification distinguish between American and British politicians on the basis of speeches?\nLet’s look at a final weighting scheme. We use almost the same approach as above, but this time round we multiply the feature weights with the logarithms of the document frequency. This is different compared to the other approaches in that it still somewhat favors frequent words, but the gap between the more and less frequent words becomes relatively small. The danger that generally frequent words, e.g. contractions, move up to the top gets smaller. In case you need convincing of this, just take a quick look at the first few document frequencies and their logarithms:\n\nhead(doc_freq)\n\nchristian     usual     think    christ      term     oblig \n      143       278      2630        30       954       559 \n\nlog(head(doc_freq))\n\nchristian     usual     think    christ      term     oblig \n 4.962845  5.627621  7.874739  3.401197  6.860664  6.326149 \n\n\nSo far, so good. Now let’s take a look at how this impacts the top thirty features:\n\nrel_weight_rep4 &lt;- (nb_model$param[\"rep\", ] * log(doc_freq))\nhead(sort(rel_weight_rep4, decreasing = T), n = 30)\n\n        tax   terrorist        bush        iraq      terror       kerri \n0.018817167 0.017883713 0.016230278 0.014788642 0.012867718 0.011382085 \n      senat     freedom        vote         war       enemi      attack \n0.010817683 0.010532787 0.010190024 0.010087330 0.009400702 0.009076270 \nafghanistan          re    militari       iraqi        must      govern \n0.008998235 0.008941882 0.008792542 0.008592989 0.008224134 0.007952034 \n      secur        peac          ve       women         got        forc \n0.007936409 0.007869396 0.007709321 0.007598905 0.007338997 0.007292564 \n     weapon         men       spend       feder       georg       money \n0.007232136 0.007110637 0.006918580 0.006885096 0.006826528 0.006569237 \n\n\nThe changes compared to the earlier weightings are relatively slight here, but there are some different features which offer a new policy flavor. While tax appears in most of the Republican top features, regardless of the approach to weighting, here we get the features money and spend, which chimes well with Republican fiscal policy.\nFinally, let’s see what this weighting yields for the Democrats:\n\nrel_weight_dem4 &lt;- (nb_model$param[\"dem\", ] * log(doc_freq))\nhead(sort(rel_weight_dem4, decreasing = T), n = 30)\n\n     health      school         kid       insur          re       thing \n0.012534281 0.010269381 0.010112629 0.009620942 0.009017136 0.008854010 \n       educ         got      colleg        care       crime   communiti \n0.008841332 0.008727588 0.008711000 0.008691112 0.008242729 0.008058909 \n      parti    children        bill     economi         lot           t \n0.007910096 0.007773338 0.007769400 0.007713709 0.007575288 0.007466922 \n        tax       chang      invest     problem       money   everybodi \n0.007428918 0.007220826 0.007173386 0.007100538 0.007069259 0.007064690 \n         ve     compani        rate          mr     centuri         don \n0.006911379 0.006909912 0.006887967 0.006873053 0.006856292 0.006854891 \n\n\nAgain, we see many familiar features, but we get a few features which bring in something new. We see problem, which is a good springboard for any policy platform; we also get percent, which may point to a mode of argumentation; and everybodi, augmenting the message of inclusion which has been part of previous feature tables in the form of communiti.\nHaving looked at all these different weightings and brought our perspective as politically interested researchers to them, we ought to issue a caveat: Obviously, we would have to go and look at the speeches in detail to draw meaningful conclusions about what the policy programs and interests of the Republican and Democratic parties are. What we wanted to demonstrate here is that the feature tables and the different weightings offer a strong flavor as to the most important topics, but this should not be confused with actual political demands.\nAfter this extensive trawl through the feature tables, two topics remain for us to cover together here. The first of these are alternatives to the Naive Bayes model. Second, and ultimately, we are going to point out some steps which could be summarized aptly under Best Practices."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#logistic-regression-model",
    "href": "tutorials/atap_docclass/atap_docclass.html#logistic-regression-model",
    "title": "Classifying American Speeches",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\nAll classification algorithms have their pros and cons, usually resulting in a trade-off the researcher has to make between the amount of computational capacity required, the accuracy of the model, the interpretability etc. With the Naive Bayes model, we came down on the side of the less computationally intensive and less accurate end. With the logistic regression model, we ramp up both the computational requirements as well as the accuracy, while still remaining easily interpretable. You’ll see what we mean in a second.\nA lot of what we are doing here should look very familiar. First, we create a new variable, called lr_model. We assign to it a text model based on the logistic regression algorithm, which we create using one of these aptly named quanteda functions: textmodel_lr. The arguments are identical to the arguments in the Naive Bayes approach. First, we throw in the training data, then we specify the document variable we want to train on. We run this command, and take a peek at the lr_model:\n\nlr_model &lt;- textmodel_lr(train_dtm, y = docvars(train_dtm, \"party\"))\nlr_model\n\n\nCall:\ntextmodel_lr.dfm(x = train_dtm, y = docvars(train_dtm, \"party\"))\n\n2,800 training documents; 11,780 fitted features.\nMethod: binomial logistic regression\n\n\nAs with the nb_model, just looking at the lr_model does not yield terrific insight, beyond the fact that the model looks the way it ought. We can tell this from the description: we input 2,800 training documents and have 11,785 features, as always in our data. Most importantly for the current purpose, we receive confirmation that the method is that of the binomial logistic regression. Perfect!\nTo see how well the logistic regression model does, we take exactly the same steps as we did for the Naive Bayes model: first, we use the model to predict the party of the speeches in the test data. Then, we create a confusion matrix. We also already draw up the confusion matrix of the nb_model, so we can easily compare the two.\n\npred_lr &lt;- predict(lr_model, newdata = test_dtm)\nconfmat_lr &lt;- table(prediction = pred_lr, PARTY = docvars(test_dtm, \"party\"))\nconfmat_lr\n\n          PARTY\nprediction dem rep\n       dem 250   8\n       rep   9 180\n\nconfmat_nb\n\n          party\nprediction dem rep\n       dem 245  20\n       rep  14 168\n\n\nAs above, we will not get exactly the same numbers here, but unless something went terribly wrong, you should see that the logistic regression model is somewhat more successful at correctly predicting the party.\nLet’s go a step further and put the proverbial and actual number on it. We discussed earlier how we can calculate the accuracy from the confusion matrix. So, we sum again the number of correct predictions and divide them by the total number of predictions. And for easier comparison, we’ll draw up the accuracy of the Naive Bayes model as well:\n\naccuracy_lr &lt;- (confmat_lr[1, 1] + confmat_lr[2, 2]) / sum(confmat_lr)\naccuracy_lr\n\n[1] 0.9619687\n\naccuracy_nb\n\n[1] 0.9239374\n\n\nWhat we see in the confusion matrix is, of course, mirrored in the accuracy of the models. In my case, the logistic regression model is approximately 2.5 percentage points more accurate than the Naive Bayes model. Does this roughly match the difference you see between your two models?"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#differences-explained",
    "href": "tutorials/atap_docclass/atap_docclass.html#differences-explained",
    "title": "Classifying American Speeches",
    "section": "Differences Explained",
    "text": "Differences Explained\nThat we get different results when creating models using different algorithms will not surprise you whatsoever. But you might have noticed a different theme running through the last few steps: we are not able to say confidently that the numbers we get are exactly the ones you are seeing. The reason, as alluded to briefly above, is that chances are really high that you are training your model on a different sample of speeches than we are.\nAnd you can see for yourself how this works. We are going to repeat the steps from the data sampling to the calculation of the accuracy. The only thing that changes is the set of training data that is sampled initially. You can either just let the code do its thing and check out the results; or you can quickly try to recall for yourself what each line does (and sneak a peek at the descriptions above, in case you can’t reconstruct why we are doing what). For didactic purposes, we obviously recommend the latter, but we can’t hold a grudge against you if you just want to see the results:\n\ntrain_dtm2 &lt;- dfm_sample(dtm, size = 2800)\ntest_dtm2 &lt;- dtm[setdiff(docnames(dtm), docnames(train_dtm2)), ]\nlr_model2 &lt;- textmodel_lr(train_dtm2, y = docvars(train_dtm2, \"party\"))\npred_lr2 &lt;- predict(lr_model2, newdata = test_dtm)\nconfmat_lr2 &lt;- table(prediction = pred_lr2, PARTY = docvars(test_dtm, \"party\"))\nconfmat_lr2\n\n          PARTY\nprediction dem rep\n       dem 258   1\n       rep   1 187\n\nconfmat_lr\n\n          PARTY\nprediction dem rep\n       dem 250   8\n       rep   9 180\n\n\nYou’ll find that the confusion matrices show a visible difference between the two logistic regression models, trained on two different samples. Constituted by the fact of how we calculate it, this difference is obviously also reflected in the accuracy:\n\naccuracy_lr2 &lt;- (confmat_lr2[1, 1] + confmat_lr2[2, 2]) / sum(confmat_lr2)\naccuracy_lr2\n\n[1] 0.9955257\n\naccuracy_lr\n\n[1] 0.9619687\n\n\nThe difference we see here in the confusion matrix and the accuracy have a pretty detrimental implication for any scientific application of document classification: the results are not robustly replicable in the face of random sampling. To prevent you from turning in naive results (pardon the pun), we’ll discuss next what you can do in the face of this issue."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#best-practices",
    "href": "tutorials/atap_docclass/atap_docclass.html#best-practices",
    "title": "Classifying American Speeches",
    "section": "Best Practices",
    "text": "Best Practices\nIn any field, whether empirical or theoretical, part of what constitutes good academic practice is to be transparent about a) what the steps you have taken to arrive from your starting point to your conclusion and b) what the limitations and affordances of your methodology are.\nAs far as the steps go, we have given you a detailed blow-by-blow instruction about how we get from the input data to meaningful results, both in terms of document classification accuracy and salient features. If you made it this far into this notebook you must also be quite convinced of the affordances of document classification. What we are going to discuss next is probably the best way to address the limitations of document classification which arise from sampling."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#loops",
    "href": "tutorials/atap_docclass/atap_docclass.html#loops",
    "title": "Classifying American Speeches",
    "section": "Loops",
    "text": "Loops\nSince it’s a bit tedious to run through the whole sequence of commands from sampling to calculating the accuracy, we can use a loop to go through this process any given number of times (although here, as with the choice of algorithm, there is a small trade-off between the number of times we can and want to do this and the benefits to robustness). Let’s take a look at how this loop thing works.\nFirst, we construct a new variable. Let’s call it accuracy. We’ll be basing our loop on this, so we will make accuracy a vector, a list of numbers, whose length corresponds to the number of times we want to run our loop. We can do this as follows:\n\naccuracy &lt;- 1:100\n\nIf you look at accuarcy, you can see that it’s just a vector containing the integers 1 through 100:\n\naccuracy\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nNow for the loop. If you’ve never come across this before, it might seem like a bit of an unwieldy construction - in which case, we recommend focusing first on the six lines of code inside the { curly brackets }:\n\nfor (i in accuracy) {\n    train_dtm &lt;- dfm_sample(dtm, size = 2800)\n    test_dtm &lt;- dtm[setdiff(docnames(dtm), docnames(train_dtm)), ]\n    lr_model &lt;- quanteda.textmodels::textmodel_nb(train_dtm, docvars(train_dtm, \"party\"))\n    pred_lr &lt;- predict(lr_model, newdata = test_dtm)\n    confmat_lr &lt;- table(prediction = pred_lr, PARTY = docvars(test_dtm, \"party\"))\n    accuracy[i] &lt;- (confmat_lr[1, 1] + confmat_lr[2, 2]) / sum(confmat_lr)\n}\n\nWhat you find inside the curly brackets should, for the most part, be familiar. In the first line, we have the sampling of the training data. This is the essential step, which allows us to identify how robust our results are. The second line defines the test data in relation to the training data. In the third line, we train our linear regression model. The fourth line uses the model to predict the parties of the speeches in the test data. The fifth line creates the by now familiar confusion matrix. And finally, in the sixth line, we calculate the accuracy, and save it to our variable, accuracy.\nThe one unfamiliar element occurs in this sixth line, where we indicate that the accuracy is to be saved to the index position [i] in the vector accuracy. This starts to make sense if we look at what is outside the curly brackets. The line which we actually start the command with is for (i in accuracy), followed by what is in the brackets.\nThis basically states that for each index, each entry, in the vector accuracy, the sequence of commands in the curly brackets should be executed. With the sixth line inside the curly bracket, we make use of the fact that we can overwrite any single value contained within the vector accuracy, without altering the length of the vector.\nWhen we execute the loop, the six lines of code inside the curly brackets run twenty times. The first time, the accuracy that we calculate is saved in the first position of the vector, overwriting the number 1 that was there before. The second time, the calculated accuracy overwrites the second position of the vector, where the number 2 was before. And so on, until each of the twenty integers has been replaced with the accuracy of twenty models trained on twenty different sets of training data.\nWe can see that this works by looking at the variable accuracy once we’ve run the loop:\n\naccuracy\n\n  [1] 0.9418345 0.9373602 0.9172260 0.9395973 0.9395973 0.9373602 0.9507830\n  [8] 0.9395973 0.9395973 0.9440716 0.9149888 0.9373602 0.9395973 0.9418345\n [15] 0.9485459 0.9351230 0.9440716 0.9552573 0.9351230 0.9440716 0.9351230\n [22] 0.9440716 0.9597315 0.9440716 0.9619687 0.9261745 0.9306488 0.9440716\n [29] 0.9373602 0.9485459 0.9619687 0.9440716 0.9440716 0.9440716 0.9328859\n [36] 0.9328859 0.9328859 0.9395973 0.9530201 0.9351230 0.9194631 0.9619687\n [43] 0.9463087 0.9351230 0.9284116 0.9395973 0.9328859 0.9217002 0.9351230\n [50] 0.9373602 0.9485459 0.9552573 0.9306488 0.9284116 0.9373602 0.9507830\n [57] 0.9574944 0.9284116 0.9328859 0.9306488 0.9261745 0.9217002 0.9306488\n [64] 0.9597315 0.9395973 0.9574944 0.9619687 0.9261745 0.9485459 0.9351230\n [71] 0.9440716 0.9395973 0.9082774 0.9440716 0.9507830 0.9418345 0.9306488\n [78] 0.9194631 0.9507830 0.9463087 0.9328859 0.9373602 0.9418345 0.9284116\n [85] 0.9373602 0.9530201 0.9373602 0.9395973 0.9507830 0.9239374 0.9619687\n [92] 0.9172260 0.9395973 0.9463087 0.9440716 0.9328859 0.9328859 0.9172260\n [99] 0.9507830 0.9418345\n\n\nWhere before we had integers, we now have values somewhere in the ballpark of 92-98% (also depending on whether we use Naive Bayes or the more performant Logistic Regression). On the one hand this shows us that there is some observable variation in the accuracy of our models which is predicated on the data sample the model is trained on. On the other hand, it shows us that, regardless of the training data, the models all reach a decent degree of accuracy somewhere above 90% also in a worst-case scenario."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#demonstrating-robustness",
    "href": "tutorials/atap_docclass/atap_docclass.html#demonstrating-robustness",
    "title": "Classifying American Speeches",
    "section": "Demonstrating Robustness",
    "text": "Demonstrating Robustness\nWhile it’s easy enough to take in the results of the loop if we’re working with a fairly low number of iterations, it becomes a bit more challenging if we consider running this loop hundreds of times. In those cases, we can summarize our results in some key statistics and show them graphically in the form of a histogram. We aren’t going for any shiny visuals here, but you can find many a good instruction into how to make those. We’ll just show you the bare-bones approaches here.\nOne of the easiest ways to get a summary of a numerical vector is the summary() command, which is part of basic R functionality. Let’s take a look:\n\nsummary(accuracy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9083  0.9329  0.9396  0.9394  0.9463  0.9620 \n\n\nWe get several statistics here. On either pole of the output we see the minimum and maximum values contained in the vector. In my case, the worst model achieved an accuracy of 91% while the best model achieved an accuracy of 97%. In between these two poles we have the median and the arithmetic mean, the colloquial (sometimes even pedestrian) average. In my case, both the medium and the mean landed on 94.2%, indicating both that the module robustly achieves an accuracy of 94% and that the accuracies are indeed normally distributed.\nWe can also visualize this normal distribution in a histogram, using the hist() command. The first argument here is the data, our accuarcy vector, and the second argument makes the whole graphic more visually palatable (try it without this specification to see what we’re talking about):\n\naccuracy %&gt;%\n    as.data.frame() %&gt;%\n    dplyr::rename(accuracy = 1) %&gt;%\n    ggplot(aes(x = accuracy)) +\n    geom_histogram() +\n    theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWhat you see in the bottom right quadrant should roughly resemble a normal distribution - although we will add the disclaimer that there is a chance that it doesn’t, on account of the fact that we don’t have that many data points. Once we reach somewhere around a thousand runs, we should more or less reliably arrive at a normal distribution. But be warned: if you do that, it’s probably worth starting the loop and then going to fix yourself a snack. Unless you have a really good computer, this could take a while.\nWhat you’ll also notice about the graph we just created is that it’s centered around the mean of our distribution. This is great when you want to see what the distribution looks like. If you’ve arrived at the point where you want to brag about how good your results are, we recommend plotting the histogram showing all of the x-axis between 0 and 1. We can do this with the additional specification of xlim=0:1. Like so:\n\naccuracy %&gt;%\n    as.data.frame() %&gt;%\n    dplyr::rename(accuracy = 1) %&gt;%\n    ggplot(aes(x = accuracy)) +\n    geom_histogram() +\n    coord_cartesian(xlim = c(0, 1)) +\n    theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNow you can see that the results cluster fairly heavily in the far right of the graph, as expected. The final metric we want to draw your attention to, in case it hasn’t sprung to mind yet, is the standard deviation. There is a technical definition that you can look up at your leisure; but in short it describes how widely the data is scattered around the mean. The heuristic is that, the higher the standard deviation, the less robust the results. Let’s take a look at the standard deviations for our accuracies:\n\nsd(accuracy)\n\n[1] 0.01147901\n\n\nIn my case, I get a standard deviation of 1.2 percentage points. This translates to the following: roughly 66% of all models achieve an accuracy of 94.2% (the mean), plus and minus 1.2 percentage points. So two thirds of the models make predictions that are 93% to 95.4% accurate.\nThere are other methods to measure robustness, a popular one is n-fold cross validation, in which we train n times, with a specific 1-1/n section used for training and 1/n for testing, in such a way that every section is used once for testing. You could program n-fold cross validation as an advanced exercise. Using a loop with random sampling is a simple way to explore robustness, albeit less systematic than cross-validation.\nIf you always want to get the same result, you can also force the sampling to always return the same documents, by setting a seed for the random calculation with the set.seed() function. While this ensures reproducibility, it may hide robustness problems, and this is why we chose not to use it.\nFor a spam filter, the performance of our system would clearly be insufficient (we can see at least one of us biting into his laptop if every 20th spam mail reached our inbox). But as you have seen, there are various ways in which a higher accuracy can be achieved, from the specific pre-processing steps to the choice of algorithms. While there are dozens, if not hundreds, of other parameters that can be tweaked, after working your way through this introduction you should be ready to start your own document classification experiments and do cool stuff! Let’s get on it!"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#footnotes",
    "href": "tutorials/atap_docclass/atap_docclass.html#footnotes",
    "title": "Classifying American Speeches",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs an aside, we once again see some of the potential harm done by pre-processing: we have the feature us ranking really highly here, but we cannot be sure whether this is a plural, inclusive pronoun, or whether this is a botched version of the United States. Depending on what you are working towards, you might be interested in keeping abbreviations like this intact.↩︎"
  },
  {
    "objectID": "tutorials/regression/regression.html#simple-linear-regression",
    "href": "tutorials/regression/regression.html#simple-linear-regression",
    "title": "Introduction",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nThis section focuses on a very widely used statistical method which is called regression. Regressions are used when we try to understand how independent variables correlate with a dependent or outcome variable. So, if you want to investigate how a certain factor affects an outcome, then a regression is the way to go. We will have a look at two simple examples to understand what the concepts underlying a regression mean and how a regression works. The R code, that we will use, is adapted from many highly recommendable introductions which also focus on regression (among other types of analyses), for example, Gries, Winter, Levshina, Winter or Wilcox. Baayen is also very good but probably not the first book one should read about statistics but it is highly recommendable for advanced learners.\nAlthough the basic logic underlying regressions is identical to the conceptual underpinnings of analysis of variance (ANOVA), a related method, sociolinguists have traditionally favored regression analysis in their studies while ANOVAs have been the method of choice in psycholinguistics. The preference for either method is grounded in historical happenstances and the culture of these subdisciplines rather than in methodological reasoning. However, ANOVA are more restricted in that they can only take numeric dependent variables and they have stricter model assumptions that are violated more readily. In addition, a minor difference between regressions and ANOVA lies in the fact that regressions are based on the \\(t\\)-distribution while ANOVAs use the F-distribution (however, the F-value is simply the value of t squared or t2). Both t- and F-values report on the ratio between explained and unexplained variance.\nThe idea behind regression analysis is expressed formally in the equation below where\\(f_{(x)}\\) is the y-value we want to predict, \\(\\alpha\\) is the intercept (the point where the regression line crosses the y-axis at x = 0), \\(\\beta\\) is the coefficient (the slope of the regression line).\n\\[\\begin{equation}\nf_{(x)} = \\alpha + \\beta_{i}x + \\epsilon\n\\end{equation}\\]\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (x) and add the value of the intercept (point where line crosses the y-axis at x = 0).\nHowever, the idea behind regressions can best be described graphically: imagine a cloud of points (like the points in the scatterplot in the upper left panel below). Regressions aim to find that line which has the minimal summed distance between points and the line (like the line in the lower panels). Technically speaking, the aim of a regression is to find the line with the minimal deviance (or the line with the minimal sum of residuals). Residuals are the distance between the line and the points (the red lines) and it is also called variance.\nThus, regression lines are those lines where the sum of the red lines should be minimal. The slope of the regression line is called coefficient and the point where the regression line crosses the y-axis at x = 0 is called the intercept.\n\n\n\n\n\n\n\n\n\nA word about standard errors (SE) is in order here because most commonly used statistics programs will provide SE values when reporting regression models. The SE is a measure that tells us how much the coefficients were to vary if the same regression were applied to many samples from the same population. A relatively small SE value therefore indicates that the coefficients will remain very stable if the same regression model is fitted to many different samples with identical parameters. In contrast, a large SE tells you that the model is volatile and not very stable or reliable as the coefficients vary substantially if the model is applied to many samples.\nMathematically, the SE is the standard deviation (SD) divided by the square root of the sample size (N) (see below).The SD is the square root of the deviance (that is, the SD is the square root of the sum of the mean \\(\\bar{x}\\) minus each data point (xi) squared divided by the sample size (N) minus 1).\n\\[\\begin{equation}\nStandard Error (SE) = \\frac{\\sum (\\bar{x}-x_{i})^2/N-1}{\\sqrt{N}} = \\frac{SD}{\\sqrt{N}}\n\\end{equation}\\]\n\nExample 1: Preposition Use across Real-Time\nWe will now turn to our first example. In this example, we will investigate whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly synthetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and increase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on.\nThe analysis is based on data extracted from the Penn Corpora of Historical English (see http://www.ling.upenn.edu/hist-corpora/), that consists of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora.\nThen, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative frequency of prepositions (dependent or outcome variable).\nA regression analysis will follow the steps described below:\n\nExtraction and processing of the data\nData visualization\nApplying the regression analysis to the data\nDiagnosing the regression model and checking whether or not basic model assumptions have been violated.\n\nIn a first step, we load functions that we may need (which in this case is a function that we will use to summarize the results of the analysis).\n\n# load functions\nsource(\"https://slcladal.github.io/rscripts/slrsummary.r\")\n\nAfter preparing our session, we can now load and inspect the data to get a first impression of its properties.\n\n# load data\nslrdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/sld.rda\", \"rb\"))\n\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nInspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted to character variables. If unchecked, then such issues could go unnoticed and cause trouble.\nWe will now plot the data to get a better understanding of what the data looks like.\n\np1 &lt;- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth()\np2 &lt;- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\") # with linear model smoothing!\n# display plots\nggpubr::ggarrange(p1, p2, ncol = 2, nrow = 1)\n\n\n\n\n\n\n\n\nBefore beginning with the regression analysis, we will center the year. We center the values of year by subtracting each value from the mean of year. This can be useful when dealing with numeric variables because if we did not center year, we would get estimated values for year 0 (a year when English did not even exist yet). If a variable is centered, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, centering can be very helpful, especially with respect to the interpretation of the results that regression models report.\n\n# center date\nslrdata$Date &lt;- slrdata$Date - mean(slrdata$Date) \n\nWe will now begin the regression analysis by generating a first regression model and inspect its results.\n\n# create initial model\nm1.lm &lt;- lm(Prepositions ~ Date, data = slrdata)\n# inspect results\nsummary(m1.lm)\n\n\nCall:\nlm(formula = Prepositions ~ Date, data = slrdata)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-69.1012471 -13.8549421   0.5779091  13.3208913  62.8580401 \n\nCoefficients:\n                   Estimate      Std. Error   t value             Pr(&gt;|t|)    \n(Intercept) 132.19009310987   0.83863748040 157.62483 &lt; 0.0000000000000002 ***\nDate          0.01732180307   0.00726746646   2.38347             0.017498 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.4339648 on 535 degrees of freedom\nMultiple R-squared:  0.010507008,   Adjusted R-squared:  0.00865748837 \nF-statistic: 5.68093894 on 1 and 535 DF,  p-value: 0.017498081\n\n\nThe summary output starts by repeating the regression equation. Then, the model provides the distribution of the residuals. The residuals should be distributed normally with the absolute values of the Min and Max as well as the 1Q (first quartile) and 3Q (third quartile) being similar or ideally identical. In our case, the values are very similar which suggests that the residuals are distributed evenly and follow a normal distribution. The next part of the report is the coefficients table. The estimate for the intercept is the value of y at x = 0. The estimate for Date represents the slope of the regression line and tells us that with each year, the predicted frequency of prepositions increase by .01732 prepositions. The t-value is the Estimate divided by the standard error (Std. Error). Based on the t-value, the p-value can be calculated manually as shown below.\n\n# use pt function (which uses t-values and the degrees of freedom)\n2*pt(-2.383, nrow(slrdata)-1)\n\n[1] 0.0175196401501\n\n\nThe R2-values tell us how much variance is explained by our model. The baseline value represents a model that uses merely the mean. 0.0105 means that our model explains only 1.05 percent of the variance (0.010 x 100) - which is a tiny amount. The problem of the multiple R2 is that it will increase even if we add variables that explain almost no variance. Hence, multiple R2 encourages the inclusion of junk variables.\n\\[\\begin{equation}\nR^2 = R^2_{multiple} = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar y)^2}\n\\end{equation}\\]\nThe adjusted R2-value takes the number of predictors into account and, thus, the adjusted R2 will always be lower than the multiple R2. This is so because the adjusted R2 penalizes models for having predictors. The equation for the adjusted R2 below shows that the amount of variance that is explained by all the variables in the model (the top part of the fraction) must outweigh the inclusion of the number of variables (k) (lower part of the fraction). Thus, the adjusted R2 will decrease when variables are added that explain little or even no variance while it will increase if variables are added that explain a lot of variance.\n\\[\\begin{equation}\nR^2_{adjusted} = 1 - (\\frac{(1 - R^2)(n - 1)}{n - k - 1})\n\\end{equation}\\]\nIf there is a big difference between the two R2-values, then the model contains (many) predictors that do not explain much variance which is not good. The F-statistic and the associated p-value tell us that the model, despite explaining almost no variance, is still significantly better than an intercept-only base-line model (or using the overall mean to predict the frequency of prepositions per text).\nWe can test this and also see where the F-values comes from by comparing the\n\n# create intercept-only base-line model\nm0.lm &lt;- lm(Prepositions ~ 1, data = slrdata)\n# compare the base-line and the more saturated model\nanova(m1.lm, m0.lm, test = \"F\")\n\nAnalysis of Variance Table\n\nModel 1: Prepositions ~ Date\nModel 2: Prepositions ~ 1\n  Res.Df         RSS Df   Sum of Sq       F   Pr(&gt;F)  \n1    535 202058.2576                                  \n2    536 204203.8289 -1 -2145.57126 5.68094 0.017498 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F- and p-values are exactly those reported by the summary which shows where the F-values comes from and what it means; namely it denote the difference between the base-line and the more saturated model.\nThe degrees of freedom associated with the residual standard error are the number of cases in the model minus the number of predictors (including the intercept). The residual standard error is square root of the sum of the squared residuals of the model divided by the degrees of freedom. Have a look at he following to clear this up:\n\n# DF = N - number of predictors (including intercept)\nDegreesOfFreedom &lt;- nrow(slrdata)-length(coef(m1.lm))\n# sum of the squared residuals\nSumSquaredResiduals &lt;- sum(resid(m1.lm)^2)\n# Residual Standard Error\nsqrt(SumSquaredResiduals/DegreesOfFreedom); DegreesOfFreedom\n\n[1] 19.4339647585\n\n\n[1] 535\n\n\nWe will now check if mathematical assumptions have been violated (homogeneity of variance) or whether the data contains outliers. We check this using diagnostic plots.\n\n# generate data\ndf2 &lt;- data.frame(id = 1:length(resid(m1.lm)),\n                 residuals = resid(m1.lm),\n                 standard = rstandard(m1.lm),\n                 studend = rstudent(m1.lm))\n# generate plots\np1 &lt;- ggplot(df2, aes(x = id, y = residuals)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Residuals\", x = \"Index\")\np2 &lt;- ggplot(df2, aes(x = id, y = standard)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Standardized Residuals\", x = \"Index\")\np3 &lt;- ggplot(df2, aes(x = id, y = studend)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Studentized Residuals\", x = \"Index\")\n# display plots\nggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)\n\n\n\n\n\n\n\n\nThe left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (center panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals (Field, Miles, and Field, 268–69):\n\nPoints with values higher than 3.29 should be removed from the data.\nIf more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.\nIf more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.\n\nThe right panel shows the * studentized residuals* (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student’s t-distribution to diagnose our model.\nAdjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.\nThe plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.\nWe will now generate more diagnostic plots.\n\n# generate plots\nautoplot(m1.lm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\n\n\n\n\nThe diagnostic plots are very positive and we will go through why this is so for each panel. The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic (in such cases, it can help to remove data points that are too influential (outliers)).\nThe graphic in the upper right panel indicates whether the residuals are normally distributed (which is desirable) or whether the residuals do not follow a normal distribution. If the points lie on the line, the residuals follow a normal distribution. For example, if the points are not on the line at the top and bottom, it shows that the model does not predict small and large values well and that it therefore does not have a good fit.\nThe graphic in the lower left panel provides information about homoscedasticity. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line, we are dealing with heteroscedasticity, that is, a correlation between independent variables and the residuals, which is very problematic for regressions.\nThe graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (this would be problematic). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook’s distance, which shows how the regression changes when a model without this data point is calculated. The cook distance thus shows the influence a data point has on the regression as a whole. Data points that have a Cook’s distance value greater than 1 are problematic (Field, Miles, and Field, 269).\nThe so-called leverage is also a measure that indicates how strongly a data point affects the accuracy of the regression. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this:\n\\[\\begin{equation}\nLeverage = \\frac{3(k + 1)}{n} |  \\frac{2(k + 1)}{n}\n\\end{equation}\\]\nWe will look more closely at leverage in the context of multiple linear regression and will therefore end the current analysis by summarizing the results of the regression analysis in a table.\n\n# create summary table\nslrsummary(m1.lm)  \n\n\n\nParametersEstimatePearson's rStd. Errort valuePr(&gt;|t|)P-value sig.(Intercept)132.190.84157.620p &lt; .001***Date0.020.10.012.380.0175p &lt; .05*Model statisticsValueNumber of cases in model537Residual standard error on 535 DF19.43Multiple R-squared0.0105Adjusted R-squared0.0087F-statistic (1, 535)5.68Model p-value0.0175\n\n\nAn alternative but less informative summary table of the results of a regression analysis can be generated using the tab_model function from the sjPlot package (Lüdecke) (as is shown below).\n\n# generate summary table\nsjPlot::tab_model(m1.lm) \n\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n132.19\n130.54 – 133.84\n&lt;0.001\n\n\nDate\n0.02\n0.00 – 0.03\n0.017\n\n\nObservations\n537\n\n\nR2 / R2 adjusted\n0.011 / 0.009\n\n\n\n\n\n\n\n\nTypically, the results of regression analyses are presented in such tables as they include all important measures of model quality and significance, as well as the magnitude of the effects.\nIn addition, the results of simple linear regressions should be summarized in writing.\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(m1.lm)\n\nWe fitted a linear model (estimated using OLS) to predict Prepositions with\nDate (formula: Prepositions ~ Date). The model explains a statistically\nsignificant and very weak proportion of variance (R2 = 0.01, F(1, 535) = 5.68,\np = 0.017, adj. R2 = 8.66e-03). The model's intercept, corresponding to Date =\n0, is at 132.19 (95% CI [130.54, 133.84], t(535) = 157.62, p &lt; .001). Within\nthis model:\n\n  - The effect of Date is statistically significant and positive (beta = 0.02,\n95% CI [3.05e-03, 0.03], t(535) = 2.38, p = 0.017; Std. beta = 0.10, 95% CI\n[0.02, 0.19])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nWe can use this output to write up a final report:\nA simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic or disproportionately influential data points (outliers) and performed significantly better compared to an intercept-only base line model but only explained .87 percent of the variance (adjusted R2: .0087, F-statistic (1, 535): 5,68, p-value: 0.0175*). The final minimal adequate linear regression model is based on 537 data points and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions (coefficient estimate: .02 (standardized : 0.10, 95% CI [0.02, 0.19]), SE: 0.01, t-value535: 2.38, p-value: .0175*). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nExample 2: Teaching Styles\nIn the previous example, we dealt with two numeric variables, while the following example deals with a categorical independent variable and a numeric dependent variable. The ability for regressions to handle very different types of variables makes regressions a widely used and robust method of analysis.\nIn this example, we are dealing with two groups of students that have been randomly assigned to be exposed to different teaching methods. Both groups undergo a language learning test after the lesson with a maximum score of 20 points.\nThe question that we will try to answer is whether the students in group A have performed significantly better than those in group B which would indicate that the teaching method to which group A was exposed works better than the teaching method to which group B was exposed.\nLet’s move on to implementing the regression in R. In a first step, we load the data set and inspect its structure.\n\n# load data\nslrdata2  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/sgd.rda\", \"rb\"))\n\n\n\nGroupScoreA15A12A11A18A15A15A9A19A14A13A11A12A18A15A16\n\n\nNow, we graphically display the data. In this case, a boxplot represents a good way to visualize the data.\n\n# extract means\nslrdata2 %&gt;%\n  dplyr::group_by(Group) %&gt;%\n  dplyr::mutate(Mean = round(mean(Score), 1), SD = round(sd(Score), 1)) %&gt;%\n  ggplot(aes(Group, Score)) + \n  geom_boxplot(fill=c(\"orange\", \"darkgray\")) +\n  geom_text(aes(label = paste(\"M = \", Mean, sep = \"\"), y = 1)) +\n  geom_text(aes(label = paste(\"SD = \", SD, sep = \"\"), y = 0)) +\n  theme_bw(base_size = 15) +\n  labs(x = \"Group\") +                      \n  labs(y = \"Test score (Points)\", cex = .75) +   \n  coord_cartesian(ylim = c(0, 20)) +  \n  guides(fill = FALSE)                \n\n\n\n\n\n\n\n\nThe data indicate that group A did significantly better than group B. We will test this impression by generating the regression model and creating the model and extracting the model summary.\n\n# generate regression model\nm2.lm &lt;- lm(Score ~ Group, data = slrdata2) \n# inspect results\nsummary(m2.lm)                             \n\n\nCall:\nlm(formula = Score ~ Group, data = slrdata2)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-6.76666667 -1.93333333  0.15000000  2.06666667  6.23333333 \n\nCoefficients:\n                Estimate   Std. Error  t value               Pr(&gt;|t|)    \n(Intercept) 14.933333333  0.534571121 27.93517 &lt; 0.000000000000000222 ***\nGroupB      -3.166666667  0.755997730 -4.18873            0.000096692 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.92796662 on 58 degrees of freedom\nMultiple R-squared:  0.232249929,   Adjusted R-squared:  0.219012859 \nF-statistic:  17.545418 on 1 and 58 DF,  p-value: 0.0000966923559\n\n\nThe model summary reports that Group A performed significantly better compared with Group B. This is shown by the fact that the p-value (the value in the column with the header (Pr(&gt;|t|)) is smaller than .001 as indicated by the three * after the p-values). Also, the negative Estimate for Group B indicates that Group B has lower scores than Group A. We will now generate the diagnostic graphics.3\n\npar(mfrow = c(1, 3))        # plot window: 1 plot/row, 3 plots/column\nplot(resid(m2.lm))     # generate diagnostic plot\nplot(rstandard(m2.lm)) # generate diagnostic plot\nplot(rstudent(m2.lm)); par(mfrow = c(1, 1))  # restore normal plot window\n\n\n\n\n\n\n\n\nThe graphics do not indicate outliers or other issues, so we can continue with more diagnostic graphics.\n\npar(mfrow = c(2, 2)) # generate a plot window with 2x2 panels\nplot(m2.lm); par(mfrow = c(1, 1)) # restore normal plot window\n\n\n\n\n\n\n\n\nThese graphics also show no problems. In this case, the data can be summarized in the next step.\n\n# tabulate results\nslrsummary(m2.lm)\n\n\n\nParametersEstimatePearson's rStd. Errort valuePr(&gt;|t|)P-value sig.(Intercept)14.930.5327.940p &lt; .001***GroupB-3.170.480.76-4.190.0001p &lt; .001***Model statisticsValueNumber of cases in model60Residual standard error on 58 DF2.93Multiple R-squared0.2322Adjusted R-squared0.219F-statistic (1, 58)17.55Model p-value0.0001\n\n\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(m2.lm)\n\nWe fitted a linear model (estimated using OLS) to predict Score with Group\n(formula: Score ~ Group). The model explains a statistically significant and\nmoderate proportion of variance (R2 = 0.23, F(1, 58) = 17.55, p &lt; .001, adj. R2\n= 0.22). The model's intercept, corresponding to Group = A, is at 14.93 (95% CI\n[13.86, 16.00], t(58) = 27.94, p &lt; .001). Within this model:\n\n  - The effect of Group [B] is statistically significant and negative (beta =\n-3.17, 95% CI [-4.68, -1.65], t(58) = -4.19, p &lt; .001; Std. beta = -0.96, 95%\nCI [-1.41, -0.50])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nWe can use this output to write up a final report:\nA simple linear regression was fitted to the data. A visual assessment of the model diagnostics did not indicate any problematic or disproportionately influential data points (outliers). The final linear regression model is based on 60 data points, performed significantly better than an intercept-only base line model (F (1, 58): 17.55, p-value &lt;. 001\\(***\\)), and reported that the model explained 21.9 percent of variance which confirmed a good model fit. According to this final model, group A scored significantly better on the language learning test than group B (coefficient: -3.17, 95% CI [-4.68, -1.65], Std. : -0.96, 95% CI [-1.41, -0.50], SE: 0.48, t-value58: -4.19, p-value &lt;. 001\\(***\\)). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "tutorials/regression/regression.html#multiple-linear-regression",
    "href": "tutorials/regression/regression.html#multiple-linear-regression",
    "title": "Introduction",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nIn contrast to simple linear regression, which estimates the effect of a single predictor, multiple linear regression estimates the effect of various predictor (see the equation below). A multiple linear regression can thus test the effects of various predictors simultaneously.\n\\[\\begin{equation}\n\nf_{(x)} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i+1} + \\dots + \\beta_{n}x_{i+n} + \\epsilon\n\n\\end{equation}\\]\nThere exists a wealth of literature focusing on multiple linear regressions and the concepts it is based on. For instance, there are Achen, Bortz, Crawley, Faraway, Field, Miles, and Field, Gries, Levshina, Winter and Wilcox to name just a few. Introductions to regression modeling in R are Baayen, Crawley, Gries, or Levshina.\nThe model diagnostics we are dealing with here are partly identical to the diagnostic methods discussed in the section on simple linear regression. Because of this overlap, diagnostics will only be described in more detail if they have not been described in the section on simple linear regression.\n\n\n\n\nEXCURSION\n\n\n\n\n\n\n`\n\n\nA note on sample size and power\n\nA brief note on minimum necessary sample or data set size appears necessary here. Although there appears to be a general assumption that 25 data points per group are sufficient, this is not necessarily correct (it is merely a general rule of thumb that is actually often incorrect). Such rules of thumb are inadequate because the required sample size depends on the number of variables in a given model, the size of the effect and the variance of the effect - in other words, the minimum necessary sample size relates to statistical power (see here for a tutorial on power). If a model contains many variables, then this requires a larger sample size than a model which only uses very few predictors.\nAlso, to detect an effect with a very minor effect size, one needs a substantially larger sample compared to cases where the effect is very strong. In fact, when dealing with small effects, model require a minimum of 600 cases to reliably detect these effects. Finally, effects that are very robust and do not vary much require a much smaller sample size compared with effects that are spurious and vary substantially. Since the sample size depends on the effect size and variance as well as the number of variables, there is no final one-size-fits-all answer to what the best sample size is.\nAnother, slightly better but still incorrect, rule of thumb is that the more data, the better. This is not correct because models based on too many cases are prone for overfitting and thus report correlations as being significant that are not. However, given that there are procedures that can correct for overfitting, larger data sets are still preferable to data sets that are simply too small to warrant reliable results. In conclusion, it remains true that the sample size depends on the effect under investigation.\n\n\n`\n\nDespite there being no ultimate rule of thumb, Field, Miles, and Field (273–75), based on Green, provide data-driven suggestions for the minimal size of data required for regression models that aim to find medium sized effects (k = number of predictors; categorical variables with more than two levels should be transformed into dummy variables):\n\nIf one is merely interested in the overall model fit (something I have not encountered), then the sample size should be at least 50 + k (k = number of predictors in model).\nIf one is only interested in the effect of specific variables, then the sample size should be at least 104 + k (k = number of predictors in model).\nIf one is only interested in both model fit and the effect of specific variables, then the sample size should be at least the higher value of 50 + k or 104 + k (k = number of predictors in model).\n\nYou will see in the R code below that there is already a function that tests whether the sample size is sufficient.\n\nExample: Gifts and Availability\nThe example we will go through here is taken from Field, Miles, and Field. In this example, the research question is if the money that men spend on presents for women depends on the women’s attractiveness and their relationship status. To answer this research question, we will implement a multiple linear regression and start by loading the data and inspect its structure and properties.\n\n# load data\nmlrdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nThe data set consist of three variables stored in three columns. The first column contains the relationship status of the present giver (in this study this were men), the second whether the man is interested in the woman (the present receiver in this study), and the third column represents the money spend on the present. The data set represents 100 cases and the mean amount of money spend on a present is 88.38 dollars. In a next step, we visualize the data to get a more detailed impression of the relationships between variables.\n\n# create plots\np1 &lt;- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot() + \n  theme_bw()\n# plot 2\np2 &lt;- ggplot(mlrdata, aes(attraction, money)) +\n  geom_boxplot() +\n  theme_bw()\n# plot 3\np3 &lt;- ggplot(mlrdata, aes(x = money)) +\n  geom_histogram(aes(y=..density..)) +            \n  theme_bw() +         \n  geom_density(alpha=.2, fill = \"gray50\") \n# plot 4\np4 &lt;- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot(aes(fill = factor(status))) + \n  scale_fill_manual(values = c(\"grey30\", \"grey70\")) + \n  facet_wrap(~ attraction) + \n  guides(fill = \"none\") +\n  theme_bw()\n# show plots\ngridExtra::grid.arrange(grobs = list(p1, p2, p3, p4), widths = c(1, 1), layout_matrix = rbind(c(1, 2), c(3, 4)))\n\n\n\n\n\n\n\n\nThe upper left figure consists of a boxplot which shows how much money was spent by relationship status. The figure suggests that men spend more on women if they are not in a relationship. The next figure shows the relationship between the money spend on presents and whether or not the men were interested in the women.\nThe boxplot in the upper right panel suggests that men spend substantially more on women if the men are interested in them. The next figure depicts the distribution of the amounts of money spend on the presents for the women. In addition, the figure indicates the existence of two outliers (dots in the boxplot)\nThe histogram in the lower left panel shows that, although the mean amount of money spent on presents is 88.38 dollars, the distribution peaks around 50 dollars indicating that on average, men spend about 50 dollars on presents. Finally, we will plot the amount of money spend on presents against relationship status by attraction in order to check whether the money spent on presents is affected by an interaction between attraction and relationship status.\nThe boxplot in the lower right panel confirms the existence of an interaction (a non-additive term) as men only spend more money on women if the men single and they are interested in the women. If men are not interested in the women, then the relationship has no effect as they spend an equal amount of money on the women regardless of whether they are in a relationship or not.\nWe will now start to implement the regression model. In a first step, we create two saturated models that contain all possible predictors (main effects and interactions). The two models are identical but one is generated with the lm and the other with the glm function as these functions offer different model parameters in their output.\n\nm1.mlr = lm(                      # generate lm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  data = mlrdata)                 # def. data\nm1.glm = glm(                     # generate glm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  family = gaussian,              # def. linkage function\n  data = mlrdata)                 # def. data\n\nAfter generating the saturated models we can now start with the model fitting. Model fitting refers to a process that aims at find the model that explains a maximum of variance with a minimum of predictors (see Field, Miles, and Field, 318). Model fitting is therefore based on the principle of parsimony which is related to Occam’s razor according to which explanations that require fewer assumptions are more likely to be true.\n\n\nAutomatic Model Fitting and Why You Should Not Use It\nIn this section, we will use a step-wise step-down procedure that uses decreases in AIC (Akaike Information Criterion) as the criterion to minimize the model in a step-wise manner. This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value.\nWe use this method here just so that you know it exists and how to implement it but you should rather avoid using automated model fitting. The reason for avoiding automated model fitting is that the algorithm only checks if the AIC has decreased but not if the model is stable or reliable. Thus, automated model fitting has the problem that you can never be sure that the way that lead you to the final model is reliable and that all models were indeed stable. Imagine you want to climb down from a roof top and you have a ladder. The problem is that you do not know if and how many steps are broken. This is similar to using automated model fitting. In other sections, we will explore better methods to fit models (manual step-wise step-up and step-down procedures, for example).\nThe AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same data set with the same (number of) cases (LL stands for logged likelihood or LogLikelihood and k represents the number of predictors in the model (including the intercept); the LL represents a measure of how good the model fits the data).\n\\[\\begin{equation}\nAkaike Information Criterion (AIC) = -2LL + 2k\n\\end{equation}\\]\nAn alternative to the AIC is the BIC (Bayesian Information Criterion). Both AIC and BIC penalize models for including variables in a model. The penalty of the BIC is bigger than the penalty of the AIC and it includes the number of cases in the model (LL stands for logged likelihood or LogLikelihood, k represents the number of predictors in the model (including the intercept), and N represents the number of cases in the model).\n\\[\\begin{equation}\nBayesian Information Criterion (BIC) = -2LL + 2k * log(N)\n\\end{equation}\\]\nInteractions are evaluated first and only if all insignificant interactions have been removed would the procedure start removing insignificant main effects (that are not part of significant interactions). Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model.\n\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n\nStart:  AIC=592.52\nmoney ~ 1 + attraction * status\n\n                    Df   Sum of Sq         RSS         AIC\n&lt;none&gt;                             34557.56428 592.5211556\n- attraction:status  1 24947.25481 59504.81909 644.8642395\n\n\n\nCall:\nlm(formula = money ~ 1 + attraction * status, data = mlrdata)\n\nCoefficients:\n                         (Intercept)               attractionNotInterested  \n                             99.1548                              -47.6628  \n                        statusSingle  attractionNotInterested:statusSingle  \n                             57.6928                              -63.1788  \n\n\nThe automated model fitting procedure informs us that removing predictors has not caused a decrease in the AIC. The saturated model is thus also the final minimal adequate model. We will now inspect the final minimal model and go over the model report.\n\nm2.mlr = lm(                       # generate lm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  data = mlrdata)                  # def. data\nm2.glm = glm(                      # generate glm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  family = gaussian,               # def. linkage function\n  data = mlrdata)                  # def. data\n# inspect final minimal model\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-45.0760 -14.2580   0.4596  11.9315  44.1424 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.79459947 26.13050\nstatusSingle                          57.69280000   5.36637403 10.75080\nattractionNotInterested              -47.66280000   5.36637403 -8.88175\nstatusSingle:attractionNotInterested -63.17880000   7.58919893 -8.32483\n                                                   Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.000000000000000222 ***\nstatusSingle                         &lt; 0.000000000000000222 ***\nattractionNotInterested                 0.00000000000003751 ***\nstatusSingle:attractionNotInterested    0.00000000000058085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.9729973 on 96 degrees of freedom\nMultiple R-squared:  0.852041334,   Adjusted R-squared:  0.847417626 \nF-statistic: 184.276619 on 3 and 96 DF,  p-value: &lt; 0.0000000000000002220446\n\n\nThe first element of the report is called Call and it reports the regression formula of the model. Then, the report provides the residual distribution (the range, median and quartiles of the residuals) which allows drawing inferences about the distribution of differences between observed and expected values. If the residuals are distributed non-normally, then this is a strong indicator that the model is unstable and unreliable because mathematical assumptions on which the model is based are violated.\nNext, the model summary reports the most important part: a table with model statistics of the fixed-effects structure of the model. The table contains the estimates (coefficients of the predictors), standard errors, t-values, and the p-values which show whether a predictor significantly correlates with the dependent variable that the model investigates.\nAll main effects (status and attraction) as well as the interaction between status and attraction is reported as being significantly correlated with the dependent variable (money). An interaction occurs if a correlation between the dependent variable and a predictor is affected by another predictor.\nThe top most term is called intercept and has a value of 99.15 which represents the base estimate to which all other estimates refer. To exemplify what this means, let us consider what the model would predict that a man would spend on a present if he interested in the woman but he is also in a relationship. The amount he would spend (based on the model would be 99.15 dollars (which is the intercept). This means that the intercept represents the predicted value if all predictors take the base or reference level. And since being in relationship but being interested are the case, and because the interaction does not apply, the predicted value in our example is exactly the intercept (see below).\n\n#intercept  Single  NotInterested  Single:NotInterested\n99.15     + 57.69  + 0           + 0     # 156.8 single + interested\n\n[1] 156.84\n\n99.15     + 57.69  - 47.66       - 63.18 # 46.00 single + not interested\n\n[1] 46\n\n99.15     - 0      + 0           - 0     # 99.15 relationship + interested\n\n[1] 99.15\n\n99.15     - 0      - 47.66       - 0     # 51.49 relationship + not interested\n\n[1] 51.49\n\n\nNow, let us consider what a man would spend if he is in a relationship and he is not attracted to the women. In that case, the model predicts that the man would spend only 51.49 dollars on a present: the intercept (99.15) minus 47.66 because the man is not interested (and no additional subtraction because the interaction does not apply).\nWe can derive the same results easier using the predict function.\n\n# make prediction based on the model for original data\nprediction &lt;- predict(m2.mlr, newdata = mlrdata)\n# inspect predictions\ntable(round(prediction,2))\n\n\n 46.01  51.49  99.15 156.85 \n    25     25     25     25 \n\n\nBelow the table of coefficients, the regression summary reports model statistics that provide information about how well the model performs. The difference between the values and the values in the coefficients table is that the model statistics refer to the model as a whole rather than focusing on individual predictors.\nThe multiple R2-value is a measure of how much variance the model explains. A multiple R2-value of 0 would inform us that the model does not explain any variance while a value of .852 mean that the model explains 85.2 percent of the variance. A value of 1 would inform us that the model explains 100 percent of the variance and that the predictions of the model match the observed values perfectly. Multiplying the multiple R2-value thus provides the percentage of explained variance. Models that have a multiple R2-value equal or higher than .05 are deemed substantially significant (see Szmrecsanyi, 55). It has been claimed that models should explain a minimum of 5 percent of variance but this is problematic as it is not uncommon for models to have very low explanatory power while still performing significantly and systematically better than chance. In addition, the total amount of variance is negligible in cases where one is interested in very weak but significant effects. It is much more important for model to perform significantly better than minimal base-line models because if this is not the case, then the model does not have any predictive and therefore no explanatory power.\nThe adjusted R2-value considers the amount of explained variance in light of the number of predictors in the model (it is thus somewhat similar to the AIC and BIC) and informs about how well the model would perform if it were applied to the population that the sample is drawn from. Ideally, the difference between multiple and adjusted R2-value should be very small as this means that the model is not overfitted. If, however, the difference between multiple and adjusted R2-value is substantial, then this would strongly suggest that the model is unstable and overfitted to the data while being inadequate for drawing inferences about the population. Differences between multiple and adjusted R2-values indicate that the data contains outliers that cause the distribution of the data on which the model is based to differ from the distributions that the model mathematically requires to provide reliable estimates. The difference between multiple and adjusted R2-value in our model is very small (85.2-84.7=.05) and should not cause concern.\nBefore continuing, we will calculate the confidence intervals of the coefficients.\n\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n\n                                              2.5 %         97.5 %\n(Intercept)                           91.6225795890 106.6870204110\nstatusSingle                          47.0406317400  68.3449682600\nattractionNotInterested              -58.3149682600 -37.0106317400\nstatusSingle:attractionNotInterested -78.2432408219 -48.1143591781\n\n# create and compare baseline- and minimal adequate model\nm0.mlr &lt;- lm(money ~1, data = mlrdata)\nanova(m0.mlr, m2.mlr)\n\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(&gt;F)    \n1     99 233562.28650                                                    \n2     96  34557.56428  3 199004.7222 184.27662 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we compare the final minimal adequate model to the base-line model to test whether then final model significantly outperforms the baseline model.\n\n# compare baseline- and minimal adequate model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(&gt;F)    \n(Intercept) 781015.8300  1 2169.64133 &lt; 0.000000000000000222 ***\nResiduals    34557.5643 96                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe comparison between the two model confirms that the minimal adequate model performs significantly better (makes significantly more accurate estimates of the outcome variable) compared with the baseline model.\n\n\nModel diagnostics\nWe now check the model performance using the model_performance() function and to diagnose the model, we are using the check_model() function from the performance package (ludeke2021performance?).\nWe start by checking the performance of the null-model against the performance values of the final minimal adequate model. Ideally, the final minimal adequate model would show higher explanatory power (higher R2 values) and better parsimony (lower AIC and AICc as well as BIC values).\n\nperformance::model_performance(m0.mlr)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n--------------------------------------------------------------------\n1063.391 | 1063.515 | 1068.601 | 0.000 |     0.000 | 48.328 | 48.572\n\nperformance::model_performance(m2.mlr)\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n-----------------------------------------------------------------\n878.309 | 878.947 | 891.335 | 0.852 |     0.847 | 18.590 | 18.973\n\n\nThe output confirms that the final minimal adequate model performs substantively better than the null model (higher explanatory power and better parsimony). We now diagnose the model using the check_model() function.\n\nperformance::check_model(m2.mlr)\n\n\n\n\n\n\n\n\n\n\nOutlier Detection\nIn a next step, we now need to look for outliers check whether removing data points disproportionately decreases model fit. To begin with, we generate diagnostic plots.\n\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe plots do not show severe problems such as funnel shaped patterns or drastic deviations from the diagonal line in Normal Q-Q plot (have a look at the explanation of what to look for and how to interpret these diagnostic plots in the section on simple linear regression) but data points 52, 64, and 83 are repeatedly indicated as potential outliers.\n\n# determine a cutoff for data points that have D-values higher than 4/(n-k-1)\ncutoff &lt;- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 3 rows/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n\n[1] 52 83\n\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nThe graphs indicate that data points 52, 64, and 83 may be problematic. We will therefore statistically evaluate whether these data points need to be removed. In order to find out which data points require removal, we extract the influence measure statistics and add them to out data set.\n\n# extract influence statistics\ninfl &lt;- influence.measures(m2.mlr)\n# add infl. statistics to data\nmlrdata &lt;- data.frame(mlrdata, infl[[1]], infl[[2]])\n# annotate too influential data points\nremove &lt;- apply(infl$is.inf, 1, function(x) {\n  ifelse(x == TRUE, return(\"remove\"), return(\"keep\")) } )\n# add annotation to data\nmlrdata &lt;- data.frame(mlrdata, remove)\n# number of rows before removing outliers\nnrow(mlrdata)\n\n[1] 100\n\n# remove outliers\nmlrdata &lt;- mlrdata[mlrdata$remove == \"keep\", ]\n# number of rows after removing outliers\nnrow(mlrdata)\n\n[1] 98\n\n\nThe difference in row in the data set before and after removing data points indicate that two data points which represented outliers have been removed.\n\n\n\nNOTEIn general, outliers should not simply be removed unless there are good reasons for it (this could be that the outliers represent measurement errors). If a data set contains outliers, one should rather switch to methods that are better at handling outliers, e.g. by using weights to account for data points with high leverage. One alternative would be to switch to a robust regression (see here). However, here we show how to proceed by removing outliers as this is a common, though potentially problematic, method of dealing with outliers.\n\n\n\n\n\n\n\n\nRerun Regression\nAs we have decided to remove the outliers which means that we are now dealing with a different data set, we need to rerun the regression analysis. As the steps are identical to the regression analysis performed above, the steps will not be described in greater detail.\n\n# recreate regression models on new data\nm0.mlr = lm(money ~ 1, data = mlrdata)\nm0.glm = glm(money ~ 1, family = gaussian, data = mlrdata)\nm1.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm1.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n\nStart:  AIC=570.29\nmoney ~ (status + attraction)^2\n\n                    Df Sum of Sq         RSS         AIC\n&lt;none&gt;                           30411.31714 570.2850562\n- status:attraction  1 21646.862 52058.17914 620.9646729\n\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nCoefficients:\n                         (Intercept)                          statusSingle  \n                          99.1548000                            55.8535333  \n             attractionNotInterested  statusSingle:attractionNotInterested  \n                         -47.6628000                           -59.4613667  \n\n\n\n# create new final models\nm2.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm2.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# inspect final minimal model\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.000000000000000222 ***\nstatusSingle                         &lt; 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,   Adjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: &lt; 0.0000000000000002220446\n\n\n\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n\n                                              2.5 %         97.5 %\n(Intercept)                           92.0121609656 106.2974390344\nstatusSingle                          45.6476377202  66.0594289465\nattractionNotInterested              -57.7640169936 -37.5615830064\nstatusSingle:attractionNotInterested -73.8946826590 -45.0280506744\n\n\n\n# compare baseline with final model\nanova(m0.mlr, m2.mlr)\n\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(&gt;F)    \n1     97 213227.06081                                                    \n2     94  30411.31714  3 182815.7437 188.35839 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# compare baseline with final model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(&gt;F)    \n(Intercept) 760953.2107  1 2352.07181 &lt; 0.000000000000000222 ***\nResiduals    30411.3171 94                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nAdditional Model Diagnostics\nAfter rerunning the regression analysis on the updated data set, we again create diagnostic plots in order to check whether there are potentially problematic data points.\n\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n# determine a cutoff for data points that have\n# D-values higher than 4/(n-k-1)\ncutoff &lt;- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 1 row/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n\n84 88 \n82 86 \n\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nAlthough the diagnostic plots indicate that additional points may be problematic, but these data points deviate substantially less from the trend than was the case with the data points that have already been removed. To make sure that retaining the data points that are deemed potentially problematic by the diagnostic plots, is acceptable, we extract diagnostic statistics and add them to the data.\n\n# add model diagnostics to the data\nmlrdata &lt;- mlrdata %&gt;%\n  dplyr::mutate(residuals = resid(m2.mlr),\n                standardized.residuals = rstandard(m2.mlr),\n                studentized.residuals = rstudent(m2.mlr),\n                cooks.distance = cooks.distance(m2.mlr),\n                dffit = dffits(m2.mlr),\n                leverage = hatvalues(m2.mlr),\n                covariance.ratios = covratio(m2.mlr),\n                fitted = m2.mlr$fitted.values)\n\nWe can now use these diagnostic statistics to create more precise diagnostic plots.\n\n# plot 5\np5 &lt;- ggplot(mlrdata,\n             aes(studentized.residuals)) +\n  theme(legend.position = \"none\")+\n  geom_histogram(aes(y=..density..),\n                 binwidth = .2,\n                 colour=\"black\",\n                 fill=\"gray90\") +\n  labs(x = \"Studentized Residual\", y = \"Density\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE),\n                            sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)),\n                colour = \"red\", size = 1) +\n  theme_bw(base_size = 8)\n# plot 6\np6 &lt;- ggplot(mlrdata, aes(fitted, studentized.residuals)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"Red\")+\n  theme_bw(base_size = 8)+\n  labs(x = \"Fitted Values\",\n       y = \"Studentized Residual\")\n# plot 7\np7 &lt;- qplot(sample = mlrdata$studentized.residuals) +\n  theme_bw(base_size = 8) +\n  labs(x = \"Theoretical Values\",\n       y = \"Observed Values\")\ngridExtra::grid.arrange(p5, p6, p7, nrow = 1)\n\n\n\n\n\n\n\n\nThe new diagnostic plots do not indicate outliers that require removal. With respect to such data points the following parameters should be considered:\n\nData points with standardized residuals &gt; 3.29 should be removed (Field, Miles, and Field, 269)\nIf more than 1 percent of data points have standardized residuals exceeding values &gt; 2.58, then the error rate of the model is unacceptable (Field, Miles, and Field, 269).\nIf more than 5 percent of data points have standardized residuals exceeding values &gt; 1.96, then the error rate of the model is unacceptable (Field, Miles, and Field, 269)\nIn addition, data points with Cook’s D-values &gt; 1 should be removed (Field, Miles, and Field, 269)\nAlso, data points with leverage values higher than \\(3(k + 1)/N\\) or \\(2(k + 1)/N\\) (k = Number of predictors, N = Number of cases in model) should be removed (Field, Miles, and Field, 270)\nThere should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead.\nPredictors cannot substantially correlate with each other (multicollinearity) (see the subsection on (multi-)collinearity in the section of multiple binomial logistic regression for more details about (multi-)collinearity). If a model contains predictors that have variance inflation factors (VIF) &gt; 10 the model is unreliable (Myers) and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic (Szmrecsanyi, 215) Indeed, Zuur, Ieno, and Elphick propose that variables with VIFs exceeding 3 should be removed!\n\n\n\n\nNOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See Gries for a more elaborate explanation.\n\n\n\n\n\n\n\nThe mean value of VIFs should be ~ 1 (Bowerman and O’Connell).\n\nThe following code chunk evaluates these criteria.\n\n# 1: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$standardized.residuals &gt; 3.29)\n\nnamed integer(0)\n\n# 2: optimal = 1\n# (listed data points should be removed)\nstdres_258 &lt;- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) &gt; 2.58, 1, 0) } ))\n(sum(stdres_258) / length(stdres_258)) * 100\n\n[1] 0\n\n# 3: optimal = 5\n# (listed data points should be removed)\nstdres_196 &lt;- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) &gt; 1.96, 1, 0) } ))\n(sum(stdres_196) / length(stdres_196)) * 100\n\n[1] 6.12244897959\n\n# 4: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$cooks.distance &gt; 1)\n\nnamed integer(0)\n\n# 5: optimal = 0\n# (data points should be removed if cooks distance is close to 1)\nwhich(mlrdata$leverage &gt;= (3*mean(mlrdata$leverage)))\n\nnamed integer(0)\n\n# 6: checking autocorrelation:\n# Durbin-Watson test (optimal: high p-value)\ndwt(m2.mlr)\n\n lag  Autocorrelation D-W Statistic p-value\n   1 -0.0143324675649  1.9680423527   0.624\n Alternative hypothesis: rho != 0\n\n# 7: test multicollinearity 1\nvif(m2.mlr)\n\n                        statusSingle              attractionNotInterested \n                                2.00                                 1.96 \nstatusSingle:attractionNotInterested \n                                2.96 \n\n# 8: test multicollinearity 2\n1/vif(m2.mlr)\n\n                        statusSingle              attractionNotInterested \n                      0.500000000000                       0.510204081633 \nstatusSingle:attractionNotInterested \n                      0.337837837838 \n\n# 9: mean vif should not exceed 1\nmean(vif(m2.mlr))\n\n[1] 2.30666666667\n\n\nExcept for the mean VIF value (2.307) which should not exceed 1, all diagnostics are acceptable. We will now test whether the sample size is sufficient for our model. With respect to the minimal sample size and based on Green, Field, Miles, and Field (273–74) offer the following rules of thumb for an adequate sample size (k = number of predictors; categorical predictors with more than two levels should be recoded as dummy variables):\n\nif you are interested in the overall model: 50 + 8k (k = number of predictors)\nif you are interested in individual predictors: 104 + k\nif you are interested in both: take the higher value!\n\n\n\nEvaluation of Sample Size\nAfter performing the diagnostics, we will now test whether the sample size is adequate and what the values of R would be based on a random distribution in order to be able to estimate how likely a \\(\\beta\\)-error is given the present sample size (see Field, Miles, and Field, 274). Beta errors (or \\(\\beta\\)-errors) refer to the erroneous assumption that a predictor is not significant (based on the analysis and given the sample) although it does have an effect in the population. In other words, \\(\\beta\\)-error means to overlook a significant effect because of weaknesses of the analysis. The test statistics ranges between 0 and 1 where lower values are better. If the values approximate 1, then there is serious concern as the model is not reliable given the sample size. In such cases, unfortunately, the best option is to increase the sample size.\n\n# load functions\nsource(\"https://slcladal.github.io/rscripts/SampleSizeMLR.r\")\nsource(\"https://slcladal.github.io/rscripts/ExpR.r\")\n# check if sample size is sufficient\nsmplesz(m2.mlr)\n\n[1] \"Sample too small: please increase your sample by  9  data points\"\n\n# check beta-error likelihood\nexpR(m2.mlr)\n\n[1] \"Based on the sample size expect a false positive correlation of 0.0309 between the predictors and the predicted\"\n\n\nThe function smplesz reports that the sample size is insufficient by 9 data points according to Green. The likelihood of \\(\\beta\\)-errors, however, is very small (0.0309). As a last step, we summarize the results of the regression analysis.\n\n# tabulate model results\nsjPlot::tab_model(m0.glm, m2.glm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nmoney\nmoney\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n88.12\n78.72 – 97.52\n&lt;0.001\n99.15\n92.10 – 106.21\n&lt;0.001\n\n\nstatus [Single]\n\n\n\n55.85\n45.78 – 65.93\n&lt;0.001\n\n\nattraction\n[NotInterested]\n\n\n\n-47.66\n-57.63 – -37.69\n&lt;0.001\n\n\nstatus [Single] ×\nattraction\n[NotInterested]\n\n\n\n-59.46\n-73.71 – -45.21\n&lt;0.001\n\n\nObservations\n98\n98\n\n\nR2\n-0.000\n0.857\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values in this report is incorrect! As we have seen above, and is also shown in the table below, the correct R2 values are: multiple R2 0.8574, adjusted R2 0.8528.\n\n\n\n\n\n\nAdditionally, we can inspect the summary of the regression model as shown below to extract additional information.\n\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.000000000000000222 ***\nstatusSingle                         &lt; 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,   Adjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: &lt; 0.0000000000000002220446\n\n\nAlthough Field, Miles, and Field suggest that the main effects of the predictors involved in the interaction should not be interpreted, they are interpreted here to illustrate how the results of a multiple linear regression can be reported.\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(m2.mlr)\n\nWe fitted a linear model (estimated using OLS) to predict money with status and\nattraction (formula: money ~ (status + attraction)^2). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.86,\nF(3, 94) = 188.36, p &lt; .001, adj. R2 = 0.85). The model's intercept,\ncorresponding to status = Relationship and attraction = Interested, is at 99.15\n(95% CI [92.01, 106.30], t(94) = 27.56, p &lt; .001). Within this model:\n\n  - The effect of status [Single] is statistically significant and positive (beta\n= 55.85, 95% CI [45.65, 66.06], t(94) = 10.87, p &lt; .001; Std. beta = 1.19, 95%\nCI [0.97, 1.41])\n  - The effect of attraction [NotInterested] is statistically significant and\nnegative (beta = -47.66, 95% CI [-57.76, -37.56], t(94) = -9.37, p &lt; .001; Std.\nbeta = -1.02, 95% CI [-1.23, -0.80])\n  - The effect of status [Single] × attraction [NotInterested] is statistically\nsignificant and negative (beta = -59.46, 95% CI [-73.89, -45.03], t(94) =\n-8.18, p &lt; .001; Std. beta = -1.27, 95% CI [-1.58, -0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nWe can use this output to write up a final report:\nA multiple linear regression was fitted to the data using an automated, step-wise, AIC-based (Akaike’s Information Criterion) procedure. The model fitting arrived at a final minimal model. During the model diagnostics, two outliers were detected and removed. Further diagnostics did not find other issues after the removal.\nThe final minimal adequate regression model is based on 98 data points and performs highly significantly better than a minimal baseline model (multiple R2: .857, adjusted R2: .853, F-statistic (3, 94): 154.4, AIC: 850.4, BIC: 863.32, p&lt;.001\\(***\\)). The final minimal adequate regression model reports attraction and status as significant main effects. The relationship status of men correlates highly significantly and positively with the amount of money spend on the women’s presents (SE: 5.14, t-value: 10.87, p&lt;.001\\(***\\)). This shows that men spend 156.8 dollars on presents if they are single while they spend 99,15 dollars if they are in a relationship. Whether men are attracted to women also correlates highly significantly and positively with the money they spend on women (SE: 5.09, t-values: -9.37, p&lt;.001\\(***\\)). If men are not interested in women, they spend 47.66 dollar less on a present for women compared with women the men are interested in.\nFurthermore, the final minimal adequate regression model reports a highly significant interaction between relationship status and attraction (SE: 7.27, t-value: -8.18, p&lt;.001\\(***\\)): If men are single but they are not interested in a women, a man would spend only 59.46 dollars on a present compared to all other constellations."
  },
  {
    "objectID": "tutorials/regression/regression.html#multiple-binomial-logistic-regression",
    "href": "tutorials/regression/regression.html#multiple-binomial-logistic-regression",
    "title": "Introduction",
    "section": "Multiple Binomial Logistic Regression",
    "text": "Multiple Binomial Logistic Regression\nLogistic regression is a multivariate analysis technique that builds on and is very similar in terms of its implementation to linear regression but logistic regressions take dependent variables that represent nominal rather than numeric scaling (Harrell Jr). The difference requires that the linear regression must be modified in certain ways to avoid producing non-sensical outcomes. The most fundamental difference between logistic and linear regressions is that logistic regression work on the probabilities of an outcome (the likelihood), rather than the outcome itself. In addition, the likelihoods on which the logistic regression works must be logged (logarithmized) in order to avoid produce predictions that produce values greater than 1 (instance occurs) and 0 (instance does not occur). You can check this by logging the values from -10 to 10 using the plogis function as shown below.\n\nround(plogis(-10:10), 5)\n\n [1] 0.00005 0.00012 0.00034 0.00091 0.00247 0.00669 0.01799 0.04743 0.11920\n[10] 0.26894 0.50000 0.73106 0.88080 0.95257 0.98201 0.99331 0.99753 0.99909\n[19] 0.99966 0.99988 0.99995\n\n\nIf we visualize these logged values, we get an S-shaped curve which reflects the logistic function.\n\n\n\n\n\n\n\n\n\nTo understand what this mean, we will use a very simple example. In this example, we want to see whether the height of men affect their likelihood of being in a relationship. The data we use represents a data set consisting of two variables: height and relationship.\n\n\n\n\n\n\n\n\n\nThe left panel of the Figure above shows that a linear model would predict values for the relationship status, which represents a factor (0 = Single and 1 = In a Relationship), that are nonsensical because values above 1 or below 0 do not make sense. In contrast to a linear regression, which predicts actual values, such as the frequencies of prepositions in a certain text, a logistic regression predicts probabilities of events (for example, being in a relationship) rather than actual values. The center panel shows the predictions of a logistic regression and we see that a logistic regression also has an intercept and a (very steep) slope but that the regression line also predicts values that are above 1 and below 0. However, when we log the predicted values we these predicted values are transformed into probabilities with values between 0 and 1. And the logged regression line has a S-shape which reflects the logistic function. Furthermore, we can then find the optimal line (the line with the lowest residual deviance) by comparing the sum of residuals - just as we did for a simple linear model and that way, we find the regression line for a logistic regression.\n\nExample 1: EH in Kiwi English\nTo exemplify how to implement a logistic regression in R (see Agresti; Agresti and Kateri) for very good and thorough introductions to this topic], we will analyze the use of the discourse particle eh in New Zealand English and test which factors correlate with its occurrence. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender, ethnicity, and age of that speaker and whether or not the speech unit contained an eh. To begin with, we clean the current work space, set option, install and activate relevant packages, load customized functions, and load the example data set.\n\n# load data\nblrdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/bld.rda\", \"rb\"))\n\n\n\nIDGenderAgeEthnicityEH&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha0\n\n\nThe summary of the data show that the data set contains 25,821 observations of five variables. The variable ID contains strings that represent a combination file and speaker of a speech unit. The second variable represents the gender, the third the age, and the fourth the ethnicity of speakers. The fifth variable represents whether or not a speech unit contained the discourse particle eh.\nNext, we factorize the variables in our data set. In other words, we specify that the strings represent variable levels and define new reference levels because as a default R will use the variable level which first occurs in alphabet ordering as the reference level for each variable, we redefine the variable levels for Age and Ethnicity.\n\nblrdata &lt;- blrdata %&gt;%\n  # factorize variables\n  dplyr::mutate(Age = factor(Age),\n                Gender = factor(Gender),\n                Ethnicity = factor(Ethnicity),\n                ID = factor(ID),\n                EH = factor(EH)) %&gt;%\n  # relevel Age (Reference = Young) and Ethnicity (Reference= Pakeha))\n  dplyr::mutate(Age = relevel(Age, \"Young\"),\n                Ethnicity = relevel(Ethnicity, \"Pakeha\"))\n\nAfter preparing the data, we will now plot the data to get an overview of potential relationships between variables.\n\nblrdata %&gt;%\n  dplyr::mutate(EH = ifelse(EH == \"0\", 0, 1)) %&gt;%\n  ggplot(aes(Age, EH, color = Gender)) +\n  facet_wrap(~Ethnicity) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of eh\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nWith respect to main effects, the Figure above indicates that men use eh more frequently than women, that young speakers use it more frequently compared with old speakers, and that speakers that are descendants of European settlers (Pakeha) use eh more frequently compared with Maori (the native inhabitants of New Zealand).\nThe plots in the lower panels do not indicate significant interactions between use of eh and the Age, Gender, and Ethnicity of speakers. In a next step, we will start building the logistic regression model.\n\n\nModel Building\nAs a first step, we need to define contrasts and use the datadist function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. Field, Miles, and Field (414–27) and Gries provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more.\n\n# set contrasts\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\n# extract distribution summaries for all potential variables\nblrdata.dist &lt;- datadist(blrdata)\n# store distribution summaries for all potential variables\noptions(datadist = \"blrdata.dist\")\n\nNext, we generate a minimal model that predicts the use of eh solely based on the intercept.\n\n# baseline glm model\nm0.blr = glm(EH ~ 1, family = binomial, data = blrdata)\n\n\n\nModel fitting\nWe will now start with the model fitting procedure. In the present case, we will use a manual step-wise step-up procedure during which predictors are added to the model if they significantly improve the model fit. In addition, we will perform diagnostics as we fit the model at each step of the model fitting process rather than after the fitting.\nWe will test two things in particular: whether the data has incomplete information or complete separation and if the model suffers from (multi-)collinearity.\nIncomplete information or complete separation means that the data does not contain all combinations of the predictor or the dependent variable. This is important because if the data does not contain cases of all combinations, the model will assume that it has found a perfect predictor. In such cases the model overestimates the effect of that that predictor and the results of that model are no longer reliable. For example, if eh was only used by young speakers in the data, the model would jump on that fact and say Ha! If there is an old speaker, that means that that speaker will never ever and under no circumstances say eh* - I can therefore ignore all other factors!*\nMulticollinearity means that predictors correlate and have shared variance. This means that whichever predictor is included first will take all the variance that it can explain and the remaining part of the variable that is shared will not be attributed to the other predictor. This may lead to reporting that a factor is not significant because all of the variance it can explain is already accounted for. However, if the other predictor were included first, then the original predictor would be returned as insignificant. This means that- depending on the order in which predictors are added - the results of the regression can differ dramatically and the model is therefore not reliable. Multicollinearity is actually a very common problem and there are various ways to deal with it but it cannot be ignored (at least in regression analyses).\nWe will start by adding Age to the minimal adequate model.\n\n# check incomplete information\nifelse(min(ftable(blrdata$Age, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\n# add age to the model\nm1.blr = glm(EH ~ Age, family = binomial, data = blrdata)\n# check multicollinearity (vifs should have values of 3 or lower for main effects)\nifelse(max(vif(m1.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\n# check if adding Age significantly improves model fit\nanova(m1.blr, m0.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age\nModel 2: EH ~ 1\n  Resid. Df  Resid. Dev Df     Deviance               Pr(&gt;Chi)    \n1     25819 32376.86081                                           \n2     25820 33007.75469 -1 -630.8938871 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs the data does not contain incomplete information, the vif values are below 3, and adding Age has significantly improved the model fit (the p-value of the ANOVA is lower than .05). We therefore proceed with Age included.\nWe continue by adding Gender. We add a second ANOVA test to see if including Gender affects the significance of other predictors in the model. If this were the case - if adding Gender would cause Age to become insignificant - then we could change the ordering in which we include predictors into our model.\n\nifelse(min(ftable(blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm2.blr &lt;- update(m1.blr, . ~ . +Gender)\nifelse(max(vif(m2.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m2.blr, m1.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender\nModel 2: EH ~ Age\n  Resid. Df  Resid. Dev Df    Deviance               Pr(&gt;Chi)    \n1     25818 32139.54089                                          \n2     25819 32376.86081 -1 -237.319914 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m2.blr, test = \"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: EH\n          LR Chisq Df             Pr(&gt;Chisq)    \nAge    668.6350712  1 &lt; 0.000000000000000222 ***\nGender 237.3199140  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, including Gender significantly improves model fit and the data does not contain incomplete information or complete separation. Also, including Gender does not affect the significance of Age. Now, we include Ethnicity.\n\nifelse(min(ftable(blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm3.blr &lt;- update(m2.blr, . ~ . +Ethnicity)\nifelse(max(vif(m3.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m3.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(&gt;Chi)\n1     25817 32139.27988                          \n2     25818 32139.54089 -1 -0.2610145356  0.60942\n\n\nSince adding Ethnicity does not significantly improve the model fit, we do not need to test if its inclusion affects the significance of other predictors. We continue without Ethnicity and include the interaction between Age and Gender.\n\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm4.blr &lt;- update(m2.blr, . ~ . +Age*Gender)\nifelse(max(vif(m4.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m4.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Age:Gender\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(&gt;Chi)\n1     25817 32139.41665                          \n2     25818 32139.54089 -1 -0.1242399135  0.72448\n\n\nThe interaction between Age and Gender is not significant which means that men and women do not behave differently with respect to their use of EH as they age. Also, the data does not contain incomplete information and the model does not suffer from multicollinearity - the predictors are not collinear. We can now include if there is a significant interaction between Age and Ethnicity.\n\nifelse(min(ftable(blrdata$Age, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm5.blr &lt;- update(m2.blr, . ~ . +Age*Ethnicity)\nifelse(max(vif(m5.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m5.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Age:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df    Deviance Pr(&gt;Chi)\n1     25816 32136.47224                        \n2     25818 32139.54089 -2 -3.06865451   0.2156\n\n\nAgain, no incomplete information or multicollinearity and no significant interaction. Now, we test if there exists a significant interaction between Gender and Ethnicity.\n\nifelse(min(ftable(blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm6.blr &lt;- update(m2.blr, . ~ . +Gender*Ethnicity)\nifelse(max(vif(m6.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m6.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Gender:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(&gt;Chi)\n1     25816 32139.26864                          \n2     25818 32139.54089 -2 -0.2722521853  0.87273\n\n\nAs the interaction between Gender and Ethnicity is not significant, we continue without it. In a final step, we include the three-way interaction between Age, Gender, and Ethnicity.\n\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm7.blr &lt;- update(m2.blr, . ~ . +Gender*Ethnicity)\nifelse(max(vif(m7.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m7.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Gender:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(&gt;Chi)\n1     25816 32139.26864                          \n2     25818 32139.54089 -2 -0.2722521853  0.87273\n\n\nWe have found our final minimal adequate model because the 3-way interaction is also insignificant. As we have now arrived at the final minimal adequate model (m2.blr), we generate a final minimal model using the lrm function.\n\nm2.lrm &lt;- lrm(EH ~ Age+Gender, data = blrdata, x = T, y = T, linear.predictors = T)\nm2.lrm\n\nLogistic Regression Model\n\nlrm(formula = EH ~ Age + Gender, data = blrdata, x = T, y = T, \n    linear.predictors = T)\n\n                       Model Likelihood        Discrimination    Rank Discrim.    \n                             Ratio Test               Indexes          Indexes    \nObs         25821    LR chi2     868.21        R2       0.046    C       0.602    \n 0          17114    d.f.             2      R2(2,25821)0.033    Dxy     0.203    \n 1           8707    Pr(&gt; chi2) &lt;0.0001    R2(2,17312.8)0.049    gamma   0.302    \nmax |deriv| 3e-10                              Brier    0.216    tau-a   0.091    \n\n             Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept    -0.2324 0.0223 -10.44 &lt;0.0001 \nAge=Old      -0.8305 0.0335 -24.78 &lt;0.0001 \nGender=Women -0.4201 0.0273 -15.42 &lt;0.0001 \n\n\n\nanova(m2.lrm)\n\n                Wald Statistics          Response: EH \n\n Factor     Chi-Square d.f. P     \n Age        614.04     1    &lt;.0001\n Gender     237.65     1    &lt;.0001\n TOTAL      802.65     2    &lt;.0001\n\n\nAfter fitting the model, we validate the model to avoid arriving at a final minimal model that is overfitted to the data at hand.\n\n\nModel Validation\nTo validate a model, you can apply the validate function and apply it to a saturated model. The output of the validate function shows how often predictors are retained if the sample is re-selected with the same size but with placing back drawn data points. The execution of the function requires some patience as it is rather computationally expensive and it is, therefore, commented out below.\n\n# model validation (remove # to activate: output too long for website)\nm7.lrm &lt;- lrm(EH ~ (Age+Gender+Ethnicity)^3, data = blrdata, x = T, y = T, linear.predictors = T)\n#validate(m7.lrm, bw = T, B = 200)\n\nThe validate function shows that retaining two predictors (Age and Gender) is the best option and thereby confirms our final minimal adequate model as the best minimal model. In addition, we check whether we need to include a penalty for data points because they have too strong of an impact of the model fit. To see whether a penalty is warranted, we apply the pentrace function to the final minimal adequate model.\n\npentrace(m2.lrm, seq(0, 0.8, by = 0.05)) # determine penalty\n\n\nBest penalty:\n\n penalty            df\n     0.8 1.99925395138\n\n penalty            df           aic           bic         aic.c\n    0.00 2.00000000000 864.213801108 847.895914321 864.213336316\n    0.05 1.99995335085 864.213893816 847.896387637 864.213429042\n    0.10 1.99990670452 864.213985335 847.896859740 864.213520579\n    0.15 1.99986006100 864.214075641 847.897330609 864.213610904\n    0.20 1.99981342030 864.214164764 847.897800270 864.213700044\n    0.25 1.99976678241 864.214252710 847.898268733 864.213788009\n    0.30 1.99972014734 864.214339446 847.898735961 864.213874762\n    0.35 1.99967351509 864.214424993 847.899201978 864.213960327\n    0.40 1.99962688564 864.214509360 847.899666792 864.214044712\n    0.45 1.99958025902 864.214592526 847.900130382 864.214127896\n    0.50 1.99953363520 864.214674504 847.900592761 864.214209892\n    0.55 1.99948701420 864.214755279 847.901053914 864.214290685\n    0.60 1.99944039601 864.214834874 847.901513865 864.214370299\n    0.65 1.99939378063 864.214913276 847.901972599 864.214448719\n    0.70 1.99934716807 864.214990480 847.902430112 864.214525941\n    0.75 1.99930055832 864.215066506 847.902886425 864.214601985\n    0.80 1.99925395138 864.215141352 847.903341534 864.214676849\n\n\nThe values are so similar that a penalty is unnecessary. In a next step, we rename the final models.\n\nlr.glm &lt;- m2.blr  # rename final minimal adequate glm model\nlr.lrm &lt;- m2.lrm  # rename final minimal adequate lrm model\n\nNow, we calculate a Model Likelihood Ratio Test to check if the final model performs significantly better than the initial minimal base-line model. The result of this test is provided as a default if we call a summary of the lrm object.\n\nmodelChi &lt;- lr.glm$null.deviance - lr.glm$deviance\nchidf &lt;- lr.glm$df.null - lr.glm$df.residual\nchisq.prob &lt;- 1 - pchisq(modelChi, chidf)\nmodelChi; chidf; chisq.prob\n\n[1] 868.213801105\n\n\n[1] 2\n\n\n[1] 0\n\n\nThe code above provides three values: a \\(\\chi\\)2, the degrees of freedom, and a p-value. The p-value is lower than .05 and the results of the Model Likelihood Ratio Test therefore confirm that the final minimal adequate model performs significantly better than the initial minimal base-line model. Another way to extract the model likelihood test statistics is to use an ANOVA to compare the final minimal adequate model to the minimal base-line model.\nA handier way to get these statistics is by performing an ANOVA on the final minimal model which, if used this way, is identical to a Model Likelihood Ratio test.\n\nanova(m0.glm, lr.glm, test = \"Chi\") # Model Likelihood Ratio Test\n\nWarning in anova.glmlist(c(list(object), dotargs), dispersion = dispersion, :\nmodels with response '\"EH\"' removed because response differs from model 1\n\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: money\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df  Resid. Dev Pr(&gt;Chi)\nNULL                    97 213227.0608         \n\n\nIn a next step, we calculate pseudo-R2 values which represent the amount of residual variance that is explained by the final minimal adequate model. We cannot use the ordinary R2 because the model works on the logged probabilities rather than the values of the dependent variable.\n\n# calculate pseudo R^2\n# number of cases\nncases &lt;- length(fitted(lr.glm))\nR2.hl &lt;- modelChi/lr.glm$null.deviance\nR.cs &lt;- 1 - exp ((lr.glm$deviance - lr.glm$null.deviance)/ncases)\nR.n &lt;- R.cs /( 1- ( exp (-(lr.glm$null.deviance/ ncases))))\n# function for extracting pseudo-R^2\nlogisticPseudoR2s &lt;- function(LogModel) {\n  dev &lt;- LogModel$deviance\n    nullDev &lt;- LogModel$null.deviance\n    modelN &lt;-  length(LogModel$fitted.values)\n    R.l &lt;-  1 -  dev / nullDev\n    R.cs &lt;- 1- exp ( -(nullDev - dev) / modelN)\n    R.n &lt;- R.cs / ( 1 - ( exp (-(nullDev / modelN))))\n    cat(\"Pseudo R^2 for logistic regression\\n\")\n    cat(\"Hosmer and Lemeshow R^2  \", round(R.l, 3), \"\\n\")\n    cat(\"Cox and Snell R^2        \", round(R.cs, 3), \"\\n\")\n    cat(\"Nagelkerke R^2           \", round(R.n, 3),    \"\\n\") }\nlogisticPseudoR2s(lr.glm)\n\nPseudo R^2 for logistic regression\nHosmer and Lemeshow R^2   0.026 \nCox and Snell R^2         0.033 \nNagelkerke R^2            0.046 \n\n\nThe low pseudo-R2 values show that our model has very low explanatory power. For instance, the value of Hosmer and Lemeshow R2 (0.026) “is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables” (Field, Miles, and Field, 317). In essence, all the pseudo-R2 values are measures of how substantive the model is (how much better it is compared to a baseline model). Next, we extract the confidence intervals for the coefficients of the model.\n\n# extract the confidence intervals for the coefficients\nconfint(lr.glm)\n\n                      2.5 %          97.5 %\n(Intercept) -0.276050866644 -0.188778707888\nAgeOld      -0.896486392283 -0.765095825337\nGenderWomen -0.473530977599 -0.366703827312\n\n\nDespite having low explanatory and predictive power, the age of speakers and their gender are significant as the confidence intervals of the coefficients do not overlap with 0.\n\n\nEffect Size\nIn a next step, we compute odds ratios and their confidence intervals. Odds Ratios represent a common measure of effect size and can be used to compare effect sizes across models. Odds ratios rang between 0 and infinity. Values of 1 indicate that there is no effect. The further away the values are from 1, the stronger the effect. If the values are lower than 1, then the variable level correlates negatively with the occurrence of the outcome (the probability decreases) while values above 1 indicate a positive correlation and show that the variable level causes an increase in the probability of the outcome (the occurrence of EH).\n\nexp(lr.glm$coefficients) # odds ratios\n\n   (Intercept)         AgeOld    GenderWomen \n0.792642499264 0.435815384592 0.656972294902 \n\nexp(confint(lr.glm))     # confidence intervals of the odds ratios\n\nWaiting for profiling to be done...\n\n\n                     2.5 %         97.5 %\n(Intercept) 0.758774333475 0.827969709589\nAgeOld      0.408000698617 0.465289342330\nGenderWomen 0.622799290895 0.693014866728\n\n\nThe odds ratios confirm that older speakers use eh significantly less often compared with younger speakers and that women use eh less frequently than men as the confidence intervals of the odds rations do not overlap with 1. In a next step, we calculate the prediction accuracy of the model.\n\n\nPrediction Accuracy\nIn order to calculate the prediction accuracy of the model, we generate a variable called Prediction that contains the predictions of pour model and which we add to the data. Then, we use the confusionMatrix function from the caret package (Kuhn 2021) to extract the prediction accuracy.\n\n# create variable with contains the prediction of the model\nblrdata &lt;- blrdata %&gt;%\n  dplyr::mutate(Prediction = predict(lr.glm, type = \"response\"),\n                Prediction = ifelse(Prediction &gt; .5, 1, 0),\n                Prediction = factor(Prediction, levels = c(\"0\", \"1\")),\n                EH = factor(EH, levels = c(\"0\", \"1\")))\n# create a confusion matrix with compares observed against predicted values\ncaret::confusionMatrix(blrdata$Prediction, blrdata$EH)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 17114  8707\n         1     0     0\n                                                    \n               Accuracy : 0.66279385                \n                 95% CI : (0.656990096, 0.668560948)\n    No Information Rate : 0.66279385                \n    P-Value [Acc &gt; NIR] : 0.5029107                 \n                                                    \n                  Kappa : 0                         \n                                                    \n Mcnemar's Test P-Value : &lt; 0.00000000000000022     \n                                                    \n            Sensitivity : 1.00000000                \n            Specificity : 0.00000000                \n         Pos Pred Value : 0.66279385                \n         Neg Pred Value :        NaN                \n             Prevalence : 0.66279385                \n         Detection Rate : 0.66279385                \n   Detection Prevalence : 1.00000000                \n      Balanced Accuracy : 0.50000000                \n                                                    \n       'Positive' Class : 0                         \n                                                    \n\n\nWe can see that out model has never predicted the use of eh which is common when dealing with rare phenomena. This is expected as the event s so rare that the probability of it not occurring substantively outweighs the probability of it occurring. As such, the prediction accuracy of our model is not significantly better compared to the prediction accuracy of the baseline model which is the no-information rate (NIR)) (p = 0.5029).\nWe can use the plot_model function from the sjPlot package (Lüdecke) to visualize the effects.\n\n# predicted probability\nefp1 &lt;- plot_model(lr.glm, type = \"pred\", terms = c(\"Age\"), axis.lim = c(0, 1)) \n# predicted percentage\nefp2 &lt;- plot_model(lr.glm, type = \"pred\", terms = c(\"Gender\"), axis.lim = c(0, 1)) \ngridExtra::grid.arrange(efp1, efp2, nrow = 1)\n\n\n\n\n\n\n\n\nAnd we can also combine the visualization of the effects in a single plot as shown below.\n\nsjPlot::plot_model(lr.glm, type = \"pred\", terms = c(\"Age\", \"Gender\"), axis.lim = c(0, 1)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Predicted Probabilty of eh\", title = \"\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\n\n\nModel Diagnostics\nWe are now in a position to perform model diagnostics and test if the model violates distributional requirements. In a first step, we test for the existence of multicollinearity.\n\n\nMulticollinearity\nMulticollinearity means that predictors in a model can be predicted by other predictors in the model (this means that they share variance with other predictors). If this is the case, the model results are unreliable because the presence of absence of one predictor has substantive effects on at least one other predictor.\nTo check whether the final minimal model contains predictors that correlate with each other, we extract variance inflation factors (VIF). If a model contains predictors that have variance inflation factors (VIF) &gt; 10 the model is unreliable (Myers). Gries shows that a VIF of 10 means that that predictor is explainable (predictable) from the other predictors in the model with an R2 of .9 (a VIF of 5 means that predictor is explainable (predictable) from the other predictors in the model with an R2 of .8).Indeed, predictors with VIF values greater than 4 are usually already problematic but, for large data sets, even VIFs greater than 2 can lead inflated standard errors (Jaeger 2013). Also, VIFs of 2.5 can be problematic (Szmrecsanyi, 215) and (Zuur, Ieno, and Elphick) proposes that variables with VIFs exceeding 3 should be removed.\n\n\n\nNOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See Gries or the excursion below for a more elaborate explanation.\n\n\n\n\n\n\n\n\n\n\nEXCURSION\n\n\n\n\n\n\n`\nWhat is multicollinearity?\n\n\nAnswer\n\n\nDuring the workshop on mixed-effects modeling, we talked about (multi-)collinearity and someone asked if collinearity reflected shared variance (what I thought) or predictability of variables (what the other person thought). Both answers are correct! We will see below why…\n\n\n\n\n(Multi-)collinearity reflects the predictability of predictors based on the values of other predictors!\n\n\n\n\nTo test this, I generate a data set with 4 independent variables a, b, c, and d as well as two potential response variables r1 (which is random) and r2 (where the first 50 data points are the same as in r1 but for the second 50 data points I have added a value of 50 to the data points 51 to 100 from r1). This means that the predictors a and d should both strongly correlate with r2.\n\n# load packages\nlibrary(dplyr)\nlibrary(rms)\n# create data set\n# responses\n# 100 random numbers\nr1 &lt;- rnorm(100, 50, 10)\n# 50 smaller + 50 larger numbers\nr2 &lt;- c(r1[1:50], r1[51:100] + 50)\n# predictors\na &lt;- c(rep(\"1\", 50), rep (\"0\", 50))\nb &lt;- rep(c(rep(\"1\", 25), rep (\"0\", 25)), 2)\nc &lt;- rep(c(rep(\"1\", 10), rep(\"0\", 10)), 5)\nd &lt;- c(rep(\"1\", 47), rep (\"0\", 3), rep (\"0\", 47), rep (\"1\", 3))\n# create data set\ndf &lt;- data.frame(r1, r2, a, b, c, d)\n:::\n::: {.cell} ::: {.cell-output-display}\n\n\nFirst 10 rows of df data.\n\n\nr1\nr2\na\nb\nc\nd\n\n\n\n\n55.3805120133\n55.3805120133\n1\n1\n1\n1\n\n\n50.5169933942\n50.5169933942\n1\n1\n1\n1\n\n\n67.5135734932\n67.5135734932\n1\n1\n1\n1\n\n\n60.1733973728\n60.1733973728\n1\n1\n1\n1\n\n\n72.3678161278\n72.3678161278\n1\n1\n1\n1\n\n\n52.4309002788\n52.4309002788\n1\n1\n1\n1\n\n\n55.6904236277\n55.6904236277\n1\n1\n1\n1\n\n\n44.2366484882\n44.2366484882\n1\n1\n1\n1\n\n\n55.5987335816\n55.5987335816\n1\n1\n1\n1\n\n\n58.3120761737\n58.3120761737\n1\n1\n1\n1\n\n\n\n\n\n\n::: :::\nHere are the visualizations of r1 and r2\n::: {.cell} ::: {.cell-output-display}  :::\n::: {.cell-output-display}  ::: :::\nFit first model\nNow, I fit a first model. As the response is random, we do not expect any of the predictors to have a significant effect and we expect the R2 to be rather low.\n::: {.cell}\nm1 &lt;- lm(r1 ~ a + b + c + d, data = df)\n# inspect model\nsummary(m1)\n::: {.cell-output .cell-output-stdout}\n\nCall:\nlm(formula = r1 ~ a + b + c + d, data = df)\n\nResiduals:\n          Min            1Q        Median            3Q           Max \n-25.076373713  -7.774694324   0.933388851   6.630369791  22.021652947 \n\nCoefficients:\n                Estimate   Std. Error  t value             Pr(&gt;|t|)    \n(Intercept) 46.077236782  2.059313248 22.37505 &lt; 0.0000000000000002 ***\na1           5.957479586  4.593681927  1.29689             0.197812    \nb1           5.148440925  2.098541787  2.45334             0.015977 *  \nc1          -0.213502165  2.188893729 -0.09754             0.922504    \nd1          -5.232737413  4.515342995 -1.15888             0.249410    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.4927089 on 95 degrees of freedom\nMultiple R-squared:  0.075627922, Adjusted R-squared:  0.0367069924 \nF-statistic: 1.94311705 on 4 and 95 DF,  p-value: 0.109586389\n::: :::\nWe now check for (multi-)collinearity using the vif function from the rms package (Harrell Jr 2021). Variables a and d should have high variance inflation factor values (vif-values) because they overlap very much!\n::: {.cell}\n# extract vifs\nrms::vif(m1)\n::: {.cell-output .cell-output-stdout}\n           a1            b1            c1            d1 \n4.79166666667 1.00000000000 1.08796296296 4.62962962963 \n::: :::\nVariables a and d do indeed have high vif-values.\nFit second model\nWe now fit a second model to the response which has higher values for the latter part of the response. Both a and d strongly correlate with the response. But because a and d are collinear, d should not be reported as being significant by the model. The R2 of the model should be rather high (given the correlation between the response r2 and a and d).\n::: {.cell}\nm2 &lt;- lm(r2 ~ a + b + c + d, data = df)\n# inspect model\nsummary(m2)\n::: {.cell-output .cell-output-stdout}\n\nCall:\nlm(formula = r2 ~ a + b + c + d, data = df)\n\nResiduals:\n          Min            1Q        Median            3Q           Max \n-25.076373713  -7.774694324   0.933388851   6.630369791  22.021652947 \n\nCoefficients:\n                 Estimate    Std. Error  t value               Pr(&gt;|t|)    \n(Intercept)  96.077236782   2.059313248 46.65499 &lt; 0.000000000000000222 ***\na1          -44.042520414   4.593681927 -9.58763  0.0000000000000012574 ***\nb1            5.148440925   2.098541787  2.45334               0.015977 *  \nc1           -0.213502165   2.188893729 -0.09754               0.922504    \nd1           -5.232737413   4.515342995 -1.15888               0.249410    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.4927089 on 95 degrees of freedom\nMultiple R-squared:  0.851726565, Adjusted R-squared:  0.845483473 \nF-statistic: 136.427041 on 4 and 95 DF,  p-value: &lt; 0.0000000000000002220446\n::: :::\nAgain, we extract the vif-values.\n::: {.cell}\n# extract vifs\nrms::vif(m2)\n::: {.cell-output .cell-output-stdout}\n           a1            b1            c1            d1 \n4.79166666667 1.00000000000 1.08796296296 4.62962962963 \n::: :::\nThe vif-values are identical which shows that what matters is if the variables are predictable. To understand how we arrive at vif-values, we inspect the model matrix.\n::: {.cell}\n# inspect model matrix\nmm &lt;- model.matrix(m2)\n:::\n::: {.cell} ::: {.cell-output-display}\n\n\nFirst 15 rows of the model matrix.\n\n\n(Intercept)\na1\nb1\nc1\nd1\n\n\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n1\n\n\n\n\n\n\n::: :::\nWe now fit a linear model in which we predict d from the other predictors in the model matrix.\n::: {.cell}\nmt &lt;- lm(mm[,5] ~ mm[,1:4])\nsummary(mt)$r.squared\n::: {.cell-output .cell-output-stdout}\n[1] 0.784\n::: :::\nThe R2 shows that the values of d are explained to 78.4 percent by the values of the other predictors in the model.\nNow, we can write a function (taken from Gries) that converts this R2 value\n::: {.cell}\nR2.to.VIF &lt;- function(some.modelmatrix.r2) {\nreturn(1/(1-some.modelmatrix.r2)) } \nR2.to.VIF(0.784)\n::: {.cell-output .cell-output-stdout}\n[1] 4.62962962963\n::: :::\nThe function outputs the vif-value of d. This shows that the vif-value of d represents its predictability from the other predictors in the model matrix which represents the amount of shared variance between d and the other predictors in the model.\n\n\n`\n\n\nWe now extract and check the VIFs of the model.\n\nvif(lr.glm)\n\n       AgeOld   GenderWomen \n1.00481494539 1.00481494539 \n\n\nIn addition, predictors with 1/VIF values \\(&lt;\\) .1 must be removed (data points with values above .2 are considered problematic) (Menard) and the mean value of VIFs should be \\(~\\) 1 (Bowerman and O’Connell).\n\nmean(vif(lr.glm))\n\n[1] 1.00481494539\n\n\n\nOutlier detection\nIn order to detect potential outliers, we will calculate diagnostic parameters and add these to our data set.\n\ninfl &lt;- influence.measures(lr.glm) # calculate influence statistics\nblrdata &lt;- data.frame(blrdata, infl[[1]], infl[[2]]) # add influence statistics\n\nIn a next step, we use these diagnostic parameters to check if there are data points which should be removed as they unduly affect the model fit.\n\n\nSample Size\nWe now check whether the sample size is sufficient for our analysis (Green).\n\nif you are interested in the overall model: 50 + 8k (k = number of predictors)\nif you are interested in individual predictors: 104 + k\nif you are interested in both: take the higher value!\n\n\n# function to evaluate sample size\nsmplesz &lt;- function(x) {\n  ifelse((length(x$fitted) &lt; (104 + ncol(summary(x)$coefficients)-1)) == TRUE,\n    return(\n      paste(\"Sample too small: please increase your sample by \",\n      104 + ncol(summary(x)$coefficients)-1 - length(x$fitted),\n      \" data points\", collapse = \"\")),\n    return(\"Sample size sufficient\")) }\n# apply unction to model\nsmplesz(lr.glm)\n\n[1] \"Sample size sufficient\"\n\n\nAccording to rule of thumb provided in Green, the sample size is sufficient for our analysis.\n\n\nSummarizing Results\nAs a final step, we summarize our findings in tabulated form.\n\nsjPlot::tab_model(lr.glm)\n\n\n\n\n \nEH\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.79\n0.76 – 0.83\n&lt;0.001\n\n\nAge [Old]\n0.44\n0.41 – 0.47\n&lt;0.001\n\n\nGender [Women]\n0.66\n0.62 – 0.69\n&lt;0.001\n\n\nObservations\n25821\n\n\nR2 Tjur\n0.032\n\n\n\n\n\n\n\n\nA more detailed summary table can be retrieved as follows:\n\n# load function\nsource(\"https://slcladal.github.io/rscripts/blrsummary.r\")\n# calculate accuracy \npredict.acc &lt;- caret::confusionMatrix(blrdata$Prediction, blrdata$EH)\npredict.acc &lt;- predict.acc[3]$overall[[1]]\n# create summary table\nblrsummarytb &lt;- blrsummary(lr.glm, lr.lrm, predict.acc) \n\n\n\nStatisticsEstimateVIFOddsRatioCI(2.5%)CI(97.5%)Std. Errorz valuePr(&gt;|z|)Significance(Intercept)-0.230.790.760.830.02-10.440p &lt; .001***AgeOld-0.8310.440.410.470.03-24.780p &lt; .001***GenderWomen-0.4210.660.620.690.03-15.420p &lt; .001***Model statisticsValueNumber of cases in model25821Observed misses0 :17114Observed successes1 :8707Null deviance33007.75Residual deviance32139.54R2 (Nagelkerke)0.046R2 (Hosmer & Lemeshow)0.026R2 (Cox & Snell)0.033C0.602Somers' Dxy0.203AIC32145.54Prediction accuracy0.66%Model Likelihood Ratio TestModel L.R.: 868.21df: 2p-value: 0sig: p &lt; .001***\n\n\nR2 (Hosmer & Lemeshow)\nHosmer and Lemeshow’s R2 “is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables. It can vary between 0 (indicating that the predictors are useless at predicting the outcome variable) and 1 (indicating that the model predicts the outcome variable perfectly)” (Field, Miles, and Field, 317).\nR2 (Cox & Snell)\n“Cox and Snell’s R2 (1989) is based on the deviance of the model (-2LL(new») and the deviance of the baseline model (-2LL(baseline), and the sample size, n […]. However, this statistic never reaches its theoretical maximum of 1.\nR2 (Nagelkerke)\nSince R2 (Cox & Snell) never reaches its theoretical maximum of 1, Nagelkerke (1991) suggested Nagelkerke’s R2 (Field, Miles, and Field, 317–18).\nSomers’ Dxy\nSomers’ Dxy is a rank correlation between predicted probabilities and observed responses ranges between 0 (randomness) and 1 (perfect prediction). Somers’ Dxy should have a value higher than .5 for the model to be meaningful (Baayen, 204).\nC\nC is an index of concordance between the predicted probability and the observed response. When C takes the value 0.5, the predictions are random, when it is 1, prediction is perfect. A value above 0.8 indicates that the model may have some real predictive capacity (Baayen, 204).\nAkaike information criteria (AIC)\nAkaike information criteria (AlC = -2LL + 2k) provide a value that reflects a ratio between the number of predictors in the model and the variance that is explained by these predictors. Changes in AIC can serve as a measure of whether the inclusion of a variable leads to a significant increase in the amount of variance that is explained by the model. “You can think of this as the price you pay for something: you get a better value of R2, but you pay a higher price, and was that higher price worth it? These information criteria help you to decide. The BIC is the same as the AIC but adjusts the penalty included in the AlC (i.e., 2k) by the number of cases: BlC = -2LL + 2k x log(n) in which n is the number of cases in the model” (Field, Miles, and Field, 318).\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(lr.glm)\n\nWe fitted a logistic model (estimated using ML) to predict EH with Age and\nGender (formula: EH ~ Age + Gender). The model's explanatory power is weak\n(Tjur's R2 = 0.03). The model's intercept, corresponding to Age = Young and\nGender = Men, is at -0.23 (95% CI [-0.28, -0.19], p &lt; .001). Within this model:\n\n  - The effect of Age [Old] is statistically significant and negative (beta =\n-0.83, 95% CI [-0.90, -0.77], p &lt; .001; Std. beta = -0.83, 95% CI [-0.90,\n-0.77])\n  - The effect of Gender [Women] is statistically significant and negative (beta\n= -0.42, 95% CI [-0.47, -0.37], p &lt; .001; Std. beta = -0.42, 95% CI [-0.47,\n-0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n\nWe can use this output to write up a final report:\nWe fitted a logistic model (estimated using ML) to predict the use of the utterance-final discourse particle eh with Age and Gender (formula: EH ~ Age + Gender). The model’s explanatory power is weak (Tjur’s R2 = 0.03). The model’s intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p &lt; .001). Within this model:\n\nThe effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p &lt; .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\nThe effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p &lt; .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using\n\n\nOrdinal Regression\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable (Agresti). For this reason, ordinal regression is one of the key methods in analysing Likert data.\nTo see how an ordinal regression is implemented in R, we load and inspect the ´ordinaldata´ data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students’ final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (very likely, somewhat likely, and unlikely) and reflects the committees’ assessment of whether the student is likely to succeed in the program.\n\n# load data\nordata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/ord.rda\", \"rb\")) %&gt;%\n  dplyr::rename(Recommend = 1, \n              Internal = 2, \n              Exchange = 3, \n              FinalScore = 4) %&gt;%\n  dplyr::mutate(FinalScore = round(FinalScore, 2))\n\n\n\nRecommendInternalExchangeFinalScorevery likely003.26somewhat likely103.21unlikely113.94somewhat likely002.81somewhat likely002.53unlikely012.59somewhat likely002.56somewhat likely002.73unlikely003.00somewhat likely103.50unlikely113.65somewhat likely002.84very likely013.90somewhat likely002.68unlikely103.57\n\n\nIn a first step, we need to relevel the ordinal variable to represent an ordinal factor (or a progression from “unlikely” over “somewhat likely” to “very likely”. And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n# relevel data\nordata &lt;- ordata %&gt;%\n  dplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %&gt;%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %&gt;%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n\nNow that the dependent variable is releveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n1.900000 2.720000 2.990000 2.998925 3.270000 4.000000 \n\n\n[1] 0.397940933861\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n# visualize data\nggplot(ordata, aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\n\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modeling by using the polr function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course.\n\n# fit ordered logit model and store results 'm'\nm.or &lt;- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n# summarize model\nsummary(m.or)\n\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                          Value  Std. Error     t value\nInternalInternal   1.0476639736 0.265789132 3.941711114\nExchangeNoExchange 0.0586810805 0.297858820 0.197009713\nFinalScore         0.6157435960 0.260631273 2.362508493\n\nIntercepts:\n                            Value       Std. Error  t value    \nunlikely|somewhat likely    2.261997617 0.882173598 2.564118471\nsomewhat likely|very likely 4.357441779 0.904467831 4.817685749\n\nResidual Deviance: 717.024871356 \nAIC: 727.024871356 \n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n## store table\n(ctable &lt;- coef(summary(m.or)))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639736459 0.265789131533 3.941711113632\nExchangeNoExchange          0.0586810804767 0.297858819642 0.197009712679\nFinalScore                  0.6157435959832 0.260631273036 2.362508492595\nunlikely|somewhat likely    2.2619976171265 0.882173597890 2.564118471169\nsomewhat likely|very likely 4.3574417786829 0.904467830674 4.817685749458\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n## calculate and store p values\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable &lt;- cbind(ctable, \"p value\" = p))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639736459 0.265789131533 3.941711113632\nExchangeNoExchange          0.0586810804767 0.297858819642 0.197009712679\nFinalScore                  0.6157435959832 0.260631273036 2.362508492595\nunlikely|somewhat likely    2.2619976171265 0.882173597890 2.564118471169\nsomewhat likely|very likely 4.3574417786829 0.904467830674 4.817685749458\n                                        p value\nInternalInternal            0.00008090238230957\nExchangeNoExchange          0.84381993712009706\nFinalScore                  0.01815172553987886\nunlikely|somewhat likely    0.01034382311355037\nsomewhat likely|very likely 0.00000145232867123\n\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n# extract profiled confidence intervals\nci &lt;- confint(m.or)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m.or), ci))\n\n                              OR          2.5 %        97.5 %\nInternalInternal   2.85098336080 1.695837799502 4.81711408248\nExchangeNoExchange 1.06043699275 0.595033205643 1.91977108407\nFinalScore         1.85103250827 1.113625249814 3.09849059341\n\n\nThe odds ratios show that internal students are 2.85 times or 285 percent as likely as non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 times or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant.\n\n\nPoisson Regression\nThis section is based on this tutorials on how to perform a Poisson regression in R.\n\n\n\nNOTEPoisson regressions are used to analyze data where the dependent variable represents counts.\n\n\n\n\n\n\nThis applied particularly to counts that are based on observations of something that is measured in set intervals. For instances the number of pauses in two-minute-long conversations. Poisson regressions are particularly appealing when dealing with rare events, i.e. when something only occurs very infrequently. In such cases, normal linear regressions do not work because the instances that do occur are automatically considered outliers. Therefore, it is useful to check if the data conform to a Poisson distribution.\nHowever, the tricky thing about Poisson regressions is that the data has to conform to the Poisson distribution which is, according to my experience, rarely the case, unfortunately. The Gaussian Normal Distribution is very flexible because it is defined by two parameters, the mean (mu, i.e. \\(\\mu\\)) and the standard deviation (sigma, i.e. \\(\\sigma\\)). This allows the normal distribution to take very different shapes (for example, very high and slim (compressed) or very wide and flat). In contrast, the Poisson is defined by only one parameter (lambda, i.e. \\(\\lambda\\)) which mean that if we have a mean of 2, then the standard deviation is also 2 (actually we would have to say that the mean is \\(\\lambda\\) and the standard deviation is also \\(\\lambda\\) or \\(\\lambda\\) = \\(\\mu\\) = \\(\\sigma\\)). This is much trickier for natural data as this means that the Poisson distribution is very rigid.\n\n\n\n\n\n\n\n\n\nAs we can see, as \\(\\lambda\\) takes on higher values, the distribution becomes wider and flatter - a compressed distribution with a high mean can therefore not be Poisson-distributed. We will now start by loading the data.\n\n# load data\npoissondata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/prd.rda\", \"rb\"))\n\n\n\nIdPausesLanguageAlcohol450German411080Russian41150German44670German421530German40510Russian421640German461330German4020German33530German4610German401280English38160German441060German37890German40\n\n\nWe will clean the data by factorizing Id which is currently considered a numeric variable rather than a factor.\n\n# process data\npoissondata &lt;- poissondata %&gt;%\n  mutate(Id = factor(Id, levels = 1:200, labels = 1:200))\n# inspect data\nstr(poissondata)\n\n'data.frame':   200 obs. of  4 variables:\n $ Id      : Factor w/ 200 levels \"1\",\"2\",\"3\",\"4\",..: 45 108 15 67 153 51 164 133 2 53 ...\n $ Pauses  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Language: chr  \"German\" \"Russian\" \"German\" \"German\" ...\n $ Alcohol : int  41 41 44 42 40 42 46 40 33 46 ...\n\n\nFirst, we check if the conditions for a Poisson regression are met.\n\n# output the results\ngf = vcd::goodfit(poissondata$Pauses, \n                  type= \"poisson\", \n                  method= \"ML\")\n# inspect results\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df            P(&gt; X^2)\nLikelihood Ratio 33.0122916717  5 0.00000374234139957\n\n\nIf the p-values is smaller than .05, then data is not Poisson distributed which means that it differs significantly from a Poisson distribution and is very likely over-dispersed. We will check the divergence from a Poisson distribution visually by plotting the observed counts against the expected counts if the data were Poisson distributed.\n\nplot(gf,main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\nAlthough the goodfit function reported that the data differs significantly from the Poisson distribution, the fit is rather good. We can use an additional Levene’s test to check if variance homogeneity is given.\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\n\n# check homogeneity\nleveneTest(poissondata$Pauses, poissondata$Language, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df  F value        Pr(&gt;F)    \ngroup   2 17.15274 0.00000013571 ***\n      197                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test indicates that variance homogeneity is also violated. Since both the approximation to a Poisson distribution and variance homogeneity are violated, we should switch either to a quasi-Poisson model or a negative binomial model. However, as we are only interested in how to implement a Poisson model here, we continue despite the fact that this could not be recommended if we were actually interested in accurate results based on a reliable model.\nIn a next step, we summarize Progression by inspecting the means and standard deviations of the individual variable levels.\n\n# extract mean and standard deviation\nwith(poissondata, tapply(Pauses, Language, function(x) {\n  sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n\n               English                 German                Russian \n\"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\" \"M (SD) = 0.20 (0.40)\" \n\n\nNow, we visualize the data.\n\n# plot data\nggplot(poissondata, aes(Pauses, fill = Language)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  scale_fill_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\n\n\n\n\n\n# calculate Poisson regression\nm1.poisson &lt;- glm(Pauses ~ Language + Alcohol, family=\"poisson\", data=poissondata)\n# inspect model\nsummary(m1.poisson)\n\n\nCall:\nglm(formula = Pauses ~ Language + Alcohol, family = \"poisson\", \n    data = poissondata)\n\nCoefficients:\n                     Estimate    Std. Error  z value         Pr(&gt;|z|)    \n(Intercept)     -4.1632652529  0.6628774832 -6.28060 0.00000000033728 ***\nLanguageGerman  -0.7140499158  0.3200148750 -2.23130         0.025661 *  \nLanguageRussian -1.0838591456  0.3582529824 -3.02540         0.002483 ** \nAlcohol          0.0701523975  0.0105992050  6.61865 0.00000000003625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.6722345  on 199  degrees of freedom\nResidual deviance: 189.4496199  on 196  degrees of freedom\nAIC: 373.5045031\n\nNumber of Fisher Scoring iterations: 6\n\n\nIn addition to the Estimates for the coefficients, we could also calculate the confidence intervals for the coefficients (LL stands for lower limit and UL for upper limit in the table below).\n\n# calculate model\ncov.m1 &lt;- sandwich::vcovHC(m1.poisson, type=\"HC0\")\n# extract standard error\nstd.err &lt;- sqrt(diag(cov.m1))\n# extract robust estimates\nr.est &lt;- cbind(Estimate= coef(m1.poisson), \n               \"Robust SE\" = std.err,\n               \"Pr(&gt;|z|)\" = 2 * pnorm(abs(coef(m1.poisson)/std.err),\n                                      lower.tail=FALSE),\nLL = coef(m1.poisson) - 1.96 * std.err,\nUL = coef(m1.poisson) + 1.96 * std.err)\n# inspect data\nr.est\n\n                        Estimate       Robust SE                 Pr(&gt;|z|)\n(Intercept)     -4.1632652529178 0.6480942940026 0.0000000001328637743949\nLanguageGerman  -0.7140499157783 0.2986422497410 0.0168031204011183689373\nLanguageRussian -1.0838591456208 0.3210481575684 0.0007354744824167337974\nAlcohol          0.0701523974937 0.0104351647012 0.0000000000178397516955\n                              LL               UL\n(Intercept)     -5.4335300691629 -2.8930004366726\nLanguageGerman  -1.2993887252708 -0.1287111062859\nLanguageRussian -1.7131135344549 -0.4546047567867\nAlcohol          0.0496994746793  0.0906053203082\n\n\nWe can now calculate the p-value of the model.\n\nwith(m1.poisson, cbind(res.deviance = deviance, df = df.residual,\n  p = pchisq(deviance, df.residual, lower.tail=FALSE)))\n\n     res.deviance  df              p\n[1,] 189.44961991 196 0.618227445717\n\n\nNow, we check, if removing Language leads to a significant decrease in model fit.\n\n# remove Language from the model\nm2.poisson &lt;- update(m1.poisson, . ~ . -Language)\n# check if dropping Language causes a significant decrease in model fit\nanova(m2.poisson, m1.poisson, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Pauses ~ Alcohol\nModel 2: Pauses ~ Language + Alcohol\n  Resid. Df  Resid. Dev Df   Deviance   Pr(&gt;Chi)    \n1       198 204.0213018                             \n2       196 189.4496199  2 14.5716819 0.00068517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now calculate robust coefficients using the msm package (Jackson 2011).\n\n# get estimates\ns &lt;- msm::deltamethod(list(~ exp(x1), ~ exp(x2), ~ exp(x3), ~ exp(x4)), \n                                                coef(m1.poisson), cov.m1)\n# exponentiate old estimates dropping the p values\nrexp.est &lt;- exp(r.est[, -3])\n# replace SEs with estimates for exponentiated coefficients\nrexp.est[, \"Robust SE\"] &lt;- s\n# display results\nrexp.est\n\n                       Estimate       Robust SE               LL\n(Intercept)     0.0155566784084 0.0100821945101 0.00436765044896\nLanguageGerman  0.4896571063601 0.1462322998451 0.27269843575906\nLanguageRussian 0.3382875026084 0.1086065794408 0.18030353649622\nAlcohol         1.0726716412682 0.0111935052470 1.05095521026088\n                             UL\n(Intercept)     0.0554097096208\nLanguageGerman  0.8792279322820\nLanguageRussian 0.6346987787641\nAlcohol         1.0948368101199\n\n\n\n# extract predicted values\n(s1 &lt;- data.frame(Alcohol = mean(poissondata$Alcohol),\n  Language = factor(1:3, levels = 1:3, labels = names(table(poissondata$Language)))))\n\n  Alcohol Language\n1  52.645  English\n2  52.645   German\n3  52.645  Russian\n\n# show results\npredict(m1.poisson, s1, type=\"response\", se.fit=TRUE)\n\n$fit\n             1              2              3 \n0.624944591447 0.306008560283 0.211410945109 \n\n$se.fit\n              1               2               3 \n0.0862811728183 0.0883370633684 0.0705010813453 \n\n$residual.scale\n[1] 1\n\n\n\n## calculate and store predicted values\npoissondata$Predicted &lt;- predict(m1.poisson, type=\"response\")\n## order by program and then by math\npoissondata &lt;- poissondata[with(poissondata, order(Language, Alcohol)), ]\n\n\n## create the plot\nggplot(poissondata, aes(x = Alcohol, y = Predicted, colour = Language)) +\n  geom_point(aes(y = Pauses), alpha=.5, \n             position=position_jitter(h=.2)) +\n  geom_line(size = 1) +\n  labs(x = \"Alcohol (ml)\", y = \"Expected number of pauses\") +\n  scale_color_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\n\n\n\n\n\n\nRobust Regression\nRobust regression represent an alternative to simple linear models which can handle overly influential data points (outliers). Robust regressions allow us to retain outliers in the data rather than having to remove them from the data by adding weights (Rousseeuw and Leroy). Thus, robust regressions are used when there are outliers present in the data and we can thus not use traditional models but we have no good argument to remove these data points.\n\n\n\nNOTERobust regressions allow us to handle overly influential data points (outliers) by using weights. Thus, robust regressions enable us to retain all data points.\n\n\n\n\n\n\nWe begin by loading a data set (the mlrdata set which have used for multiple linear regression).\n\n# load data\nrobustdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nWe first fit an ordinary linear model (and although we know from the section on multiple regression that the interaction between status and attraction is significant, we will disregard this for now as this will help to explain the weighing procedure which is the focus of this section).\n\n# create model\nslm &lt;- lm(money ~ status+attraction, data = robustdata)\n# inspect model\nsummary(slm)\n\n\nCall:\nlm(formula = money ~ status + attraction, data = robustdata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-60.87070 -15.78645  -2.61010  13.88770  59.93710 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             114.94950000   4.28993616  26.79515\nstatusSingle             26.10340000   4.95359160   5.26959\nattractionNotInterested -79.25220000   4.95359160 -15.99894\n                                      Pr(&gt;|t|)    \n(Intercept)             &lt; 0.000000000000000222 ***\nstatusSingle                     0.00000082576 ***\nattractionNotInterested &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.767958 on 97 degrees of freedom\nMultiple R-squared:  0.745229335,   Adjusted R-squared:  0.739976331 \nF-statistic: 141.867286 on 2 and 97 DF,  p-value: &lt; 0.0000000000000002220446\n\n\nWe now check whether the model is well fitted using diagnostic plots.\n\n# generate plots\nautoplot(slm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\n\n\n\n\nThe diagnostic plots indicate that there are three outliers in the data (data points 52, 83 and possibly 64). Therefore, we need to evaluate if the outliers severely affect the fit of the model.\n\nrobustdata[c(52, 64, 83),]\n\n   status    attraction  money\n52 Single NotInterested   0.93\n64 Single NotInterested  84.28\n83 Single    Interested 200.99\n\n\nWe can now calculate Cook’s distance and standardized residuals check if the values of the potentially problematic points have unacceptably high values (-2 &lt; ok &lt; 2).\n\nCooksDistance &lt;- cooks.distance(slm)\nStandardizedResiduals &lt;- stdres(slm)\na &lt;- cbind(robustdata, CooksDistance, StandardizedResiduals)\na[CooksDistance &gt; 4/100, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n\n\nWe will calculate the absolute value and reorder the table so that it is easier to check the values.\n\nAbsoluteStandardizedResiduals &lt;- abs(StandardizedResiduals)\na &lt;- cbind(robustdata, CooksDistance, StandardizedResiduals, AbsoluteStandardizedResiduals)\nasorted &lt;- a[order(-AbsoluteStandardizedResiduals), ]\nasorted[1:10, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n78       Single    Interested 188.76 0.0394313968241         1.95572122040\n21 Relationship NotInterested  81.90 0.0369837409025         1.89404933081\n24 Relationship NotInterested  81.56 0.0364414260634         1.88011125419\n   AbsoluteStandardizedResiduals\n52                 2.49535435377\n83                 2.45708203516\n88                 2.16151282221\n84                 2.15782333134\n1                  2.07565427025\n65                 2.03662765573\n67                 1.98907421786\n78                 1.95572122040\n21                 1.89404933081\n24                 1.88011125419\n\n\nAs Cook’s distance and the standardized residuals do have unacceptable values, we re-calculate the linear model as a robust regression and inspect the results\n\n# create robust regression model\nrmodel &lt;- robustbase::lmrob(money ~ status + attraction, data = robustdata)\n# inspect model\nsummary(rmodel)\n\n\nCall:\nrobustbase::lmrob(formula = money ~ status + attraction, data = robustdata)\n \\--&gt; method = \"MM\"\nResiduals:\n         Min           1Q       Median           3Q          Max \n-61.14269796 -15.20405781  -1.48712081  14.43502508  62.42342804 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             113.18405781   3.89777692  29.03811\nstatusSingle             25.38251415   5.08841106   4.98830\nattractionNotInterested -76.49387400   5.06626449 -15.09867\n                                      Pr(&gt;|t|)    \n(Intercept)             &lt; 0.000000000000000222 ***\nstatusSingle                      0.0000026725 ***\nattractionNotInterested &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 22.3497532 \nMultiple R-squared:  0.740716949,   Adjusted R-squared:  0.735370907 \nConvergence in 11 IRWLS iterations\n\nRobustness weights: \n 10 weights are ~= 1. The remaining 90 ones are summarized as\n       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. \n0.415507215 0.856134403 0.947485769 0.889078657 0.986192099 0.998890516 \nAlgorithmic parameters: \n           tuning.chi                    bb            tuning.psi \n1.5476399999999999046 0.5000000000000000000 4.6850610000000001421 \n           refine.tol               rel.tol             scale.tol \n0.0000001000000000000 0.0000001000000000000 0.0000000001000000000 \n            solve.tol              zero.tol           eps.outlier \n0.0000001000000000000 0.0000000001000000000 0.0010000000000000000 \n                eps.x     warn.limit.reject     warn.limit.meanrw \n0.0000000000018189894 0.5000000000000000000 0.5000000000000000000 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nThe output shows that both status and attraction are significant but, as we have seen above, the effect that really matters is the interaction between status and attraction.\nWe will briefly check the weights to understand the process of weighing better. The idea of weighing is to downgrade data points that are too influential while not punishing data points that have a good fit and are thus less influential. This means that the problematic data points should have lower weights than other data points (the maximum is 1 - so points can only be made “lighter”).\n\nhweights &lt;- data.frame(status = robustdata$status, resid = rmodel$resid, weight = rmodel$rweights)\nhweights2 &lt;- hweights[order(rmodel$rweights), ]\nhweights2[1:15, ]\n\n         status          resid         weight\n83       Single  62.4234280426 0.415507214894\n52       Single -61.1426979566 0.434323578289\n88       Single  55.2134280426 0.521220526760\n84       Single  55.1234280426 0.522529106622\n78       Single  50.1934280426 0.593234343069\n65       Single -49.9526979566 0.596626306571\n1  Relationship  49.6398161925 0.601024865582\n67       Single -48.7926979566 0.612874578464\n21 Relationship  45.2098161925 0.661914499988\n24 Relationship  44.8698161925 0.666467581490\n39 Relationship -43.8940578083 0.679427975993\n79       Single  40.8234280426 0.719104361372\n58       Single -40.5226979566 0.722893449575\n89       Single  39.9734280426 0.729766992775\n95       Single  39.8234280426 0.731633375015\n\n\nThe values of the weights support our assumption that those data points that were deemed too influential are made lighter as they now only have weights of 0.415507214894 and 0.434323578289 respectively. This was, however, not the focus of this sections as this section merely served to introduce the concept of weights and how they can be used in the context of a robust linear regression.\n\n\nMixed-Effects Regression\nIn contrast to fixed-effects regression models, mixed-effects models assume a hierarchical data structure in which data points are grouped or nested in higher order categories (e.g. students within classes). Mixed-effects models are rapidly increasing in use in data analysis because they allow us to incorporate hierarchical or nested data structures. Mixed-effects models are, of course, an extension of fixed-effects regression models and also multivariate and come in different types.\nIn the following, we will go over the most relevant and frequently used types of mixed-effect regression models, mixed-effects linear regression models and mixed-effects binomial logistic regression models.\nThe major difference between these types of models is that they take different types of dependent variables. While linear models take numeric dependent variables, logistic models take nominal variables.\n\nLinear Mixed-Effects Regression\nThe following focuses on an extension of ordinary multiple linear regressions: mixed-effects regression linear regression. Mixed-effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. (multi-)collinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of mixed-effects regression modeling are that they are prone to producing high \\(\\beta\\)-errors (see Johnson) and that they require rather large data sets.\n\nIntroduction\nSo far, the regression models that we have used only had fixed-effects. Having only fixed-effects means that all data points are treated as if they are completely independent and thus on the same hierarchical level. However, it is very common that the data is nested in the sense that data points are not independent because they are, for instance produced by the same speaker or are grouped by some other characteristic. In such cases, the data is considered hierarchical and statistical models should incorporate such structural features of the data they work upon. Fortunately, modeling hierarchical or nested data structures is very easy thanks to the lme4 package (Bates et al. 2015).\nWith respect to regression modeling, hierarchical structures are incorporated by what is called random effects. When models only have a fixed-effects structure, then they make use of only a single intercept and/or slope (as in the left panel in the figure below), while mixed effects models have intercepts for each level of a random effect. If the random effect structure represents speakers then this would mean that a mixed-model would have a separate intercept and/or slope for each speaker (in addition to the overall intercept that is shown as an orange line in the figure below).\n\n\n\n\n\n\n\n\n\nThe idea behind regression analysis is expressed formally in the equation below where\\(f_{(x)}\\) is the y-value we want to predict, \\(\\alpha\\) is the intercept (the point where the regression line crosses the y-axis at x = 0), \\(\\beta\\) is the coefficient (the slope of the regression line), and x is the value of a predictor (e.g. 180cm - if we would like to predict the weight of a person based on their height). The \\(\\epsilon\\) is an error term that reflects the difference between the predicted value and the (actually) observed value (\\(\\epsilon\\) is thus a residual that is important as regressions assume that residuals are, e.g., normally distributed).\n\\[\\begin{equation}\nf_{(x)} = \\alpha + \\beta x + \\epsilon\n\\end{equation}\\]\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (\\(x\\)) and add the value of the intercept (point where line crosses the y-axis at x = 0).\nThe equation below represents a formal representation of a mixed-effects regression with varying intercepts (see Winter, 235).\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta x + \\epsilon\n\\end{equation}\\]\nIn this random intercept model, each level of a random variable has a different intercept. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the product of the predictor coefficient and the value of x.\nFinally, the equation below represents a formal representation of a mixed-effects regression with varying intercepts and varying slopes (see Winter, 235).\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta_{i}x + \\epsilon\n\\end{equation}\\]\nIn this last model, each level of a random variable has a different intercept and a different slope. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the coefficient of that random effect level multiplied by the value of x.\n\n\nRandom Effects\nRandom Effects can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to fixed-effects models, that have only 1 intercept and one slope (left panel in the figure above), mixed-effects models can therefore have various random intercepts (center panel) or various random slopes, or both, various random intercepts and various random slopes (right panel).\nWhat features do distinguish random and fixed effects?\n\nRandom effects represent a higher level variable under which data points are grouped. This implies that random effects must be categorical (or nominal but they a´cannot be continuous!) (see Winter, 236).\nRandom effects represent a sample of an infinite number of possible levels. For instance, speakers, trials, items, subjects, or words represent a potentially infinite pool of elements from which many different samples can be drawn. Thus, random effects represent a random sample sample. Fixed effects, on the other hand, typically do not represent a random sample but a fixed set of variable levels (e.g. Age groups, or parts-of-speech).\nRandom effects typically represent many different levels while fixed effects typically have only a few. Zuur, Hilbe, and Ieno propose that a variable may be used as a fixed effect if it has less than 5 levels while it should be treated as a random effect if it has more than 10 levels. Variables with 5 to 10 levels can be used as both. However, this is a rule of thumb and ignores the theoretical reasons (random sample and nestedness) for considering something as a random effect and it also is at odds with the way that repeated measures are models (namely as mixed effects) although they typically only have very few levels.\nFixed effects represent an effect that if we draw many samples, the effect would be consistent across samples (Winter) while random effects should vary for each new sample that is drawn.\n\nIn the following, we will only focus on models with random intercepts because this is the more common method and because including both random intercepts and random slopes requires larger data sets (but have a better fit because intercepts are not forced to be parallel and the lines therefore have a better fit). You should, however, always think about what random effects structure is appropriate for your model - a very recommendable explanation of how to chose which random effects structure is best (and about what the determining factors for this decision are) is give in Winter (241–44). Also, consider the center and the right plots above to understand what is meant by random intercepts and random slopes.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted (Field, Miles, and Field). We test whether including random effects is warranted by comparing a model, that bases its estimates of the depended variable solely on the base intercept (the mean), with a model, that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the random-effect model explains significantly more variance than the simple model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified if they reduce residual deviance.\n\n\nExample: Preposition Use across Time by Genre\nTo explore how to implement a mixed-effects model in R we revisit the preposition data that contains relative frequencies of prepositions in English texts written between 1150 and 1913. As a first step, and to prepare our analysis, we load necessary R packages, specify options, and load as well as provide an overview of the data.\n\n# load data\nlmmdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/lmd.rda\", \"rb\")) %&gt;%\n  # convert date into a numeric variable\n  dplyr::mutate(Date = as.numeric(Date))\n\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nThe data set contains the date when the text was written (Date), the genre of the text (Genre), the name of the text (Text), the relative frequency of prepositions in the text (Prepositions), and the region in which the text was written (Region). We now plot the data to get a first impression of its structure.\n\np1 &lt;- ggplot(lmmdata, aes(x = Date, y = Prepositions)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, color = \"red\", linetype = \"dashed\") +\n  theme_bw() +\n  labs(y = \"Frequency\\n(Prepositions)\")\np2 &lt;- ggplot(lmmdata, aes(x = reorder(Genre, -Prepositions), y = Prepositions)) +\n  geom_boxplot() +\n  theme_bw() + \n  theme(axis.text.x = element_text(angle=90)) +\n  labs(x = \"Genre\", y = \"Frequency\\n(Prepositions)\")\np3 &lt;- ggplot(lmmdata, aes(Prepositions)) +\n  geom_histogram() +\n  theme_bw() + \n  labs(y = \"Count\", x = \"Frequency (Prepositions)\")\ngridExtra::grid.arrange(grobs = list(p1, p2, p3), widths = c(1, 1), layout_matrix = rbind(c(1, 1), c(2, 3)))\n\n\n\n\n\n\n\n\nThe scatter plot in the upper panel indicates that the use of prepositions has moderately increased over time while the boxplots in the lower left panel show that the genres differ quite substantially with respect to their median frequencies of prepositions per text. Finally, the histogram in the lower right panel show that preposition use is distributed normally with a mean of 132.2 prepositions per text.\n\np4 &lt;- ggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  labs(x = \"Year\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\np5 &lt;- ggplot(lmmdata, aes(Region, Prepositions)) +\n  geom_boxplot() +\n  labs(x = \"Region\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\ngridExtra::grid.arrange(p4, p5, nrow = 1)\n\n\n\n\n\n\n\n\n\nggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  facet_wrap(~ Genre, nrow = 4) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs(x = \"Date of composition\", y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\n\n\n\n\nCentering or even scaling numeric variables is useful for later interpretation of regression models: if the date variable were not centered, the regression would show the effects of variables at year 0(!). If numeric variables are centered, other variables are variables are considered relative not to 0 but to the mean of that variable (in this case the mean of years in our data). Centering simply means that the mean of the numeric variable is subtracted from each value.\n\nlmmdata$DateUnscaled &lt;- lmmdata$Date\nlmmdata$Date &lt;- scale(lmmdata$Date, scale = F)\n\n\n\nDateGenreTextPrepositionsRegionDateUnscaled109.8696461825Sciencealbin166.01North1,73684.8696461825Educationanon139.86North1,711181.8696461825PrivateLetterausten130.78North1,808251.8696461825Educationbain151.29North1,878116.8696461825Educationbarclay145.72North1,743281.8696461825Educationbenson120.77North1,908279.8696461825Diarybenson119.17North1,906270.8696461825Philosophyboethja132.96North1,897158.8696461825Philosophyboethri130.49North1,785149.8696461825Diaryboswell135.94North1,776278.8696461825Travelbradley154.20North1,90584.8696461825Educationbrightland149.14North1,711135.8696461825Sermonburton159.71North1,76299.8696461825Sermonbutler157.49North1,726208.8696461825PrivateLettercarlyle124.16North1,835\n\n\nWe now set up a fixed-effects model with the glm function and a mixed-effects model using the glmer function from the lme4 package (Bates et al. 2015) with Genre as a random effect.\n\n# generate models\nm0.glm &lt;- glm(Prepositions ~ 1, family = gaussian, data = lmmdata)\nm0.lmer = lmer(Prepositions ~ 1 + (1|Genre), REML = T, data = lmmdata)\n\nNow that we have created the base-line models, we will test whether including a random effect structure is mathematically justified. It is important to note here that we are not going to test if including a random effect structure is theoretically motivated but simply if it causes a decrease in variance.\n\n\nTesting Random Effects\nAs a first step in the modeling process, we now need to determine whether or not including a random effect structure is justified. We do so by comparing the AIC of the base-line model without random intercepts to the AIC of the model with random intercepts.\n\nAIC(logLik(m0.glm))\n\n[1] 4718.19031114\n\nAIC(logLik(m0.lmer))\n\n[1] 4497.77554693\n\n\nThe inclusion of a random effect structure with random intercepts is justified as the AIC of the model with random intercepts is substantially lower than the AIC of the model without random intercepts.\nWhile I do not how how to test if including a random effect is justified, there are often situations, which require to test exactly which random effect structure is best. When doing this, it is important to use restricted maximum likelihood (REML = TRUE or method = REML) rather than maximum likelihood (see Pinheiro and Bates; Winter, 226).\n\n# generate models with 2 different random effect structures\nma.lmer = lmer(Prepositions ~ Date + (1|Genre), REML = T, data = lmmdata)\nmb.lmer = lmer(Prepositions ~ Date + (1 + Date | Genre), REML = T, data = lmmdata)\n# compare models\nanova(ma.lmer, mb.lmer, test = \"Chisq\", refit = F)\n\nData: lmmdata\nModels:\nma.lmer: Prepositions ~ Date + (1 | Genre)\nmb.lmer: Prepositions ~ Date + (1 + Date | Genre)\n        npar         AIC         BIC       logLik    deviance    Chisq Df\nma.lmer    4 4499.148092 4516.292084 -2245.574046 4491.148092            \nmb.lmer    6 4486.699509 4512.415498 -2237.349755 4474.699509 16.44858  2\n        Pr(&gt;Chisq)    \nma.lmer               \nmb.lmer 0.00026806 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model comparison shows that the model with the more complex random effect structure has a significantly better fit to the data compared with the model with the simpler random effect structure. However, we will continue with the model with the simpler structure because this is just an example.\n\n\n\nNOTEIn a real analysis, we would switch to a model with random intercepts and random slopes for Genre because it has a significantly better fit to the data.\n\n\n\n\n\n\n\n\nModel Fitting\nAfter having determined that including a random effect structure is justified, we can continue by fitting the model and including diagnostics as we go. Including diagnostics in the model fitting process can save time and prevent relying on models which only turn out to be unstable if we would perform the diagnostics after the fact.\nWe begin fitting our model by adding Date as a fixed effect and compare this model to our mixed-effects base-line model to see if Date improved the model fit by explaining variance and if Date significantly correlates with our dependent variable (this means that the difference between the models is the effect (size) of Date!)\n\nm1.lmer &lt;- lmer(Prepositions ~ (1|Genre) + Date, REML = T, data = lmmdata)\nanova(m1.lmer, m0.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm0.lmer: Prepositions ~ 1 + (1 | Genre)\nm1.lmer: Prepositions ~ (1 | Genre) + Date\n        npar         AIC         BIC       logLik    deviance  Chisq Df\nm0.lmer    3 4501.947337 4514.805331 -2247.973668 4495.947337          \nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736 8.9296  1\n        Pr(&gt;Chisq)   \nm0.lmer              \nm1.lmer  0.0028059 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nThe model with Date is the better model (significant p-value and lower AIC). The significant p-value shows that Date correlates significantly with Prepositions (\\(\\chi\\)2(1): 8.929600937904, p = 0.00281) . The \\(\\chi\\)2 value here is labeled Chisq and the degrees of freedom are calculated by subtracting the smaller number of DFs from the larger number of DFs.\nWe now test whether Region should also be part of the final minimal adequate model. The easiest way to add predictors is by using the update function (it saves time and typing).\n\n# generate model\nm2.lmer &lt;- update(m1.lmer, .~.+ Region)\n# test vifs\ncar::vif(m2.lmer)\n\n         Date        Region \n1.20287668037 1.20287668037 \n\n# compare models                \nanova(m2.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm2.lmer: Prepositions ~ (1 | Genre) + Date + Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm2.lmer    5 4494.624343 4516.054333 -2242.312171 4484.624343 2.39339  1\n        Pr(&gt;Chisq)\nm1.lmer           \nm2.lmer    0.12185\n\n\nThree things tell us that Region should not be included:\n\nthe AIC does not decrease,\nthe BIC increases(!), and\nthe p-value is higher than .05.\n\nThis means, that we will continue fitting the model without having Region included. Well… not quite - just as a note on including variables: while Region is not significant as a main effect, it must still be included in a model if it were part of a significant interaction. To test if this is indeed the case, we fit another model with the interaction between Date and Region as predictor.\n\n# generate model\nm3.lmer &lt;- update(m1.lmer, .~.+ Region*Date)\n# extract vifs\ncar::vif(m3.lmer)\n\n         Date        Region   Date:Region \n1.96923042279 1.20324697637 1.78000887980 \n\n# compare models                \nanova(m3.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm3.lmer: Prepositions ~ (1 | Genre) + Date + Region + Date:Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm3.lmer    6 4496.124872 4521.840861 -2242.062436 4484.124872 2.89286  2\n        Pr(&gt;Chisq)\nm1.lmer           \nm3.lmer    0.23541\n\n\nAgain, the high p-value and the increase in AIC and BIC show that we have found our minimal adequate model with only contains Date as a main effect. In a next step, we can inspect the final minimal adequate model, i.e. the most parsimonious (the model that explains a maximum of variance with a minimum of predictors).\n\n# inspect results\nsummary(m1.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Prepositions ~ (1 | Genre) + Date\n   Data: lmmdata\n\nREML criterion at convergence: 4491.1\n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-3.734915441 -0.657038004  0.005865025  0.661298615  3.596659863 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev.  \n Genre    (Intercept) 159.021120 12.6103576\n Residual             228.764179 15.1249522\nNumber of obs: 537, groups:  Genre, 16\n\nFixed effects:\n                   Estimate      Std. Error  t value\n(Intercept) 133.88516211469   3.24749296223 41.22724\nDate          0.01894493515   0.00632363682  2.99589\n\nCorrelation of Fixed Effects:\n     (Intr)\nDate 0.005 \n\n\n\n\nModel Diagnostics\nWe can now evaluate the goodness of fit of the model and check if mathematical requirements and assumptions have been violated. In a first step, we generate diagnostic plots that focus on the random effect structure.\n\nplot(m1.lmer, Genre ~ resid(.), abline = 0 ) # generate diagnostic plots\n\n\n\n\n\n\n\n\nThe plot shows that there are some outliers (points outside the boxes) and that the variability within letters is greater than in other genres we therefore examine the genres in isolation standardized residuals versus fitted values (Pinheiro and Bates, 175).\n\nplot(m1.lmer, resid(., type = \"pearson\") ~ fitted(.) | Genre, id = 0.05, \n     adj = -0.3, pch = 20, col = \"gray40\")\n\n\n\n\n\n\n\n\nThe plot shows the standardized residuals (or Pearson’s residuals) versus fitted values and suggests that there are outliers in the data (the names elements in the plots). To check if these outliers are a cause for concern, we will now use a Levene’s test to check if the variance is distributed homogeneously (homoscedasticity) or whether the assumption of variance homogeneity is violated (due to the outliers).\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\nWe use Levene’s test here merely to check if it substantiates the impressions we got from the visual inspection.\n\n# check homogeneity\nleveneTest(lmmdata$Prepositions, lmmdata$Genre, center = mean)\n\nWarning in leveneTest.default(lmmdata$Prepositions, lmmdata$Genre, center =\nmean): lmmdata$Genre coerced to factor.\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value   Pr(&gt;F)  \ngroup  15 1.74289 0.039906 *\n      521                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test shows that the variance is distributed unevenly across genres which means that we do not simply continue but should either remove problematic data points (outliers) or use a weighing method.\nIn this case, we create a new model which uses weights to compensate for heterogeneity of variance and thus the influence of outliers - which is an alternative to removing the data points and rerunning the analysis (Pinheiro and Bates, 177). However, to do so, we need to use a different function (the lme function) which means that we have to create two models: the old minimal adequate model and the new minimal adequate model with added weights. After we have created these models, we will compare them to see if including weights has improved the fit.\n\n# generate models\nm4.lme = lme(Prepositions ~ Date, random = ~1|Genre, data = lmmdata, method = \"ML\")\nm5.lme &lt;- update(m4.lme, weights = varIdent(form = ~ 1 | Genre))\n# compare models\nanova(m5.lme, m4.lme)\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm5.lme     1 19 4485.84955680 4567.28352060 -2223.92477840                     \nm4.lme     2  4 4495.01773596 4512.16172834 -2243.50886798 1 vs 2 39.1681791556\n       p-value\nm5.lme        \nm4.lme  0.0006\n\n\nThe weight model (m5.lme) that uses weights to account for unequal variance is performing significantly better than the model without weights (m4.lme) and we therefore switch to the weight model and inspect its parameters.\n\n# inspect results\nsummary(m5.lme)        \n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n           AIC          BIC        logLik\n  4485.8495568 4567.2835206 -2223.9247784\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2632323711 14.3422048362\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340715615490  0.869521038482  0.788861254265  0.911719712514 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096572672893  0.978728647599  0.784966066137  0.736986014758  1.190619042239 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929751328  0.974641038177  0.848591142831  0.970869209380  1.086232439777 \nTrialProceeding \n 1.260188475499 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640155217 3.144361414827 520 42.6045221424  0.0000\nDate          0.0217416268 0.005454723299 520  3.9858349603  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-3.319117397700 -0.679728744836  0.014685431575  0.698705968152  3.103872340648 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\nWe can also use an ANOVA display which is more to the point.\n\nanova(m5.lme)          \n\n            numDF denDF        F-value p-value\n(Intercept)     1   520 1813.892139648  &lt;.0001\nDate            1   520   15.886880331  0.0001\n\n\nAs we did before, we now check, whether the final minimal model (with weights) outperforms an intercept-only base-line model.\n\n# generate base-line model\nm0.lme = lme(Prepositions ~ 1, random = ~1|Genre, data = lmmdata, method = \"ML\", weights = varIdent(form = ~ 1 | Genre))\nanova(m0.lme, m5.lme)  # test if date is significant\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm0.lme     1 18 4496.28563021 4573.43359592 -2230.14281511                     \nm5.lme     2 19 4485.84955680 4567.28352060 -2223.92477840 1 vs 2 12.4360734115\n       p-value\nm0.lme        \nm5.lme  0.0004\n\n\nOur final minimal adequate model with weights performs significantly better than an intercept only base-line model. Before doing the final diagnostics, we well inspect the estimates for the random effect structure to check if there are values which require further inspection (e.g. because they are drastically different from all other values).\n\n# extract estimates and sd for fixed and random effects\nintervals(m5.lme, which=\"fixed\")      \n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                        lower              est.             upper\n(Intercept) 127.7983166699800 133.9640155217270 140.1297143734741\nDate          0.0110455977276   0.0217416268231   0.0324376559185\n\n\nThe random effect estimates do not show any outliers or drastically increased or decreased values which means that the random effect structure is fine.\n\n\nEffect Sizes\nWe will now extract effect sizes (in the example: the effect size of Date) and calculate normalized effect size measures (this effect size measure works for all fixed effects). When you have factorial design, you can take the square root of the squared t-value divided by the t-value squared plus the degrees of freedom to calculate the effect size:\n\\[\\begin{equation}\n\nr = \\sqrt{ \\frac{ t^2}{(t^2 + df) } } = \\sqrt{ \\frac{ 3.99^2}{(3.99^2 + 520) } } = 0.172\n\n\\end{equation}\\]\n\n\n\n\nNOTETwo words of warning though: br&gt;1. In our case, the effect we are interested in is not factorial but continuous which means that we should not use this effect size measure. We only show this here as an example for how you can calculate the effect size measure r.2. Only apply this function to main effects that are not involved in interactions as they are meaningless because the amount of variance explained by main effects involved in interactions is unclear (Field, Miles, and Field, 641).\n\n\n\n\n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n&lt;0.001\n\n\nDate\n0.02\n0.01 – 0.03\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.70\n\n\n\nτ00 Genre\n150.39\n\n\nICC\n0.42\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.432\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\nThe marginal R2 (marginal coefficient of determination) represents the variance explained by the fixed effects while the conditional R2 is interpreted as a variance explained by the entire model, including both fixed and random effects (Bartoń).\nThe respective call for the model is:\n\n# extract R2s\nr.squaredGLMM(m1.lmer)\n\n                 R2m            R2c\n[1,] 0.0121971160219 0.417270545268\n\n\nThe effects can be visualized using the plot_model function from the sjPlot package (Lüdecke).\n\nsjPlot::plot_model(m5.lme, type = \"pred\", terms = c(\"Date\")) +\n  # show uncentered date rather than centered date\n  scale_x_continuous(name = \"Date\", \n                     breaks = seq(-500, 300, 100), \n                     labels = seq(1150, 1950, 100))\n\n\n\n\n\n\n\n\nWhile we have already shown that the effect of Date is significant, it is small which means that the number of prepositions per text does not correlate very strongly with time. This suggests that other factors that are not included in the model also impact the frequency of prepositions (and probably more meaningfully, too).\nBefore turning to the diagnostics, we will use the fitted (or predicted) and the observed values with a regression line for the predicted values. This will not only show how good the model fit the data but also the direction and magnitude of the effect.\n\n# extract predicted values\nlmmdata$Predicted &lt;- predict(m5.lme, lmmdata)\n# plot predicted values\nggplot(lmmdata, aes(DateUnscaled, Predicted)) +\n  facet_wrap(~Genre) +\n  geom_point(aes(x = DateUnscaled, y = Prepositions), color = \"gray80\", size = .5) +\n  geom_smooth(aes(y = Predicted), color = \"gray20\", linetype = \"solid\", \n              se = T, method = \"lm\") +\n  guides(color=guide_legend(override.aes=list(fill=NA))) +  \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"top\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  xlab(\"Date of composition\")\n\n\n\n\n\n\n\n\n\n\nModel Diagnostics\nIn addition, we generate diagnostic plots. What we wish to see in the diagnostic plots is a cloud of dots in the middle of the window without any structure. What we do not want to see is a funnel-shaped cloud because this indicates an increase of the errors/residuals with an increase of the predictor(s) (because this would indicate heteroscedasticity) (Pinheiro and Bates, 182).\n\n# start plotting\npar(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns\nplot(m5.lme, pch = 20, col = \"black\", lty = \"dotted\"); par(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nWhat a wonderful unstructured cloud - the lack of structure tells us that the model is “healthy” and does not suffer from heteroscedasticity. We will now create more diagnostic plots to find potential problems (Pinheiro and Bates, 21).\n\n# fitted values by Genre\nplot(m5.lme, form = resid(., type = \"p\") ~ fitted(.) | Genre, abline = 0, \n     cex = .5, pch = 20, col = \"black\")\n\n\n\n\n\n\n\n\nIn contrast to the unweight model, no data points are named which indicates that the outliers do no longer have unwarranted influence on the model. Now, we check the residuals of fitted values against observed values (Pinheiro and Bates, 179). What we would like to see is a straight, upwards going line.\n\n# residuals of fitted values against observed\nqqnorm(m5.lme, pch = 20, col = \"black\")\n\n\n\n\n\n\n\n\nA beautiful, straight line! The qqplot does not indicate any problems. It is, unfortunately, rather common that the dots deviate from the straight line at the very bottom or the very top which means that the model is good at estimating values around the middle of the dependent variable but rather bad at estimating lower or higher values. Next, we check the residuals by “Genre” (Pinheiro and Bates, 179).\n\n# residuals by genre\nqqnorm(m5.lme, ~resid(.) | Genre, pch = 20, col = \"black\" )\n\n\n\n\n\n\n\n\nBeautiful straight lines - perfection! Now, we inspect the observed responses versus the within-group fitted values (Pinheiro and Bates, 178).\n\n# observed responses versus the within-group fitted values\nplot(m5.lme, Prepositions ~ fitted(.), id = 0.05, adj = -0.3, \n     xlim = c(80, 220), cex = .8, pch = 20, col = \"blue\")\n\n\n\n\n\n\n\n\nAlthough some data points are named, the plot does not show any structure, like a funnel, which would have been problematic.\n\n\nReporting Results\nBefore we do the write-up, we have a look at the model summary as this will provide us with at least some of the parameters that we want to report.\n\nsummary(m5.lme)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n           AIC          BIC        logLik\n  4485.8495568 4567.2835206 -2223.9247784\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2632323711 14.3422048362\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340715615490  0.869521038482  0.788861254265  0.911719712514 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096572672893  0.978728647599  0.784966066137  0.736986014758  1.190619042239 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929751328  0.974641038177  0.848591142831  0.970869209380  1.086232439777 \nTrialProceeding \n 1.260188475499 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640155217 3.144361414827 520 42.6045221424  0.0000\nDate          0.0217416268 0.005454723299 520  3.9858349603  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-3.319117397700 -0.679728744836  0.014685431575  0.698705968152  3.103872340648 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n&lt;0.001\n\n\nDate\n0.02\n0.01 – 0.03\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.70\n\n\n\nτ00 Genre\n150.39\n\n\nICC\n0.42\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.432\n\n\n\n\n\n\nWe can now use the information extracted above to write up a final report:\nA mixed-effect linear regression model which contained the genre of texts as random effect was fit to the data in a step-wise-step up procedure. Due to the presence of outliers in the data, weights were included into the model which led to a significantly improved model fit compared to an un-weight model (\\(\\chi\\)2(2): 39.17, p: 0.0006). The final minimal adequate model performed significantly better than an intercept-only base-line model (\\(\\chi\\)2(1): 12.44, p =.0004) and showed that the frequency of prepositions increases significantly but only marginally with the date of composition (Estimate: 0.02, CI: 0.01-0.03, p &lt; .001, marginal R2 = 0.0174, conditional R2 = 0.4324). Neither the region where the text was composed nor a higher order interaction between genre and region significantly correlated with the use of prepositions in the data.\n\n\nRemarks on Prediction\nWhile the number of intercepts, the model reports, and the way how mixed- and fixed-effects arrive at predictions differ, their predictions are extremely similar and almost identical (at least when dealing with a simple random effect structure). Consider the following example where we create analogous fixed and mixed effect models and plot their predicted frequencies of prepositions per genre across the un-centered date of composition. The predictions of the mixed-effects model are plotted as a solid red line, while the predictions of the fixed-effects model are plotted as dashed blue lines.\n\n# create lm model\nm5.lmeunweight &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmePredictions &lt;- fitted(m5.lmeunweight, lmmdata)\nm5.lm &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmPredictions &lt;- fitted(m5.lm, lmmdata)\n# plot predictions\nggplot(lmmdata, aes(x = DateUnscaled, y = lmePredictions, group = Genre)) +\n  geom_line(aes(y = lmmdata$lmePredictions), linetype = \"solid\", color = \"red\") +\n  geom_line(aes(y = lmmdata$lmPredictions), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ Genre, nrow = 4) +\n  theme_bw() +\n  labs(x = \"Date of composition\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\n\n\n\n\nThe predictions overlap almost perfectly which means that the predictions of both are almost identical - irrespective of whether genre is part of the mixed or the fixed effects structure.\n\n\n\nMixed-Effects Binomial Logistic Regression\nWe now turn to an extension of binomial logistic regression: mixed-effects binomial logistic regression. As is the case with linear mixed-effects models logistic mixed effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. multicollinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of regression modeling are that they are prone to producing high \\(\\beta\\)-errors (see Johnson) and that they require rather large data sets.\n\nIntroduction\nAs is the case with linear mixed-effects models, binomial logistic mixed-effect models are multivariate analyses that treat data points as hierarchical or grouped in some way. In other words, they take into account that the data is nested in the sense that data points are produced by the same speaker or are grouped by some other characteristics. In mixed-models, hierarchical structures are modelled as random effects. If the random effect structure represents speakers then this means that a mixed-model would have a separate intercept and/or slope for each speaker.\nRandom Effects in linear models can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to linear mixed-effects models, random effects differ in the position and the slope of the logistic function that is applied to the likelihood of the dependent variable. Random intercepts (upper right panel) or various random slopes (lower left panel), or both, various random intercepts and various random slopes (lower right panel). In the following, we will only focus on models with random intercepts because this is the by far more common method and because including both random intercepts and random slopes requires huge amounts of data. Consider the Figure below to understand what is meant by random intercepts.\n\n\n\n\n\n\n\n\n\nThe upper left panel merely shows the logistic curve representing the predictions of a fixed-effects logistic regression with a single intercept and slope. The upper right panel shows the logistic curves representing the predictions of a of a mixed-effects logistic regression with random intercepts for each level of a grouping variable. The lower left panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with one intercept but random slopes for each level of a grouping variable. The lower right panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with random intercepts and random slopes for each level of a grouping variable.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted (Field, Miles, and Field). We test whether including random effects is warranted by comparing a model, that bases its estimates of the dependent variable solely on the base intercept, with a model that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the mixed-effects model explains significantly more variance than the fixed-effects model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified.\n\n\nExample: Discourse LIKE in Irish English\nIn this example we will investigate which factors correlate with the use of final discourse like (e.g. “The weather is shite, like!”) in Irish English. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender (Gender: Men versus Women) and age of that speaker (Age: Old versus Young), whether the interlocutors were of the same or a different gender (ConversationType: SameGender versus MixedGender), and whether another final discourse like had been used up to three speech units before (Priming: NoPrime versus Prime), whether or not the speech unit contained an final discourse like (SUFLike: 1 = yes, 0 = no. To begin with, we load the data and inspect the structure of the data set,\n\n# load data\nmblrdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/mbd.rda\", \"rb\"))\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-061$CWomenYoungMixedGenderNoPrime0S1A-023$BWomenYoungMixedGenderNoPrime0S1A-054$AWomenYoungSameGenderNoPrime0S1A-090$BWomenYoungMixedGenderNoPrime0S1A-009$BWomenOldSameGenderPrime0S1A-085$EMenYoungMixedGenderPrime1S1A-003$CWomenYoungMixedGenderNoPrime1S1A-084$CWomenYoungSameGenderNoPrime0S1A-076$AWomenYoungSameGenderNoPrime0S1A-083$DMenOldMixedGenderNoPrime1S1A-068$AWomenYoungSameGenderNoPrime0S1A-066$BWomenYoungSameGenderNoPrime0S1A-061$AMenOldMixedGenderNoPrime1S1A-049$AWomenYoungSameGenderNoPrime0S1A-022$BWomenYoungMixedGenderNoPrime0\n\n\nAs all variables except for the dependent variable (SUFlike) are character strings, we factorize the independent variables.\n\n# def. variables to be factorized\nvrs &lt;- c(\"ID\", \"Age\", \"Gender\", \"ConversationType\", \"Priming\")\n# def. vector with variables\nfctr &lt;- which(colnames(mblrdata) %in% vrs)     \n# factorize variables\nmblrdata[,fctr] &lt;- lapply(mblrdata[,fctr], factor)\n# relevel Age (Young = Reference)\nmblrdata$Age &lt;- relevel(mblrdata$Age, \"Young\")\n# order data by ID\nmblrdata &lt;- mblrdata %&gt;%\n  dplyr::arrange(ID)\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderPrime0S1A-001$BWomenOldMixedGenderNoPrime1S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0\n\n\nBefore continuing, a few words about the minimum number of random effect levels and the minimum number of observations per random effect level are in order.\nWhile many data points per random variable level increases statistical power and thus to more robust estimates of the random effects (Austin and Leckie), it has been shown that small numbers of observations per random effect variable level do not cause serious bias and it does not negatively affect the estimates of the fixed-effects coefficients (Bell, Ferron, and Kromrey; Clarke; Clarke and Wheaton; Maas and Hox). The minimum number of observations per random effect variable level is therefore 1.\nIn simulation study, (Bell, Ferron, and Kromrey) tested the impact of random variable levels with only a single observation ranging from 0 to 70 percent. As long as there was a relatively high number of random effect variable levels (500 or more), small numbers of observations had almost no impact on bias and Type 1 error control.\nWe now plot the data to inspect the relationships within the data set.\n\nggplot(mblrdata, aes(Gender, SUFlike, color = Priming)) +\n  facet_wrap(Age~ConversationType) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of discourse like\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nThe upper left panel in the Figure above indicates that men use discourse like more frequently than women. The center right panel suggests that priming significantly increases the likelihood of discourse like being used. The center left panel suggests that speakers use discourse like more frequently in mixed-gender conversations. However, the lower right panel indicates an interaction between gender and conversation type as women appear to use discourse like less frequently in same gender conversations while the conversation type does not seem to have an effect on men. After visualizing the data, we will now turn to the model building process.\n\n\nModel Building\nAs a first step, we need to define contrasts and use the datadist function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. Field, Miles, and Field (414–27) and Gries provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more.\n\n# set options\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\nmblrdata.dist &lt;- datadist(mblrdata)\noptions(datadist = \"mblrdata.dist\")\n\nIn a next step, we generate fixed-effects minimal base-line models and a base-line mixed-model using the glmer function with a random intercept for ID (a lmer-object of the final minimal adequate model will be created later).\n\n# baseline model glm\nm0.glm = glm(SUFlike ~ 1, family = binomial, data = mblrdata) \n# base-line mixed-model\nm0.glmer = glmer(SUFlike ~ (1|ID), data = mblrdata, family = binomial) \n\n\n\nTesting the Random Effect\nNow, we check if including the random effect is permitted by comparing the AICs from the glm to AIC from the glmer model. If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that including random intercepts is justified.\n\naic.glmer &lt;- AIC(logLik(m0.glmer))\naic.glm &lt;- AIC(logLik(m0.glm))\naic.glmer; aic.glm\n\n[1] 1828.49227107\n\n\n[1] 1838.17334856\n\n\nThe AIC of the glmer object is smaller which shows that including the random intercepts is justified. To confirm whether the AIC reduction is sufficient for justifying the inclusion of a random-effect structure, we also test whether the mixed-effects minimal base-line model explains significantly more variance by applying a Model Likelihood Ratio Test to the fixed- and the mixed effects minimal base-line models.\n\n# test random effects\nnull.id = -2 * logLik(m0.glm) + 2 * logLik(m0.glmer)\npchisq(as.numeric(null.id), df=1, lower.tail=F) \n\n[1] 0.000631389572435\n\n# sig m0.glmer better than m0.glm\n\nThe p-value of the Model Likelihood Ratio Test is lower than .05 which shows that the inclusion of the random-effects structure is warranted. We can now continue with the model fitting process.\n\n\nModel Fitting\nThe next step is to fit the model which means that we aim to find the best model, i.e. the minimal adequate model. In this case, we will use the glmulti package to find the model with the lowest Bayesian Information Criterion (BIC) of all possible models. We add ´control = glmerControl(optimizer = “bobyqa”)´ to avoid unnecessary failures to converge.\n\n# wrapper function for linear mixed-models\nglmer.glmulti &lt;- function(formula,data, random=\"\",...){\n  glmer(paste(deparse(formula),random), \n        family = binomial, \n        data=data, \n        control=glmerControl(optimizer=\"bobyqa\"), ...)\n}\n# define formula\nform_glmulti = as.formula(paste(\"SUFlike ~ Gender + Age + ConversationType + Priming\"))\n# multi selection for glmer\nmfit &lt;- glmulti(form_glmulti,random=\"+(1 | ID)\", \n                data = mblrdata, method = \"h\", fitfunc = glmer.glmulti,\n                crit = \"bic\", intercept = TRUE, marginality = FALSE, level = 2)\n\nInitialization...\nTASK: Exhaustive screening of candidate set.\nFitting...\n\n\n\nAfter 50 models:\nBest model: SUFlike~1+Gender+ConversationType+Priming\nCrit= 1696.5877339969\nMean crit= 1753.96253323424\n\n\n\n\n\n\n\n\n\n\nAfter 100 models:\nBest model: SUFlike~1+Gender+ConversationType+Priming\nCrit= 1696.5877339969\nMean crit= 1731.89001011587\n\n\n\n\n\n\n\n\n\nCompleted.\n\n\nWe extract the best 5 models (best is here defined as the models with the lowest BIC).\n\n# extract best models\ntop &lt;- weightable(mfit)\ntop &lt;- top[1:5,]\n# inspect top 5 models\ntop\n\n                                                                                          model\n1                                             SUFlike ~ 1 + Gender + ConversationType + Priming\n2                            SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:Gender\n3 SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:Gender + Priming:ConversationType\n4                                               SUFlike ~ 1 + Gender + Priming + Priming:Gender\n5                  SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:ConversationType\n            bic         weights\n1 1696.58773400 0.2567900971793\n2 1696.76989551 0.2344349735686\n3 1696.76989551 0.2344349735686\n4 1699.62465884 0.0562494681405\n5 1699.83927868 0.0505259299850\n\n\nThe best model is has the formula SUFlike ~ 1 + Gender + ConversationType + Priming and we take this to be our final minimal adequate model, i.e. the most parsimonious model (the model which explains the relatively most variance with lowest number of predictors). Hence, we define our final minimal model and check its output.\n\nmlr.glmer &lt;- glmer(SUFlike ~ (1 | ID) + Gender + ConversationType + Priming, \n                   family = binomial,\n                   control=glmerControl(optimizer=\"bobyqa\"),\n                   data = mblrdata)\n# inspect final minimal adequate model\nsummary(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n   Data: mblrdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1668.6   1696.6   -829.3   1658.6     1995 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.579360650 -0.415523754 -0.330409042 -0.312054134  3.247885856 \n\nRandom effects:\n Groups Name        Variance     Std.Dev.   \n ID     (Intercept) 0.0837517494 0.289398945\nNumber of obs: 2000, groups:  ID, 208\n\nFixed effects:\n                               Estimate   Std. Error  z value\n(Intercept)                -1.067430840  0.149184383 -7.15511\nGenderWomen                -0.642887590  0.175327287 -3.66679\nConversationTypeSameGender -0.536428857  0.148819363 -3.60456\nPrimingPrime                1.866249307  0.163249208 11.43190\n                                         Pr(&gt;|z|)    \n(Intercept)                   0.00000000000083605 ***\nGenderWomen                            0.00024562 ***\nConversationTypeSameGender             0.00031268 ***\nPrimingPrime               &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now test whether the final minimal model performs significantly better than the minimal base-line model, and print the regression summary.\n\n# final model better than base-line model\nsigfit &lt;- anova(mlr.glmer, m0.glmer, test = \"Chi\") \n# inspect\nsigfit\n\nData: mblrdata\nModels:\nm0.glmer: SUFlike ~ (1 | ID)\nmlr.glmer: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n          npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.glmer     2 1828.492271 1839.694076 -912.2461355 1824.492271             \nmlr.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222 165.90905  3\n                      Pr(&gt;Chisq)    \nm0.glmer                            \nmlr.glmer &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# inspect final minimal adequate model\nprint(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n   Data: mblrdata\n      AIC       BIC    logLik  deviance  df.resid \n1668.5832 1696.5877 -829.2916 1658.5832      1995 \nRandom effects:\n Groups Name        Std.Dev.   \n ID     (Intercept) 0.289398945\nNumber of obs: 2000, groups:  ID, 208\nFixed Effects:\n               (Intercept)                 GenderWomen  \n              -1.067430840                -0.642887590  \nConversationTypeSameGender                PrimingPrime  \n              -0.536428857                 1.866249307  \n\n\n\n\nVisualizing Effects\nWe visualize the effects here by showing the probability of discourse like based on the predicted values.\n\nplot_model(mlr.glmer, type = \"pred\", terms = c(\"Gender\", \"Priming\", \"ConversationType\"))\n\n\n\n\n\n\n\n\nWe can see that discourse like is more likely to surface in primed contexts and among males. In conversations with both men and women, speakers use discourse like slightly less than in mixed conversations.\n\n\nExtracting Model Fit Parameters\nWe now extract model fit parameters (Baayen, 281).\n\nprobs = 1/(1+exp(-fitted(mlr.glmer)))\nprobs = binomial()$linkinv(fitted(mlr.glmer))\nsomers2(probs, as.numeric(mblrdata$SUFlike))\n\n                C               Dxy                 n           Missing \n   0.760226203516    0.520452407033 2000.000000000000    0.000000000000 \n\n\nThe two lines that start with probs are simply two different ways to do the same thing (you only need one of these).\nThe model fit parameters indicate a suboptimal fit. Both the C-value and Somers’s Dxy show poor fit between predicted and observed occurrences of discourse like. If the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity (Baayen, 204). Somers’ Dxy is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ Dxy values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction (Baayen, 204). The C.value of 0.760226203516 suggests that the model has some predictive and explanatory power, but not at an optimal level. We will now perform the model diagnostics.\n\n\nModel Diagnostics\nWe begin the model diagnostics by generating a diagnostic that plots the fitted or predicted values against the residuals.\n\nplot(mlr.glmer, pch = 20, col = \"black\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nAs a final step, we summarize our findings in tabulated form.\n\n# summarize final model\nsjPlot::tab_model(mlr.glmer)\n\n\n\n \nSUFlike\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.34\n0.26 – 0.46\n&lt;0.001\n\n\nGender [Women]\n0.53\n0.37 – 0.74\n&lt;0.001\n\n\nConversationType[SameGender]\n0.58\n0.44 – 0.78\n&lt;0.001\n\n\nPriming [Prime]\n6.46\n4.69 – 8.90\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 ID\n0.08\n\n\nICC\n0.02\n\n\nN ID\n208\n\nObservations\n2000\n\n\nMarginal R2 / Conditional R2\n0.131 / 0.152\n\n\n\n\n\n\n\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(mlr.glmer)\n\nWe fitted a logistic mixed model (estimated using ML and BOBYQA optimizer) to\npredict SUFlike with Gender, ConversationType and Priming (formula: SUFlike ~\nGender + ConversationType + Priming). The model included ID as random effect\n(formula: ~1 | ID). The model's total explanatory power is moderate\n(conditional R2 = 0.15) and the part related to the fixed effects alone\n(marginal R2) is of 0.13. The model's intercept, corresponding to Gender = Men,\nConversationType = MixedGender and Priming = NoPrime, is at -1.07 (95% CI\n[-1.36, -0.78], p &lt; .001). Within this model:\n\n  - The effect of Gender [Women] is statistically significant and negative (beta\n= -0.64, 95% CI [-0.99, -0.30], p &lt; .001; Std. beta = -0.64, 95% CI [-0.99,\n-0.30])\n  - The effect of ConversationType [SameGender] is statistically significant and\nnegative (beta = -0.54, 95% CI [-0.83, -0.24], p &lt; .001; Std. beta = -0.54, 95%\nCI [-0.83, -0.24])\n  - The effect of Priming [Prime] is statistically significant and positive (beta\n= 1.87, 95% CI [1.55, 2.19], p &lt; .001; Std. beta = 1.87, 95% CI [1.55, 2.19])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n\nWe can use this output to write up a final report:\nWe can use this output to write up a final report:\nWe fitted a logistic mixed model to predict the use of discourse like. The model included speakers as random effect (formula: ~1 | ID). The model’s total explanatory power is moderate (conditional R2 = 0.15) and the part related to the fixed effects alone (marginal R2) is of 0.13.\nRegarding fixed effects, the model reported that * women use discourse like statistically less compared to men (beta = -0.64 [-0.99, -0.30], p &lt; .001; Std. beta = -0.64 [-0.99, -0.30]) * speakers in conversations with other speakers of the same gender use discourse like significantly less compared to thier use in mixed-gender conversations (beta = -0.54 [-0.83, -0.24], p &lt; .001; Std. beta = -0.54 [-0.83, -0.24]) * Priming is significantly positively correlated with the use of discourse like (beta = 1.87 [1.55, 2.19], p &lt; .001; Std. beta = 1.87 [1.55, 2.19])\n\n\n\nMixed-Effects (Quasi-)Poisson and Negative-Binomial Regression\nLike fixed-effects Poisson models, mixed-effects Poisson models take counts as dependent variables. The data for this analysis was collected on three separate evenings (Trial). The number of the filler uhm (UHM) was counted in two-minute conversations that were either in English, German, Russian, or Mandarin (Language). In addition, the number of shots that speakers drank before they talked was recorded (Shots).\n\n# load data\ncountdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/cld.rda\", \"rb\"))\n\n\n# inspect data\ncountdata %&gt;%\n  as.data.frame() %&gt;%\n  head(15) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %&gt;%\n  flextable::border_outer()\n\nIDTrialLanguageGenderUHMShots13RussianMan0023RussianMan0033GermanMan0541GermanMan0351GermanWoman2663GermanMan1571MandarinMan1183GermanWoman0493RussianWoman00102GermanMan02113RussianMan01122GermanMan01133RussianWoman01142RussianWoman44152EnglishMan04\n\n\nSince the data contains character variables, we need to factorize the data before we can analyse it further and we also remove the ID column.\n\n# factorize variables\ncountdata &lt;- countdata %&gt;%\n  dplyr::select(-ID) %&gt;%\n  dplyr::mutate_if(is.character, factor)\n\n\n# inspect data\ncountdata %&gt;%\n  as.data.frame() %&gt;%\n  head(15) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %&gt;%\n  flextable::border_outer()\n\nTrialLanguageGenderUHMShots3RussianMan003RussianMan003GermanMan051GermanMan031GermanWoman263GermanMan151MandarinMan113GermanWoman043RussianWoman002GermanMan023RussianMan012GermanMan013RussianWoman012RussianWoman442EnglishMan04\n\n\nAfter the data is factorized, we can visualize the data.\n\ncountdata %&gt;%\n  # prepare data\n  dplyr::select(Language, Shots) %&gt;%\n  dplyr::group_by(Language) %&gt;%\n  dplyr::mutate(Mean = round(mean(Shots), 1)) %&gt;%\n  dplyr::mutate(SD = round(sd(Shots), 1)) %&gt;%\n  # start plot\n  ggplot(aes(Language, Shots, color = Language, fill = Language)) +\n  geom_violin(trim=FALSE, color = \"gray20\")+ \n  geom_boxplot(width=0.1, fill=\"white\", color = \"gray20\") +\n  geom_text(aes(y=-4,label=paste(\"mean: \", Mean, sep = \"\")), size = 3, color = \"black\") +\n  geom_text(aes(y=-5,label=paste(\"SD: \", SD, sep = \"\")), size = 3, color = \"black\") +\n  scale_fill_manual(values=rep(\"grey90\",4)) + \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"none\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  ylim(-5, 15) +\n  labs(x = \"Language\", y = \"Shots\")\n\n\n\n\n\n\n\n\nThe violin plots show that the English speakers drank more shots than speakers of other languages with Mandarin speakers drinking the fewest shots.\nIn the present case, we will a Boruta variable selection procedure to streamline the model fitting process. Thus, before fitting the model, we will test which variables have any kind of relationship with the dependent variable and therefore deserve to be evaluated in the regression modeling. As this is just an example, we will only consider variables which are deemed important and disregard both unimportant and tentative variables. We start the Boruta analysis by setting a seed and running an initial Boruta analysis.\n\n# perform variable selection\nset.seed(20191220)\nboruta &lt;- Boruta(UHM ~.,data=countdata)\nprint(boruta)\n\nBoruta performed 99 iterations in 2.79168200493 secs.\n 2 attributes confirmed important: Language, Shots;\n 1 attributes confirmed unimportant: Gender;\n 1 tentative attributes left: Trial;\n\n\nAs only Shots is confirmed as important, we will only check for the effect of Shots and include Language as a random effect in the regression modeling. Including Language as a random effect is probably not justified statistically (given that the Boruta analysis showed that it only has a tentative effect) but for theoretical reasons as the speakers are nested into Languages. Before we start with the modeling, however, we proceed by checking if the data does indeed approximate a Poisson distribution.\n\n# output the results\ngf = goodfit(countdata$UHM,type= \"poisson\", method= \"ML\")\nplot(gf, main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\nThe data does not perfectly match a distribution that would be expected if the data approximated a Poisson distribution. We will use a goodness-of-fit test to check if the data does indeed diverge significantly from being Poisson distributed. If the p-values of the goodness-of-fit test is smaller than .05, then the distribution of the data differs significantly from a Poisson distribution and, given the visualization is likely over-dispersed.\nIn case of overdispersion, we may have to use a quasi-Poisson or, even better, a negative binomial model but we will, for now continue with the Poisson model and perform diagnostics later to check if we have to switch to a more robust method. One effect of overdispersion is that the standard errors of a model are biased and quasi-Poisson models scale the standard errors to compensate bias. However, Zuur, Hilbe, and Ieno suggest to use negative-binomial model instead. This is so because the scaling of the standard errors performed by quasi-Poisson models only affects the significance of coefficients (the p-values) but it does not affect the coefficients which, however, may be affected themselves by overdispersion. Thus, the coefficients of Poisson as well as quasi-Poisson models (which are identical) may be unreliable when dealing with overdispersion. Negative binomial models, in contrast, include an additional dispersion or heterogeneity parameter which accommodates overdispersion better than merely scaling the standard errors (see Zuur, Hilbe, and Ieno, 21).\n\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df                                     P(&gt; X^2)\nLikelihood Ratio 153.422771085  5 0.000000000000000000000000000000249336691328\n\n\nThe p-value is indeed smaller than .05 which means that we should indeed use a negative-binomial model rather than a Poisson model. We will ignore this, for now, and proceed to fit a Poisson mixed-effects model and check what happens if a Poisson model is fit to over-dispersed data.\n\nMixed-Effects Poisson Regression\nIn a first step, we create mixed-effect intercept-only baseline models and then test if including “Shots” significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.glmer = glmer(UHM ~ 1 + (1 | Language), data = countdata, family = poisson,\n                 control=glmerControl(optimizer=\"bobyqa\"))\n# add Shots\nm1.glmer &lt;- update(m0.glmer, .~.+ Shots)\nAnova(m1.glmer, test = \"Chi\")           \n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: UHM\n       Chisq Df             Pr(&gt;Chisq)    \nShots 321.25  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. However, we get the warning that the fitted mixed model is (almost / near) singular. In such cases, the model should not be reported. As this is only an example, we will continue by having a look at the model summary.\n\nsummary(m1.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: UHM ~ (1 | Language) + Shots\n   Data: countdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1041.8   1054.5   -517.9   1035.8      497 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.509633852 -0.666423093 -0.592950422  0.586114082  4.338639382 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Language (Intercept) 0        0       \nNumber of obs: 500, groups:  Language, 4\n\nFixed effects:\n                 Estimate    Std. Error   z value               Pr(&gt;|z|)    \n(Intercept) -1.2789168850  0.0893313003 -14.31656 &lt; 0.000000000000000222 ***\nShots        0.2336279071  0.0130347632  17.92345 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nShots -0.806\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe model summary confirms that the number of shots does have a significantly positive effect on the number of occurrences of uhm. Furthermore, the scaled residuals are distributed very unevenly which suggests overdispersion. Including Language as a random effect is not justified given that they have 0 variance and a standard deviation of 0 (which means that Language does not account for or explain any additional variance).\nWe now check if the model suffers from overdispersion following Zuur, Hilbe, and Ieno (138).\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.glmer, type = \"pearson\")\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors &lt;- length(fixef(m1.glmer)) +1\n# calculate overdispersion\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.16901460967\n\n\nThe data is slightly over-dispersed. It would also be advisable to plot the Cook’s distance (which should not show data points with values &gt; 1). If there are data points with high Cook’s D values, we could exclude them which would, very likely reduce the overdispersion (see Zuur, Hilbe, and Ieno, 22). We ignore this, for now, and use diagnostic plots to check if the plots indicate problems.\n\ndiag_data &lt;- data.frame(PearsonResiduals, fitted(m1.glmer)) %&gt;%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 &lt;- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 &lt;- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 &lt;- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngridExtra::grid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\n\n\n\n\nThe diagnostic plots show problems as the dots in the first two plots are not random but show a pattern in the lower left corner. In addition, the variance of English (left boxplot) is notable larger than the variance of Russian (right boxplot). As a final step, we plot the predicted vales of the model to check if the predictions make sense.\n\nplot_model(m1.glmer, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe model predicts that the instances of uhm increase with the number of shots. Note that the increase is not homogeneous as the y-axis labels indicate! We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.glmer, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations (overdispersion measure &gt; 1 and excessive multicollinearity (singular fit)) had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nThe summary of the model can be extracted using the tab_model function from the sjPlot package (Lüdecke).\n\nsjPlot::tab_model(m1.glmer)\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.33\n&lt;0.001\n\n\nShots\n1.26\n1.23 – 1.30\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\n\nr.squaredGLMM(m1.glmer)\n\n                     R2m            R2c\ndelta     0.208910675833 0.208910675833\nlognormal 0.294971461467 0.294971461467\ntrigamma  0.120419615931 0.120419615931\n\n\nAlso note that our model suffers from a serious problem (near singular fit). If this were not just an example, you should not(!) report this model!\n\n\nMixed-Effects Quasi-Possion Regression\nThe Quasi-Poisson Regression is a generalization of the Poisson regression and is used when modeling an overdispersed count variable. Poisson models are based on the Poisson distribution which is defined as a distribution where the variance is equal to the mean (which is very restrictive and not often the case). Quasi-Poisson models scale the standard errors which has a positive effect when dealing with overdispersed data.\nTherefore, when the variance is greater than the mean, a Quasi-Poisson model, which assumes that the variance is a linear function of the mean, is more appropriate as it handles over-dispersed data better than normal Poisson-models.\nWe begin the model fitting process by creating a mixed- and a fixed-effects intercept-only base-line model. Unfortunately, there is not yet a procedure in place for quasi-Poisson models to test if the inclusion of random effects is justified. However, here the Boruta also provides valuable information: Language was only considered tentative but not important which suggests that it will not explain variance which means that including Language as a random effect may not be justified. This would require further inspection. Because we are only dealing with an example here, we ignore this fact (which you should not do in proper analyses) and continue right away with adding shots.\n\n# base-line mixed-model\nm0.qp = glmmPQL(UHM ~ 1, random = ~ 1 | Language, data = countdata, \n                   family = quasipoisson(link='log'))\n\niteration 1\n\n\niteration 2\n\n\niteration 3\n\n\niteration 4\n\n# add Shots\nm1.qp &lt;- update(m0.qp, .~.+ Shots)\n\niteration 1\n\nAnova(m1.qp, test = \"Chi\")           # SIG! (p&lt;0.0000000000000002 ***)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: zz\n         Chisq Df             Pr(&gt;Chisq)    \nShots 276.4523  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. We will now have a look at the model summary.\n\nsummary(m1.qp)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: countdata \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 | Language\n               (Intercept)      Residual\nStdDev: 0.0000407429998994 1.07801257915\n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  UHM ~ Shots \n                      Value       Std.Error  DF        t-value p-value\n(Intercept) -1.278929721022 0.0964886326622 495 -13.2547190870       0\nShots        0.233630231741 0.0140795660798 495  16.5935676154       0\n Correlation: \n      (Intr)\nShots -0.806\n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-1.400417640410 -0.618228066947 -0.550071160975  0.543731905020  4.024828853743 \n\nNumber of Observations: 500\nNumber of Groups: 4 \n\n\nThe model summary does not provide much information such as,e.g. AIC or BIC values. The coefficient for Shots is highly significant (p &lt;.001) and the data is notably over-dispersed (the Standardized Within-Group Residuals deviate substantially from a normal distribution with higher values having a thick tail). Also, in contrast to the Poisson model, Language does explain at least a minimal share of the variance now as the mean and standard deviation are no longer 0. Note also, that the coefficients are identical to the Poisson coefficients but the standard errors and p-values differ (the model provides t- rather than z-values).\nIn a next step, we will calculate the odds ratios of the coefficient (as we only have one). We will use the coefficients from the fixed-effects model as the coefficients for mixed- and fixed-effects models are identical (the random effect structure only affects the standard error and p-values but not the coefficients; you can check by uncommenting the summary command).\n\nm1.glm = glm(UHM ~ Shots, data = countdata, family = quasipoisson(link='log'))\nexp(coef(m1.glm))\n\n   (Intercept)          Shots \n0.278338612235 1.263174385573 \n\n\nThe standardized or \\(\\beta\\)-coefficient tells us that the likelihood of uhm increases by 1.26 (or 26.32 percent) with each additional shot.\nBefore inspecting the relationship between Shots and uhm, we will check if the overdispersion was reduced.\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.qp, type = \"pearson\")\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors &lt;- length(fixef(m1.qp)) +1\n# calculate overdispersion\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.00603621722\n\n\nThe overdispersion has indeed decreased and is not so close to 1 that overdispersion is no longer an issue.\nWe continue to diagnose the model by plotting the Pearson’s residuals against fitted values. This diagnostic plot should not show a funnel-like structure or patterning as we observed in the case of the Poisson model.\n\n# diagnostic plot\nplot(m1.qp, pch = 20, col = \"black\", lty= \"dotted\", ylab = \"Pearson's residuals\")\n\n\n\n\n\n\n\n\nIndeed, the plot exhibits a (slight) funnel shape (but not drastically so) and thus indicates heteroscedasticity. However, the patterning that we observed with the Poisson model has disappeared. We continue by plotting the random effect adjustments.\n\n# generate diagnostic plots\nplot(m1.qp, Language ~ resid(.), abline = 0, fill = \"gray70\") \n\n\n\n\n\n\n\n\nThe adjustments by “Language” are marginal (which was somewhat expected given that Language was only deemed tentative), which shows that there is very little variation between the languages and that we have no statistical reason to include Language as a random effect.\nIn a final step, we plot the fixed-effect of Shots using the predictorEffects function from the effects package (Fox and Weisberg 2019).\n\nplot_model(m1.qp, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe effects plot shows that the number of uhms increases exponentially with the number of shots a speaker has had. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.qp, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nGiven that the overdispersion measure of this Quasi-Poisson model is close to 1, that the model did not suffer from excessive multicollinearity (singular fit), and because this model shows improvements compared to the Poisson model with respect to the model diagnostics (some adjustments by Language and less patterning in the diagnostic plots), we would choose this quasi-Poisson model over the Poisson model.\nFinally, we extract the summary table of this model.\n\nsjPlot::tab_model(m1.qp)\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.34\n&lt;0.001\n\n\nShots\n1.26\n1.23 – 1.30\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.00\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\nMarginal R2 / Conditional R2\n1.000 / NA\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\n\nr.squaredGLMM(m1.qp)\n\n                      R2m             R2c\ndelta     0.1856941875702 0.1856941884278\nlognormal 0.2752763976645 0.2752763989357\ntrigamma  0.0977040660495 0.0977040665007\n\n\n\n\nMixed-Effects Negative Binomial Regression\nNegative binomial regression models are a generalization of Poisson regression which loosens the restrictive assumption that the variance is equal to the mean made by the Poisson model. This is a major advantage as the most common issue that one faces with Poisson regressions is that the data deviate too substantially from the assumed Poisson distribution.\nTo implement a Negative-Binomial Mixed-Effects Regression, we first create a mixed-effects intercept-only baseline model and then test if including Shots significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.nb = glmer.nb(UHM ~ 1 + (1 | Language), data = countdata)\n# add Shots\nm1.nb &lt;- update(m0.nb, .~.+ Shots)\n\nboundary (singular) fit: see help('isSingular')\n\nanova(m1.nb, m0.nb)           \n\nData: countdata\nModels:\nm0.nb: UHM ~ 1 + (1 | Language)\nm1.nb: UHM ~ (1 | Language) + Shots\n      npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.nb    3 1159.000894 1171.644718 -576.5004470 1153.000894             \nm1.nb    4 1051.593288 1068.451721 -521.7966442 1043.593288 109.40761  1\n                  Pr(&gt;Chisq)    \nm0.nb                           \nm1.nb &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe negative-binomial model also reports a significant impact of shots on the number of uhms. In a next step, we calculate the overdispersion.\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.nb, type = \"pearson\")\n# extract number of betas + predictors + sigma\nNumberOfPredictors &lt;- 2+1+1\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# calculate overdispersion parameter\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases / NumberOfPredictors)# show overdispersion parameter\nOverdispersion\n\n[1] 2.46949245769\n\n\nThe overdispersion has increased which is rather suboptimal. In this case, we would report the Quasi-Poisson Regression rather than the Negative Binomial Model (which is rather rare as Negative Binomial Models typically perform better than (Quasi-)Poisson models. However, this tutorial focuses merely on how to implement a Negative Binomial Mixed-Effects Regression and we thus continue with generating diagnostic plots to check for problems.\n\ndiag_data &lt;- data.frame(PearsonResiduals, fitted(m1.nb)) %&gt;%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 &lt;- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 &lt;- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 &lt;- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngridExtra::grid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\n\n\n\n\nThe diagnostics show patterning similar to the one we saw with the Poisson model which suggest that the negative binomial model is also not an optimal model for our data. We continue by plotting the predicted values and, subsequently, summarize the analysis.\n\nplot_model(m1.nb, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe effect plot shows that the predicted number of shots increases exponentially with each shot. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.nb, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nA mixed-effect negative binomial regression model which contained the language in which the conversation took place as random effect was fit to the data. Prior to the regression modeling, a Boruta analysis was applied to determine whether any of the predictors had a meaningful relationship with the dependent variable (instances of uhm). Since the Boruta analysis indicated that only the number of shots a speaker had was important, only “Shots” was tested during model fitting. The final minimal adequate model showed that the number of uhm as fillers increases significantly, and near-linearly with the number of shots speakers had (\\(\\chi\\)2(1):83.0, p &lt;.0001, \\(\\beta\\): 0.2782). An inspection of the random effect structure conveyed that there was almost no variability between languages and language did not contribute meaningfully to the model fit.\n\n\n\nMixed-Effects Multinomial Regression\nIn this section, we will focus on how to implement a mixed-effects multinomial regression model using the mblogit function from the mclogit package (see Elff 2021). As we have already gone though model fitting and model validation procedures above, we will strictly see how to implement this type of model here - we will not go through all the other steps that a proper regression analysis would require.\nWe begin the analysis by loading the example data set. The data represents observations gathered during an experiment where speakers had to report what they saw. The responses are categorized into four groups:\n\n# description data\npict  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/pict.rda\", \"rb\"))\n# inspect\nhead(pict)\n\n  Id Participant       Group Item     Response Gender Age\n1  1        G001 German_Mono    1  NumeralNoun   Male  18\n2  2        G002 German_Mono    3  NumeralNoun   Male  18\n3  3        G003 German_Mono    4  NumeralNoun   Male  18\n4  4        G004 German_Mono    6 QuantAdjNoun   Male  18\n5  5        G005 German_Mono    8  NumeralNoun   Male  18\n6  6        G006 German_Mono    9 QuantAdjNoun   Male  18\n\n\nIn a first step, we generate a baseline model that we call m0. This model only contains the random effect structure and the intercept as the sole predictor.\n\nm0.mn &lt;- mblogit(formula = Response ~ 1, \n              random = ~ 1 | Participant, \n              data = pict)\n\n\nIteration 1 - deviance = 2452.16571809 - criterion = 0.840346812509\n\n\n\nIteration 2 - deviance = 2412.75547122 - criterion = 0.0646013613319\n\n\n\nIteration 3 - deviance = 2411.26894466 - criterion = 0.00534731178597\n\n\n\nIteration 4 - deviance = 2411.2582473 - criterion = 0.0000446387422471\n\n\n\nIteration 5 - deviance = 2411.25823534 - criterion = 0.0000000031405953873\nconverged\n\n\nIn this case, the algorithm did not converge properly - if this were a real analysis, we could not simply continue but would have to inspect possible causes for this. However, as this is just a showcase, we will ignore this and move on. Next, we add the fixed effects (Gender and Group).\n\nm1.mn &lt;- mblogit(formula = Response ~ Gender + Group, \n              random = ~ 1 | Item, \n              data = pict)\n\n\nIteration 1 - deviance = 1652.54499424 - criterion = 0.800708039017\n\n\n\nIteration 2 - deviance = 1571.22116522 - criterion = 0.0814899820197\n\n\n\nIteration 3 - deviance = 1551.91023391 - criterion = 0.0274469746195\n\n\n\nIteration 4 - deviance = 1547.29823345 - criterion = 0.00512667078353\n\n\n\nIteration 5 - deviance = 1546.02170442 - criterion = 0.000173543402508\n\n\n\nIteration 6 - deviance = 1545.8333116 - criterion = 0.000000244926398008\n\n\n\nIteration 7 - deviance = 1545.82769259 - criterion = 0.000000000000785569845713\nconverged\n\n\nNow, we can compare the models to see if including the fixed-effects into the model has significantly improved the model fit.\n\nanova(m0.mn, m1.mn)\n\nAnalysis of Deviance Table\n\nModel 1: Response ~ 1\nModel 2: Response ~ Gender + Group\n  Resid. Df  Resid. Dev Df    Deviance\n1      3261 2411.258235               \n2      3249 1545.827693 12 865.4305427\n\n\nAs the second model is significantly better, we are justified to believe that our fixed effects have explanatory power. We can now use the getSummary.mmblogit function to get a summary of the model with the fixed effects.\n\n# inspect\nmclogit::getSummary.mmblogit(m1.mn)\n\n$coef\n, , NumeralNoun/BareNoun\n\n                                  est             se             stat\n(Intercept)           1.0940536961694 0.795348462841   1.375565236225\nGenderMale            0.0691676134585 0.196851703842   0.351369137826\nGroupGerman_Mono     -3.2909055852599 0.304213383695 -10.817754121410\nGroupL2_Advanced     -0.4575232522124 0.307505930002  -1.487851802435\nGroupL2_Intermediate -1.1689603301872 0.320486374900  -3.647457183015\n                                                            p             lwr\n(Intercept)          0.16895627569603502426964780624984996393 -0.464800646159\nGenderMale           0.72531143261552322165641726314788684249 -0.316654636367\nGroupGerman_Mono     0.00000000000000000000000000283642762786 -3.887152860918\nGroupL2_Advanced     0.13678998075957157776194605958153260872 -1.060223800048\nGroupL2_Intermediate 0.00026484842867640642034496312184899125 -1.797102082527\n                                 upr\n(Intercept)           2.652908038498\nGenderMale            0.454989863284\nGroupGerman_Mono     -2.694658309602\nGroupL2_Advanced      0.145177295623\nGroupL2_Intermediate -0.540818577848\n\n, , QuantAdjNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)           0.595495714810 0.498285332593   1.19508979265\nGenderMale            0.299894756058 0.248324126895   1.20767466218\nGroupGerman_Mono     -3.856907426426 0.376376472714 -10.24747216162\nGroupL2_Advanced     -2.194953767755 0.347242020175  -6.32110643363\nGroupL2_Intermediate -3.016728204963 0.398140344234  -7.57704726148\n                                                         p             lwr\n(Intercept)          0.23205194960832503658920700218004640 -0.381125591097\nGenderMale           0.22717242789340932884734058916365029 -0.186811589147\nGroupGerman_Mono     0.00000000000000000000000121478955842 -4.594591757573\nGroupL2_Advanced     0.00000000025969706979146851136980930 -2.875535621216\nGroupL2_Intermediate 0.00000000000003535080320338597665829 -3.797068940455\n                                 upr\n(Intercept)           1.572117020716\nGenderMale            0.786601101264\nGroupGerman_Mono     -3.119223095278\nGroupL2_Advanced     -1.514371914293\nGroupL2_Intermediate -2.236387469472\n\n, , QuantNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)          -2.163121210173 0.753539267072 -2.870615115491\nGenderMale           -0.371289871064 0.430045391060 -0.863373678181\nGroupGerman_Mono     -2.420925019720 0.813762214781 -2.974978409844\nGroupL2_Advanced      0.789609605279 0.683753160717  1.154816753535\nGroupL2_Intermediate -0.148789198300 0.739325816561 -0.201249834602\n                                    p             lwr             upr\n(Intercept)          0.00409674000335 -3.640031034571 -0.686211385775\nGenderMale           0.38793204670759 -1.214163349258  0.471583607130\nGroupGerman_Mono     0.00293009169381 -4.015869652670 -0.825980386770\nGroupL2_Advanced     0.24816547492527 -0.550521964042  2.129741174601\nGroupL2_Intermediate 0.84050322615521 -1.597841171600  1.300262775001\n\n\n$Item\n, , 1\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)   5.46019439094 26.1766792826   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) -1.63804668966           NaN   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     1.63405993830 11.5842291808   NA NA  NA  NA\n\n, , 2\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  -1.63804668966           NaN   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1)  1.60628957168           NaN   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     0.49903254341 2.36453656619   NA NA  NA  NA\n\n, , 3\n\n                                             est             se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  1.63405993830 11.58422918076   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) 0.49903254341  2.36453656619   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)    1.51237743196            NaN   NA NA  NA  NA\n\n\n$Groups\nGroups by Item \n            10 \n\n$sumstat\n               LR                df          deviance          McFadden \n1476.294014648252   21.000000000000 1545.827692593037    0.488495883905 \n        Cox.Snell        Nagelkerke               AIC               BIC \n   0.741897420618    0.791357248659 1587.827692593037 1692.700285072724 \n                N \n1090.000000000000 \n\n$call\nmblogit(formula = Response ~ Gender + Group, data = pict, random = ~1 | \n    Item)\n\n$contrasts\n$contrasts$Gender\n[1] \"contr.treatment\"\n\n$contrasts$Group\n[1] \"contr.treatment\"\n\n\n$xlevels\n$xlevels$Gender\n[1] \"Female\" \"Male\"  \n\n$xlevels$Group\n[1] \"English_Mono\"    \"German_Mono\"     \"L2_Advanced\"     \"L2_Intermediate\"\n\n\nThe NAs (not available information) is a result of the model having a bad fit to the data and, optimally, we would need to inspect why the model has a bad fit. Again, we ignore this and move on. Next, we check the VIFs to see if the model does not violate multicollinearity assumptions.\n\ncar::vif(m1.mn) # maybe use cut-off of 5 (maybe 10)\n\n                GVIF Df GVIF^(1/(2*Df))\nGender 3.59671433201  1   1.89650054891\nGroup  9.94593851425  3   1.46647375519\n\n\nThe VIFs are a bit high - especially the GVIF for Group would be a cause for concern if this was not just a demo analysis! However, as we only want to implement a multinomial mixed-effects model here and not provide a proper, clean analysis, we will ignore this issue here.\nIn a next step, we visualize effects to get a better understanding of how the predictors that are part of the fixed-effect structure of the mode affect the outcome (the response variable).\n\nsjPlot::plot_model(m1.mn)\n\n\n\n\n\n\n\n\nFinally, we can extract an alternative summary table produced by the tab_model function from the sjPlot package (see Lüdecke).\n\nsjPlot::tab_model(m1.mn)\n\n\n\n \nResponse: NumeralNoun\nResponse: QuantAdjNoun\nResponse: QuantNoun\n\n\nPredictors\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\n\n\n(Intercept)\n2.99\n0.63 – 14.20\n0.169\n1.81\n0.68 – 4.82\n0.232\n0.11\n0.03 – 0.50\n0.004\n\n\nGenderMale\n1.07\n0.73 – 1.58\n0.725\n1.35\n0.83 – 2.20\n0.227\n0.69\n0.30 – 1.60\n0.388\n\n\nGroupGerman_Mono\n0.04\n0.02 – 0.07\n&lt;0.001\n0.02\n0.01 – 0.04\n&lt;0.001\n0.09\n0.02 – 0.44\n0.003\n\n\nGroupL2_Advanced\n0.63\n0.35 – 1.16\n0.137\n0.11\n0.06 – 0.22\n&lt;0.001\n2.20\n0.58 – 8.42\n0.248\n\n\nGroupL2_Intermediate\n0.31\n0.17 – 0.58\n&lt;0.001\n0.05\n0.02 – 0.11\n&lt;0.001\n0.86\n0.20 – 3.67\n0.841\n\n\n\nN Item\n10\n\nObservations\n1090\n\n\n\n\n\n\n\nThis is the final step in implementing a a mixed-effects multinomial regression model using the mblogit function from the mclogit package (see Elff 2021). We are aware that the analysis shown here is supervifial(!) - but please keep in mind that we just wanted to showcase the implementation here rather than providing a properly and carefully done analysis.\n\n\nMixed-Effects Ordinal Regression\nIn this section, we will strictly focus on how to implement a mixed-effects ordinal regression model using the clmm function from the ordinal package (see Christensen 2019). This type of regression model is extremely useful when dealing with Likert data or other types of questionnaire and survey data where the responses have some kind of hierarchical structure (i.e. responses are not truly independent because they come from different points in time or from different regions). load data\n\n# rating experiment data\nratex  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/ratex.rda\", \"rb\"))\n# inspect data\nhead(ratex)\n\n  Rater Child Group       Accent AccentNumeric       Family\n1    R1  C001 Child StrongAccent             2 DomBilingual\n2    R2  C001 Child StrongAccent             2 DomBilingual\n3    R3  C001 Child StrongAccent             2 DomBilingual\n4    R4  C001 Child StrongAccent             2 DomBilingual\n5    R5  C001 Child StrongAccent             2 DomBilingual\n6    R6  C001 Child StrongAccent             2 DomBilingual\n\n\nWe now tabulate the data to get a better understanding of the data structure.\n\nratex %&gt;%\n  dplyr::group_by(Family, Accent) %&gt;%\n  dplyr::summarise(Frequency = n()) %&gt;%\n  tidyr::spread(Accent, Frequency)\n\n`summarise()` has grouped output by 'Family'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 4\n# Groups:   Family [3]\n  Family         NoAccent StrongAccent WeakAccent\n  &lt;fct&gt;             &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n1 DomBilingual         80          145        174\n2 EqualBilingual       20           22         63\n3 Monolingual         209            1         41\n\n\nNext, we visualize the data to inspect its properties.\n\nratex %&gt;%\n  ggplot(aes(Family, AccentNumeric, color = Group)) + \n  stat_summary(fun = mean, geom = \"point\") +          \n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nAn alternative plot shows other properties of the data.\n\nratex %&gt;%\n  dplyr::group_by(Family, Rater, Group) %&gt;%\n  dplyr::summarise(AccentMean = mean(AccentNumeric)) %&gt;%\n  ggplot(aes(Family, AccentMean, fill = Group)) + \n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"gray50\", \"gray85\"))\n\n`summarise()` has grouped output by 'Family', 'Rater'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nWe now start the modeling by generating a model with Family as the sole predictor.\n\n# fit baseline model\nm1.or &lt;- clmm(Accent ~ (1|Rater) + Family, link=\"logit\", data = ratex)\n# test for incomplete information\nifelse(min(ftable(ratex$Accent, ratex$Family)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\n# extract aic\naic.glmer &lt;- AIC(logLik(m1.or))\n# inspect aic\naic.glmer\n\n[1] 1380.25888675\n\n# summarize model\nsummary(m1.or)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: Accent ~ (1 | Rater) + Family\ndata:    ratex\n\n link  threshold nobs logLik  AIC     niter    max.grad cond.H \n logit flexible  755  -685.13 1380.26 371(435) 4.36e-07 5.9e+05\n\nRandom effects:\n Groups Name        Variance            Std.Dev.       \n Rater  (Intercept) 0.00000000353193271 0.0000594300657\nNumber of groups:  Rater 21 \n\nCoefficients:\n                         Estimate   Std. Error   z value             Pr(&gt;|z|)\nFamilyEqualBilingual  0.477744666  0.214313910   2.22918             0.025802\nFamilyMonolingual    -2.550224083  0.198849328 -12.82491 &lt; 0.0000000000000002\n                        \nFamilyEqualBilingual *  \nFamilyMonolingual    ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                             Estimate    Std. Error   z value\nNoAccent|StrongAccent   -1.0798112479  0.1058004233 -10.20611\nStrongAccent|WeakAccent  0.1060945018  0.0951352125   1.11520\n\n\nWe can now perform Post-Hoc tests to see which comparisons are significant.\n\nlsmeans(m1.or, pairwise~Family, adjust=\"tukey\")\n\n$lsmeans\n Family         lsmean     SE  df asymp.LCL asymp.UCL\n DomBilingual    0.487 0.0914 Inf     0.308     0.666\n EqualBilingual  0.965 0.1950 Inf     0.582     1.347\n Monolingual    -2.063 0.1740 Inf    -2.404    -1.722\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df z.ratio p.value\n DomBilingual - EqualBilingual   -0.478 0.214 Inf  -2.229  0.0664\n DomBilingual - Monolingual       2.550 0.199 Inf  12.825  &lt;.0001\n EqualBilingual - Monolingual     3.028 0.265 Inf  11.438  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nFinally, we can summarize the model.\n\nsjPlot::tab_model(m1.or)\n\n\n\n \nAccent\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\nNoAccent|StrongAccent\n0.34\n0.28 – 0.42\n&lt;0.001\n\n\nStrongAccent|WeakAccent\n1.11\n0.92 – 1.34\n0.265\n\n\nFamily [EqualBilingual]\n1.61\n1.06 – 2.45\n0.026\n\n\nFamily [Monolingual]\n0.08\n0.05 – 0.12\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Rater\n0.00\n\n\nN Rater\n21\n\nObservations\n755\n\n\nMarginal R2 / Conditional R2\n0.325 / NA\n\n\n\n\n\n\n\nAnd we can visualize the effects.\n\nplot_model(m1.or, type = \"pred\", terms = c(\"Family\"))\n\n\n\n\n\n\n\n\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results.\n\n\n\nCitation & Session Info\nSchweinberger, Martin. 2024. Fixed- and Mixed-Effects Regression Models in R. Brisbane: The University of Queensland. url: https://slcladal.github.io/regression.html (Version 2024.12.20).\n@manual{schweinberger2024regression,\n  author = {Schweinberger, Martin},\n  title = {Fixed- and Mixed-Effects Regression Models in R},\n  note = {https://slcladal.github.io/regression.html},\n  year = {2024},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2024.12.20}\n}\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] see_0.9.0           performance_0.12.4  gridExtra_2.3      \n [4] vcd_1.4-13          tibble_3.2.1        stringr_1.5.1      \n [7] sjPlot_2.8.16       robustbase_0.99-4-1 rms_6.8-2          \n[10] ordinal_2023.12-4.1 nlme_3.1-166        MuMIn_1.48.4       \n[13] mclogit_0.9.6       MASS_7.3-61         lme4_1.1-35.5      \n[16] Matrix_1.7-1        knitr_1.48          Hmisc_5.2-0        \n[19] ggfortify_0.4.17    glmulti_1.0.8       leaps_3.2          \n[22] rJava_1.0-11        emmeans_1.10.5      effects_4.2-2      \n[25] car_3.1-3           carData_3.0-5       Boruta_8.0.0       \n[28] vip_0.4.1           ggpubr_0.6.0        ggplot2_3.5.1      \n[31] flextable_0.9.7     dplyr_1.1.4        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1           klippy_0.0.0.9500       polspline_1.1.25       \n  [4] datawizard_0.13.0       hardhat_1.4.0           pROC_1.18.5            \n  [7] rpart_4.1.23            lifecycle_1.0.4         rstatix_0.7.2          \n [10] globals_0.16.3          lattice_0.22-6          insight_0.20.5         \n [13] backports_1.5.0         survey_4.4-2            magrittr_2.0.3         \n [16] rmarkdown_2.28          yaml_2.3.10             zip_2.3.1              \n [19] askpass_1.2.1           RColorBrewer_1.1-3      cowplot_1.1.3          \n [22] DBI_1.2.3               minqa_1.2.8             lubridate_1.9.3        \n [25] multcomp_1.4-26         abind_1.4-8             expm_1.0-0             \n [28] purrr_1.0.2             nnet_7.3-19             TH.data_1.1-2          \n [31] sandwich_3.1-1          ipred_0.9-15            lava_1.8.0             \n [34] gdtools_0.4.0           ggrepel_0.9.6           memisc_0.99.31.8.1     \n [37] listenv_0.9.1           MatrixModels_0.5-3      parallelly_1.38.0      \n [40] svglite_2.1.3           codetools_0.2-20        xml2_1.3.6             \n [43] tidyselect_1.2.1        ggeffects_1.7.2         farver_2.1.2           \n [46] effectsize_0.8.9        stats4_4.4.1            base64enc_0.1-3        \n [49] jsonlite_1.8.9          caret_6.0-94            e1071_1.7-16           \n [52] Formula_1.2-5           survival_3.7-0          iterators_1.0.14       \n [55] systemfonts_1.1.0       foreach_1.5.2           tools_4.4.1            \n [58] ragg_1.3.3              Rcpp_1.0.13             glue_1.8.0             \n [61] prodlim_2024.06.25      ranger_0.16.0           xfun_0.49              \n [64] mgcv_1.9-1              msm_1.8.1               withr_3.0.2            \n [67] numDeriv_2016.8-1.1     fastmap_1.2.0           mitools_2.4            \n [70] boot_1.3-31             fansi_1.0.6             SparseM_1.84-2         \n [73] openssl_2.2.2           digest_0.6.37           timechange_0.3.0       \n [76] R6_2.5.1                estimability_1.5.1      textshaping_0.4.0      \n [79] colorspace_2.1-1        utf8_1.2.4              tidyr_1.3.1            \n [82] generics_0.1.3          fontLiberation_0.1.0    data.table_1.16.2      \n [85] recipes_1.1.0           class_7.3-22            report_0.5.9           \n [88] htmlwidgets_1.6.4       parameters_0.23.0       ModelMetrics_1.2.2.2   \n [91] pkgconfig_2.0.3         gtable_0.3.6            timeDate_4041.110      \n [94] lmtest_0.9-40           htmltools_0.5.8.1       fontBitstreamVera_0.1.1\n [97] kableExtra_1.4.0        scales_1.3.0            gower_1.0.1            \n[100] rstudioapi_0.17.1       reshape2_1.4.4          uuid_1.2-1             \n[103] coda_0.19-4.1           checkmate_2.3.2         nloptr_2.1.1           \n[106] proxy_0.4-27            zoo_1.8-12              sjlabelled_1.2.0       \n[109] parallel_4.4.1          foreign_0.8-87          pillar_1.9.0           \n[112] vctrs_0.6.5             ucminf_1.2.2            xtable_1.8-4           \n[115] cluster_2.1.6           htmlTable_2.4.3         evaluate_1.0.1         \n[118] mvtnorm_1.3-1           cli_3.6.3               compiler_4.4.1         \n[121] rlang_1.1.4             future.apply_1.11.3     ggsignif_0.6.4         \n[124] labeling_0.4.3          forcats_1.0.0           plyr_1.8.9             \n[127] sjmisc_2.8.10           stringi_1.8.4           viridisLite_0.4.2      \n[130] assertthat_0.2.1        munsell_0.5.1           bayestestR_0.15.0      \n[133] quantreg_5.99           fontquiver_0.2.1        sjstats_0.19.0         \n[136] hms_1.1.3               patchwork_1.3.0         future_1.34.0          \n[139] highr_0.11              haven_2.5.4             broom_1.0.7            \n[142] DEoptimR_1.1-3          officer_0.6.7          \n\n\n\nBack to top\nBack to HOME\n\n\n\nReferences"
  },
  {
    "objectID": "tutorials/regression/regression.html#ordinal-regression",
    "href": "tutorials/regression/regression.html#ordinal-regression",
    "title": "Introduction",
    "section": "Ordinal Regression",
    "text": "Ordinal Regression\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable (Agresti). For this reason, ordinal regression is one of the key methods in analysing Likert data.\nTo see how an ordinal regression is implemented in R, we load and inspect the ´ordinaldata´ data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students’ final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (very likely, somewhat likely, and unlikely) and reflects the committees’ assessment of whether the student is likely to succeed in the program.\n\n# load data\nordata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/ord.rda\", \"rb\")) %&gt;%\n  dplyr::rename(Recommend = 1, \n              Internal = 2, \n              Exchange = 3, \n              FinalScore = 4) %&gt;%\n  dplyr::mutate(FinalScore = round(FinalScore, 2))\n\n\n\nRecommendInternalExchangeFinalScorevery likely003.26somewhat likely103.21unlikely113.94somewhat likely002.81somewhat likely002.53unlikely012.59somewhat likely002.56somewhat likely002.73unlikely003.00somewhat likely103.50unlikely113.65somewhat likely002.84very likely013.90somewhat likely002.68unlikely103.57\n\n\nIn a first step, we need to relevel the ordinal variable to represent an ordinal factor (or a progression from “unlikely” over “somewhat likely” to “very likely”. And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n# relevel data\nordata &lt;- ordata %&gt;%\n  dplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %&gt;%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %&gt;%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n\nNow that the dependent variable is releveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n1.900000 2.720000 2.990000 2.998925 3.270000 4.000000 \n\n\n[1] 0.397940933861\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n# visualize data\nggplot(ordata, aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\n\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modeling by using the polr function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course.\n\n# fit ordered logit model and store results 'm'\nm.or &lt;- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n# summarize model\nsummary(m.or)\n\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                          Value  Std. Error     t value\nInternalInternal   1.0476639736 0.265789132 3.941711114\nExchangeNoExchange 0.0586810805 0.297858820 0.197009713\nFinalScore         0.6157435960 0.260631273 2.362508493\n\nIntercepts:\n                            Value       Std. Error  t value    \nunlikely|somewhat likely    2.261997617 0.882173598 2.564118471\nsomewhat likely|very likely 4.357441779 0.904467831 4.817685749\n\nResidual Deviance: 717.024871356 \nAIC: 727.024871356 \n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n## store table\n(ctable &lt;- coef(summary(m.or)))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639736459 0.265789131533 3.941711113632\nExchangeNoExchange          0.0586810804767 0.297858819642 0.197009712679\nFinalScore                  0.6157435959832 0.260631273036 2.362508492595\nunlikely|somewhat likely    2.2619976171265 0.882173597890 2.564118471169\nsomewhat likely|very likely 4.3574417786829 0.904467830674 4.817685749458\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n## calculate and store p values\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable &lt;- cbind(ctable, \"p value\" = p))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639736459 0.265789131533 3.941711113632\nExchangeNoExchange          0.0586810804767 0.297858819642 0.197009712679\nFinalScore                  0.6157435959832 0.260631273036 2.362508492595\nunlikely|somewhat likely    2.2619976171265 0.882173597890 2.564118471169\nsomewhat likely|very likely 4.3574417786829 0.904467830674 4.817685749458\n                                        p value\nInternalInternal            0.00008090238230957\nExchangeNoExchange          0.84381993712009706\nFinalScore                  0.01815172553987886\nunlikely|somewhat likely    0.01034382311355037\nsomewhat likely|very likely 0.00000145232867123\n\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n# extract profiled confidence intervals\nci &lt;- confint(m.or)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m.or), ci))\n\n                              OR          2.5 %        97.5 %\nInternalInternal   2.85098336080 1.695837799502 4.81711408248\nExchangeNoExchange 1.06043699275 0.595033205643 1.91977108407\nFinalScore         1.85103250827 1.113625249814 3.09849059341\n\n\nThe odds ratios show that internal students are 2.85 times or 285 percent as likely as non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 times or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant."
  },
  {
    "objectID": "tutorials/regression/regression.html#poisson-regression",
    "href": "tutorials/regression/regression.html#poisson-regression",
    "title": "Introduction",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nThis section is based on this tutorials on how to perform a Poisson regression in R.\n\n\n\nNOTEPoisson regressions are used to analyze data where the dependent variable represents counts.\n\n\n\n\n\n\nThis applied particularly to counts that are based on observations of something that is measured in set intervals. For instances the number of pauses in two-minute-long conversations. Poisson regressions are particularly appealing when dealing with rare events, i.e. when something only occurs very infrequently. In such cases, normal linear regressions do not work because the instances that do occur are automatically considered outliers. Therefore, it is useful to check if the data conform to a Poisson distribution.\nHowever, the tricky thing about Poisson regressions is that the data has to conform to the Poisson distribution which is, according to my experience, rarely the case, unfortunately. The Gaussian Normal Distribution is very flexible because it is defined by two parameters, the mean (mu, i.e. \\(\\mu\\)) and the standard deviation (sigma, i.e. \\(\\sigma\\)). This allows the normal distribution to take very different shapes (for example, very high and slim (compressed) or very wide and flat). In contrast, the Poisson is defined by only one parameter (lambda, i.e. \\(\\lambda\\)) which mean that if we have a mean of 2, then the standard deviation is also 2 (actually we would have to say that the mean is \\(\\lambda\\) and the standard deviation is also \\(\\lambda\\) or \\(\\lambda\\) = \\(\\mu\\) = \\(\\sigma\\)). This is much trickier for natural data as this means that the Poisson distribution is very rigid.\n\n\n\n\n\n\n\n\n\nAs we can see, as \\(\\lambda\\) takes on higher values, the distribution becomes wider and flatter - a compressed distribution with a high mean can therefore not be Poisson-distributed. We will now start by loading the data.\n\n# load data\npoissondata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/prd.rda\", \"rb\"))\n\n\n\nIdPausesLanguageAlcohol450German411080Russian41150German44670German421530German40510Russian421640German461330German4020German33530German4610German401280English38160German441060German37890German40\n\n\nWe will clean the data by factorizing Id which is currently considered a numeric variable rather than a factor.\n\n# process data\npoissondata &lt;- poissondata %&gt;%\n  mutate(Id = factor(Id, levels = 1:200, labels = 1:200))\n# inspect data\nstr(poissondata)\n\n'data.frame':   200 obs. of  4 variables:\n $ Id      : Factor w/ 200 levels \"1\",\"2\",\"3\",\"4\",..: 45 108 15 67 153 51 164 133 2 53 ...\n $ Pauses  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Language: chr  \"German\" \"Russian\" \"German\" \"German\" ...\n $ Alcohol : int  41 41 44 42 40 42 46 40 33 46 ...\n\n\nFirst, we check if the conditions for a Poisson regression are met.\n\n# output the results\ngf = vcd::goodfit(poissondata$Pauses, \n                  type= \"poisson\", \n                  method= \"ML\")\n# inspect results\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df            P(&gt; X^2)\nLikelihood Ratio 33.0122916717  5 0.00000374234139957\n\n\nIf the p-values is smaller than .05, then data is not Poisson distributed which means that it differs significantly from a Poisson distribution and is very likely over-dispersed. We will check the divergence from a Poisson distribution visually by plotting the observed counts against the expected counts if the data were Poisson distributed.\n\nplot(gf,main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\nAlthough the goodfit function reported that the data differs significantly from the Poisson distribution, the fit is rather good. We can use an additional Levene’s test to check if variance homogeneity is given.\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\n\n# check homogeneity\nleveneTest(poissondata$Pauses, poissondata$Language, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df  F value        Pr(&gt;F)    \ngroup   2 17.15274 0.00000013571 ***\n      197                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test indicates that variance homogeneity is also violated. Since both the approximation to a Poisson distribution and variance homogeneity are violated, we should switch either to a quasi-Poisson model or a negative binomial model. However, as we are only interested in how to implement a Poisson model here, we continue despite the fact that this could not be recommended if we were actually interested in accurate results based on a reliable model.\nIn a next step, we summarize Progression by inspecting the means and standard deviations of the individual variable levels.\n\n# extract mean and standard deviation\nwith(poissondata, tapply(Pauses, Language, function(x) {\n  sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n\n               English                 German                Russian \n\"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\" \"M (SD) = 0.20 (0.40)\" \n\n\nNow, we visualize the data.\n\n# plot data\nggplot(poissondata, aes(Pauses, fill = Language)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  scale_fill_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\n\n\n\n\n\n# calculate Poisson regression\nm1.poisson &lt;- glm(Pauses ~ Language + Alcohol, family=\"poisson\", data=poissondata)\n# inspect model\nsummary(m1.poisson)\n\n\nCall:\nglm(formula = Pauses ~ Language + Alcohol, family = \"poisson\", \n    data = poissondata)\n\nCoefficients:\n                     Estimate    Std. Error  z value         Pr(&gt;|z|)    \n(Intercept)     -4.1632652529  0.6628774832 -6.28060 0.00000000033728 ***\nLanguageGerman  -0.7140499158  0.3200148750 -2.23130         0.025661 *  \nLanguageRussian -1.0838591456  0.3582529824 -3.02540         0.002483 ** \nAlcohol          0.0701523975  0.0105992050  6.61865 0.00000000003625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.6722345  on 199  degrees of freedom\nResidual deviance: 189.4496199  on 196  degrees of freedom\nAIC: 373.5045031\n\nNumber of Fisher Scoring iterations: 6\n\n\nIn addition to the Estimates for the coefficients, we could also calculate the confidence intervals for the coefficients (LL stands for lower limit and UL for upper limit in the table below).\n\n# calculate model\ncov.m1 &lt;- sandwich::vcovHC(m1.poisson, type=\"HC0\")\n# extract standard error\nstd.err &lt;- sqrt(diag(cov.m1))\n# extract robust estimates\nr.est &lt;- cbind(Estimate= coef(m1.poisson), \n               \"Robust SE\" = std.err,\n               \"Pr(&gt;|z|)\" = 2 * pnorm(abs(coef(m1.poisson)/std.err),\n                                      lower.tail=FALSE),\nLL = coef(m1.poisson) - 1.96 * std.err,\nUL = coef(m1.poisson) + 1.96 * std.err)\n# inspect data\nr.est\n\n                        Estimate       Robust SE                 Pr(&gt;|z|)\n(Intercept)     -4.1632652529178 0.6480942940026 0.0000000001328637743949\nLanguageGerman  -0.7140499157783 0.2986422497410 0.0168031204011183689373\nLanguageRussian -1.0838591456208 0.3210481575684 0.0007354744824167337974\nAlcohol          0.0701523974937 0.0104351647012 0.0000000000178397516955\n                              LL               UL\n(Intercept)     -5.4335300691629 -2.8930004366726\nLanguageGerman  -1.2993887252708 -0.1287111062859\nLanguageRussian -1.7131135344549 -0.4546047567867\nAlcohol          0.0496994746793  0.0906053203082\n\n\nWe can now calculate the p-value of the model.\n\nwith(m1.poisson, cbind(res.deviance = deviance, df = df.residual,\n  p = pchisq(deviance, df.residual, lower.tail=FALSE)))\n\n     res.deviance  df              p\n[1,] 189.44961991 196 0.618227445717\n\n\nNow, we check, if removing Language leads to a significant decrease in model fit.\n\n# remove Language from the model\nm2.poisson &lt;- update(m1.poisson, . ~ . -Language)\n# check if dropping Language causes a significant decrease in model fit\nanova(m2.poisson, m1.poisson, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Pauses ~ Alcohol\nModel 2: Pauses ~ Language + Alcohol\n  Resid. Df  Resid. Dev Df   Deviance   Pr(&gt;Chi)    \n1       198 204.0213018                             \n2       196 189.4496199  2 14.5716819 0.00068517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now calculate robust coefficients using the msm package (Jackson 2011).\n\n# get estimates\ns &lt;- msm::deltamethod(list(~ exp(x1), ~ exp(x2), ~ exp(x3), ~ exp(x4)), \n                                                coef(m1.poisson), cov.m1)\n# exponentiate old estimates dropping the p values\nrexp.est &lt;- exp(r.est[, -3])\n# replace SEs with estimates for exponentiated coefficients\nrexp.est[, \"Robust SE\"] &lt;- s\n# display results\nrexp.est\n\n                       Estimate       Robust SE               LL\n(Intercept)     0.0155566784084 0.0100821945101 0.00436765044896\nLanguageGerman  0.4896571063601 0.1462322998451 0.27269843575906\nLanguageRussian 0.3382875026084 0.1086065794408 0.18030353649622\nAlcohol         1.0726716412682 0.0111935052470 1.05095521026088\n                             UL\n(Intercept)     0.0554097096208\nLanguageGerman  0.8792279322820\nLanguageRussian 0.6346987787641\nAlcohol         1.0948368101199\n\n\n\n# extract predicted values\n(s1 &lt;- data.frame(Alcohol = mean(poissondata$Alcohol),\n  Language = factor(1:3, levels = 1:3, labels = names(table(poissondata$Language)))))\n\n  Alcohol Language\n1  52.645  English\n2  52.645   German\n3  52.645  Russian\n\n# show results\npredict(m1.poisson, s1, type=\"response\", se.fit=TRUE)\n\n$fit\n             1              2              3 \n0.624944591447 0.306008560283 0.211410945109 \n\n$se.fit\n              1               2               3 \n0.0862811728183 0.0883370633684 0.0705010813453 \n\n$residual.scale\n[1] 1\n\n\n\n## calculate and store predicted values\npoissondata$Predicted &lt;- predict(m1.poisson, type=\"response\")\n## order by program and then by math\npoissondata &lt;- poissondata[with(poissondata, order(Language, Alcohol)), ]\n\n\n## create the plot\nggplot(poissondata, aes(x = Alcohol, y = Predicted, colour = Language)) +\n  geom_point(aes(y = Pauses), alpha=.5, \n             position=position_jitter(h=.2)) +\n  geom_line(size = 1) +\n  labs(x = \"Alcohol (ml)\", y = \"Expected number of pauses\") +\n  scale_color_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))"
  },
  {
    "objectID": "tutorials/regression/regression.html#robust-regression",
    "href": "tutorials/regression/regression.html#robust-regression",
    "title": "Introduction",
    "section": "Robust Regression",
    "text": "Robust Regression\nRobust regression represent an alternative to simple linear models which can handle overly influential data points (outliers). Robust regressions allow us to retain outliers in the data rather than having to remove them from the data by adding weights (Rousseeuw and Leroy). Thus, robust regressions are used when there are outliers present in the data and we can thus not use traditional models but we have no good argument to remove these data points.\n\n\n\nNOTERobust regressions allow us to handle overly influential data points (outliers) by using weights. Thus, robust regressions enable us to retain all data points.\n\n\n\n\n\n\nWe begin by loading a data set (the mlrdata set which have used for multiple linear regression).\n\n# load data\nrobustdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/mld.rda\", \"rb\"))\n\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nWe first fit an ordinary linear model (and although we know from the section on multiple regression that the interaction between status and attraction is significant, we will disregard this for now as this will help to explain the weighing procedure which is the focus of this section).\n\n# create model\nslm &lt;- lm(money ~ status+attraction, data = robustdata)\n# inspect model\nsummary(slm)\n\n\nCall:\nlm(formula = money ~ status + attraction, data = robustdata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-60.87070 -15.78645  -2.61010  13.88770  59.93710 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             114.94950000   4.28993616  26.79515\nstatusSingle             26.10340000   4.95359160   5.26959\nattractionNotInterested -79.25220000   4.95359160 -15.99894\n                                      Pr(&gt;|t|)    \n(Intercept)             &lt; 0.000000000000000222 ***\nstatusSingle                     0.00000082576 ***\nattractionNotInterested &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.767958 on 97 degrees of freedom\nMultiple R-squared:  0.745229335,   Adjusted R-squared:  0.739976331 \nF-statistic: 141.867286 on 2 and 97 DF,  p-value: &lt; 0.0000000000000002220446\n\n\nWe now check whether the model is well fitted using diagnostic plots.\n\n# generate plots\nautoplot(slm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\n\n\n\n\nThe diagnostic plots indicate that there are three outliers in the data (data points 52, 83 and possibly 64). Therefore, we need to evaluate if the outliers severely affect the fit of the model.\n\nrobustdata[c(52, 64, 83),]\n\n   status    attraction  money\n52 Single NotInterested   0.93\n64 Single NotInterested  84.28\n83 Single    Interested 200.99\n\n\nWe can now calculate Cook’s distance and standardized residuals check if the values of the potentially problematic points have unacceptably high values (-2 &lt; ok &lt; 2).\n\nCooksDistance &lt;- cooks.distance(slm)\nStandardizedResiduals &lt;- stdres(slm)\na &lt;- cbind(robustdata, CooksDistance, StandardizedResiduals)\na[CooksDistance &gt; 4/100, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n\n\nWe will calculate the absolute value and reorder the table so that it is easier to check the values.\n\nAbsoluteStandardizedResiduals &lt;- abs(StandardizedResiduals)\na &lt;- cbind(robustdata, CooksDistance, StandardizedResiduals, AbsoluteStandardizedResiduals)\nasorted &lt;- a[order(-AbsoluteStandardizedResiduals), ]\nasorted[1:10, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n78       Single    Interested 188.76 0.0394313968241         1.95572122040\n21 Relationship NotInterested  81.90 0.0369837409025         1.89404933081\n24 Relationship NotInterested  81.56 0.0364414260634         1.88011125419\n   AbsoluteStandardizedResiduals\n52                 2.49535435377\n83                 2.45708203516\n88                 2.16151282221\n84                 2.15782333134\n1                  2.07565427025\n65                 2.03662765573\n67                 1.98907421786\n78                 1.95572122040\n21                 1.89404933081\n24                 1.88011125419\n\n\nAs Cook’s distance and the standardized residuals do have unacceptable values, we re-calculate the linear model as a robust regression and inspect the results\n\n# create robust regression model\nrmodel &lt;- robustbase::lmrob(money ~ status + attraction, data = robustdata)\n# inspect model\nsummary(rmodel)\n\n\nCall:\nrobustbase::lmrob(formula = money ~ status + attraction, data = robustdata)\n \\--&gt; method = \"MM\"\nResiduals:\n         Min           1Q       Median           3Q          Max \n-61.14269796 -15.20405781  -1.48712081  14.43502508  62.42342804 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             113.18405781   3.89777692  29.03811\nstatusSingle             25.38251415   5.08841106   4.98830\nattractionNotInterested -76.49387400   5.06626449 -15.09867\n                                      Pr(&gt;|t|)    \n(Intercept)             &lt; 0.000000000000000222 ***\nstatusSingle                      0.0000026725 ***\nattractionNotInterested &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 22.3497532 \nMultiple R-squared:  0.740716949,   Adjusted R-squared:  0.735370907 \nConvergence in 11 IRWLS iterations\n\nRobustness weights: \n 10 weights are ~= 1. The remaining 90 ones are summarized as\n       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. \n0.415507215 0.856134403 0.947485769 0.889078657 0.986192099 0.998890516 \nAlgorithmic parameters: \n           tuning.chi                    bb            tuning.psi \n1.5476399999999999046 0.5000000000000000000 4.6850610000000001421 \n           refine.tol               rel.tol             scale.tol \n0.0000001000000000000 0.0000001000000000000 0.0000000001000000000 \n            solve.tol              zero.tol           eps.outlier \n0.0000001000000000000 0.0000000001000000000 0.0010000000000000000 \n                eps.x     warn.limit.reject     warn.limit.meanrw \n0.0000000000018189894 0.5000000000000000000 0.5000000000000000000 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nThe output shows that both status and attraction are significant but, as we have seen above, the effect that really matters is the interaction between status and attraction.\nWe will briefly check the weights to understand the process of weighing better. The idea of weighing is to downgrade data points that are too influential while not punishing data points that have a good fit and are thus less influential. This means that the problematic data points should have lower weights than other data points (the maximum is 1 - so points can only be made “lighter”).\n\nhweights &lt;- data.frame(status = robustdata$status, resid = rmodel$resid, weight = rmodel$rweights)\nhweights2 &lt;- hweights[order(rmodel$rweights), ]\nhweights2[1:15, ]\n\n         status          resid         weight\n83       Single  62.4234280426 0.415507214894\n52       Single -61.1426979566 0.434323578289\n88       Single  55.2134280426 0.521220526760\n84       Single  55.1234280426 0.522529106622\n78       Single  50.1934280426 0.593234343069\n65       Single -49.9526979566 0.596626306571\n1  Relationship  49.6398161925 0.601024865582\n67       Single -48.7926979566 0.612874578464\n21 Relationship  45.2098161925 0.661914499988\n24 Relationship  44.8698161925 0.666467581490\n39 Relationship -43.8940578083 0.679427975993\n79       Single  40.8234280426 0.719104361372\n58       Single -40.5226979566 0.722893449575\n89       Single  39.9734280426 0.729766992775\n95       Single  39.8234280426 0.731633375015\n\n\nThe values of the weights support our assumption that those data points that were deemed too influential are made lighter as they now only have weights of 0.415507214894 and 0.434323578289 respectively. This was, however, not the focus of this sections as this section merely served to introduce the concept of weights and how they can be used in the context of a robust linear regression."
  },
  {
    "objectID": "tutorials/regression/regression.html#linear-mixed-effects-regression",
    "href": "tutorials/regression/regression.html#linear-mixed-effects-regression",
    "title": "Introduction",
    "section": "Linear Mixed-Effects Regression",
    "text": "Linear Mixed-Effects Regression\nThe following focuses on an extension of ordinary multiple linear regressions: mixed-effects regression linear regression. Mixed-effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. (multi-)collinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of mixed-effects regression modeling are that they are prone to producing high \\(\\beta\\)-errors (see Johnson) and that they require rather large data sets.\n\nIntroduction\nSo far, the regression models that we have used only had fixed-effects. Having only fixed-effects means that all data points are treated as if they are completely independent and thus on the same hierarchical level. However, it is very common that the data is nested in the sense that data points are not independent because they are, for instance produced by the same speaker or are grouped by some other characteristic. In such cases, the data is considered hierarchical and statistical models should incorporate such structural features of the data they work upon. Fortunately, modeling hierarchical or nested data structures is very easy thanks to the lme4 package (Bates et al. 2015).\nWith respect to regression modeling, hierarchical structures are incorporated by what is called random effects. When models only have a fixed-effects structure, then they make use of only a single intercept and/or slope (as in the left panel in the figure below), while mixed effects models have intercepts for each level of a random effect. If the random effect structure represents speakers then this would mean that a mixed-model would have a separate intercept and/or slope for each speaker (in addition to the overall intercept that is shown as an orange line in the figure below).\n\n\n\n\n\n\n\n\n\nThe idea behind regression analysis is expressed formally in the equation below where\\(f_{(x)}\\) is the y-value we want to predict, \\(\\alpha\\) is the intercept (the point where the regression line crosses the y-axis at x = 0), \\(\\beta\\) is the coefficient (the slope of the regression line), and x is the value of a predictor (e.g. 180cm - if we would like to predict the weight of a person based on their height). The \\(\\epsilon\\) is an error term that reflects the difference between the predicted value and the (actually) observed value (\\(\\epsilon\\) is thus a residual that is important as regressions assume that residuals are, e.g., normally distributed).\n\\[\\begin{equation}\nf_{(x)} = \\alpha + \\beta x + \\epsilon\n\\end{equation}\\]\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (\\(x\\)) and add the value of the intercept (point where line crosses the y-axis at x = 0).\nThe equation below represents a formal representation of a mixed-effects regression with varying intercepts (see Winter, 235).\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta x + \\epsilon\n\\end{equation}\\]\nIn this random intercept model, each level of a random variable has a different intercept. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the product of the predictor coefficient and the value of x.\nFinally, the equation below represents a formal representation of a mixed-effects regression with varying intercepts and varying slopes (see Winter, 235).\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta_{i}x + \\epsilon\n\\end{equation}\\]\nIn this last model, each level of a random variable has a different intercept and a different slope. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the coefficient of that random effect level multiplied by the value of x.\n\n\nRandom Effects\nRandom Effects can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to fixed-effects models, that have only 1 intercept and one slope (left panel in the figure above), mixed-effects models can therefore have various random intercepts (center panel) or various random slopes, or both, various random intercepts and various random slopes (right panel).\nWhat features do distinguish random and fixed effects?\n\nRandom effects represent a higher level variable under which data points are grouped. This implies that random effects must be categorical (or nominal but they a´cannot be continuous!) (see Winter, 236).\nRandom effects represent a sample of an infinite number of possible levels. For instance, speakers, trials, items, subjects, or words represent a potentially infinite pool of elements from which many different samples can be drawn. Thus, random effects represent a random sample sample. Fixed effects, on the other hand, typically do not represent a random sample but a fixed set of variable levels (e.g. Age groups, or parts-of-speech).\nRandom effects typically represent many different levels while fixed effects typically have only a few. Zuur, Hilbe, and Ieno propose that a variable may be used as a fixed effect if it has less than 5 levels while it should be treated as a random effect if it has more than 10 levels. Variables with 5 to 10 levels can be used as both. However, this is a rule of thumb and ignores the theoretical reasons (random sample and nestedness) for considering something as a random effect and it also is at odds with the way that repeated measures are models (namely as mixed effects) although they typically only have very few levels.\nFixed effects represent an effect that if we draw many samples, the effect would be consistent across samples (Winter) while random effects should vary for each new sample that is drawn.\n\nIn the following, we will only focus on models with random intercepts because this is the more common method and because including both random intercepts and random slopes requires larger data sets (but have a better fit because intercepts are not forced to be parallel and the lines therefore have a better fit). You should, however, always think about what random effects structure is appropriate for your model - a very recommendable explanation of how to chose which random effects structure is best (and about what the determining factors for this decision are) is give in Winter (241–44). Also, consider the center and the right plots above to understand what is meant by random intercepts and random slopes.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted (Field, Miles, and Field). We test whether including random effects is warranted by comparing a model, that bases its estimates of the depended variable solely on the base intercept (the mean), with a model, that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the random-effect model explains significantly more variance than the simple model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified if they reduce residual deviance.\n\n\nExample: Preposition Use across Time by Genre\nTo explore how to implement a mixed-effects model in R we revisit the preposition data that contains relative frequencies of prepositions in English texts written between 1150 and 1913. As a first step, and to prepare our analysis, we load necessary R packages, specify options, and load as well as provide an overview of the data.\n\n# load data\nlmmdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/lmd.rda\", \"rb\")) %&gt;%\n  # convert date into a numeric variable\n  dplyr::mutate(Date = as.numeric(Date))\n\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nThe data set contains the date when the text was written (Date), the genre of the text (Genre), the name of the text (Text), the relative frequency of prepositions in the text (Prepositions), and the region in which the text was written (Region). We now plot the data to get a first impression of its structure.\n\np1 &lt;- ggplot(lmmdata, aes(x = Date, y = Prepositions)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, color = \"red\", linetype = \"dashed\") +\n  theme_bw() +\n  labs(y = \"Frequency\\n(Prepositions)\")\np2 &lt;- ggplot(lmmdata, aes(x = reorder(Genre, -Prepositions), y = Prepositions)) +\n  geom_boxplot() +\n  theme_bw() + \n  theme(axis.text.x = element_text(angle=90)) +\n  labs(x = \"Genre\", y = \"Frequency\\n(Prepositions)\")\np3 &lt;- ggplot(lmmdata, aes(Prepositions)) +\n  geom_histogram() +\n  theme_bw() + \n  labs(y = \"Count\", x = \"Frequency (Prepositions)\")\ngridExtra::grid.arrange(grobs = list(p1, p2, p3), widths = c(1, 1), layout_matrix = rbind(c(1, 1), c(2, 3)))\n\n\n\n\n\n\n\n\nThe scatter plot in the upper panel indicates that the use of prepositions has moderately increased over time while the boxplots in the lower left panel show that the genres differ quite substantially with respect to their median frequencies of prepositions per text. Finally, the histogram in the lower right panel show that preposition use is distributed normally with a mean of 132.2 prepositions per text.\n\np4 &lt;- ggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  labs(x = \"Year\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\np5 &lt;- ggplot(lmmdata, aes(Region, Prepositions)) +\n  geom_boxplot() +\n  labs(x = \"Region\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\ngridExtra::grid.arrange(p4, p5, nrow = 1)\n\n\n\n\n\n\n\n\n\nggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  facet_wrap(~ Genre, nrow = 4) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs(x = \"Date of composition\", y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\n\n\n\n\nCentering or even scaling numeric variables is useful for later interpretation of regression models: if the date variable were not centered, the regression would show the effects of variables at year 0(!). If numeric variables are centered, other variables are variables are considered relative not to 0 but to the mean of that variable (in this case the mean of years in our data). Centering simply means that the mean of the numeric variable is subtracted from each value.\n\nlmmdata$DateUnscaled &lt;- lmmdata$Date\nlmmdata$Date &lt;- scale(lmmdata$Date, scale = F)\n\n\n\nDateGenreTextPrepositionsRegionDateUnscaled109.8696461825Sciencealbin166.01North1,73684.8696461825Educationanon139.86North1,711181.8696461825PrivateLetterausten130.78North1,808251.8696461825Educationbain151.29North1,878116.8696461825Educationbarclay145.72North1,743281.8696461825Educationbenson120.77North1,908279.8696461825Diarybenson119.17North1,906270.8696461825Philosophyboethja132.96North1,897158.8696461825Philosophyboethri130.49North1,785149.8696461825Diaryboswell135.94North1,776278.8696461825Travelbradley154.20North1,90584.8696461825Educationbrightland149.14North1,711135.8696461825Sermonburton159.71North1,76299.8696461825Sermonbutler157.49North1,726208.8696461825PrivateLettercarlyle124.16North1,835\n\n\nWe now set up a fixed-effects model with the glm function and a mixed-effects model using the glmer function from the lme4 package (Bates et al. 2015) with Genre as a random effect.\n\n# generate models\nm0.glm &lt;- glm(Prepositions ~ 1, family = gaussian, data = lmmdata)\nm0.lmer = lmer(Prepositions ~ 1 + (1|Genre), REML = T, data = lmmdata)\n\nNow that we have created the base-line models, we will test whether including a random effect structure is mathematically justified. It is important to note here that we are not going to test if including a random effect structure is theoretically motivated but simply if it causes a decrease in variance.\n\n\nTesting Random Effects\nAs a first step in the modeling process, we now need to determine whether or not including a random effect structure is justified. We do so by comparing the AIC of the base-line model without random intercepts to the AIC of the model with random intercepts.\n\nAIC(logLik(m0.glm))\n\n[1] 4718.19031114\n\nAIC(logLik(m0.lmer))\n\n[1] 4497.77554693\n\n\nThe inclusion of a random effect structure with random intercepts is justified as the AIC of the model with random intercepts is substantially lower than the AIC of the model without random intercepts.\nWhile I do not how how to test if including a random effect is justified, there are often situations, which require to test exactly which random effect structure is best. When doing this, it is important to use restricted maximum likelihood (REML = TRUE or method = REML) rather than maximum likelihood (see Pinheiro and Bates; Winter, 226).\n\n# generate models with 2 different random effect structures\nma.lmer = lmer(Prepositions ~ Date + (1|Genre), REML = T, data = lmmdata)\nmb.lmer = lmer(Prepositions ~ Date + (1 + Date | Genre), REML = T, data = lmmdata)\n# compare models\nanova(ma.lmer, mb.lmer, test = \"Chisq\", refit = F)\n\nData: lmmdata\nModels:\nma.lmer: Prepositions ~ Date + (1 | Genre)\nmb.lmer: Prepositions ~ Date + (1 + Date | Genre)\n        npar         AIC         BIC       logLik    deviance    Chisq Df\nma.lmer    4 4499.148092 4516.292084 -2245.574046 4491.148092            \nmb.lmer    6 4486.699509 4512.415498 -2237.349755 4474.699509 16.44858  2\n        Pr(&gt;Chisq)    \nma.lmer               \nmb.lmer 0.00026806 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model comparison shows that the model with the more complex random effect structure has a significantly better fit to the data compared with the model with the simpler random effect structure. However, we will continue with the model with the simpler structure because this is just an example.\n\n\n\nNOTEIn a real analysis, we would switch to a model with random intercepts and random slopes for Genre because it has a significantly better fit to the data.\n\n\n\n\n\n\n\n\nModel Fitting\nAfter having determined that including a random effect structure is justified, we can continue by fitting the model and including diagnostics as we go. Including diagnostics in the model fitting process can save time and prevent relying on models which only turn out to be unstable if we would perform the diagnostics after the fact.\nWe begin fitting our model by adding Date as a fixed effect and compare this model to our mixed-effects base-line model to see if Date improved the model fit by explaining variance and if Date significantly correlates with our dependent variable (this means that the difference between the models is the effect (size) of Date!)\n\nm1.lmer &lt;- lmer(Prepositions ~ (1|Genre) + Date, REML = T, data = lmmdata)\nanova(m1.lmer, m0.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm0.lmer: Prepositions ~ 1 + (1 | Genre)\nm1.lmer: Prepositions ~ (1 | Genre) + Date\n        npar         AIC         BIC       logLik    deviance  Chisq Df\nm0.lmer    3 4501.947337 4514.805331 -2247.973668 4495.947337          \nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736 8.9296  1\n        Pr(&gt;Chisq)   \nm0.lmer              \nm1.lmer  0.0028059 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nThe model with Date is the better model (significant p-value and lower AIC). The significant p-value shows that Date correlates significantly with Prepositions (\\(\\chi\\)2(1): 8.929600937904, p = 0.00281) . The \\(\\chi\\)2 value here is labeled Chisq and the degrees of freedom are calculated by subtracting the smaller number of DFs from the larger number of DFs.\nWe now test whether Region should also be part of the final minimal adequate model. The easiest way to add predictors is by using the update function (it saves time and typing).\n\n# generate model\nm2.lmer &lt;- update(m1.lmer, .~.+ Region)\n# test vifs\ncar::vif(m2.lmer)\n\n         Date        Region \n1.20287668037 1.20287668037 \n\n# compare models                \nanova(m2.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm2.lmer: Prepositions ~ (1 | Genre) + Date + Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm2.lmer    5 4494.624343 4516.054333 -2242.312171 4484.624343 2.39339  1\n        Pr(&gt;Chisq)\nm1.lmer           \nm2.lmer    0.12185\n\n\nThree things tell us that Region should not be included:\n\nthe AIC does not decrease,\nthe BIC increases(!), and\nthe p-value is higher than .05.\n\nThis means, that we will continue fitting the model without having Region included. Well… not quite - just as a note on including variables: while Region is not significant as a main effect, it must still be included in a model if it were part of a significant interaction. To test if this is indeed the case, we fit another model with the interaction between Date and Region as predictor.\n\n# generate model\nm3.lmer &lt;- update(m1.lmer, .~.+ Region*Date)\n# extract vifs\ncar::vif(m3.lmer)\n\n         Date        Region   Date:Region \n1.96923042279 1.20324697637 1.78000887980 \n\n# compare models                \nanova(m3.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm3.lmer: Prepositions ~ (1 | Genre) + Date + Region + Date:Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm3.lmer    6 4496.124872 4521.840861 -2242.062436 4484.124872 2.89286  2\n        Pr(&gt;Chisq)\nm1.lmer           \nm3.lmer    0.23541\n\n\nAgain, the high p-value and the increase in AIC and BIC show that we have found our minimal adequate model with only contains Date as a main effect. In a next step, we can inspect the final minimal adequate model, i.e. the most parsimonious (the model that explains a maximum of variance with a minimum of predictors).\n\n# inspect results\nsummary(m1.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Prepositions ~ (1 | Genre) + Date\n   Data: lmmdata\n\nREML criterion at convergence: 4491.1\n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-3.734915441 -0.657038004  0.005865025  0.661298615  3.596659863 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev.  \n Genre    (Intercept) 159.021120 12.6103576\n Residual             228.764179 15.1249522\nNumber of obs: 537, groups:  Genre, 16\n\nFixed effects:\n                   Estimate      Std. Error  t value\n(Intercept) 133.88516211469   3.24749296223 41.22724\nDate          0.01894493515   0.00632363682  2.99589\n\nCorrelation of Fixed Effects:\n     (Intr)\nDate 0.005 \n\n\n\n\nModel Diagnostics\nWe can now evaluate the goodness of fit of the model and check if mathematical requirements and assumptions have been violated. In a first step, we generate diagnostic plots that focus on the random effect structure.\n\nplot(m1.lmer, Genre ~ resid(.), abline = 0 ) # generate diagnostic plots\n\n\n\n\n\n\n\n\nThe plot shows that there are some outliers (points outside the boxes) and that the variability within letters is greater than in other genres we therefore examine the genres in isolation standardized residuals versus fitted values (Pinheiro and Bates, 175).\n\nplot(m1.lmer, resid(., type = \"pearson\") ~ fitted(.) | Genre, id = 0.05, \n     adj = -0.3, pch = 20, col = \"gray40\")\n\n\n\n\n\n\n\n\nThe plot shows the standardized residuals (or Pearson’s residuals) versus fitted values and suggests that there are outliers in the data (the names elements in the plots). To check if these outliers are a cause for concern, we will now use a Levene’s test to check if the variance is distributed homogeneously (homoscedasticity) or whether the assumption of variance homogeneity is violated (due to the outliers).\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\nWe use Levene’s test here merely to check if it substantiates the impressions we got from the visual inspection.\n\n# check homogeneity\nleveneTest(lmmdata$Prepositions, lmmdata$Genre, center = mean)\n\nWarning in leveneTest.default(lmmdata$Prepositions, lmmdata$Genre, center =\nmean): lmmdata$Genre coerced to factor.\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value   Pr(&gt;F)  \ngroup  15 1.74289 0.039906 *\n      521                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test shows that the variance is distributed unevenly across genres which means that we do not simply continue but should either remove problematic data points (outliers) or use a weighing method.\nIn this case, we create a new model which uses weights to compensate for heterogeneity of variance and thus the influence of outliers - which is an alternative to removing the data points and rerunning the analysis (Pinheiro and Bates, 177). However, to do so, we need to use a different function (the lme function) which means that we have to create two models: the old minimal adequate model and the new minimal adequate model with added weights. After we have created these models, we will compare them to see if including weights has improved the fit.\n\n# generate models\nm4.lme = lme(Prepositions ~ Date, random = ~1|Genre, data = lmmdata, method = \"ML\")\nm5.lme &lt;- update(m4.lme, weights = varIdent(form = ~ 1 | Genre))\n# compare models\nanova(m5.lme, m4.lme)\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm5.lme     1 19 4485.84955680 4567.28352060 -2223.92477840                     \nm4.lme     2  4 4495.01773596 4512.16172834 -2243.50886798 1 vs 2 39.1681791556\n       p-value\nm5.lme        \nm4.lme  0.0006\n\n\nThe weight model (m5.lme) that uses weights to account for unequal variance is performing significantly better than the model without weights (m4.lme) and we therefore switch to the weight model and inspect its parameters.\n\n# inspect results\nsummary(m5.lme)        \n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n           AIC          BIC        logLik\n  4485.8495568 4567.2835206 -2223.9247784\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2632323711 14.3422048362\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340715615490  0.869521038482  0.788861254265  0.911719712514 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096572672893  0.978728647599  0.784966066137  0.736986014758  1.190619042239 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929751328  0.974641038177  0.848591142831  0.970869209380  1.086232439777 \nTrialProceeding \n 1.260188475499 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640155217 3.144361414827 520 42.6045221424  0.0000\nDate          0.0217416268 0.005454723299 520  3.9858349603  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-3.319117397700 -0.679728744836  0.014685431575  0.698705968152  3.103872340648 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\nWe can also use an ANOVA display which is more to the point.\n\nanova(m5.lme)          \n\n            numDF denDF        F-value p-value\n(Intercept)     1   520 1813.892139648  &lt;.0001\nDate            1   520   15.886880331  0.0001\n\n\nAs we did before, we now check, whether the final minimal model (with weights) outperforms an intercept-only base-line model.\n\n# generate base-line model\nm0.lme = lme(Prepositions ~ 1, random = ~1|Genre, data = lmmdata, method = \"ML\", weights = varIdent(form = ~ 1 | Genre))\nanova(m0.lme, m5.lme)  # test if date is significant\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm0.lme     1 18 4496.28563021 4573.43359592 -2230.14281511                     \nm5.lme     2 19 4485.84955680 4567.28352060 -2223.92477840 1 vs 2 12.4360734115\n       p-value\nm0.lme        \nm5.lme  0.0004\n\n\nOur final minimal adequate model with weights performs significantly better than an intercept only base-line model. Before doing the final diagnostics, we well inspect the estimates for the random effect structure to check if there are values which require further inspection (e.g. because they are drastically different from all other values).\n\n# extract estimates and sd for fixed and random effects\nintervals(m5.lme, which=\"fixed\")      \n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                        lower              est.             upper\n(Intercept) 127.7983166699800 133.9640155217270 140.1297143734741\nDate          0.0110455977276   0.0217416268231   0.0324376559185\n\n\nThe random effect estimates do not show any outliers or drastically increased or decreased values which means that the random effect structure is fine.\n\n\nEffect Sizes\nWe will now extract effect sizes (in the example: the effect size of Date) and calculate normalized effect size measures (this effect size measure works for all fixed effects). When you have factorial design, you can take the square root of the squared t-value divided by the t-value squared plus the degrees of freedom to calculate the effect size:\n\\[\\begin{equation}\n\nr = \\sqrt{ \\frac{ t^2}{(t^2 + df) } } = \\sqrt{ \\frac{ 3.99^2}{(3.99^2 + 520) } } = 0.172\n\n\\end{equation}\\]\n\n\n\n\nNOTETwo words of warning though: br&gt;1. In our case, the effect we are interested in is not factorial but continuous which means that we should not use this effect size measure. We only show this here as an example for how you can calculate the effect size measure r.2. Only apply this function to main effects that are not involved in interactions as they are meaningless because the amount of variance explained by main effects involved in interactions is unclear (Field, Miles, and Field, 641).\n\n\n\n\n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n&lt;0.001\n\n\nDate\n0.02\n0.01 – 0.03\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.70\n\n\n\nτ00 Genre\n150.39\n\n\nICC\n0.42\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.432\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\nThe marginal R2 (marginal coefficient of determination) represents the variance explained by the fixed effects while the conditional R2 is interpreted as a variance explained by the entire model, including both fixed and random effects (Bartoń).\nThe respective call for the model is:\n\n# extract R2s\nr.squaredGLMM(m1.lmer)\n\n                 R2m            R2c\n[1,] 0.0121971160219 0.417270545268\n\n\nThe effects can be visualized using the plot_model function from the sjPlot package (Lüdecke).\n\nsjPlot::plot_model(m5.lme, type = \"pred\", terms = c(\"Date\")) +\n  # show uncentered date rather than centered date\n  scale_x_continuous(name = \"Date\", \n                     breaks = seq(-500, 300, 100), \n                     labels = seq(1150, 1950, 100))\n\n\n\n\n\n\n\n\nWhile we have already shown that the effect of Date is significant, it is small which means that the number of prepositions per text does not correlate very strongly with time. This suggests that other factors that are not included in the model also impact the frequency of prepositions (and probably more meaningfully, too).\nBefore turning to the diagnostics, we will use the fitted (or predicted) and the observed values with a regression line for the predicted values. This will not only show how good the model fit the data but also the direction and magnitude of the effect.\n\n# extract predicted values\nlmmdata$Predicted &lt;- predict(m5.lme, lmmdata)\n# plot predicted values\nggplot(lmmdata, aes(DateUnscaled, Predicted)) +\n  facet_wrap(~Genre) +\n  geom_point(aes(x = DateUnscaled, y = Prepositions), color = \"gray80\", size = .5) +\n  geom_smooth(aes(y = Predicted), color = \"gray20\", linetype = \"solid\", \n              se = T, method = \"lm\") +\n  guides(color=guide_legend(override.aes=list(fill=NA))) +  \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"top\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  xlab(\"Date of composition\")\n\n\n\n\n\n\n\n\n\n\nModel Diagnostics\nIn addition, we generate diagnostic plots. What we wish to see in the diagnostic plots is a cloud of dots in the middle of the window without any structure. What we do not want to see is a funnel-shaped cloud because this indicates an increase of the errors/residuals with an increase of the predictor(s) (because this would indicate heteroscedasticity) (Pinheiro and Bates, 182).\n\n# start plotting\npar(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns\nplot(m5.lme, pch = 20, col = \"black\", lty = \"dotted\"); par(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nWhat a wonderful unstructured cloud - the lack of structure tells us that the model is “healthy” and does not suffer from heteroscedasticity. We will now create more diagnostic plots to find potential problems (Pinheiro and Bates, 21).\n\n# fitted values by Genre\nplot(m5.lme, form = resid(., type = \"p\") ~ fitted(.) | Genre, abline = 0, \n     cex = .5, pch = 20, col = \"black\")\n\n\n\n\n\n\n\n\nIn contrast to the unweight model, no data points are named which indicates that the outliers do no longer have unwarranted influence on the model. Now, we check the residuals of fitted values against observed values (Pinheiro and Bates, 179). What we would like to see is a straight, upwards going line.\n\n# residuals of fitted values against observed\nqqnorm(m5.lme, pch = 20, col = \"black\")\n\n\n\n\n\n\n\n\nA beautiful, straight line! The qqplot does not indicate any problems. It is, unfortunately, rather common that the dots deviate from the straight line at the very bottom or the very top which means that the model is good at estimating values around the middle of the dependent variable but rather bad at estimating lower or higher values. Next, we check the residuals by “Genre” (Pinheiro and Bates, 179).\n\n# residuals by genre\nqqnorm(m5.lme, ~resid(.) | Genre, pch = 20, col = \"black\" )\n\n\n\n\n\n\n\n\nBeautiful straight lines - perfection! Now, we inspect the observed responses versus the within-group fitted values (Pinheiro and Bates, 178).\n\n# observed responses versus the within-group fitted values\nplot(m5.lme, Prepositions ~ fitted(.), id = 0.05, adj = -0.3, \n     xlim = c(80, 220), cex = .8, pch = 20, col = \"blue\")\n\n\n\n\n\n\n\n\nAlthough some data points are named, the plot does not show any structure, like a funnel, which would have been problematic.\n\n\nReporting Results\nBefore we do the write-up, we have a look at the model summary as this will provide us with at least some of the parameters that we want to report.\n\nsummary(m5.lme)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n           AIC          BIC        logLik\n  4485.8495568 4567.2835206 -2223.9247784\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2632323711 14.3422048362\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340715615490  0.869521038482  0.788861254265  0.911719712514 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096572672893  0.978728647599  0.784966066137  0.736986014758  1.190619042239 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929751328  0.974641038177  0.848591142831  0.970869209380  1.086232439777 \nTrialProceeding \n 1.260188475499 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640155217 3.144361414827 520 42.6045221424  0.0000\nDate          0.0217416268 0.005454723299 520  3.9858349603  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-3.319117397700 -0.679728744836  0.014685431575  0.698705968152  3.103872340648 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n&lt;0.001\n\n\nDate\n0.02\n0.01 – 0.03\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.70\n\n\n\nτ00 Genre\n150.39\n\n\nICC\n0.42\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.432\n\n\n\n\n\n\nWe can now use the information extracted above to write up a final report:\nA mixed-effect linear regression model which contained the genre of texts as random effect was fit to the data in a step-wise-step up procedure. Due to the presence of outliers in the data, weights were included into the model which led to a significantly improved model fit compared to an un-weight model (\\(\\chi\\)2(2): 39.17, p: 0.0006). The final minimal adequate model performed significantly better than an intercept-only base-line model (\\(\\chi\\)2(1): 12.44, p =.0004) and showed that the frequency of prepositions increases significantly but only marginally with the date of composition (Estimate: 0.02, CI: 0.01-0.03, p &lt; .001, marginal R2 = 0.0174, conditional R2 = 0.4324). Neither the region where the text was composed nor a higher order interaction between genre and region significantly correlated with the use of prepositions in the data.\n\n\nRemarks on Prediction\nWhile the number of intercepts, the model reports, and the way how mixed- and fixed-effects arrive at predictions differ, their predictions are extremely similar and almost identical (at least when dealing with a simple random effect structure). Consider the following example where we create analogous fixed and mixed effect models and plot their predicted frequencies of prepositions per genre across the un-centered date of composition. The predictions of the mixed-effects model are plotted as a solid red line, while the predictions of the fixed-effects model are plotted as dashed blue lines.\n\n# create lm model\nm5.lmeunweight &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmePredictions &lt;- fitted(m5.lmeunweight, lmmdata)\nm5.lm &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmPredictions &lt;- fitted(m5.lm, lmmdata)\n# plot predictions\nggplot(lmmdata, aes(x = DateUnscaled, y = lmePredictions, group = Genre)) +\n  geom_line(aes(y = lmmdata$lmePredictions), linetype = \"solid\", color = \"red\") +\n  geom_line(aes(y = lmmdata$lmPredictions), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ Genre, nrow = 4) +\n  theme_bw() +\n  labs(x = \"Date of composition\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\n\n\n\n\nThe predictions overlap almost perfectly which means that the predictions of both are almost identical - irrespective of whether genre is part of the mixed or the fixed effects structure."
  },
  {
    "objectID": "tutorials/regression/regression.html#mixed-effects-binomial-logistic-regression",
    "href": "tutorials/regression/regression.html#mixed-effects-binomial-logistic-regression",
    "title": "Introduction",
    "section": "Mixed-Effects Binomial Logistic Regression",
    "text": "Mixed-Effects Binomial Logistic Regression\nWe now turn to an extension of binomial logistic regression: mixed-effects binomial logistic regression. As is the case with linear mixed-effects models logistic mixed effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. multicollinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of regression modeling are that they are prone to producing high \\(\\beta\\)-errors (see Johnson) and that they require rather large data sets.\n\nIntroduction\nAs is the case with linear mixed-effects models, binomial logistic mixed-effect models are multivariate analyses that treat data points as hierarchical or grouped in some way. In other words, they take into account that the data is nested in the sense that data points are produced by the same speaker or are grouped by some other characteristics. In mixed-models, hierarchical structures are modelled as random effects. If the random effect structure represents speakers then this means that a mixed-model would have a separate intercept and/or slope for each speaker.\nRandom Effects in linear models can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to linear mixed-effects models, random effects differ in the position and the slope of the logistic function that is applied to the likelihood of the dependent variable. Random intercepts (upper right panel) or various random slopes (lower left panel), or both, various random intercepts and various random slopes (lower right panel). In the following, we will only focus on models with random intercepts because this is the by far more common method and because including both random intercepts and random slopes requires huge amounts of data. Consider the Figure below to understand what is meant by random intercepts.\n\n\n\n\n\n\n\n\n\nThe upper left panel merely shows the logistic curve representing the predictions of a fixed-effects logistic regression with a single intercept and slope. The upper right panel shows the logistic curves representing the predictions of a of a mixed-effects logistic regression with random intercepts for each level of a grouping variable. The lower left panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with one intercept but random slopes for each level of a grouping variable. The lower right panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with random intercepts and random slopes for each level of a grouping variable.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted (Field, Miles, and Field). We test whether including random effects is warranted by comparing a model, that bases its estimates of the dependent variable solely on the base intercept, with a model that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the mixed-effects model explains significantly more variance than the fixed-effects model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified.\n\n\nExample: Discourse LIKE in Irish English\nIn this example we will investigate which factors correlate with the use of final discourse like (e.g. “The weather is shite, like!”) in Irish English. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender (Gender: Men versus Women) and age of that speaker (Age: Old versus Young), whether the interlocutors were of the same or a different gender (ConversationType: SameGender versus MixedGender), and whether another final discourse like had been used up to three speech units before (Priming: NoPrime versus Prime), whether or not the speech unit contained an final discourse like (SUFLike: 1 = yes, 0 = no. To begin with, we load the data and inspect the structure of the data set,\n\n# load data\nmblrdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/mbd.rda\", \"rb\"))\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-061$CWomenYoungMixedGenderNoPrime0S1A-023$BWomenYoungMixedGenderNoPrime0S1A-054$AWomenYoungSameGenderNoPrime0S1A-090$BWomenYoungMixedGenderNoPrime0S1A-009$BWomenOldSameGenderPrime0S1A-085$EMenYoungMixedGenderPrime1S1A-003$CWomenYoungMixedGenderNoPrime1S1A-084$CWomenYoungSameGenderNoPrime0S1A-076$AWomenYoungSameGenderNoPrime0S1A-083$DMenOldMixedGenderNoPrime1S1A-068$AWomenYoungSameGenderNoPrime0S1A-066$BWomenYoungSameGenderNoPrime0S1A-061$AMenOldMixedGenderNoPrime1S1A-049$AWomenYoungSameGenderNoPrime0S1A-022$BWomenYoungMixedGenderNoPrime0\n\n\nAs all variables except for the dependent variable (SUFlike) are character strings, we factorize the independent variables.\n\n# def. variables to be factorized\nvrs &lt;- c(\"ID\", \"Age\", \"Gender\", \"ConversationType\", \"Priming\")\n# def. vector with variables\nfctr &lt;- which(colnames(mblrdata) %in% vrs)     \n# factorize variables\nmblrdata[,fctr] &lt;- lapply(mblrdata[,fctr], factor)\n# relevel Age (Young = Reference)\nmblrdata$Age &lt;- relevel(mblrdata$Age, \"Young\")\n# order data by ID\nmblrdata &lt;- mblrdata %&gt;%\n  dplyr::arrange(ID)\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderPrime0S1A-001$BWomenOldMixedGenderNoPrime1S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0\n\n\nBefore continuing, a few words about the minimum number of random effect levels and the minimum number of observations per random effect level are in order.\nWhile many data points per random variable level increases statistical power and thus to more robust estimates of the random effects (Austin and Leckie), it has been shown that small numbers of observations per random effect variable level do not cause serious bias and it does not negatively affect the estimates of the fixed-effects coefficients (Bell, Ferron, and Kromrey; Clarke; Clarke and Wheaton; Maas and Hox). The minimum number of observations per random effect variable level is therefore 1.\nIn simulation study, (Bell, Ferron, and Kromrey) tested the impact of random variable levels with only a single observation ranging from 0 to 70 percent. As long as there was a relatively high number of random effect variable levels (500 or more), small numbers of observations had almost no impact on bias and Type 1 error control.\nWe now plot the data to inspect the relationships within the data set.\n\nggplot(mblrdata, aes(Gender, SUFlike, color = Priming)) +\n  facet_wrap(Age~ConversationType) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of discourse like\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nThe upper left panel in the Figure above indicates that men use discourse like more frequently than women. The center right panel suggests that priming significantly increases the likelihood of discourse like being used. The center left panel suggests that speakers use discourse like more frequently in mixed-gender conversations. However, the lower right panel indicates an interaction between gender and conversation type as women appear to use discourse like less frequently in same gender conversations while the conversation type does not seem to have an effect on men. After visualizing the data, we will now turn to the model building process.\n\n\nModel Building\nAs a first step, we need to define contrasts and use the datadist function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. Field, Miles, and Field (414–27) and Gries provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more.\n\n# set options\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\nmblrdata.dist &lt;- datadist(mblrdata)\noptions(datadist = \"mblrdata.dist\")\n\nIn a next step, we generate fixed-effects minimal base-line models and a base-line mixed-model using the glmer function with a random intercept for ID (a lmer-object of the final minimal adequate model will be created later).\n\n# baseline model glm\nm0.glm = glm(SUFlike ~ 1, family = binomial, data = mblrdata) \n# base-line mixed-model\nm0.glmer = glmer(SUFlike ~ (1|ID), data = mblrdata, family = binomial) \n\n\n\nTesting the Random Effect\nNow, we check if including the random effect is permitted by comparing the AICs from the glm to AIC from the glmer model. If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that including random intercepts is justified.\n\naic.glmer &lt;- AIC(logLik(m0.glmer))\naic.glm &lt;- AIC(logLik(m0.glm))\naic.glmer; aic.glm\n\n[1] 1828.49227107\n\n\n[1] 1838.17334856\n\n\nThe AIC of the glmer object is smaller which shows that including the random intercepts is justified. To confirm whether the AIC reduction is sufficient for justifying the inclusion of a random-effect structure, we also test whether the mixed-effects minimal base-line model explains significantly more variance by applying a Model Likelihood Ratio Test to the fixed- and the mixed effects minimal base-line models.\n\n# test random effects\nnull.id = -2 * logLik(m0.glm) + 2 * logLik(m0.glmer)\npchisq(as.numeric(null.id), df=1, lower.tail=F) \n\n[1] 0.000631389572435\n\n# sig m0.glmer better than m0.glm\n\nThe p-value of the Model Likelihood Ratio Test is lower than .05 which shows that the inclusion of the random-effects structure is warranted. We can now continue with the model fitting process.\n\n\nModel Fitting\nThe next step is to fit the model which means that we aim to find the best model, i.e. the minimal adequate model. In this case, we will use the glmulti package to find the model with the lowest Bayesian Information Criterion (BIC) of all possible models. We add ´control = glmerControl(optimizer = “bobyqa”)´ to avoid unnecessary failures to converge.\n\n# wrapper function for linear mixed-models\nglmer.glmulti &lt;- function(formula,data, random=\"\",...){\n  glmer(paste(deparse(formula),random), \n        family = binomial, \n        data=data, \n        control=glmerControl(optimizer=\"bobyqa\"), ...)\n}\n# define formula\nform_glmulti = as.formula(paste(\"SUFlike ~ Gender + Age + ConversationType + Priming\"))\n# multi selection for glmer\nmfit &lt;- glmulti(form_glmulti,random=\"+(1 | ID)\", \n                data = mblrdata, method = \"h\", fitfunc = glmer.glmulti,\n                crit = \"bic\", intercept = TRUE, marginality = FALSE, level = 2)\n\nInitialization...\nTASK: Exhaustive screening of candidate set.\nFitting...\n\n\n\nAfter 50 models:\nBest model: SUFlike~1+Gender+ConversationType+Priming\nCrit= 1696.5877339969\nMean crit= 1753.96253323424\n\n\n\n\n\n\n\n\n\n\nAfter 100 models:\nBest model: SUFlike~1+Gender+ConversationType+Priming\nCrit= 1696.5877339969\nMean crit= 1731.89001011587\n\n\n\n\n\n\n\n\n\nCompleted.\n\n\nWe extract the best 5 models (best is here defined as the models with the lowest BIC).\n\n# extract best models\ntop &lt;- weightable(mfit)\ntop &lt;- top[1:5,]\n# inspect top 5 models\ntop\n\n                                                                                          model\n1                                             SUFlike ~ 1 + Gender + ConversationType + Priming\n2                            SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:Gender\n3 SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:Gender + Priming:ConversationType\n4                                               SUFlike ~ 1 + Gender + Priming + Priming:Gender\n5                  SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:ConversationType\n            bic         weights\n1 1696.58773400 0.2567900971793\n2 1696.76989551 0.2344349735686\n3 1696.76989551 0.2344349735686\n4 1699.62465884 0.0562494681405\n5 1699.83927868 0.0505259299850\n\n\nThe best model is has the formula SUFlike ~ 1 + Gender + ConversationType + Priming and we take this to be our final minimal adequate model, i.e. the most parsimonious model (the model which explains the relatively most variance with lowest number of predictors). Hence, we define our final minimal model and check its output.\n\nmlr.glmer &lt;- glmer(SUFlike ~ (1 | ID) + Gender + ConversationType + Priming, \n                   family = binomial,\n                   control=glmerControl(optimizer=\"bobyqa\"),\n                   data = mblrdata)\n# inspect final minimal adequate model\nsummary(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n   Data: mblrdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1668.6   1696.6   -829.3   1658.6     1995 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.579360650 -0.415523754 -0.330409042 -0.312054134  3.247885856 \n\nRandom effects:\n Groups Name        Variance     Std.Dev.   \n ID     (Intercept) 0.0837517494 0.289398945\nNumber of obs: 2000, groups:  ID, 208\n\nFixed effects:\n                               Estimate   Std. Error  z value\n(Intercept)                -1.067430840  0.149184383 -7.15511\nGenderWomen                -0.642887590  0.175327287 -3.66679\nConversationTypeSameGender -0.536428857  0.148819363 -3.60456\nPrimingPrime                1.866249307  0.163249208 11.43190\n                                         Pr(&gt;|z|)    \n(Intercept)                   0.00000000000083605 ***\nGenderWomen                            0.00024562 ***\nConversationTypeSameGender             0.00031268 ***\nPrimingPrime               &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now test whether the final minimal model performs significantly better than the minimal base-line model, and print the regression summary.\n\n# final model better than base-line model\nsigfit &lt;- anova(mlr.glmer, m0.glmer, test = \"Chi\") \n# inspect\nsigfit\n\nData: mblrdata\nModels:\nm0.glmer: SUFlike ~ (1 | ID)\nmlr.glmer: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n          npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.glmer     2 1828.492271 1839.694076 -912.2461355 1824.492271             \nmlr.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222 165.90905  3\n                      Pr(&gt;Chisq)    \nm0.glmer                            \nmlr.glmer &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# inspect final minimal adequate model\nprint(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n   Data: mblrdata\n      AIC       BIC    logLik  deviance  df.resid \n1668.5832 1696.5877 -829.2916 1658.5832      1995 \nRandom effects:\n Groups Name        Std.Dev.   \n ID     (Intercept) 0.289398945\nNumber of obs: 2000, groups:  ID, 208\nFixed Effects:\n               (Intercept)                 GenderWomen  \n              -1.067430840                -0.642887590  \nConversationTypeSameGender                PrimingPrime  \n              -0.536428857                 1.866249307  \n\n\n\n\nVisualizing Effects\nWe visualize the effects here by showing the probability of discourse like based on the predicted values.\n\nplot_model(mlr.glmer, type = \"pred\", terms = c(\"Gender\", \"Priming\", \"ConversationType\"))\n\n\n\n\n\n\n\n\nWe can see that discourse like is more likely to surface in primed contexts and among males. In conversations with both men and women, speakers use discourse like slightly less than in mixed conversations.\n\n\nExtracting Model Fit Parameters\nWe now extract model fit parameters (Baayen, 281).\n\nprobs = 1/(1+exp(-fitted(mlr.glmer)))\nprobs = binomial()$linkinv(fitted(mlr.glmer))\nsomers2(probs, as.numeric(mblrdata$SUFlike))\n\n                C               Dxy                 n           Missing \n   0.760226203516    0.520452407033 2000.000000000000    0.000000000000 \n\n\nThe two lines that start with probs are simply two different ways to do the same thing (you only need one of these).\nThe model fit parameters indicate a suboptimal fit. Both the C-value and Somers’s Dxy show poor fit between predicted and observed occurrences of discourse like. If the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity (Baayen, 204). Somers’ Dxy is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ Dxy values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction (Baayen, 204). The C.value of 0.760226203516 suggests that the model has some predictive and explanatory power, but not at an optimal level. We will now perform the model diagnostics.\n\n\nModel Diagnostics\nWe begin the model diagnostics by generating a diagnostic that plots the fitted or predicted values against the residuals.\n\nplot(mlr.glmer, pch = 20, col = \"black\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nAs a final step, we summarize our findings in tabulated form.\n\n# summarize final model\nsjPlot::tab_model(mlr.glmer)\n\n\n\n \nSUFlike\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.34\n0.26 – 0.46\n&lt;0.001\n\n\nGender [Women]\n0.53\n0.37 – 0.74\n&lt;0.001\n\n\nConversationType[SameGender]\n0.58\n0.44 – 0.78\n&lt;0.001\n\n\nPriming [Prime]\n6.46\n4.69 – 8.90\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 ID\n0.08\n\n\nICC\n0.02\n\n\nN ID\n208\n\nObservations\n2000\n\n\nMarginal R2 / Conditional R2\n0.131 / 0.152\n\n\n\n\n\n\n\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(mlr.glmer)\n\nWe fitted a logistic mixed model (estimated using ML and BOBYQA optimizer) to\npredict SUFlike with Gender, ConversationType and Priming (formula: SUFlike ~\nGender + ConversationType + Priming). The model included ID as random effect\n(formula: ~1 | ID). The model's total explanatory power is moderate\n(conditional R2 = 0.15) and the part related to the fixed effects alone\n(marginal R2) is of 0.13. The model's intercept, corresponding to Gender = Men,\nConversationType = MixedGender and Priming = NoPrime, is at -1.07 (95% CI\n[-1.36, -0.78], p &lt; .001). Within this model:\n\n  - The effect of Gender [Women] is statistically significant and negative (beta\n= -0.64, 95% CI [-0.99, -0.30], p &lt; .001; Std. beta = -0.64, 95% CI [-0.99,\n-0.30])\n  - The effect of ConversationType [SameGender] is statistically significant and\nnegative (beta = -0.54, 95% CI [-0.83, -0.24], p &lt; .001; Std. beta = -0.54, 95%\nCI [-0.83, -0.24])\n  - The effect of Priming [Prime] is statistically significant and positive (beta\n= 1.87, 95% CI [1.55, 2.19], p &lt; .001; Std. beta = 1.87, 95% CI [1.55, 2.19])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n\nWe can use this output to write up a final report:\nWe can use this output to write up a final report:\nWe fitted a logistic mixed model to predict the use of discourse like. The model included speakers as random effect (formula: ~1 | ID). The model’s total explanatory power is moderate (conditional R2 = 0.15) and the part related to the fixed effects alone (marginal R2) is of 0.13.\nRegarding fixed effects, the model reported that * women use discourse like statistically less compared to men (beta = -0.64 [-0.99, -0.30], p &lt; .001; Std. beta = -0.64 [-0.99, -0.30]) * speakers in conversations with other speakers of the same gender use discourse like significantly less compared to thier use in mixed-gender conversations (beta = -0.54 [-0.83, -0.24], p &lt; .001; Std. beta = -0.54 [-0.83, -0.24]) * Priming is significantly positively correlated with the use of discourse like (beta = 1.87 [1.55, 2.19], p &lt; .001; Std. beta = 1.87 [1.55, 2.19])"
  },
  {
    "objectID": "tutorials/regression/regression.html#mixed-effects-quasi-poisson-and-negative-binomial-regression",
    "href": "tutorials/regression/regression.html#mixed-effects-quasi-poisson-and-negative-binomial-regression",
    "title": "Introduction",
    "section": "Mixed-Effects (Quasi-)Poisson and Negative-Binomial Regression",
    "text": "Mixed-Effects (Quasi-)Poisson and Negative-Binomial Regression\nLike fixed-effects Poisson models, mixed-effects Poisson models take counts as dependent variables. The data for this analysis was collected on three separate evenings (Trial). The number of the filler uhm (UHM) was counted in two-minute conversations that were either in English, German, Russian, or Mandarin (Language). In addition, the number of shots that speakers drank before they talked was recorded (Shots).\n\n# load data\ncountdata  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/cld.rda\", \"rb\"))\n\n\n# inspect data\ncountdata %&gt;%\n  as.data.frame() %&gt;%\n  head(15) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %&gt;%\n  flextable::border_outer()\n\nIDTrialLanguageGenderUHMShots13RussianMan0023RussianMan0033GermanMan0541GermanMan0351GermanWoman2663GermanMan1571MandarinMan1183GermanWoman0493RussianWoman00102GermanMan02113RussianMan01122GermanMan01133RussianWoman01142RussianWoman44152EnglishMan04\n\n\nSince the data contains character variables, we need to factorize the data before we can analyse it further and we also remove the ID column.\n\n# factorize variables\ncountdata &lt;- countdata %&gt;%\n  dplyr::select(-ID) %&gt;%\n  dplyr::mutate_if(is.character, factor)\n\n\n# inspect data\ncountdata %&gt;%\n  as.data.frame() %&gt;%\n  head(15) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %&gt;%\n  flextable::border_outer()\n\nTrialLanguageGenderUHMShots3RussianMan003RussianMan003GermanMan051GermanMan031GermanWoman263GermanMan151MandarinMan113GermanWoman043RussianWoman002GermanMan023RussianMan012GermanMan013RussianWoman012RussianWoman442EnglishMan04\n\n\nAfter the data is factorized, we can visualize the data.\n\ncountdata %&gt;%\n  # prepare data\n  dplyr::select(Language, Shots) %&gt;%\n  dplyr::group_by(Language) %&gt;%\n  dplyr::mutate(Mean = round(mean(Shots), 1)) %&gt;%\n  dplyr::mutate(SD = round(sd(Shots), 1)) %&gt;%\n  # start plot\n  ggplot(aes(Language, Shots, color = Language, fill = Language)) +\n  geom_violin(trim=FALSE, color = \"gray20\")+ \n  geom_boxplot(width=0.1, fill=\"white\", color = \"gray20\") +\n  geom_text(aes(y=-4,label=paste(\"mean: \", Mean, sep = \"\")), size = 3, color = \"black\") +\n  geom_text(aes(y=-5,label=paste(\"SD: \", SD, sep = \"\")), size = 3, color = \"black\") +\n  scale_fill_manual(values=rep(\"grey90\",4)) + \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"none\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  ylim(-5, 15) +\n  labs(x = \"Language\", y = \"Shots\")\n\n\n\n\n\n\n\n\nThe violin plots show that the English speakers drank more shots than speakers of other languages with Mandarin speakers drinking the fewest shots.\nIn the present case, we will a Boruta variable selection procedure to streamline the model fitting process. Thus, before fitting the model, we will test which variables have any kind of relationship with the dependent variable and therefore deserve to be evaluated in the regression modeling. As this is just an example, we will only consider variables which are deemed important and disregard both unimportant and tentative variables. We start the Boruta analysis by setting a seed and running an initial Boruta analysis.\n\n# perform variable selection\nset.seed(20191220)\nboruta &lt;- Boruta(UHM ~.,data=countdata)\nprint(boruta)\n\nBoruta performed 99 iterations in 2.79168200493 secs.\n 2 attributes confirmed important: Language, Shots;\n 1 attributes confirmed unimportant: Gender;\n 1 tentative attributes left: Trial;\n\n\nAs only Shots is confirmed as important, we will only check for the effect of Shots and include Language as a random effect in the regression modeling. Including Language as a random effect is probably not justified statistically (given that the Boruta analysis showed that it only has a tentative effect) but for theoretical reasons as the speakers are nested into Languages. Before we start with the modeling, however, we proceed by checking if the data does indeed approximate a Poisson distribution.\n\n# output the results\ngf = goodfit(countdata$UHM,type= \"poisson\", method= \"ML\")\nplot(gf, main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\nThe data does not perfectly match a distribution that would be expected if the data approximated a Poisson distribution. We will use a goodness-of-fit test to check if the data does indeed diverge significantly from being Poisson distributed. If the p-values of the goodness-of-fit test is smaller than .05, then the distribution of the data differs significantly from a Poisson distribution and, given the visualization is likely over-dispersed.\nIn case of overdispersion, we may have to use a quasi-Poisson or, even better, a negative binomial model but we will, for now continue with the Poisson model and perform diagnostics later to check if we have to switch to a more robust method. One effect of overdispersion is that the standard errors of a model are biased and quasi-Poisson models scale the standard errors to compensate bias. However, Zuur, Hilbe, and Ieno suggest to use negative-binomial model instead. This is so because the scaling of the standard errors performed by quasi-Poisson models only affects the significance of coefficients (the p-values) but it does not affect the coefficients which, however, may be affected themselves by overdispersion. Thus, the coefficients of Poisson as well as quasi-Poisson models (which are identical) may be unreliable when dealing with overdispersion. Negative binomial models, in contrast, include an additional dispersion or heterogeneity parameter which accommodates overdispersion better than merely scaling the standard errors (see Zuur, Hilbe, and Ieno, 21).\n\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df                                     P(&gt; X^2)\nLikelihood Ratio 153.422771085  5 0.000000000000000000000000000000249336691328\n\n\nThe p-value is indeed smaller than .05 which means that we should indeed use a negative-binomial model rather than a Poisson model. We will ignore this, for now, and proceed to fit a Poisson mixed-effects model and check what happens if a Poisson model is fit to over-dispersed data.\n\nMixed-Effects Poisson Regression\nIn a first step, we create mixed-effect intercept-only baseline models and then test if including “Shots” significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.glmer = glmer(UHM ~ 1 + (1 | Language), data = countdata, family = poisson,\n                 control=glmerControl(optimizer=\"bobyqa\"))\n# add Shots\nm1.glmer &lt;- update(m0.glmer, .~.+ Shots)\nAnova(m1.glmer, test = \"Chi\")           \n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: UHM\n       Chisq Df             Pr(&gt;Chisq)    \nShots 321.25  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. However, we get the warning that the fitted mixed model is (almost / near) singular. In such cases, the model should not be reported. As this is only an example, we will continue by having a look at the model summary.\n\nsummary(m1.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: UHM ~ (1 | Language) + Shots\n   Data: countdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1041.8   1054.5   -517.9   1035.8      497 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.509633852 -0.666423093 -0.592950422  0.586114082  4.338639382 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Language (Intercept) 0        0       \nNumber of obs: 500, groups:  Language, 4\n\nFixed effects:\n                 Estimate    Std. Error   z value               Pr(&gt;|z|)    \n(Intercept) -1.2789168850  0.0893313003 -14.31656 &lt; 0.000000000000000222 ***\nShots        0.2336279071  0.0130347632  17.92345 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nShots -0.806\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe model summary confirms that the number of shots does have a significantly positive effect on the number of occurrences of uhm. Furthermore, the scaled residuals are distributed very unevenly which suggests overdispersion. Including Language as a random effect is not justified given that they have 0 variance and a standard deviation of 0 (which means that Language does not account for or explain any additional variance).\nWe now check if the model suffers from overdispersion following Zuur, Hilbe, and Ieno (138).\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.glmer, type = \"pearson\")\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors &lt;- length(fixef(m1.glmer)) +1\n# calculate overdispersion\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.16901460967\n\n\nThe data is slightly over-dispersed. It would also be advisable to plot the Cook’s distance (which should not show data points with values &gt; 1). If there are data points with high Cook’s D values, we could exclude them which would, very likely reduce the overdispersion (see Zuur, Hilbe, and Ieno, 22). We ignore this, for now, and use diagnostic plots to check if the plots indicate problems.\n\ndiag_data &lt;- data.frame(PearsonResiduals, fitted(m1.glmer)) %&gt;%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 &lt;- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 &lt;- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 &lt;- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngridExtra::grid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\n\n\n\n\nThe diagnostic plots show problems as the dots in the first two plots are not random but show a pattern in the lower left corner. In addition, the variance of English (left boxplot) is notable larger than the variance of Russian (right boxplot). As a final step, we plot the predicted vales of the model to check if the predictions make sense.\n\nplot_model(m1.glmer, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe model predicts that the instances of uhm increase with the number of shots. Note that the increase is not homogeneous as the y-axis labels indicate! We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.glmer, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations (overdispersion measure &gt; 1 and excessive multicollinearity (singular fit)) had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nThe summary of the model can be extracted using the tab_model function from the sjPlot package (Lüdecke).\n\nsjPlot::tab_model(m1.glmer)\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.33\n&lt;0.001\n\n\nShots\n1.26\n1.23 – 1.30\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\n\nr.squaredGLMM(m1.glmer)\n\n                     R2m            R2c\ndelta     0.208910675833 0.208910675833\nlognormal 0.294971461467 0.294971461467\ntrigamma  0.120419615931 0.120419615931\n\n\nAlso note that our model suffers from a serious problem (near singular fit). If this were not just an example, you should not(!) report this model!\n\n\nMixed-Effects Quasi-Possion Regression\nThe Quasi-Poisson Regression is a generalization of the Poisson regression and is used when modeling an overdispersed count variable. Poisson models are based on the Poisson distribution which is defined as a distribution where the variance is equal to the mean (which is very restrictive and not often the case). Quasi-Poisson models scale the standard errors which has a positive effect when dealing with overdispersed data.\nTherefore, when the variance is greater than the mean, a Quasi-Poisson model, which assumes that the variance is a linear function of the mean, is more appropriate as it handles over-dispersed data better than normal Poisson-models.\nWe begin the model fitting process by creating a mixed- and a fixed-effects intercept-only base-line model. Unfortunately, there is not yet a procedure in place for quasi-Poisson models to test if the inclusion of random effects is justified. However, here the Boruta also provides valuable information: Language was only considered tentative but not important which suggests that it will not explain variance which means that including Language as a random effect may not be justified. This would require further inspection. Because we are only dealing with an example here, we ignore this fact (which you should not do in proper analyses) and continue right away with adding shots.\n\n# base-line mixed-model\nm0.qp = glmmPQL(UHM ~ 1, random = ~ 1 | Language, data = countdata, \n                   family = quasipoisson(link='log'))\n\niteration 1\n\n\niteration 2\n\n\niteration 3\n\n\niteration 4\n\n# add Shots\nm1.qp &lt;- update(m0.qp, .~.+ Shots)\n\niteration 1\n\nAnova(m1.qp, test = \"Chi\")           # SIG! (p&lt;0.0000000000000002 ***)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: zz\n         Chisq Df             Pr(&gt;Chisq)    \nShots 276.4523  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. We will now have a look at the model summary.\n\nsummary(m1.qp)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: countdata \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 | Language\n               (Intercept)      Residual\nStdDev: 0.0000407429998994 1.07801257915\n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  UHM ~ Shots \n                      Value       Std.Error  DF        t-value p-value\n(Intercept) -1.278929721022 0.0964886326622 495 -13.2547190870       0\nShots        0.233630231741 0.0140795660798 495  16.5935676154       0\n Correlation: \n      (Intr)\nShots -0.806\n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-1.400417640410 -0.618228066947 -0.550071160975  0.543731905020  4.024828853743 \n\nNumber of Observations: 500\nNumber of Groups: 4 \n\n\nThe model summary does not provide much information such as,e.g. AIC or BIC values. The coefficient for Shots is highly significant (p &lt;.001) and the data is notably over-dispersed (the Standardized Within-Group Residuals deviate substantially from a normal distribution with higher values having a thick tail). Also, in contrast to the Poisson model, Language does explain at least a minimal share of the variance now as the mean and standard deviation are no longer 0. Note also, that the coefficients are identical to the Poisson coefficients but the standard errors and p-values differ (the model provides t- rather than z-values).\nIn a next step, we will calculate the odds ratios of the coefficient (as we only have one). We will use the coefficients from the fixed-effects model as the coefficients for mixed- and fixed-effects models are identical (the random effect structure only affects the standard error and p-values but not the coefficients; you can check by uncommenting the summary command).\n\nm1.glm = glm(UHM ~ Shots, data = countdata, family = quasipoisson(link='log'))\nexp(coef(m1.glm))\n\n   (Intercept)          Shots \n0.278338612235 1.263174385573 \n\n\nThe standardized or \\(\\beta\\)-coefficient tells us that the likelihood of uhm increases by 1.26 (or 26.32 percent) with each additional shot.\nBefore inspecting the relationship between Shots and uhm, we will check if the overdispersion was reduced.\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.qp, type = \"pearson\")\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors &lt;- length(fixef(m1.qp)) +1\n# calculate overdispersion\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.00603621722\n\n\nThe overdispersion has indeed decreased and is not so close to 1 that overdispersion is no longer an issue.\nWe continue to diagnose the model by plotting the Pearson’s residuals against fitted values. This diagnostic plot should not show a funnel-like structure or patterning as we observed in the case of the Poisson model.\n\n# diagnostic plot\nplot(m1.qp, pch = 20, col = \"black\", lty= \"dotted\", ylab = \"Pearson's residuals\")\n\n\n\n\n\n\n\n\nIndeed, the plot exhibits a (slight) funnel shape (but not drastically so) and thus indicates heteroscedasticity. However, the patterning that we observed with the Poisson model has disappeared. We continue by plotting the random effect adjustments.\n\n# generate diagnostic plots\nplot(m1.qp, Language ~ resid(.), abline = 0, fill = \"gray70\") \n\n\n\n\n\n\n\n\nThe adjustments by “Language” are marginal (which was somewhat expected given that Language was only deemed tentative), which shows that there is very little variation between the languages and that we have no statistical reason to include Language as a random effect.\nIn a final step, we plot the fixed-effect of Shots using the predictorEffects function from the effects package (Fox and Weisberg 2019).\n\nplot_model(m1.qp, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe effects plot shows that the number of uhms increases exponentially with the number of shots a speaker has had. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.qp, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nGiven that the overdispersion measure of this Quasi-Poisson model is close to 1, that the model did not suffer from excessive multicollinearity (singular fit), and because this model shows improvements compared to the Poisson model with respect to the model diagnostics (some adjustments by Language and less patterning in the diagnostic plots), we would choose this quasi-Poisson model over the Poisson model.\nFinally, we extract the summary table of this model.\n\nsjPlot::tab_model(m1.qp)\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.34\n&lt;0.001\n\n\nShots\n1.26\n1.23 – 1.30\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.00\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\nMarginal R2 / Conditional R2\n1.000 / NA\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\n\nr.squaredGLMM(m1.qp)\n\n                      R2m             R2c\ndelta     0.1856941875702 0.1856941884278\nlognormal 0.2752763976645 0.2752763989357\ntrigamma  0.0977040660495 0.0977040665007\n\n\n\n\nMixed-Effects Negative Binomial Regression\nNegative binomial regression models are a generalization of Poisson regression which loosens the restrictive assumption that the variance is equal to the mean made by the Poisson model. This is a major advantage as the most common issue that one faces with Poisson regressions is that the data deviate too substantially from the assumed Poisson distribution.\nTo implement a Negative-Binomial Mixed-Effects Regression, we first create a mixed-effects intercept-only baseline model and then test if including Shots significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.nb = glmer.nb(UHM ~ 1 + (1 | Language), data = countdata)\n# add Shots\nm1.nb &lt;- update(m0.nb, .~.+ Shots)\n\nboundary (singular) fit: see help('isSingular')\n\nanova(m1.nb, m0.nb)           \n\nData: countdata\nModels:\nm0.nb: UHM ~ 1 + (1 | Language)\nm1.nb: UHM ~ (1 | Language) + Shots\n      npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.nb    3 1159.000894 1171.644718 -576.5004470 1153.000894             \nm1.nb    4 1051.593288 1068.451721 -521.7966442 1043.593288 109.40761  1\n                  Pr(&gt;Chisq)    \nm0.nb                           \nm1.nb &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe negative-binomial model also reports a significant impact of shots on the number of uhms. In a next step, we calculate the overdispersion.\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.nb, type = \"pearson\")\n# extract number of betas + predictors + sigma\nNumberOfPredictors &lt;- 2+1+1\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# calculate overdispersion parameter\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases / NumberOfPredictors)# show overdispersion parameter\nOverdispersion\n\n[1] 2.46949245769\n\n\nThe overdispersion has increased which is rather suboptimal. In this case, we would report the Quasi-Poisson Regression rather than the Negative Binomial Model (which is rather rare as Negative Binomial Models typically perform better than (Quasi-)Poisson models. However, this tutorial focuses merely on how to implement a Negative Binomial Mixed-Effects Regression and we thus continue with generating diagnostic plots to check for problems.\n\ndiag_data &lt;- data.frame(PearsonResiduals, fitted(m1.nb)) %&gt;%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 &lt;- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 &lt;- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 &lt;- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngridExtra::grid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\n\n\n\n\nThe diagnostics show patterning similar to the one we saw with the Poisson model which suggest that the negative binomial model is also not an optimal model for our data. We continue by plotting the predicted values and, subsequently, summarize the analysis.\n\nplot_model(m1.nb, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe effect plot shows that the predicted number of shots increases exponentially with each shot. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.nb, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nA mixed-effect negative binomial regression model which contained the language in which the conversation took place as random effect was fit to the data. Prior to the regression modeling, a Boruta analysis was applied to determine whether any of the predictors had a meaningful relationship with the dependent variable (instances of uhm). Since the Boruta analysis indicated that only the number of shots a speaker had was important, only “Shots” was tested during model fitting. The final minimal adequate model showed that the number of uhm as fillers increases significantly, and near-linearly with the number of shots speakers had (\\(\\chi\\)2(1):83.0, p &lt;.0001, \\(\\beta\\): 0.2782). An inspection of the random effect structure conveyed that there was almost no variability between languages and language did not contribute meaningfully to the model fit."
  },
  {
    "objectID": "tutorials/regression/regression.html#mixed-effects-multinomial-regression",
    "href": "tutorials/regression/regression.html#mixed-effects-multinomial-regression",
    "title": "Introduction",
    "section": "Mixed-Effects Multinomial Regression",
    "text": "Mixed-Effects Multinomial Regression\nIn this section, we will focus on how to implement a mixed-effects multinomial regression model using the mblogit function from the mclogit package (see Elff 2021). As we have already gone though model fitting and model validation procedures above, we will strictly see how to implement this type of model here - we will not go through all the other steps that a proper regression analysis would require.\nWe begin the analysis by loading the example data set. The data represents observations gathered during an experiment where speakers had to report what they saw. The responses are categorized into four groups:\n\n# description data\npict  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/pict.rda\", \"rb\"))\n# inspect\nhead(pict)\n\n  Id Participant       Group Item     Response Gender Age\n1  1        G001 German_Mono    1  NumeralNoun   Male  18\n2  2        G002 German_Mono    3  NumeralNoun   Male  18\n3  3        G003 German_Mono    4  NumeralNoun   Male  18\n4  4        G004 German_Mono    6 QuantAdjNoun   Male  18\n5  5        G005 German_Mono    8  NumeralNoun   Male  18\n6  6        G006 German_Mono    9 QuantAdjNoun   Male  18\n\n\nIn a first step, we generate a baseline model that we call m0. This model only contains the random effect structure and the intercept as the sole predictor.\n\nm0.mn &lt;- mblogit(formula = Response ~ 1, \n              random = ~ 1 | Participant, \n              data = pict)\n\n\nIteration 1 - deviance = 2452.16571809 - criterion = 0.840346812509\n\n\n\nIteration 2 - deviance = 2412.75547122 - criterion = 0.0646013613319\n\n\n\nIteration 3 - deviance = 2411.26894466 - criterion = 0.00534731178597\n\n\n\nIteration 4 - deviance = 2411.2582473 - criterion = 0.0000446387422471\n\n\n\nIteration 5 - deviance = 2411.25823534 - criterion = 0.0000000031405953873\nconverged\n\n\nIn this case, the algorithm did not converge properly - if this were a real analysis, we could not simply continue but would have to inspect possible causes for this. However, as this is just a showcase, we will ignore this and move on. Next, we add the fixed effects (Gender and Group).\n\nm1.mn &lt;- mblogit(formula = Response ~ Gender + Group, \n              random = ~ 1 | Item, \n              data = pict)\n\n\nIteration 1 - deviance = 1652.54499424 - criterion = 0.800708039017\n\n\n\nIteration 2 - deviance = 1571.22116522 - criterion = 0.0814899820197\n\n\n\nIteration 3 - deviance = 1551.91023391 - criterion = 0.0274469746195\n\n\n\nIteration 4 - deviance = 1547.29823345 - criterion = 0.00512667078353\n\n\n\nIteration 5 - deviance = 1546.02170442 - criterion = 0.000173543402508\n\n\n\nIteration 6 - deviance = 1545.8333116 - criterion = 0.000000244926398008\n\n\n\nIteration 7 - deviance = 1545.82769259 - criterion = 0.000000000000785569845713\nconverged\n\n\nNow, we can compare the models to see if including the fixed-effects into the model has significantly improved the model fit.\n\nanova(m0.mn, m1.mn)\n\nAnalysis of Deviance Table\n\nModel 1: Response ~ 1\nModel 2: Response ~ Gender + Group\n  Resid. Df  Resid. Dev Df    Deviance\n1      3261 2411.258235               \n2      3249 1545.827693 12 865.4305427\n\n\nAs the second model is significantly better, we are justified to believe that our fixed effects have explanatory power. We can now use the getSummary.mmblogit function to get a summary of the model with the fixed effects.\n\n# inspect\nmclogit::getSummary.mmblogit(m1.mn)\n\n$coef\n, , NumeralNoun/BareNoun\n\n                                  est             se             stat\n(Intercept)           1.0940536961694 0.795348462841   1.375565236225\nGenderMale            0.0691676134585 0.196851703842   0.351369137826\nGroupGerman_Mono     -3.2909055852599 0.304213383695 -10.817754121410\nGroupL2_Advanced     -0.4575232522124 0.307505930002  -1.487851802435\nGroupL2_Intermediate -1.1689603301872 0.320486374900  -3.647457183015\n                                                            p             lwr\n(Intercept)          0.16895627569603502426964780624984996393 -0.464800646159\nGenderMale           0.72531143261552322165641726314788684249 -0.316654636367\nGroupGerman_Mono     0.00000000000000000000000000283642762786 -3.887152860918\nGroupL2_Advanced     0.13678998075957157776194605958153260872 -1.060223800048\nGroupL2_Intermediate 0.00026484842867640642034496312184899125 -1.797102082527\n                                 upr\n(Intercept)           2.652908038498\nGenderMale            0.454989863284\nGroupGerman_Mono     -2.694658309602\nGroupL2_Advanced      0.145177295623\nGroupL2_Intermediate -0.540818577848\n\n, , QuantAdjNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)           0.595495714810 0.498285332593   1.19508979265\nGenderMale            0.299894756058 0.248324126895   1.20767466218\nGroupGerman_Mono     -3.856907426426 0.376376472714 -10.24747216162\nGroupL2_Advanced     -2.194953767755 0.347242020175  -6.32110643363\nGroupL2_Intermediate -3.016728204963 0.398140344234  -7.57704726148\n                                                         p             lwr\n(Intercept)          0.23205194960832503658920700218004640 -0.381125591097\nGenderMale           0.22717242789340932884734058916365029 -0.186811589147\nGroupGerman_Mono     0.00000000000000000000000121478955842 -4.594591757573\nGroupL2_Advanced     0.00000000025969706979146851136980930 -2.875535621216\nGroupL2_Intermediate 0.00000000000003535080320338597665829 -3.797068940455\n                                 upr\n(Intercept)           1.572117020716\nGenderMale            0.786601101264\nGroupGerman_Mono     -3.119223095278\nGroupL2_Advanced     -1.514371914293\nGroupL2_Intermediate -2.236387469472\n\n, , QuantNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)          -2.163121210173 0.753539267072 -2.870615115491\nGenderMale           -0.371289871064 0.430045391060 -0.863373678181\nGroupGerman_Mono     -2.420925019720 0.813762214781 -2.974978409844\nGroupL2_Advanced      0.789609605279 0.683753160717  1.154816753535\nGroupL2_Intermediate -0.148789198300 0.739325816561 -0.201249834602\n                                    p             lwr             upr\n(Intercept)          0.00409674000335 -3.640031034571 -0.686211385775\nGenderMale           0.38793204670759 -1.214163349258  0.471583607130\nGroupGerman_Mono     0.00293009169381 -4.015869652670 -0.825980386770\nGroupL2_Advanced     0.24816547492527 -0.550521964042  2.129741174601\nGroupL2_Intermediate 0.84050322615521 -1.597841171600  1.300262775001\n\n\n$Item\n, , 1\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)   5.46019439094 26.1766792826   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) -1.63804668966           NaN   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     1.63405993830 11.5842291808   NA NA  NA  NA\n\n, , 2\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  -1.63804668966           NaN   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1)  1.60628957168           NaN   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     0.49903254341 2.36453656619   NA NA  NA  NA\n\n, , 3\n\n                                             est             se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  1.63405993830 11.58422918076   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) 0.49903254341  2.36453656619   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)    1.51237743196            NaN   NA NA  NA  NA\n\n\n$Groups\nGroups by Item \n            10 \n\n$sumstat\n               LR                df          deviance          McFadden \n1476.294014648252   21.000000000000 1545.827692593037    0.488495883905 \n        Cox.Snell        Nagelkerke               AIC               BIC \n   0.741897420618    0.791357248659 1587.827692593037 1692.700285072724 \n                N \n1090.000000000000 \n\n$call\nmblogit(formula = Response ~ Gender + Group, data = pict, random = ~1 | \n    Item)\n\n$contrasts\n$contrasts$Gender\n[1] \"contr.treatment\"\n\n$contrasts$Group\n[1] \"contr.treatment\"\n\n\n$xlevels\n$xlevels$Gender\n[1] \"Female\" \"Male\"  \n\n$xlevels$Group\n[1] \"English_Mono\"    \"German_Mono\"     \"L2_Advanced\"     \"L2_Intermediate\"\n\n\nThe NAs (not available information) is a result of the model having a bad fit to the data and, optimally, we would need to inspect why the model has a bad fit. Again, we ignore this and move on. Next, we check the VIFs to see if the model does not violate multicollinearity assumptions.\n\ncar::vif(m1.mn) # maybe use cut-off of 5 (maybe 10)\n\n                GVIF Df GVIF^(1/(2*Df))\nGender 3.59671433201  1   1.89650054891\nGroup  9.94593851425  3   1.46647375519\n\n\nThe VIFs are a bit high - especially the GVIF for Group would be a cause for concern if this was not just a demo analysis! However, as we only want to implement a multinomial mixed-effects model here and not provide a proper, clean analysis, we will ignore this issue here.\nIn a next step, we visualize effects to get a better understanding of how the predictors that are part of the fixed-effect structure of the mode affect the outcome (the response variable).\n\nsjPlot::plot_model(m1.mn)\n\n\n\n\n\n\n\n\nFinally, we can extract an alternative summary table produced by the tab_model function from the sjPlot package (see Lüdecke).\n\nsjPlot::tab_model(m1.mn)\n\n\n\n \nResponse: NumeralNoun\nResponse: QuantAdjNoun\nResponse: QuantNoun\n\n\nPredictors\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\n\n\n(Intercept)\n2.99\n0.63 – 14.20\n0.169\n1.81\n0.68 – 4.82\n0.232\n0.11\n0.03 – 0.50\n0.004\n\n\nGenderMale\n1.07\n0.73 – 1.58\n0.725\n1.35\n0.83 – 2.20\n0.227\n0.69\n0.30 – 1.60\n0.388\n\n\nGroupGerman_Mono\n0.04\n0.02 – 0.07\n&lt;0.001\n0.02\n0.01 – 0.04\n&lt;0.001\n0.09\n0.02 – 0.44\n0.003\n\n\nGroupL2_Advanced\n0.63\n0.35 – 1.16\n0.137\n0.11\n0.06 – 0.22\n&lt;0.001\n2.20\n0.58 – 8.42\n0.248\n\n\nGroupL2_Intermediate\n0.31\n0.17 – 0.58\n&lt;0.001\n0.05\n0.02 – 0.11\n&lt;0.001\n0.86\n0.20 – 3.67\n0.841\n\n\n\nN Item\n10\n\nObservations\n1090\n\n\n\n\n\n\n\nThis is the final step in implementing a a mixed-effects multinomial regression model using the mblogit function from the mclogit package (see Elff 2021). We are aware that the analysis shown here is supervifial(!) - but please keep in mind that we just wanted to showcase the implementation here rather than providing a properly and carefully done analysis."
  },
  {
    "objectID": "tutorials/regression/regression.html#mixed-effects-ordinal-regression",
    "href": "tutorials/regression/regression.html#mixed-effects-ordinal-regression",
    "title": "Introduction",
    "section": "Mixed-Effects Ordinal Regression",
    "text": "Mixed-Effects Ordinal Regression\nIn this section, we will strictly focus on how to implement a mixed-effects ordinal regression model using the clmm function from the ordinal package (see Christensen 2019). This type of regression model is extremely useful when dealing with Likert data or other types of questionnaire and survey data where the responses have some kind of hierarchical structure (i.e. responses are not truly independent because they come from different points in time or from different regions). load data\n\n# rating experiment data\nratex  &lt;- base::readRDS(url(\"https://slcladal.github.io/data/ratex.rda\", \"rb\"))\n# inspect data\nhead(ratex)\n\n  Rater Child Group       Accent AccentNumeric       Family\n1    R1  C001 Child StrongAccent             2 DomBilingual\n2    R2  C001 Child StrongAccent             2 DomBilingual\n3    R3  C001 Child StrongAccent             2 DomBilingual\n4    R4  C001 Child StrongAccent             2 DomBilingual\n5    R5  C001 Child StrongAccent             2 DomBilingual\n6    R6  C001 Child StrongAccent             2 DomBilingual\n\n\nWe now tabulate the data to get a better understanding of the data structure.\n\nratex %&gt;%\n  dplyr::group_by(Family, Accent) %&gt;%\n  dplyr::summarise(Frequency = n()) %&gt;%\n  tidyr::spread(Accent, Frequency)\n\n`summarise()` has grouped output by 'Family'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 4\n# Groups:   Family [3]\n  Family         NoAccent StrongAccent WeakAccent\n  &lt;fct&gt;             &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n1 DomBilingual         80          145        174\n2 EqualBilingual       20           22         63\n3 Monolingual         209            1         41\n\n\nNext, we visualize the data to inspect its properties.\n\nratex %&gt;%\n  ggplot(aes(Family, AccentNumeric, color = Group)) + \n  stat_summary(fun = mean, geom = \"point\") +          \n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nAn alternative plot shows other properties of the data.\n\nratex %&gt;%\n  dplyr::group_by(Family, Rater, Group) %&gt;%\n  dplyr::summarise(AccentMean = mean(AccentNumeric)) %&gt;%\n  ggplot(aes(Family, AccentMean, fill = Group)) + \n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"gray50\", \"gray85\"))\n\n`summarise()` has grouped output by 'Family', 'Rater'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nWe now start the modeling by generating a model with Family as the sole predictor.\n\n# fit baseline model\nm1.or &lt;- clmm(Accent ~ (1|Rater) + Family, link=\"logit\", data = ratex)\n# test for incomplete information\nifelse(min(ftable(ratex$Accent, ratex$Family)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\n# extract aic\naic.glmer &lt;- AIC(logLik(m1.or))\n# inspect aic\naic.glmer\n\n[1] 1380.25888675\n\n# summarize model\nsummary(m1.or)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: Accent ~ (1 | Rater) + Family\ndata:    ratex\n\n link  threshold nobs logLik  AIC     niter    max.grad cond.H \n logit flexible  755  -685.13 1380.26 371(435) 4.36e-07 5.9e+05\n\nRandom effects:\n Groups Name        Variance            Std.Dev.       \n Rater  (Intercept) 0.00000000353193271 0.0000594300657\nNumber of groups:  Rater 21 \n\nCoefficients:\n                         Estimate   Std. Error   z value             Pr(&gt;|z|)\nFamilyEqualBilingual  0.477744666  0.214313910   2.22918             0.025802\nFamilyMonolingual    -2.550224083  0.198849328 -12.82491 &lt; 0.0000000000000002\n                        \nFamilyEqualBilingual *  \nFamilyMonolingual    ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                             Estimate    Std. Error   z value\nNoAccent|StrongAccent   -1.0798112479  0.1058004233 -10.20611\nStrongAccent|WeakAccent  0.1060945018  0.0951352125   1.11520\n\n\nWe can now perform Post-Hoc tests to see which comparisons are significant.\n\nlsmeans(m1.or, pairwise~Family, adjust=\"tukey\")\n\n$lsmeans\n Family         lsmean     SE  df asymp.LCL asymp.UCL\n DomBilingual    0.487 0.0914 Inf     0.308     0.666\n EqualBilingual  0.965 0.1950 Inf     0.582     1.347\n Monolingual    -2.063 0.1740 Inf    -2.404    -1.722\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df z.ratio p.value\n DomBilingual - EqualBilingual   -0.478 0.214 Inf  -2.229  0.0664\n DomBilingual - Monolingual       2.550 0.199 Inf  12.825  &lt;.0001\n EqualBilingual - Monolingual     3.028 0.265 Inf  11.438  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nFinally, we can summarize the model.\n\nsjPlot::tab_model(m1.or)\n\n\n\n \nAccent\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\nNoAccent|StrongAccent\n0.34\n0.28 – 0.42\n&lt;0.001\n\n\nStrongAccent|WeakAccent\n1.11\n0.92 – 1.34\n0.265\n\n\nFamily [EqualBilingual]\n1.61\n1.06 – 2.45\n0.026\n\n\nFamily [Monolingual]\n0.08\n0.05 – 0.12\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Rater\n0.00\n\n\nN Rater\n21\n\nObservations\n755\n\n\nMarginal R2 / Conditional R2\n0.325 / NA\n\n\n\n\n\n\n\nAnd we can visualize the effects.\n\nplot_model(m1.or, type = \"pred\", terms = c(\"Family\"))\n\n\n\n\n\n\n\n\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results."
  },
  {
    "objectID": "tutorials/regression/regression.html#footnotes",
    "href": "tutorials/regression/regression.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m extremely grateful to Stefan Thomas Gries who provided very helpful feedback and pointed out many errors in previous versions of this tutorial. All remaining errors are, of course, my own.↩︎\nI’m very grateful to Antonio Dello Iacono who pointed out that the plots require additional discussion.↩︎\nI’m very grateful to Antonio Dello Iacono who pointed out that a previous version of this tutorial misinterpreted the results as I erroneously reversed the group results.↩︎"
  },
  {
    "objectID": "tutorials/sentiment/sentiment.html#preparation-and-session-set-up",
    "href": "tutorials/sentiment/sentiment.html#preparation-and-session-set-up",
    "title": "Introduction",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"tibble\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"textdata\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"sentimentr\")\ninstall.packages(\"zoo\")\ninstall.packages(\"flextable\")\ninstall.packages(\"syuzhet\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# activate packages\nlibrary(dplyr)\nlibrary(flextable)\nlibrary(ggplot2)\nlibrary(Hmisc)\nlibrary(sentimentr)\nlibrary(stringr)\nlibrary(textdata)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(tidytext)\nlibrary(zoo)\nlibrary(syuzhet)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/sentiment/sentiment.html#getting-started",
    "href": "tutorials/sentiment/sentiment.html#getting-started",
    "title": "Introduction",
    "section": "Getting started",
    "text": "Getting started\nIn the following, we will perform a SA to investigate the emotionality of five different novels. We will start with the first example and load five pieces of literature.\n\ndarwin &lt;- base::readRDS(url(\"https://slcladal.github.io/data/origindarwin.rda\", \"rb\"))\ntwain &lt;- base::readRDS(url(\"https://slcladal.github.io/data/twainhuckfinn.rda\", \"rb\"))\norwell &lt;- base::readRDS(url(\"https://slcladal.github.io/data/orwell.rda\", \"rb\"))\nlovecraft &lt;- base::readRDS(url(\"https://slcladal.github.io/data/lovecraftcolor.rda\", \"rb\"))\n\n\n\n.THE ORIGIN OF SPECIES BY CHARLES DARWIN AN HISTORICAL SKETCH OF THE PROGRESS OF OPINION ON THE ORIGIN OF SPECIES INTRODUCTION When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the organic beings in- habiting South America, and in the geological relations of the \n\n\nWrite function to clean data\n\ntxtclean &lt;- function(x, title) {\n    require(dplyr)\n    require(stringr)\n    require(tibble)\n    x &lt;- x %&gt;%\n        iconv(to = \"UTF-8\") %&gt;%\n        base::tolower() %&gt;%\n        paste0(collapse = \" \") %&gt;%\n        stringr::str_squish() %&gt;%\n        stringr::str_split(\" \") %&gt;%\n        unlist() %&gt;%\n        tibble::tibble() %&gt;%\n        dplyr::select(word = 1, everything()) %&gt;%\n        dplyr::mutate(novel = title) %&gt;%\n        dplyr::anti_join(stop_words) %&gt;%\n        dplyr::mutate(word = str_remove_all(word, \"\\\\W\")) %&gt;%\n        dplyr::filter(word != \"\")\n}\n\nProcess and clean texts.\n\n# process text data\ndarwin_clean &lt;- txtclean(darwin, \"darwin\")\nlovecraft_clean &lt;- txtclean(lovecraft, \"lovecraft\")\norwell_clean &lt;- txtclean(orwell, \"orwell\")\ntwain_clean &lt;- txtclean(twain, \"twain\")\n\n\n\nwordnovelorigindarwinspeciesdarwincharlesdarwindarwindarwinhistoricaldarwinsketchdarwinprogressdarwinopiniondarwinorigindarwinspeciesdarwin"
  },
  {
    "objectID": "tutorials/sentiment/sentiment.html#binning",
    "href": "tutorials/sentiment/sentiment.html#binning",
    "title": "Introduction",
    "section": "Binning",
    "text": "Binning\nThe following code chunk uses binning to determine the polarity and subsequently displaying changes in polarity across the development of the novels’ plots.\n\nnovels_bin &lt;- novels_anno %&gt;%\n    dplyr::group_by(novel) %&gt;%\n    dplyr::filter(is.na(sentiment) | sentiment == \"negative\" | sentiment == \"positive\") %&gt;%\n    dplyr::mutate(\n        sentiment = as.character(sentiment),\n        sentiment = case_when(\n            is.na(sentiment) ~ \"0\",\n            TRUE ~ sentiment\n        ),\n        sentiment = case_when(\n            sentiment == \"0\" ~ 0,\n            sentiment == \"positive\" ~ 1,\n            TRUE ~ -1\n        ),\n        id = 1:n(),\n        index = as.numeric(cut2(id, m = 100))\n    ) %&gt;%\n    dplyr::group_by(novel, index) %&gt;%\n    dplyr::summarize(\n        index = unique(index),\n        polarity = mean(sentiment)\n    )\n\n\n\nnovelindexpolaritydarwin10.03960396darwin20.12000000darwin30.11000000darwin40.09000000darwin50.01000000darwin60.11000000darwin70.03000000darwin80.08000000darwin90.04000000darwin100.01000000\n\n\nWe now have an average polarity for each bin and can plot this polarity over the development of the story.\n\nggplot(novels_bin, aes(index, polarity)) +\n    facet_wrap(vars(novel), scales = \"free_x\") +\n    geom_smooth(se = F, col = \"black\") +\n    theme_bw() +\n    labs(\n        y = \"polarity ratio (mean by bin)\",\n        x = \"index (bin)\"\n    )"
  },
  {
    "objectID": "tutorials/sentiment/sentiment.html#moving-average",
    "href": "tutorials/sentiment/sentiment.html#moving-average",
    "title": "Introduction",
    "section": "Moving average",
    "text": "Moving average\nAnother method for tracking changes in polarity over time is to calculate rolling or moving means. It should be noted thought that rolling means are not an optimal method for tracking changes over time and rather represent a method for smoothing chaotic time-series data. However, they can be used to complement the analysis of changes that are detected by binning.\nTo calculate moving averages, we will assign words with positive polarity a value +1 and words with negative polarity a value of -1 (neutral words are coded as 0). A rolling mean calculates the mean over a fixed window span. Once the initial mean is calculated, the window is shifted to the next position and the mean is calculated for that window of values, and so on. We set the window size to 100 words which represents an arbitrary value.\n\nnovels_change &lt;- novels_anno %&gt;%\n    dplyr::filter(is.na(sentiment) | sentiment == \"negative\" | sentiment == \"positive\") %&gt;%\n    dplyr::group_by(novel) %&gt;%\n    dplyr::mutate(\n        sentiment = as.character(sentiment),\n        sentiment = case_when(\n            is.na(sentiment) ~ \"0\",\n            TRUE ~ sentiment\n        ),\n        sentiment = case_when(\n            sentiment == \"0\" ~ 0,\n            sentiment == \"positive\" ~ 1,\n            TRUE ~ -1\n        ),\n        id = 1:n()\n    ) %&gt;%\n    dplyr::summarise(\n        id = id,\n        rmean = rollapply(sentiment, 100, mean, align = \"right\", fill = NA)\n    ) %&gt;%\n    na.omit()\n\n\n\nnovelidrmeandarwin1000.04darwin1010.04darwin1020.04darwin1030.04darwin1040.04darwin1050.04darwin1060.04darwin1070.04darwin1080.04darwin1090.04\n\n\nWe will now display the values of the rolling mean to check if three are notable trends in how the polarity shifts over the course of the unfolding of the story within George Orwell’s Nineteen Eighty-Four.\n\nggplot(novels_change, aes(id, rmean)) +\n    facet_wrap(vars(novel), scales = \"free_x\") +\n    geom_smooth(se = F, col = \"black\") +\n    theme_bw() +\n    labs(\n        y = \"polarity ratio (rolling mean, k = 100)\",\n        x = \"index (word in monograph)\"\n    )\n\n\n\n\n\n\n\n\nThe difference between the rolling mean and the binning is quite notable and results from the fact, that rolling means represent a smoothing method rather than a method to track changes over time."
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#wordclouds",
    "href": "tutorials/textanalysis/textanalysis.html#wordclouds",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Wordclouds",
    "text": "Wordclouds\nAlternatively, word frequency lists can be visually represented as word clouds, though they provide less detailed information. Word clouds are visual representations where words appear larger based on their frequency, offering a quick visual summary of word importance in a dataset.\n\n# create a word cloud visualization\ntext %&gt;%\n  # Convert text data to a quanteda corpus\n  quanteda::corpus() %&gt;%\n  # tokenize the corpus, removing punctuation\n  quanteda::tokens(remove_punct = TRUE) %&gt;%\n  # remove English stopwords\n  quanteda::tokens_remove(stopwords(\"english\")) %&gt;%\n  # create a document-feature matrix (DFM)\n  quanteda::dfm() %&gt;%\n  # generate a word cloud using textplot_wordcloud\n  quanteda.textplots::textplot_wordcloud(\n    # maximum words to display in the word cloud\n    max_words = 150,\n    # determine the maximum size of words\n    max_size = 10,\n    # determine the minimum size of words\n    min_size = 1.5,\n    # Define a color palette for the word cloud\n    color = scales::viridis_pal(option = \"A\")(10)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nThe textplot_wordcloud function creates a word cloud visualization of text data in R. Its main arguments are x (a Document-Feature Matrix or DFM), max_words (maximum words to display), and color (color palette for the word cloud).\n\n\n\n\n\n\nAnother form of word clouds, known as comparison clouds, is helpful in discerning disparities between texts. For instance, we can load various texts and assess how they vary in terms of word frequencies. To illustrate this, we’ll load Herman Melville’s Moby Dick, George Orwell’s 1984, and Charles Darwin’s Origin.\nFirst, we’ll load these texts and combine them into single documents.\n\n# load data\norwell_sep &lt;- base::readRDS(url(\"https://slcladal.github.io/data/orwell.rda\", \"rb\"))\norwell &lt;- orwell_sep %&gt;%\n  paste0(collapse = \" \")\nmelville_sep &lt;- base::readRDS(url(\"https://slcladal.github.io/data/melville.rda\", \"rb\"))\nmelville &lt;- melville_sep %&gt;%\n  paste0(collapse = \" \")\ndarwin_sep &lt;- base::readRDS(url(\"https://slcladal.github.io/data/darwin.rda\", \"rb\"))\ndarwin &lt;- darwin_sep %&gt;%\n  paste0(collapse = \" \")\n\nNow, we generate a corpus object from these texts and create a variable with the author name.\n\ncorp_dom &lt;- quanteda::corpus(c(darwin, melville, orwell))\nattr(corp_dom, \"docvars\")$Author &lt;- c(\"Darwin\", \"Melville\", \"Orwell\")\n\nNow, we can remove so-called stopwords (non-lexical function words) and punctuation and generate the comparison cloud.\n\n# create a comparison word cloud for a corpus\ncorp_dom %&gt;%\n  # tokenize the corpus, removing punctuation, symbols, and numbers\n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE,\n    remove_numbers = TRUE\n  ) %&gt;%\n  # remove English stopwords\n  quanteda::tokens_remove(stopwords(\"english\")) %&gt;%\n  # create a Document-Feature Matrix (DFM)\n  quanteda::dfm() %&gt;%\n  # group the DFM by the 'Author' column from 'corp_dom'\n  quanteda::dfm_group(groups = corp_dom$Author) %&gt;%\n  # trim the DFM, keeping terms that occur at least 10 times\n  quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE) %&gt;%\n  # generate a comparison word cloud\n  quanteda.textplots::textplot_wordcloud(\n    # create a comparison word cloud\n    comparison = TRUE,\n    # set colors for different groups\n    color = c(\"darkgray\", \"orange\", \"purple\"),\n    # define the maximum number of words to display in the word cloud\n    max_words = 150\n  )"
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#frequency-changes",
    "href": "tutorials/textanalysis/textanalysis.html#frequency-changes",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Frequency changes",
    "text": "Frequency changes\nWe can also explore how the term alice is used throughout the chapters of our example text. To begin, let’s extract the word count for each chapter.\n\n# extract the number of words per chapter\nWords &lt;- text_chapters %&gt;%\n  # split each chapter into words based on spaces\n  stringr::str_split(\" \") %&gt;%\n  # measure the length (number of words) in each chapter\n  lengths()\n# display the resulting data, which contains the word counts per chapter\nWords\n\n [1]    8 2364 2125 1765 2616 2339 2609 2307 2487 2272 2058 1886 2153\n\n\nNext, we extract the number of matches in each chapter.\n\n# extract the number of matches of \"alice\" per chapter\nMatches &lt;- text_chapters %&gt;%\n  # count the number of times \"alice\" appears in each chapter\n  stringr::str_count(\"alice\")\n# display the resulting data, which shows the number of matches of \"alice\" per chapter\nMatches\n\n [1]  1 28 26 23 31 35 43 51 39 52 30 16 23\n\n\nNow, we extract the names of the chapters and create a table with the chapter names and the relative frequency of matches per 1,000 words.\n\n# extract chapters\nChapters &lt;- paste0(\"chapter\", 0:(length(text_chapters) - 1))\nChapters\n\n [1] \"chapter0\"  \"chapter1\"  \"chapter2\"  \"chapter3\"  \"chapter4\"  \"chapter5\" \n [7] \"chapter6\"  \"chapter7\"  \"chapter8\"  \"chapter9\"  \"chapter10\" \"chapter11\"\n[13] \"chapter12\"\n\n\nNext, we combine the information in a single data frame and add a column containing the relative frequency of alice in each chapter.\n\n# create table of results\ntb &lt;- data.frame(Chapters, Matches, Words) %&gt;%\n  # create new variable with the relative frequency\n  dplyr::mutate(Frequency = round(Matches / Words * 1000, 2)) %&gt;%\n  # reorder chapters\n  dplyr::mutate(Chapters = factor(Chapters, levels = c(paste0(\"chapter\", 0:12))))\n\n\n\nChaptersMatchesWordsFrequencychapter018125.00chapter1282,36411.84chapter2262,12512.24chapter3231,76513.03chapter4312,61611.85chapter5352,33914.96chapter6432,60916.48chapter7512,30722.11chapter8392,48715.68chapter9522,27222.89chapter10302,05814.58chapter11161,8868.48chapter12232,15310.68\n\n\nNow, let’s visualize the relative frequencies of our search term in each chapter.\n\n# create a plot using ggplot\nggplot(tb, aes(x = Chapters, y = Frequency, group = 1)) +\n  # add a smoothed line (trendline) in purple color\n  geom_smooth(color = \"purple\") +\n  # add a line plot in dark gray color\n  geom_line(color = \"darkgray\") +\n  # remove fill from the legend\n  guides(color = guide_legend(override.aes = list(fill = NA))) +\n  # set a white and black theme\n  theme_bw() +\n  # rotate x-axis text by 45 degrees and adjust alignment\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  # customize the y-axis label\n  scale_y_continuous(name = \"Relative Frequency (per 1,000 words)\")"
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#dispersion-plots",
    "href": "tutorials/textanalysis/textanalysis.html#dispersion-plots",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Dispersion plots",
    "text": "Dispersion plots\nTo show when in a text or in a collection of texts certain terms occur, we can use dispersion plots. The quanteda package offers a very easy-to-use function textplot_xray to generate dispersion plots.\n\n# add chapter names\nnames(text_chapters) &lt;- Chapters\n# generate corpus from chapters\n\n# Assuming text_chapters is your character vector\n# text_corpus &lt;- quanteda::corpus(text_chapters)\n\n# Create a tokens object\ntext_corpus &lt;- quanteda::tokens(text_chapters)\n\n# generate dispersion plots\nquanteda.textplots::textplot_xray(kwic(text_corpus, pattern = \"alice\"),\n  kwic(text_corpus, pattern = \"hatter\"),\n  sort = T\n)\n\n\n\n\n\n\n\n\nWe can modify the plot by saving it into an object and then use ggplot to modify it appearance.\n\n# generate and save dispersion plots\ndp &lt;- quanteda.textplots::textplot_xray(\n  kwic(text_corpus, pattern = \"alice\"),\n  kwic(text_corpus, pattern = \"cat\")\n)\n# modify plot\ndp + aes(color = keyword) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#over--and-underuse",
    "href": "tutorials/textanalysis/textanalysis.html#over--and-underuse",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Over- and underuse",
    "text": "Over- and underuse\nFrequency data serves as a valuable lens through which we can explore the essence of a text. For instance, when we examine private dialogues, we often encounter higher occurrences of second-person pronouns compared to more formal text types like scripted monologues or speeches. This insight holds the potential to aid in text classification and assessing text formality.\nTo illustrate, consider the following statistics: the counts of second-person pronouns, you and your, as well as the total word count excluding these pronouns in private dialogues versus scripted monologues within the Irish segment of the International Corpus of English (ICE). Additionally, the tables provide the percentage of second-person pronouns in both text types, enabling us to discern whether private dialogues indeed contain more of these pronouns compared to scripted monologues, such as speeches.\n\n\n.Private dialoguesScripted monologuesyou, your6761659Other words259625105295Percent2.600.63\n\n\nThis straightforward example highlights that second-person pronouns constitute 2.6 percent of all words in private dialogues, yet they represent only 0.63 percent in scripted speeches. To vividly illustrate such variations, we can employ association and mosaic plots, which offer effective visual presentations.\n\n# create a matrix 'd' with the specified values and dimensions\nd &lt;- matrix(c(6761, 659, 259625, 105295), nrow = 2, byrow = TRUE)\n# assign column names to the matrix\ncolnames(d) &lt;- c(\"D\", \"M\")\n# assign row names to the matrix\nrownames(d) &lt;- c(\"you, your\", \"Other words\")\n# generate an association plot using 'assocplot' function\nassocplot(d)\n\n\n\n\n\n\n\n\nIn an association plot, bars above the dashed line signify relative overuse, while bars below indicate relative underuse. Accordingly, the plot reveals that in monologues, there’s an underuse of you and your and an overuse of other words. Conversely, in dialogues, the opposite patterns emerge: an overuse of you and your and an underuse of other words. This visual representation helps us grasp the distinctive word usage patterns between these text types."
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#footnotes",
    "href": "tutorials/textanalysis/textanalysis.html#footnotes",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#preparation-and-session-set-up",
    "href": "tutorials/basicstatz/basicstatz.html#preparation-and-session-set-up",
    "title": "Basic Inferential Statistics using R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"flextable\")\ninstall.packages(\"e1071\")\ninstall.packages(\"lawstat\")\ninstall.packages(\"fGarch\")\ninstall.packages(\"gridExtra\")\ninstall.packages(\"cfa\")\ninstall.packages(\"effectsize\")\ninstall.packages(\"report\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow, we load the packages that we will need in this tutorial.\n\n# load packages\nlibrary(dplyr) # for data processing\nlibrary(ggplot2) # for data vis\nlibrary(tidyr) # for data transformation\nlibrary(flextable) # for creating tables\nlibrary(e1071) # for checking assumptions\nlibrary(lawstat) # for statistical tests\nlibrary(fGarch) # for statistical tests\nlibrary(gridExtra) # for plotting\nlibrary(cfa) # for stats\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nAlso we load some sample data sets that we will use in this tutorial.\n\n# data for indep. t-test\nitdata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/itdata.rda\", \"rb\"))\n# data for paired t-test\nptdata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/ptdata.rda\", \"rb\"))\n# data for fishers exact test\nfedata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/fedata.rda\", \"rb\"))\n# data for mann-whitney u-test\nmwudata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/mwudata.rda\", \"rb\"))\n# data for wilcox test\nuhmdata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/uhmdata.rda\", \"rb\"))\n# data for friedmann-test\nfrdata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/frdata.rda\", \"rb\"))\n# data for x2-test\nx2data &lt;- base::readRDS(url(\"https://slcladal.github.io/data/x2data.rda\", \"rb\"))\n# data for extensions of x2-test\nx2edata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/x2edata.rda\", \"rb\"))\n# multi purpose data\nmdata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/mdata.rda\", \"rb\"))\n\nOnce you have installed R, RStudio, and once you have also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#visual-inspection-of-normality",
    "href": "tutorials/basicstatz/basicstatz.html#visual-inspection-of-normality",
    "title": "Basic Inferential Statistics using R",
    "section": "Visual inspection of normality",
    "text": "Visual inspection of normality\nTo check if data are distributed normally, we extract a sample of words uttered my 100 men and 100 women from a sample corpus (the details of the sample corpus are represented by the mdata data set).\n\nndata &lt;- mdata %&gt;%\n  dplyr::rename(\n    Gender = sex,\n    Words = word.count\n  ) %&gt;%\n  dplyr::select(Gender, Words) %&gt;%\n  dplyr::filter(\n    !is.na(Words),\n    !is.na(Gender)\n  ) %&gt;%\n  dplyr::group_by(Gender) %&gt;%\n  dplyr::sample_n(100)\n\nThe table below shows the first ten lines of the women sample.\n\n\nGenderWordsfemale513female126female96female1,160female376female10female3female520female416female824\n\n\nWe can now go ahead and visualize the data to check for normality.\n\nHistograms\nThe first type of visualization we are going to use are histograms (with densities) as histograms will give us an impression about the distribution of the values. If the histograms and density functions follow a symmetric bell-shaped course, then the data are approximately normally distributed.\n\nggplot(ndata, aes(x = Words)) +\n  facet_grid(~Gender) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(aes(y = ..density..), color = \"red\") +\n  theme_bw() +\n  labs(title = \"Histograms with denisty of words uttered by men and women in a sample corpus\")\n\n\n\n\n\n\n\n\nThe histograms shows that the words uttered by men and women in the sample corpus are non-normal (this means that the errors will also be distributed non-normally).\n\n\nQuantile-Quantile Plots\nQuantile-Quantile Plots (or QQ-Plots) compare the quantiles between two distributions - typically the observed distribution and an assumed distribution like the normal distribution. If the points fall on the diagonal line, then the distributions match. If the points differ from the diagonal line, then the distributions differ.\nA very clear and concise explanation of how to manually create QQ-plots is shown is this StatQuest video by Josh Starmer (I can highly recommend all of his really informative, overall fantastic, and entertaining videos!) and explained in this TowardsDataScience post.\n\nggplot(ndata, aes(sample = Words)) +\n  facet_grid(~Gender) +\n  geom_qq() +\n  geom_qq_line(color = \"red\") +\n  theme_bw() +\n  labs(title = \"QQ-plot of words uttered by men and women in a sample corpus\", x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe deviation of the points from the line show that the data differs substantively from a normal distribution."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#statistical-measures",
    "href": "tutorials/basicstatz/basicstatz.html#statistical-measures",
    "title": "Basic Inferential Statistics using R",
    "section": "Statistical measures",
    "text": "Statistical measures\nAnother way to test if data are distributed normally is to calculate certain parameter which tell us about the skewness (whether the distribution is asymmetrical) or the kurtosis (is the data is too spiky or too flat) of the data.\n\nSkewness\nSkewed distributions are asymmetrical and they can be positively or negatively skewed. The tail of a negatively skewed distribution points towards negative values (to the left) which means that the distribution leans towards the right (towards positive values) while the tail of a positively skewed distribution points towards positive values (to the right) which means that the distribution leans towards the left (towards negative values).\nAnother characteristic of skewed distributions is that the mean and median of a distribution differ. If the median is greater than the mean, the distribution is negatively skewed (the long tail points towards negative values). If the median is smaller than the mean, the distribution is positively skewed (the long tail points towards positive values).\nNegatively skewed\n\nTail points towards negative values (to the left)\nMedian is greater than the mean\n\nPositively skewed\n\nTail points towards positive values (to the right)\nMedian is lower than the mean\n\n\n\n\n\n\n\n\n\n\nAs we need to test skewness within groups, we start by extracting the word counts of only women and then test if the distribution of the women’s word counts are normal.\n\n# extract word counts for one group\nwords_women &lt;- ndata %&gt;%\n  dplyr::filter(Gender == \"female\") %&gt;%\n  dplyr::pull(Words)\n# inspect\nhead(words_women)\n\n[1]  513  126   96 1160  376   10\n\n\nTo see if a distribution is skewed, we can use the summary function to check if the mean and the median differ.\n\nsummary(words_women)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   95.25  360.50  501.25  756.25 2482.00 \n\n\nIn our example, the mean is larger than the median which suggests that the data are positively skewed.\nWe apply the skewness function from the e1071 package to the sample scores to calculated skewness. The skewness function allows to calculate skewness in three different ways:\nType 1 (This is the typical definition used in many older textbooks): \\[\\begin{equation}\ng_1 = m_3 / m_2^{(3/2)}.\n\\end{equation}\\]\nType 2 (Used in SAS and SPSS): \\[\\begin{equation}\nG_1 = g_1 * sqrt(n(n-1)) / (n-2)\n\\end{equation}\\]\nType 3 (Used in MINITAB and BMDP): \\[\\begin{equation}\nb_1 = m_3 / s^3 = g_1 ((n-1)/n)^{(3/2)}\n\\end{equation}\\]\nAll three formulas have in common that the more negative values are, the more strongly positively skewed are the data (distribution leans to the left) and the more positive the values are, the more strongly negatively skewed are the data (distribution leans to the right). Here we use the second formula (by setting type = 2) that is also used in SPSS.\n\nskewness(words_women, type = 2)\n\n[1] 1.540359\n\n\nIf the reported skewness value is negative, then the distribution is positively skewed. If the value is positive, then the distribution is negatively skewed. If the value is lower than -1 or greater than +1, then the distribution can be assumed to be substantively skewed (Hair et al.).\n\n\nKurtosis\nAnother way in which distributions can differ from the normal distribution relates to the thickness of the tails and the spikiness of the distribution. If distributions are bell-shaped like the normal distribution, they are called mesokurtic. If distributions are symmetrical but they are more spiky than the normal distribution, they are called leptokurtic. If symmetrical distributions are flatter and have bigger tails than the normal distribution, the distributions are called platykurtic.\n\n\n\n\n\n\n\n\n\nTo show how we can calculate kurtosis (or if a distribution suffers from kurtosis), we apply the kurtosis function from the e1071 package to the sample scores we generated above /when we calculated skewness).\n\nkurtosis(words_women)\n\n[1] 2.179125\n\n\nAs the kurtosis value is positive, the distribution is leptokurtic (if it were negative, the distribution would be platykurtic). As a rule of thumb, values greater than +1 indicate that the distribution is too peaked while values lower than –1 indicate that the distribution is substantively platykurtic (Hair et al., pp61).\nThe kurtosis score can thus be interpreted as follows:\n\nA values of 0 means that the distribution is perfectly mesokurtic with Values between -0.5 and 0.5 suggesting that the distribution is approximately mesokurtic\nValues between -0.5 and -1 mean that the distribution is moderately platykurtic with values smaller than -1 indicating that the distribution is platykurtic.\nValues between 0.5 and 1 mean that the distribution is moderately leptokurtic with values greater than 1 indicating that the distribution is leptokurtic"
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#statistical-test-of-assumptions",
    "href": "tutorials/basicstatz/basicstatz.html#statistical-test-of-assumptions",
    "title": "Basic Inferential Statistics using R",
    "section": "Statistical test of assumptions",
    "text": "Statistical test of assumptions\nThe two most common tests to check assumptions are the Shapiro-Wilk test, which tests if the data differ significantly from a normal distribution, and the Levene’s test which tests if the variances are of two groups are approximately equal.\n\nShapiro-Wilk test\nThe Shapiro-Wilk test (cf. Shapiro and Wilk) can be used to check if the data differs significantly from a normal distribution (within groups). However, it has been shown to be too lenient when dealing with small sample sizes (below 50 to 100 cases per variable level), but too strict when dealing with larger sample sizes (500 or more cases per variable level). A such, the Shapiro-Wilk test should only be used in combination with visual inspection on the data.\nIn this example, we will only test if the words uttered by women differ significantly from a normal distribution (we use the words_women vector created above). Once we have a a vector of word counts of one group only (in this case the women’s word counts), we can continue by performing the Shapiro-Wilk test.\n\nshapiro.test(words_women)\n\n\n    Shapiro-Wilk normality test\n\ndata:  words_women\nW = 0.84256, p-value = 6.357e-09\n\n\nIf the p-value of the Shapiro-Wilk test is greater than .05, the data do not support the hypothesis that they differ from normality. In other words, if the p-value is greater than .05, we can assume that the data are approximately normally distributed.\nThe output of the Shapiro-Wilk test shown above thus indicates that our data differs significantly from normal (W = 0.7925, p &lt; .001***). For more information about the implementation of the Shapiro-Wilk test in R, type ?shapiro.test into the console.\n\n\nLevene’s test\nThe Levene’s test (cf. Levene) evaluates if the variances of two groups is approximately equal - this is referred to as homoskedasticity. This is important, because unequal variances - or heteroskedasticity - strongly suggest that there is another factor, a confound, that is not included in the model but that significantly affects the dependent variable which renders results of an analysis unreliable.\nTo implement a Levene’s test in R, we need to install and load thelawstat package.\n\nlevene.test(mdata$word.count, mdata$sex)\n\n\n    Modified robust Brown-Forsythe Levene-type test based on the absolute\n    deviations from the median\n\ndata:  mdata$word.count\nTest Statistic = 0.0050084, p-value = 0.9436\n\n\nIf the p-values of the Levene’s test is greater than .05, we can assume that the variances are approximately equal. Thus, the output of the Levene’s test shown above thus indicates that the variances of men and women in our data are approximately equal (W = 0.005, p = .9436). For more information about the implementation of the Levene’s test in R, type ?levene.test into the console."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#students-t-test",
    "href": "tutorials/basicstatz/basicstatz.html#students-t-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Student’s t-test",
    "text": "Student’s t-test\nThere are two basic types of t-tests: the dependent or paired t-test and the independent t-test. Paired t-test are used when the data points are not independent, for example, because they come form the same subjects in a pre-post test design. In contrast, Independent t-tests are used when the data points are independent and come from two different groups (e.g., from learners and native speakers or from men and women).\nThe assumptions of the Student’s t-test are that\n\nthe dependent variable is a continuous, numeric variable;\nthe independent variable is a nominal variable (two levels / groups)\nthe variances within each group are approximately normal;\nthe errors within each group are approximately normal (this implies that the distributions of the scores of each group are approximately normal).\n\nIf the variances are not normal, then this indicates that another important variable is confounding the results. In such cases, you should go back to the data and check what other variable could cause the unequal variances. If you decide to proceed with the analysis, you can switch to a Welch t-test which does not assume equal variances within each group.\n\nPaired t-test\nPaired t-tests take into account that the scores (or values) come from the same individuals in two conditions (e.g. before and after a treatment).There are two equations for the paired t-test that are used.\n\\[\\begin{equation}\nt = \\frac{\\sum D}{\\sqrt{\\frac{N \\sum D^2 - (\\sum D)^2}{N-1}}}\n\\end{equation}\\]\nor\n\\[\\begin{equation}\nt = \\frac{\\bar D}{\\frac{s_D}{\\sqrt{N}}}\n\\end{equation}\\]\nTo show how a paired t-tests works, we will test if a treatment (a teaching method) reduces the number of spelling errors in a long essay of 6 students. In a first step, we generate some data representing the errors in two essays of the same length written before and after the teaching method was used for 8 weeks.\n\nPretest &lt;- c(78, 65, 71, 68, 76, 59)\nPosttest &lt;- c(71, 62, 70, 60, 66, 48)\nptd &lt;- data.frame(Pretest, Posttest)\n\nThe data look like as shown in the table below.\n\n\nPretestPosttest787165627170686076665948\n\n\nTo perform a paired t-test in R, we use the t.test function and specify the argument paired as TRUE.\n\nt.test(ptd$Pretest,\n  ptd$Posttest,\n  paired = TRUE,\n  conf.level = 0.95\n)\n\n\n    Paired t-test\n\ndata:  ptd$Pretest and ptd$Posttest\nt = 4.1523, df = 5, p-value = 0.00889\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  2.539479 10.793854\nsample estimates:\nmean difference \n       6.666667 \n\n\nIn addition to testing if the groups differ significantly, we also want to calculate the effect size of the difference. We can use the effectsize package to extract Cohen’s \\(d\\) which is the standard effect size measure for t-tests.\n\neffectsize::cohens_d(\n  x = ptd$Pretest,\n  y = ptd$Posttest,\n  paired = TRUE\n)\n\nCohen's d |       95% CI\n------------------------\n1.70      | [0.37, 2.96]\n\n\nTo check if the effect is small or big - that is if a Cohen’s \\(d\\) value can be interpreted as being small or big, we can use the following overview.\n\n\nEffectSizedReferenceVery small0.01Sawilowsky (2009)Small0.20Cohen (1988)Medium0.50Cohen (1988)Large0.80Cohen (1988)Very large1.20Sawilowsky (2009)Huge2.00Sawilowsky (2009)\n\n\nThe classification combines Sawilowsky and Cohen. The analysis can be summarized using the reports package (Makowski et al. 2021) as follows.\n\nreport::report(t.test(ptd$Pretest, ptd$Posttest, paired = TRUE, conf.level = 0.95))\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Paired t-test testing the difference between ptd$Pretest and ptd$Posttest\n(mean difference = 6.67) suggests that the effect is positive, statistically\nsignificant, and large (difference = 6.67, 95% CI [2.54, 10.79], t(5) = 4.15, p\n= 0.009; Cohen's d = 1.70, 95% CI [0.37, 2.96])\n\n\nWe can use this output to write up a final report:\nA paired t-test test was applied to the data and it confirmed that the number of spelling errors after the 8 weeks of using the new teaching method significantly decreased (t5: 4.1523, p = .009**). The treatment had a very large, statistically significant, positive effect (Cohen’s \\(d\\) = 1.70 [CIs: 0.41, 3.25]) (cf. Sawilowsky).\n\n\nIndependent t-tests\nIndependent t-tests are used very widely and they determine if the means of two groups are significantly different. As such, t-tests are used when we have a normally distributed (or parametric), numeric dependent variable and a nominal predictor variable.\n\\[\\begin{equation}\nt = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{\\frac{s^2_p}{N_1} + \\frac{s^2_p}{N_2}}}\n\\end{equation}\\]\nwhere\n\\[\\begin{equation}\ns^2_p = \\frac{(N_1 - 1)s^2_1 + (N_2 - 1)s^2_2}{N_1 + N_2 - 2}\n\\end{equation}\\]\nWe now load some data that we can apply a t-test to. The data represents scores on a proficiency test of native speakers and learners of English. We want to use a t-test to determine if the native speakers and learners differ in their proficiency.\n\n# load data\ntdata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/d03.rda\", \"rb\")) %&gt;%\n  dplyr::rename(\n    NativeSpeakers = 1,\n    Learners = 2\n  ) %&gt;%\n  tidyr::gather(Group, Score, NativeSpeakers:Learners) %&gt;%\n  dplyr::mutate(Group = factor(Group))\n\n\n\nGroupScoreNativeSpeakers6NativeSpeakers65NativeSpeakers12NativeSpeakers56NativeSpeakers45NativeSpeakers84NativeSpeakers38NativeSpeakers46NativeSpeakers64NativeSpeakers24\n\n\nWe now apply an independent t-test to the data.\n\nt.test(Score ~ Group,\n  var.equal = T,\n  data = tdata\n)\n\n\n    Two Sample t-test\n\ndata:  Score by Group\nt = -0.054589, df = 18, p-value = 0.9571\nalternative hypothesis: true difference in means between group Learners and group NativeSpeakers is not equal to 0\n95 percent confidence interval:\n -19.74317  18.74317\nsample estimates:\n      mean in group Learners mean in group NativeSpeakers \n                        43.5                         44.0 \n\n\nAs the p-value is higher than .05, we cannot reject the H-0- and we thus have to conclude that our evidence does not suffice to say that learners and Native Speakers differ in their proficiency. However, we still extract the effect size, again using Cohen’s \\(d\\). In contract to the extraction of the effect size for paired t-tests, however, we will set the argument paired to FALSE (in fact, we could simply leave it out as the paired = FALSE is the default).\n\neffectsize::cohens_d(tdata$Score ~ tdata$Group,\n  paired = FALSE\n)\n\nCohen's d |        95% CI\n-------------------------\n-0.02     | [-0.90, 0.85]\n\n- Estimated using pooled SD.\n\n\nThe analysis can be summarized using the reports package (Makowski et al. 2021) as follows.\n\nreport::report(t.test(Score ~ Group, var.equal = T, data = tdata))\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Two Sample t-test testing the difference of Score by Group (mean in group\nLearners = 43.50, mean in group NativeSpeakers = 44.00) suggests that the\neffect is negative, statistically not significant, and very small (difference =\n-0.50, 95% CI [-19.74, 18.74], t(18) = -0.05, p = 0.957; Cohen's d = -0.03, 95%\nCI [-0.95, 0.90])\n\n\nWe can use this output to write up a final report:\nAn independent t-test test was applied to the data and it reported that the scores between the two groups did not differ significantly (t18: -0.0546, p = .9571). In addition to not differing significantly, the effect size of the difference between the groups was also very small (Cohen’s \\(d\\) = -0.03 [CIs: -0.95, 0.90]) (cf. Sawilowsky)."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#fishers-exact-test",
    "href": "tutorials/basicstatz/basicstatz.html#fishers-exact-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\nFisher’s Exact test is very useful because it does not rely on distributional assumptions relying on normality. Instead, Fisher’s Exact Test calculates the probabilities of all possible outcomes and uses these to determine significance. To understand how a Fisher’s Exact test, we will use a very simple example.\nImagine you are interested in adjective modification and you want to find out if very and truly differ in their collocational preferences. So you extract all instances of cool, all instances of very, and all instances of truly from a corpus. Now that you have gathered this data, you want to test if truly and very differ with respect to their preference to co-occur with cool. Accordingly, you tabulate the results and get the following table.\n\n\nAdverbwith coolwith other adjectivestruly540very1741\n\n\nTo perform a Fisher’s Exact test, we first create a table with these results and then use the fisher.test function to perform the Fisher’s Exact Test to see if very and truly differ in their preference to co-occur with cool (as shown below). The null hypothesis is that there is no difference between the adverbs.\n\n# create table\ncoolmx &lt;- matrix(\n  c(5, 17, 40, 41),\n  nrow = 2, # number of rows of the table\n  # def. dimension names\n  dimnames = list(\n    Adverbs = c(\"truly\", \"very\"),\n    Adjectives = c(\"cool\", \"other adjective\")\n  )\n)\n# perform test\nfisher.test(coolmx)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  coolmx\np-value = 0.03024\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.08015294 0.96759831\nsample estimates:\nodds ratio \n 0.3048159 \n\n\nThe results of the Fisher’s Exact test show that the p-value is lower than .05, which means we reject the null hypothesis, and we are therefore justified in assuming that very and truly differ in their collocational preferences to co-occur with cool.\nThe analysis can be summarized as follows:\nA Fisher’s Exact test was applied to the data to determine if there was a significant difference in the modification of adjective cool. Specifically, we tested if the preference of cool to be modified by very and by truly differed significantly. The results of the Fisher’s Exact test confirmed that there was a statistically significant difference in the modification preference of cool(p: .030*). However, the effect size of the preference is small (Odds Ratio: 0.305)."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#mann-whitney-u-test",
    "href": "tutorials/basicstatz/basicstatz.html#mann-whitney-u-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Mann-Whitney U-Test",
    "text": "Mann-Whitney U-Test\nIt is actually quite common that numeric depend variables need to be transformed or converted into ranks, i.e. ordinal variables, because the distribution of residuals does not allow the application of parametric tests such as t-tests or linear regression. In such cases, as we are dealing with rank (ordinal) data, the application of a chi-square test is unwarranted and we need to use another test. There are different alternatives depending on whether the data are paired (coming from the same individuals) or if all observations are independent.\nThe non-parametric alternative for independent t-tests, i.e. for data where we are dealing with two separate groups and a numeric dependent variable that violates parametric assumptions (or an ordinal dependent variable), is the Mann-Whitney U-test. In contrast, if the groups under investigation represent identical participants that are tested under two conditions, the appropriate alternative is a Wilcoxon Signed Rank test (which is thus the alternative for paired t-test).\nImagine we wanted to determine if two language families differed with respect to the size of their phoneme inventories. You have already ranked the inventory sizes and would now like to now if language family correlates with inventory size. As such, we are dealing with two independent groups and we want to implement a non-parametric alternative of a t-test. To answer this question, you create the table shown below.\n\n# create table\nRank &lt;- c(1, 3, 5, 6, 8, 9, 10, 11, 17, 19, 2, 4, 7, 12, 13, 14, 15, 16, 18, 20)\nLanguageFamily &lt;- c(rep(\"Kovati\", 10), rep(\"Urudi\", 10))\nlftb &lt;- data.frame(LanguageFamily, Rank)\n\n\n\nLanguageFamilyRankKovati1Kovati3Kovati5Kovati6Kovati8Kovati9Kovati10Kovati11Kovati17Kovati19Urudi2Urudi4Urudi7Urudi12Urudi13Urudi14Urudi15Urudi16Urudi18Urudi20\n\n\nWe will also briefly inspect the data visually using a box plot.\n\nggplot(lftb, aes(x = LanguageFamily, y = Rank, fill = LanguageFamily)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"orange\", \"darkgrey\")) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nTo use the Mann-Whitney U test, the dependent variable (Rank) must be ordinal and independent variable (Group) must be a binary factor. We briefly check this by inspecting the structure of the data.\n\n# inspect structure\nstr(lftb)\n\n'data.frame':   20 obs. of  2 variables:\n $ LanguageFamily: chr  \"Kovati\" \"Kovati\" \"Kovati\" \"Kovati\" ...\n $ Rank          : num  1 3 5 6 8 9 10 11 17 19 ...\n\n\nAs the variables are what we need them to be, we can now perform the Mann-Whitney U test on the table. The null hypothesis is that there is no difference between the 2 groups.\n\n# perform test\nwilcox.test(lftb$Rank ~ lftb$LanguageFamily)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  lftb$Rank by lftb$LanguageFamily\nW = 34, p-value = 0.2475\nalternative hypothesis: true location shift is not equal to 0\n\n\nSince the p-value is greater than 0.05, we fail to reject the null hypothesis. The results of the Mann-Whitney U test tell us that the two language families do not differ significantly with respect to their phoneme inventory size.\nThe analysis can be summarized using the reports package (Makowski et al. 2021) as follows.\n\nreport::report(wilcox.test(lftb$Rank ~ lftb$LanguageFamily))\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum exact test testing the difference in ranks between\nlftb$Rank and lftb$LanguageFamily suggests that the effect is negative,\nstatistically not significant, and large (W = 34.00, p = 0.247; r (rank\nbiserial) = -0.32, 95% CI [-0.69, 0.18])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is no statistically significant relationship between the size of phoneme inventories and being member of selected language families. Despite being statistically insignificant, the effect may be large (W = 34.00, p = 0.247; r (rank biserial) = -0.32, 95% CI [-0.69, 0.18]).\nMann-Whitney U tests with continuity correction\nThe Mann-Whitney U test can also be used with continuity correction. A continuity correction is necessary when both variables represent numeric values that are non-normal. In the following example, we want to test if the reaction time for identifying a word as real is correlated with its token frequency.\nFor this example, we generate data is deliberately non-normal.\n\n\nFrequencyNormalizedReactionReaction20,0002,460.5810,0002,373.3906,6671,761.82675,0002,239.92924,0002,114.72003,3331,924.0962,8571,882.11962,5001,248.95282,2222,165.61982,0002,242.7700\n\n\nWhen we plot the data, we see that both the frequency of words (Frequency) and the reaction times that it took subjects to recognize the token as a word (Reaction) are non-normal (in this case, the distributions are negative skewed).\n\n\n\n\n\n\n\n\n\nBoth variables are negatively skewed (non-normally distributed) but we can use the wilcox.test function to perform the Mann-Whitney U test with continuity correction which takes the skewness into account. The null hypothesis is that there is no difference between the 2 groups. Although the output states that the test that was performed is a Wilcoxon rank sum test with continuity correction, we have actually performed a Mann-Whitney U test - this is because the nomenclature for the tests is not unanimous.\n\n# perform test\nwilcox.test(wxdata$Reaction, wxdata$Frequency)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  wxdata$Reaction and wxdata$Frequency\nW = 7572.5, p-value = 3.291e-10\nalternative hypothesis: true location shift is not equal to 0\n\n\nAgain, we use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(wilcox.test(wxdata$Reaction, wxdata$Frequency))\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in\nranks between wxdata$Reaction and wxdata$Frequency suggests that the effect is\npositive, statistically significant, and very large (W = 7572.50, p &lt; .001; r\n(rank biserial) = 0.51, 95% CI [0.39, 0.62])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is a strong, positive, statistically significant relationship between the reaction time for identifying a word as real and its token frequency (W = 7613.50, p &lt; .001; r (rank biserial) = 0.52, 95% CI [0.40, 0.63])."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#wilcoxon-rank-sum-test",
    "href": "tutorials/basicstatz/basicstatz.html#wilcoxon-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Wilcoxon rank sum test",
    "text": "Wilcoxon rank sum test\nThe Wilcoxon rank sum test is the non-parametric alternative to a paired or dependent t-test. Thus, a Wilcoxon rank sum test is used when the data represent the same individuals that were tested under two condition. To tell R that we are dealing with paired data, we have to set the argument paired to TRUE while we can still use the wilcox.test function (as we did before when using the Mann-Whitney U test which is applied to independent groups).\nIn this example, the same individuals had to read tongue twisters when they were sober and when they were intoxicated. A Wilcoxon signed rank test with continuity correction is used to test if the number of errors that occur when reading tongue twisters correlates with being sober/intoxicated. Again, we create fictitious data.\n\n# create data\nsober &lt;- sample(0:9, 15, replace = T)\nintoxicated &lt;- sample(3:12, 15, replace = T)\n# tabulate data\nintoxtb &lt;- data.frame(sober, intoxicated)\n\n\n\nsoberintoxicated51124771121556310290116941081161027511\n\n\nNow, we briefly plot the data.\n\nintoxtb2 &lt;- data.frame(\n  c(\n    rep(\"sober\", nrow(intoxtb)),\n    rep(\"intoxicated\", nrow(intoxtb))\n  ),\n  c(intoxtb$sober, intoxtb$intoxicated)\n) %&gt;%\n  dplyr::rename(\n    State = 1,\n    Errors = 2\n  )\nggplot(intoxtb2, aes(State, Errors)) +\n  geom_boxplot(fill = c(\"orange\", \"darkgrey\"), width = 0.5) +\n  labs(y = \"Number of errors\", x = \"State\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe boxes indicate a significant difference. Finally, we perform the Wilcoxon signed rank test with continuity correction. The null hypothesis is that the two groups are the same.\n\n# perform test\nwilcox.test(intoxtb$intoxicated, intoxtb$sober, paired = T)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  intoxtb$intoxicated and intoxtb$sober\nV = 105, p-value = 0.001072\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe p-value is lower than 0.05 (rejecting the null hypothesis) which means that the number of errors when reading tongue twisters is affected by one’s state (sober/intoxicated) - at least in this fictitious example.\nAgain, we use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(wilcox.test(intoxtb$intoxicated, intoxtb$sober, paired = T))\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon signed rank test with continuity correction testing the difference\nin ranks between intoxtb$intoxicated and intoxtb$sober suggests that the effect\nis positive, statistically significant, and very large (W = 105.00, p = 0.001;\nr (rank biserial) = 1.00, 95% CI [1.00, 1.00])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is a very large, positive, statistically significant relationship between the number of errors produced in tongue twisters and being intoxicated (W = 6.50, p = 0.003; r (rank biserial) = -0.89, 95% CI [-0.97, -0.64])."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#kruskal-wallis-rank-sum-test",
    "href": "tutorials/basicstatz/basicstatz.html#kruskal-wallis-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Kruskal-Wallis Rank Sum Test",
    "text": "Kruskal-Wallis Rank Sum Test\nThe Kruskal-Wallis rank sum test is a type of ANOVA (Analysis of Variance). For this reason, the Kruskal Wallis Test is also referred to as a one-way Anova by ranks which can handle numeric and ordinal data.\nIn the example below, uhm represents the number of filled pauses in a short 5 minute interview while speaker represents whether the speaker was a native speaker or a learner of English. As before, the data is generated and thus artificial.\n\n# create data\nuhms &lt;- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)\nSpeaker &lt;- c(rep(\"Learner\", 5), rep(\"NativeSpeaker\", 5))\n# create table\nuhmtb &lt;- data.frame(Speaker, uhms)\n\n\n\nSpeakeruhmsLearner15Learner13Learner10Learner8Learner37NativeSpeaker23NativeSpeaker31NativeSpeaker52NativeSpeaker11NativeSpeaker17\n\n\nNow, we briefly plot the data.\n\nggplot(uhmtb, aes(Speaker, uhms)) +\n  geom_boxplot(fill = c(\"orange\", \"darkgrey\")) +\n  theme_bw() +\n  labs(x = \"Speaker type\", y = \"Errors\")\n\n\n\n\n\n\n\n\nNow, we test for statistical significance. The null hypothesis is that there is no difference between the groups.\n\nkruskal.test(uhmtb$Speaker ~ uhmtb$uhms)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  uhmtb$Speaker by uhmtb$uhms\nKruskal-Wallis chi-squared = 9, df = 9, p-value = 0.4373\n\n\nThe p-value is greater than 0.05, therefore we fail to reject the null hypothesis. The Kruskal-Wallis test does not report a significant difference for the number of uhms produced by native speakers and learners of English in the fictitious data."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#the-friedman-rank-sum-test",
    "href": "tutorials/basicstatz/basicstatz.html#the-friedman-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "The Friedman Rank Sum Test",
    "text": "The Friedman Rank Sum Test\nThe Friedman rank sum test is also called a randomized block design and it is used when the correlation between a numeric dependent variable, a grouping factor and a blocking factor is tested. The Friedman rank sum test assumes that each combination of the grouping factor (Gender) and the blocking factor (Age) occur only once. Thus, imagine that the values of uhms represent the means of the respective groups.\n\n# create data\nuhms &lt;- c(7.2, 9.1, 14.6, 13.8)\nGender &lt;- c(\"Female\", \"Male\", \"Female\", \"Male\")\nAge &lt;- c(\"Young\", \"Young\", \"Old\", \"Old\")\n# create table\nuhmtb2 &lt;- data.frame(Gender, Age, uhms)\n\n\n\nGenderAgeuhmsFemaleYoung7.2MaleYoung9.1FemaleOld14.6MaleOld13.8\n\n\nWe now perform the Friedman rank sum test.\n\nfriedman.test(uhms ~ Age | Gender, data = uhmtb2)\n\n\n    Friedman rank sum test\n\ndata:  uhms and Age and Gender\nFriedman chi-squared = 2, df = 1, p-value = 0.1573\n\n\nIn our example, age does not affect the use of filled pauses even if we control for gender as the p-value is higher than .05."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#pearsonss-chi-square-test",
    "href": "tutorials/basicstatz/basicstatz.html#pearsonss-chi-square-test",
    "title": "Basic Inferential Statistics using R",
    "section": "(Pearsons’s) Chi-Square Test",
    "text": "(Pearsons’s) Chi-Square Test\nOne of the most frequently used statistical test in linguistics is the \\(\\chi\\)2 test (or Pearsons’s chi-square test, chi-squared test, or chi-square test). We will use a simple, practical example to explore how this test works. In this example, we will test whether speakers of American English (AmE) and speakers of British English (BrE) differ in their use of the near-synonyms sort of and kind of as in “He’s sort of stupid” and “He’s kind of stupid”. As a first step, we formulate the hypothesis that we want to test (H1) and its null hypothesis (H0). The alternative- or test hypothesis reads:\nH1: Speakers of AmE and BrE differ with respect to their preference for sort of and kind of.\nwhile the Null Hypothesis (H0) states\nH0: Speakers of AmE and BrE do not differ with respect to their preference for sort of and kind of.\nThe H0 claims the non-existence of something (which is the more conservative position) and in our example the non-existence of a correlation between variety of English and the use of sort of and kind of. The question now arises what has to be the case in order to reject the H0 in favor of the H1.\nTo answer this question, we require information about the probability of error, i.e. the probability that the H0 does indeed hold for the entire population. Before performing the chi-square test, we follow the convention that the required significance level is 5 percent. In other words, we will reject the H0 if the likelihood for the H\\(_{0}\\) being true is less than 5 percent given the distribution of the data. In that case, i.e. in case that the likelihood for the H0 being true is less than 5 percent, we consider the result of the chi-square test as statistically significant. This means that the observed distribution makes it very unlikely that there is no correlation between the variety of English and the use of sort of and kind of.\nLet us now assume that we have performed a search for sort of and kind of in two corpora representing American and British English and that we have obtained the following frequencies:\n\n\nHedgeBrEAmEkindof181655sortof17767\n\n\nIn a first step, we now have to calculate the row and column sums of our table.\nHedgeBrEAmETotalkindof181655836sortof17767244Total3587221,080\nNext, we calculate, the values that would have expected if there was no correlation between variety of English and the use of sort of and kind of. In order to get these expected frequencies, we apply the equation below to all cells in our table.\n\\[\\begin{equation}\n\\frac{Column total*Row total}{Overall total}\n\\end{equation}\\]\nIn our example this means that for the cell with [+]BrE [+]kindof we get:\n\\[\\begin{equation}\n\\frac{836*358}{1080} = \\frac{299288}{1080} = 277.1185\n\\end{equation}\\]\nFor the entire table this means we get the following expected values:\nHedgeBrEAmETotalkindof277.11850558.8815836sortof80.88148163.1185244Total358.00000722.00001,080\nIn a next step, we calculate the contribution of each cell to the overall \\(\\chi\\)2 value (\\(\\chi\\)2 contribution). To get \\(\\chi\\)2 contribution for each cell, we apply the equation below to each cell.\n\\[\\begin{equation}\n\\frac{(observed – expected)^{2}}{expected}\n\\end{equation}\\]\nIn our example this means that for the cell with [+]BrE [+]kindof we get:\n\\[\\begin{equation}\n\\frac{(181 – 277.1185)^{2}}{277.1185} = \\frac{-96.1185^{2}}{277.1185} = \\frac{9238.766}{277.1185} = 33.33868\n\\end{equation}\\]\nFor the entire table this means we get the following \\(\\chi^{2}\\) values:\nHedgeBrEAmETotalkindof33.3386916.5308249.86951sortof114.2260256.63839170.86440Total147.5647073.16921220.73390\nThe sum of \\(\\chi\\)2 contributions in our example is 220.7339. To see if this value is statistically significant, we need to calculate the degrees of freedom because the \\(\\chi\\) distribution differs across degrees of freedom. Degrees of freedom are calculated according to the equation below.\n\\[\\begin{equation}\nDF = (rows -1) * (columns – 1) = (2-1) * (2-1) = 1 * 1 = 1\n\\end{equation}\\]\nIn a last step, we check whether the \\(\\chi\\)2 value that we have calculated is higher than a critical value (in which case the correlation in our table is significant). Degrees of freedom are relevant here because the critical values are dependent upon the degrees of freedom: the more degrees of freedom, the higher the critical value, i.e. the harder it is to breach the level of significance.\nSince there is only 1 degree of freedom in our case, we need to consider only the first column in the table of critical values below.\nDFp&lt;.05p&lt;.01p&lt;.00113.846.6410.8325.999.2113.8237.8211.3516.2749.4913.2818.47511.0715.0920.52\nSince the \\(\\chi\\)2 value that we have calculated is much higher than the critical value provided for p&lt;.05, we can reject the H0 and may now claim that speakers of AmE and BrE differ with respect to their preference for sort of and kind of.\nBefore we summarize the results, we will calculate the effect size which is a measure for how strong the correlations are.\n\nEffect Sizes in Chi-Square\nEffect sizes are important because they correlations may be highly significant but the effect between variables can be extremely weak. The effect size is therefore a measure how strong the correlation or the explanatory and predictive power between variables is.\nThe effect size measure for \\(\\chi\\)2 tests can be either the \\(\\phi\\)-coefficient (phi-coefficient) or Cramer’s \\(\\phi\\) (Cramer’s phi). The \\(\\phi\\)-coefficient is used when dealing with 2x2 tables while Cramer’s \\(\\phi\\) is used when dealing with tables with more than 4 cells. The \\(\\phi\\) coefficient can be calculated by using the equation below (N = overall sample size).\n\\[\\begin{equation}\n\\phi = \\sqrt{\\frac{\\chi^{2}}{N}}\n\\end{equation}\\]\nIn our case, this means:\n\\[\\begin{equation}\n\\phi = \\sqrt{\\frac{220.7339}{1080}} = \\sqrt{0.2043832} = 0.4520876\n\\end{equation}\\]\nThe \\(\\phi\\) coefficient varies between 0 (no effect) and 1 (perfect correlation). For the division into weak, moderate and strong effects one can follow the division for \\(\\omega\\) (small omega), so that with values beginning with .1 represent weak, values between 0.3 and .5 represent moderate and values above .5 represent strong effects (Bühner and Ziegler, 266). So, in this example we are dealing with a medium-sized effect/correlation.\n\n\nChi-Square in R\nBefore we summarize the results, we will see how to perform a chi-square test in R. In addition to what we have done above, we will also visualize the data. To begin with, we will have a look at the data set (which is the same data we have used above).\n\n# inspect data\nchidata %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(\"Hedge\") %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::border_outer()\n\nHedgeBrEAmEkindof181655sortof17767\n\n\nWe will now visualize the data with an association. Bars above the dashed line indicate that a feature combination occurs more frequently than expected by chance. The width of the bars indicates the frequency of the feature combination.\n\n\n\n\n\n\n\n\n\nThe fact that the bars are distributed complimentary (top left red and below bar; top right black above bar; bottom left black above bar; bottom right red below bar) indicates that the use of sort of and kind of differs across AmE and BrE. We will check whether the mosaic plot confirms this impression.\n\nmosaicplot(chidata, shade = TRUE, type = \"pearson\", main = \"\") # mosaic plot\n\n\n\n\n\n\n\n\nThe color contrasts in the mosaic plot substantiate the impression that the two varieties of English differ significantly. To ascertain whether the differences are statistically significant, we can now apply the chi-square test.\n\n# perform chi square test without Yate's correction\nchisq.test(chidata, corr = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  chidata\nX-squared = 220.73, df = 1, p-value &lt; 2.2e-16\n\n\nThe results reported by R are identical to the results we derived by hand and confirm that BrE and AmE differ significantly in their use of sort of and kind of.\n\n\n\n\nNOTE\n\nWhen conducting a \\(\\chi\\)2 test of independence and in order to interpret the Pearson’s \\(\\chi\\)2 statistic, by default, most of the statistical packages, including R, assume that the observed frequencies in a contingency table can be approximated by the continuous \\(\\chi\\)2 distribution.  To avoid-reduce the error that the approximation introduces, the chisq.test function in base R includes the correct argument that, by default, is set to TRUE. This argument integrates the Frank Yate’s adjustment that aims at compensating for deviations from the (smooth) theoretical chi-squared distribution and it is considered especially useful if the frequency in each cell is less than a small number. Some statisticians set this number to 5 and others to 10. Although this case scenario is relevant to linguistic data, this is not always the case. Moreover, there is a strong tendency in other fields to avoid the Yates’ continuity correction altogether due to the overestimated amount of adjustment that it introduces. That is why, it is helpful to know how to avoid the Yates’ continuity correction. To do that, you simply set the correct argument to FALSE, as follows (see also above):\n\n# X2-test without Yate's correction\nchisq.test(chidata, correct = FALSE)\n\nAlso, in case you would like to deactivate the scientific notation used for displaying the p-value, as it is displayed in the output of the chisq.test function, you can do the following:\n\nformat(chisq.test(chidata)$p.value, scientific = FALSE)\n\n\n\n\n\n\n\nIn a next step, we calculate the effect size.\n\n# calculate effect size\nsqrt(chisq.test(chidata, corr = F)$statistic / sum(chidata) * (min(dim(chidata)) - 1))\n\nX-squared \n0.4520877 \n\n\nThe \\(\\phi\\) coefficient of .45 shows that variety of English correlates moderately with the use of sort of and kind of. We will now summarize the results.\n\n\nSummarizing Chi-Square Results\nThe results of our analysis can be summarized as follows: A \\(\\chi\\)2-test confirms a highly significant correlation of moderate size between the variety of English and the use of the near-synonymous hedges sort of and kind of (\\(\\chi\\)2 = 220.73, df = 1, p &lt; .001***, \\(\\phi\\) = .452).\n\n\nRequirements of Chi-Square\nChi-square tests depend on certain requirements that, if violated, negatively affect the reliability of the results of the test. To provide reliable results, 80 percent of cells in a table to which the chi-square test is applied have to have expected values of 5 or higher and at most 20 percent of expected values can be smaller than 5 (see Bortz, Lienert, and Boehnke, 98). In addition, none of the expected values can be smaller than 1 (see Bortz, Lienert, and Boehnke, 136) because then, the estimation, which relies on the \\(\\chi\\)2-distribution, becomes too imprecise to allow meaningful inferences (Cochran).\nIf these requirements are violated, then the Fisher’s Exact Test is more reliable and offers the additional advantage that these tests can also be applied to data that represent very small sample sizes. When applying the Fisher’s Exact Test, the probabilities for all possible outcomes are calculated and the summed probability for the observed or more extreme results are determined. If this sum of probabilities exceeds five percent, then the result is deemed statistically significant.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nImagine you are interested in whether older or younger speakers tend to refer to themselves linguistically. The underlying hypothesis is that - contrary to common belief - older people are more narcissistic compared with younger people. Given this research question, perform a chi-square test and summarize the results on the data below.\n\n\n\nAge1SGPNPN without 1SGTotalYoung6143104Old423678Total10379182\n\n\n\n\nAnswer\n\n\n    # generate data\n    ex1dat &lt;- matrix(c(61, 43, 42, 36), ncol = 2, byrow = T)\n    # add column names\n    colnames(ex1dat) &lt;- c(\"1SGPN\", \"PN without 1SG\")\n    # perform x2-test\n    x2_ex1 &lt;- chisq.test(ex1dat)\n    # inspect results\n    x2_ex1\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  ex1dat\nX-squared = 0.2465, df = 1, p-value = 0.6195\n\n\nGiven the data, we cannot reject the H0 according to which old people are more narcissistic compared to young people measured by their use of 1st person pronouns in conversation (\\(\\chi\\)~{1}: 0.2465026, p: 0.6195485).\n\n\nImagine you are interested in whether young men or young women exhibit a preference for the word whatever because you have made the unsystematic, anecdotal observation that young men use this word more frequently than young women. Given this research question, perform a chi-square test and summarize the results on the data below.\n\n\n\nItemYoungMalesYoungFemalesTotalwhatever175571other words3451289165521261680Total3451459166071261752\n\n\n\n\nAnswer\n\n\n    # generate data\n    ex2dat &lt;- matrix(c(17, 55, 345128, 916552), ncol = 2, byrow = T)\n    # add column names\n    colnames(ex2dat) &lt;- c(\"YoungMales\", \"YoungFemales\")\n    # perform x2-test\n    x2_ex2 &lt;- chisq.test(ex2dat)\n    # inspect results\n    x2_ex2\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  ex2dat\nX-squared = 0.33682, df = 1, p-value = 0.5617\n\n\nGiven the data, we cannot reject the H0 according to which there is no difference between young men and young women in their use of the word whatever (\\(\\chi\\)~{1}: 0.3368202, p: 0.5616705).\n\n\nFind a partner and discuss the relationship between significance and effect size. Then, go and find another partner and discuss problems that may arise when testing the frequency of certain words compared with the overall frequency of words in a corpus."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#extensions-of-chi-square",
    "href": "tutorials/basicstatz/basicstatz.html#extensions-of-chi-square",
    "title": "Basic Inferential Statistics using R",
    "section": "Extensions of Chi-Square",
    "text": "Extensions of Chi-Square\nIn the following, we will have a look at tests and methods that can be used if the requirements for ordinary (Pearson’s) chi-square tests are violated and their use would be inappropriate\n\nThe Yates-Correction\nIf all requirements for ordinary chi-square tests are acceptable and only the sample size is the issue, then applying a so-called Yates-correction may be appropriate. This type of correction is applied in cases where the overall sample size lies in-between 60 and 15 cases (Bortz, Lienert, and Boehnke, 91). The difference between the ordinary chi-square and a Yates-corrected chi-square lies in the fact that the Yates-corrected chi-square is calculated according to the equation below.\n\\[\\begin{equation}\n\\frac{(|observed – expected|-0.5)^{2}}{expected}\n\\end{equation}\\]\nAccording to this formula, we would get the values shown below rather than the values tabulated above. It is important to note here that this is only a demonstration because a Yates-Correction would actually be inappropriate as our sample size exceeds 60 cases.\n\n\nVariantBrEAmETotalkind of32.9927113.0407146.0335sort of16.359356.050772.41Total49.352169.0914218.4434\n\n\nIf the Yates-correction were applied, then this results in a slightly lower \\(\\chi\\)2-value and thus in more conservative results compared with the traditional test according to Pearson.\n\n\nChi-Square within 2-by-k tables\nAlthough the \\(\\chi\\)2-test is widely used, it is often used inappropriately. This is especially the case when chi-square tests are applied to data representing tables with more than two rows and more than two columns. It is important to note that applying the common Pearson’s’ chi-square test to sub-tables of a larger table is inappropriate because, in such cases, a modified variant of Pearson’s’ chi-square test is warranted. We will go through two examples that represent different scenarios where we are dealing with sub-samples of larger tables and a modified version of the \\(\\chi\\)2-test should be used rather than Pearson’s’ chi-square.\nIn this first example, we are dealing with a table consisting of two columns and multiple rows, a so-called 2*k table (two-by-k table). In order to test if a feature combination, that is represented by a row in the 2*k table, is significantly more common compared with other feature combinations, we need to implement the \\(\\chi\\)2-equation from (Bortz, Lienert, and Boehnke, 126–27).\nIn this example, we want to find out whether soft and hard X-rays differ in their effect on grasshopper larva. The question is whether the larva reach or do not reach a certain life cycle depending on whether they are exposed to soft X-rays, hard X-rays, light, or beta rays. The data for this example is provided below.\nTreatmentMitosis not reachedMitosis reachedTotalX-ray soft211435X-ray hard181331Beta-rays241236Light133043Total7669145\nIf we would apply an ordinary chi-square test, we would ignore that all data were collected together and using only a sub-sample would ignore the data set of which the sub-sample is part of. In other words, the sub-sample is not independent from the other data (as it represents a subsection of the whole data set). However, for exemplary reasons, we will apply an ordinary chi-square test first and then compare its results to results provided by the correct version of the chi-square test. In a first step, we create a table with all the data.\n\n# create data\nwholetable &lt;- matrix(c(21, 14, 18, 13, 24, 12, 13, 30), byrow = T, nrow = 4)\ncolnames(wholetable) &lt;- c(\"reached\", \"notreached\") # add column names\nrownames(wholetable) &lt;- c(\"rsoft\", \"rhard\", \"beta\", \"light\") # add row names\n\n\n\nTreatmentreachednotreachedrsoft2114rhard1813beta2412light1330\n\n\nNow, we extract the sub-sample from the data.\n\nsubtable &lt;- wholetable[1:2, ] # extract subtable\n\n\n\nTreatmentreachednotreachedrsoft2114rhard1813\n\n\nNext, we apply the ordinary chi-square test to the sub-sample.\n\n# simple x2-test\nchisq.test(subtable, corr = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  subtable\nX-squared = 0.025476, df = 1, p-value = 0.8732\n\n\nFinally, we perform the correct chi-square test.\n\n# load function for correct chi-square\nsource(\"https://slcladal.github.io/rscripts/x2.2k.r\")\nx2.2k(wholetable, 1, 2)\n\n$Description\n[1] \"rsoft  against  rhard  by  reached  vs  notreached\"\n\n$`Chi-Squared`\n[1] 0.025\n\n$df\n[1] 1\n\n$`p-value`\n[1] 0.8744\n\n$Phi\n[1] 0.013\n\n$Report\n[1] \"Conclusion: the null hypothesis cannot be rejected! Results are not significant!\"\n\n\nBelow is a table comparing the results of the two chi-square tests.\n\n\nStatisticchi-squarechi-square in 2*k-tableschi-squared0.02550.025p-value0.87320.8744\n\n\nThe comparison shows that, in this example, the results of the two tests are very similar but this may not always be the case.\n\n\nChi-Square within z-by-k tables\nAnother application in which the \\(\\chi\\)2 test is often applied incorrectly is when ordinary Parsons’s \\(\\chi\\)2 tests are used to test portions of tables with more than two rows and more than two columns, that is z*k tables (z: row, k: column). An example is discussed by Gries who also wrote the R Script for the correct version of the \\(\\chi\\)2 test.\nLet’s first load the data discussed in the example of Gries 9. The example deals with metaphors across registers. Based on a larger table, a \\(\\chi\\)2 confirmed that registers differ with respect to the frequency of EMOTION metaphors. The more refined question is whether the use of the metaphors EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE differs between spoken conversation and fiction.\n\n# create table\nwholetable &lt;- matrix(c(8, 31, 44, 36, 5, 14, 25, 38, 4, 22, 17, 12, 8, 11, 16, 24), ncol = 4)\nattr(wholetable, \"dimnames\") &lt;- list(\n  Register = c(\"acad\", \"spoken\", \"fiction\", \"new\"),\n  Metaphor = c(\"Heated fluid\", \"Light\", \"NatForce\", \"Other\")\n)\n\nBased on the table above, we can extract the following sub-table.\nRegisterHeated fluidLightNatForceOtheracad8548spoken31142211fiction44251716new36381224\nIf we used an ordinary Pearson’s \\(\\chi\\)2 test (the use of which would be inappropriate here), it would reveal that spoken conversations do not differ significantly from fiction in their use of EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE (\\(\\chi\\)2=3.3016, df=1, p=.069, \\(\\phi\\)=.2057).\n\n# create table\nsubtable &lt;- matrix(c(14, 25, 22, 17), ncol = 2)\nchisq.results &lt;- chisq.test(subtable, correct = FALSE) # WRONG!\nphi.coefficient &lt;- sqrt(chisq.results$statistic / sum(subtable) * (min(dim(subtable)) - 1))\nchisq.results\n\n\n    Pearson's Chi-squared test\n\ndata:  subtable\nX-squared = 3.3016, df = 1, p-value = 0.06921\n\nphi.coefficient\n\nX-squared \n0.2057378 \n\n\nThe correct analysis takes into account that it is a sub-table that is not independent of the overall table. This means that the correct analysis should take into account the total number of cases, as well as the row and column totals (cf. Bortz, Lienert, and Boehnke, 144–48).\nIn order to perform the correct analysis, we must either implement the equation proposed in Bortz, Lienert, and Boehnke or read in the function written by Gries and apply it to the sub-table.\n\n# load function for chi square test for subtables\nsource(\"https://slcladal.github.io/rscripts/sub.table.r\")\n# apply test\nresults &lt;- sub.table(wholetable, 2:3, 2:3, out = \"short\")\n# inspect results\nresults\n\n$`Whole table`\n         Metaphor\nRegister  Heated fluid Light NatForce Other Sum\n  acad               8     5        4     8  25\n  spoken            31    14       22    11  78\n  fiction           44    25       17    16 102\n  new               36    38       12    24 110\n  Sum              119    82       55    59 315\n\n$`Sub-table`\n         Metaphor\nRegister  Light NatForce Sum\n  spoken     14       22  36\n  fiction    25       17  42\n  Sum        39       39  78\n\n$`Chi-square tests`\n                                  Chi-square Df    p-value\nCells of sub-table to whole table  7.2682190  3 0.06382273\nRows (within sub-table)            0.2526975  1 0.61518204\nColumns (within sub-table)         3.1519956  1 0.07583417\nContingency (within sub-table)     3.8635259  1 0.04934652\n\n\nThe results show that the difference is, in fact, statistically significant at an \\(\\alpha\\)-level of .05 (\\(\\chi^{2}\\)=3.864, df=1, p&lt;.05*)."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#configural-frequency-analysis-cfa",
    "href": "tutorials/basicstatz/basicstatz.html#configural-frequency-analysis-cfa",
    "title": "Basic Inferential Statistics using R",
    "section": "Configural Frequency Analysis (CFA)",
    "text": "Configural Frequency Analysis (CFA)\nConfigural Frequency Analysis (CFA) is a multivariate extension of the \\(\\chi^{2}\\)-test. If we perform a \\(\\chi^{2}\\)-test on a table with more than 2 rows and 2 columns, the test tells us that somewhere in that table there is at least one cell tat differs significantly from the expected value. However, we do not know which cell or cells this is. To determine which of the cells differ significantly from the expected value, we use the CFA.\nTo perform a CFA in R, we need to load the cfa package and load some data (as shown below).\n\n# load package\nlibrary(cfa)\n# load data\ncfadata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/cfd.rda\", \"rb\"))\n\n\n\nVarietyAgeGenderClassFrequencyAmericanOldManMiddle7BritishOldWomanMiddle9BritishOldManMiddle6AmericanOldWomanWorking2AmericanOldManMiddle5BritishOldManMiddle8AmericanOldManMiddle8BritishOldManMiddle6BritishOldManMiddle3AmericanOldWomanMiddle8\n\n\nIn a next step, we define the configurations and separate them from the counts. The configurations are the independent variables in this design and the counts represent the dependent variable.\n\n# define configurations\nconfigs &lt;- cfadata %&gt;%\n  dplyr::select(Variety, Age, Gender, Class)\n# define counts\ncounts &lt;- cfadata$Frequency\n\nNow that configurations and counts are separated, we can perform the configural frequency analysis.\n\n# perform cfa\ncfa(configs, counts)\n\n\n*** Analysis of configuration frequencies (CFA) ***\n\n                         label   n   expected            Q       chisq\n1     American Old Man Working   9  17.269530 0.0074991397 3.959871781\n2    American Young Man Middle  20  13.322419 0.0060338993 3.346996519\n3    British Old Woman Working  33  24.277715 0.0079603059 3.133665860\n4   British Young Woman Middle  12  18.728819 0.0061100471 2.417504403\n5  American Young Woman Middle  10   6.362422 0.0032663933 2.079707490\n6      British Old Man Working  59  50.835658 0.0076361897 1.311214959\n7     British Young Man Middle  44  39.216698 0.0044257736 0.583424432\n8    American Old Woman Middle  81  76.497023 0.0043152503 0.265066491\n9     British Old Woman Middle 218 225.181379 0.0080255135 0.229025170\n10     American Old Man Middle 156 160.178850 0.0043537801 0.109020569\n11  American Old Woman Working   8   8.247454 0.0002225797 0.007424506\n12      British Old Man Middle 470 471.512390 0.0023321805 0.004851037\n      p.chisq sig.chisq          z        p.z sig.z\n1  0.04659725     FALSE -2.1267203 0.98327834 FALSE\n2  0.06732776     FALSE  1.7026500 0.04431680 FALSE\n3  0.07669111     FALSE  1.6871254 0.04578962 FALSE\n4  0.11998594     FALSE -1.6845116 0.95395858 FALSE\n5  0.14926878     FALSE  1.2474422 0.10611771 FALSE\n6  0.25217480     FALSE  1.1002146 0.13561931 FALSE\n7  0.44497317     FALSE  0.6962784 0.24312726 FALSE\n8  0.60666058     FALSE  0.4741578 0.31769368 FALSE\n9  0.63224759     FALSE -0.5726832 0.71657040 FALSE\n10 0.74126197     FALSE -0.3993470 0.65518123 FALSE\n11 0.93133480     FALSE -0.2612337 0.60304386 FALSE\n12 0.94447273     FALSE -0.1217934 0.54846869 FALSE\n\n\nSummary statistics:\n\nTotal Chi squared         =  17.44777 \nTotal degrees of freedom  =  11 \np                         =  2.9531e-05 \nSum of counts             =  1120 \n\nLevels:\n\nVariety     Age  Gender   Class \n      2       2       2       2 \n\n\nThe output table contains the configurations (here called lables), then the observed frequencies (the counts) and the expected frequencies, the Q and the \\(\\chi^{2}\\) statistic as well as the p-value associated with the \\(\\chi^{2}\\) value. In addition, the table provides the z-transformed \\(\\chi^{2}\\)-values and their p-value.\nIf a p-value is below the level of significance - typically below .05 - the configuration occurs with a frequency that differs significantly from the expected frequency. If the observed value is higher than the expected value, then the configuration occurs significantly more frequently than would be expected by chance. If the observed value is lower than the expected value, then the configuration occurs significantly less frequently than would be expected by chance."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#hierarchical-configural-frequency-analysis-hcfa",
    "href": "tutorials/basicstatz/basicstatz.html#hierarchical-configural-frequency-analysis-hcfa",
    "title": "Basic Inferential Statistics using R",
    "section": "Hierarchical Configural Frequency Analysis (HCFA)",
    "text": "Hierarchical Configural Frequency Analysis (HCFA)\nA hierarchical alternative to CFA is Hierarchical Configural Frequency Analysis (HCFA). In contrast to CFA, in HCFA, the data is assumed to be nested! We begin by defining the configurations and separate them from the counts.\n\n# define configurations\nconfigs &lt;- cfadata %&gt;%\n  dplyr::select(Variety, Age, Gender, Class)\n# define counts\ncounts &lt;- cfadata$Frequency\n\nNow that configurations and counts are separated, we can perform the hierarchical configural frequency analysis.\n\n# perform cfa\nhcfa(configs, counts)\n\n\n*** Hierarchical CFA ***\n\n                     Overall chi squared df          p order\nVariety Age Class              12.218696  4 0.01579696     3\nVariety Gender Class            8.773578  4 0.06701496     3\nVariety Age Gender              7.974102  4 0.09253149     3\nVariety Class                   6.078225  1 0.01368582     2\nVariety Class                   6.078225  1 0.01368582     2\nAge Gender Class                5.164357  4 0.27084537     3\nVariety Age                     4.466643  1 0.03456284     2\nVariety Age                     4.466643  1 0.03456284     2\nAge Gender                      1.934543  1 0.16426233     2\nAge Gender                      1.934543  1 0.16426233     2\nAge Class                       1.673538  1 0.19578534     2\nAge Class                       1.673538  1 0.19578534     2\nGender Class                    1.546666  1 0.21362833     2\nGender Class                    1.546666  1 0.21362833     2\nVariety Gender                  1.120155  1 0.28988518     2\nVariety Gender                  1.120155  1 0.28988518     2\n\n\nAccording to the HCFA, only a single configuration (Variety : Age : Class) is significant (X2 = 12.21, p = .016)."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#footnotes",
    "href": "tutorials/basicstatz/basicstatz.html#footnotes",
    "title": "Basic Inferential Statistics using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Language Technology and Data Analysis Laboratory (LADAL)",
    "section": "",
    "text": "WELCOME\n\nWelcomes to the website of the Language Technology and Data Analysis Laboratory (LADAL). LADAL (pronounced lah’dahl) is a free, open-source, collaborative support infrastructure for digital and computational humanities established 2019 by the School of Languages and Cultures at the University of Queensland. LADAL aims at assisting anyone interested in working with language data in matters relating to data processing, visualization, and analysis and offers guidance on matters relating to language technology and digital research tools. To this end, LADAL offers introductions to topics and concepts related to digital and computational humanities, online tutorials, interactive Jupyter notebooks, and events including workshops and webinar series.\n\nLADAL is part of the Language Data Commons of Australia (LDaCAATAP) and the Australian Text Analytics Platform (ATAP). The aim of LDaCA and ATAP is to provide researchers with a Notebook environment – in other words a tool set - that is more powerful and customisable than standard packages, while being accessible to a large number of researchers who do not have strong coding skills.\nThe Language Data Commons of Australia (LDaCAATAP) and the Australian Text Analytics Platform (ATAP) projects received investment (https://doi.org/10.47486/PL074) from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS).\n\n\nThe Australian Text Analytics Platform (ATAP) projects received investment (https://doi.org/10.47486/PL074) from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS).\n\n\n\n\nSince going public January 1, 2021, LADAL has received almost 1,100,000 page views of more than 500,000 active users in nearly 750,000 engaged sessions! The majority of LADAL users access the LADAL website from the USA (app. 20%), Great Britain and Germany (app. 6%), Australia and China (app. 5%), India (4%), and the Netherlands (3%). The highest number of users per day was May 18, 2022 with 981 users.\n\n\n\n\n\n\n\n\n\nGet in Touch!\nTo get in touch with us here at LADAL, maybe because you are interested in becoming a contributor or because you found an error on our site, you can simply write an email to ladal@uq.edu.au, reach out to us on Twitter (@slcladal), send us a message on Facebook (see our Facebook page or you can sign up to the LADAL email list. To subscribe to our email list, simply write an email to ladal@uq.edu.au with the subject email list.\n\n\nGoals\nThe LADAL aims to help develop computational and digital skills by providing information and practical, hands-on tutorials on data and text analytics as well as on statistical methods relevant for language research. In addition, the LADAL provides self-guided study materials relevant for computational Natural Language Processing. In order to be attractive to both beginners and people with advanced skills, the LADAL website covers topics and introduces methods relevant for people coming with different degrees of prior knowledge and experience - ranging from introductions to concepts of quantitative reasoning to step-by-step guides on advanced statistical modeling.\nSince the primary concern of the LADAL is to introduce computational methods that are relevant to research involving natural language, the focus of this website is placed on linguistic data and methods relevant for text analytics. As such, the LADAL provides resources for (computational) text analytics and offers introductions to quantitative reasoning, research designs, and computational methods including data visualization and statistics. The areas covered on the LADAL website are\n\n\n\nintroductions to quantitative reasoning and basic concepts in empirical language studies.\nintroductions to R as programming environment for processing natural language data.\ntutorials on data visualization and data analytics (statistics and machine learning).\ntutorials on text analysis, text mining, distant reading, and corpus linguistics.\n\n\nThe resources and events offered by LADAL also aim at promoting and informing about Best Practices in data handling such as transparency, reproducibility and the FAIR principles (data should be findable, accessible, interoperable, and reusable).\n\n\n\nUser Stories\nBelow are selected user stories of people that have used LADAL resources in their research, training, or teaching.\n\n\n\nUSER STORIESWe are currently looking for user stories (also known as testimonials) to see and show what people use LADAL resources for. If you have used LADAL resources - be it by simply copying some code, attending a workshop, learning about a method using a tutorial, or in any other way - we would be extremely grateful, if you would send us your user story! To submit your user story, simply write up a paragraph describing how you have used LADAL resources and what you have used them for and send it to ladal@uq.edu.au. We really appreciate any feedback from you about this!\n\n\n\n\n\nPaula Rautionaho (University researcher, University of Eastern Finland)\nI learned about the LADAL website at a conference and since then I’ve been going through the contents bit by bit, starting with R and Data science basics and the many tutorials available. The website is great for acquiring basic knowledge, and I’ve also used it to find information on specific methods that I need for my research. The way the code is explained in detail and exemplified through actual studies, and the fact that the code is downloadable from the site, are extremely useful and helpful. What I usually do is download the string of code, go through each line to understand what’s going on and then modify it to my needs. The website also helps in understanding the output of statistical analyses, which for me is what sets this resource apart from many others.\n\n\n\n\nLaura Janda (Professor of Russian, The Arctic University of Norway, Tromsø)\nLADAL is a tremendously valuable resource that I recommend to all my students in my Quantitative Methods in Linguistics course. Given the broad portfolio of various courses that I teach plus my numerous other commitments, combined with the rapid pace of developments in both R itself and its application to linguistic analyses, it is not possible for me to keep apace with all of the developments all of the time. It is very important to have an authoritative and comprehensive resource that represents current best practices in the field, and that is exactly what LADAL is.\n\n\n\n\nRobert Daugs (Postdoctoral Researcher, University of Kiel)\nIt’s great to see how the content has evolved and it seems that whenever I come across a method I hear about in another talk or read in a paper and wish to implement in my own research (or just try it out), a corresponding script with meaty instructions is already available at LADAL. The tutorial I probably came back to more than once actually covers mixed-effects regression modeling. Given this method’s value in corpus-based, variationist linguistics and elsewhere, I think it’s great that this tutorial made the cut and I’m looking forward to any further updates the LADAL crowd might have planned for this.\nThanks for such a wonderful, open access resource!\n\n\n\n\nTouba Warsi (Analyst - School of Medicine, University of California San Diego)\nI was very happy to find the text analysis tutorial (https://slcladal.github.io/textanalysis.html) for text analysis in R. Up-to-date and super helpful! I use it for mining open-ended student survey comments with the hope of identifying themes, glean feedback without resorting to human coding of the comments.\nPlease keep up the great work!\n\n\n\n\nSimone Griesser (Senior Research and Teaching Fellow, FHNW University of Applied Sciences and Arts Northwestern Switzerland)\nDear LADAL Team\nI hope you are well. Thanks for your wonderful website. I find it tremendously useful. Following your call for user stories I am e-mailing you.\nI have used LADAL’s POS-tagging in German for a project which aims to develop language patterns to recognise brand personality in LinkedIn and Facebook posts. The project is for a social media company advising SME’s on their social media strategy and activities. The rationale for using word types as a language characteristic is that active and dynamic brand personalities should use more verbs, especially active verbs. Community-focused brands should use more personal and possessive pronouns. Brand personalities focussing on competency with facts and figures should use more adjectives.\nThe language patterns will include other language characteristics such as language arousal, dominance, concreteness as well as key words specific to the different brand personality types. If feasible I also like to include tenses in German.\n\n\n\n\nAudience\nThe LADAL resources are aimed at researchers in HASS (Humanities, Arts, and the Social Sciences) and we aspire to attract complete novices as well as expert users. And, while the focus of the LADAL website is placed on handling data that represents natural language, anyone who has an interest in quantitative methods, data visualization, statistics, or R is welcome to explore this webpage.\n\n\nAt LADAL, we aim to reach out and make our resources available to the research community and anyone interested Language Technology, Data Analysis, and using computational means to extract, process, visualize and analyze language data. To this end, we offer workshops, give presentations and talks, organize webinars (see, e.g., the LADAL Webinar Series 2021).\n\nIn addition, we provide resources on the LADAL website and on the LADAL YouTube channel, we announce updates on Twitter (@slcladal) as well as on our NEWS site and via our Facebook page. To get in touch, you can contact us on Twitter or send us an email via ladal@uq.edu.au.\n\n\n\n\nProgramming\n\nThe LADAL primarily uses the programming language R because R is extremely flexible, relatively easy to learn, free and open source, and R has a substantive and very friendly user community. R is not merely a software package but a fully-fledged programming environment which allows complex Natural Language Processing, statistics and data visualizations and it can also be used to create websites or apps, and has direct pipelines for version control (Git). This website as well as the self-guided study materials offered by the LADAL use are written in R-markdown - a way to combine R-code with text. The flexibility of R makes it a sensible choice for researchers that strive for high quality and extreme flexibility while following best practices that enable complete replicability and full transparency. If you want to learn more about R and why we use it, please check out our Why R? page.\n\nAs computation is becoming ever more prevalent across disciplines as well as in both the social and economic domains, the LADAL offers a resource space for R that make it accessible to lay users as well as expert programmers. That said, we will expand the resources provided by the LADAL to other tools and environments and include tutorials based on Python in the future.\n\n\n\nLicensing\nThe LADAL website was created by Martin Schweinberger. It was freely released under GNU General Public License, Version 3, June 2007.\n\n\n\nCitation\nIf you use (parts of) LADAL tutorials for your own research or in your teaching materials, please cite the individual subpages as shown at the bottom of each page or reference it as:\nSchweinberger, Martin. `2023. The Language Technology and Data Analysis Laboratory (LADAL). Brisbane: The University of Queensland, School of Languages and Cultures. url: https://slcladal.github.io/index.html (Version 2023.09.14).\n@manual{uqslc2023ladal,\n  author = {Schweinberger, Martin},\n  title = {The Language Technology and Data Analysis Laboratory (LADAL)},\n  note = {https://ladal.edu.au},\n  year = {2023},\n  organization = {The University of Queensland, School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2023.09.14}\n}\n\nDisclaimer\n\nThe content of this website is free and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute the content of LADAL resources given you adhere to the licensing. The content of this website is distributed under the terms of the GNU General Public License, Version 3, June 2007.\n\n\nShare and Enjoy!\n\nBack to top"
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "TOOLS",
    "section": "",
    "text": "This page lists the tools provided by LADAL. To access a tool, simply click on the  badge or on the linked name of the tool, and the tool (an interactive Jupyter notebook) will launch, allowing you to perform the task.\n\nOverview\n\n\n\nTool\nContent\nLaunch Tool\n\n\n\n\nConcordancing Tool\nAllows users to create Keyword-in-Context displays of words or phrases in a single text or collection of texts and to download the resulting table.\n\n\n\nCollocation Tool\nTool that calculates various association measures (collocation statistics) based on textual input and allows users to download the resulting table.\n\n\n\nKeyword Tool\nTool that calculates various keyness measures based on textual input and allows users to download the resulting table.\n\n\n\nPos-tagging Tool\nTools that allows users to add part-of-speech tags to texts or collections of texts in more than 60 languages and allows users to download the resulting pos-tagged texts.\n\n\n\nCorpus-cleaning Tool\nTool that allows users to remove and replace words, tags, and other elements from the text data that you upload (of course you can then download the cleaned texts too).\n\n\n\nNetwork-Analysis Tool\nTool that allows users to generate and download network graphs based on their own data.\n\n\n\nTopic-Model Tool\nTool that allows users to generate their own custom topic models using unsupervised and supervised LDA and download the results in an MS Excel spreadsheet.\n\n\n\nSentiment-Analysis Tool\nTool that allows users to perform a sentiment analysis and download the results in an MS Excel spreadsheet.\n\n\n\n\n\n\nReporting Errors\nIf something’s amiss or if a tool isn’t cooperating, don’t hesitate to contact Martin at m.schweinberger@uq.edu.au. We’ll do our best to investigate and resolve the issue as quickly as we can. Thanks for bringing it to our attention!\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "TUTORIALS",
    "section": "",
    "text": "This page presents the tutorials provided by LADAL. To access a tutorial, click on its title, and you will be directed to the respective website.\n\nData Science Basics\n\nWorking with Computers\nThis tutorial provides advice and general tips on how to keep your computer clean and running smoothly, how to organize files and folders, and how to store your data safely and orderly.\nData Management and Reproducibility\nThis tutorial introduces basic data management techniques, version control measures, and issues relating to reproducible research.\nIntroduction to Quantitative Reasoning\nThis tutorial takes a philosophical or history-of-ideas approach and introduces the logical and cognitive underpinnings of the the scientific method.\nBasic Concepts in Quantitative Research\nThis tutorial introduces basic concepts of data analysis and quantitative research.\n\n\nR Basics\n\nWhy R?\nThis site provides our reasoning for focusing (almost exclusively) on R in LADAL.\nGetting started\nThis tutorial shows how to get started with R and it specifically focuses on R for analyzing language data but it offers valuable information for anyone who wants to get started with R.\nLoading and saving data\nThis tutorial shows how you can load and save different types of data when working with R.\nString Processing\nThis tutorial introduces string processing and this can be used when working with language data.\nRegular Expressions\nThis tutorial introduces regular expressions and how they can be used when working with language data.\nHandling tables in R\nThis tutorial shows how to work with tables and how to tabulate data in R.\n\n\nData Visualization\n\nIntroduction to Data Viz\nThis tutorial introduces data visualization using R and shows how to modify different types of visualizations in the ggplot framework in R.\nData Visualization with R\nThis tutorial introduces different types of data visualization and how to prepare your data for different plot types.\nIntroduction to Geospatial Data Visualization\nThis tutorial introduces geo-spatial data visualization in R.\nInteractive Charts\nThis tutorial shows how to generate interactive data visualizations in R.\n\n\nStatistics\n\nDescriptive Statistics\nThis tutorial focuses on how to describe and summarize data in R.\nBasic Inferential Statistics\nThis tutorial introduces basic inferential procedures for null-hypothesis hypothesis testing.\nRegression Analysis\nThis tutorial introduces regression analyses (also called regression modeling) using R. Regression models are among the most widely used quantitative methods in the language sciences to assess if and how predictors (variables or interactions between variables) correlate with a certain response.\n\nTree-Based Models\nThis tutorial focuses on tree-based models and their implementation in R.\nCluster and Correspondence Analysis\nThis tutorial introduces classification and clustering using R. Cluster analyses fall within the domain of classification methods which are used to find groups or patterns in data or to predict group membership.\n\n\nIntroduction to Lexical Similarity\nThis tutorial introduces Text Similarity, i.e. how close or similar two pieces of text are with respect to either their use of words or characters (lexical similarity) or in terms of meaning (semantic similarity).\nSemantic Vector Space Models\nThis tutorial introduces Semantic Vector Space (SVM) modeling R. SVMs are used to find groups or patterns in data or to predict group membership.\nDimension Reduction Methods\nThis tutorial introduces selected dimension reduction methods (Principal Component Analysis, Factor Analysis, and Multidimensional Scaling) which allow to detect and evaluate structures, called components, latent variables, or factors, underlying observed variables.\nPower Analysis\nThis tutorial introduces power analysis using R. Power analysis is a method primarily used to determine the appropriate sample size for empirical studies.\n\n\nText Analytics\n\nIntroduction to Text Analysis\nThis tutorial introduces Text Analysis, i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text.\nPractical Overview of Selected Text Analytics Methods\nThis tutorial showcases some basic but useful methods for text analysis and serves as a practical overview or introduction to Text analytics and distant reading.\nConcordancing (keywords-in-context)\nThis tutorial introduces how to find words or phrases in text and display concordances, a so-called keyword-in-context (KWIC) display, with R.\nCollocation and N-gram Analysis\nThis tutorial introduces collocation analysis and identifying N-grams with R and shows how to extract and visualize semantic links between words.\nKeyness and Keyword Analysis\nThis tutorial introduces keyness analysis and identifying keywords with R and shows how to visualize keywords.\n\nNetwork Analysis\nThis tutorial introduces network analysis using R. Network analysis is a method for visualization that can be used to represent various types of data.\nTopic Modeling\nThis tutorial introduces topic modeling using R.\nSentiment Analysis\nThis tutorial introduces sentiment analysis (SA) and shows how to perform a SA in R.\nTagging and Parsing\nThis tutorial introduces part-of-speech tagging and syntactic parsing using R.\nAutomated Text Summarization\nThis tutorial shows how to summarize texts automatically using R by extracting the most prototypical sentences.\nSpell Checking\nThis tutorial shows how to implement and use spell checking in R when working with text data.\n\n\nCase Studies\n\nClassifying American Speeches\nThis tutorial shows how to perform document classification using R. It was created by Gerold Schneider and Max Lauber for the Australian Text Analytics Platform (ATAP).\nCorpus Linguistics with R\nThis section presents different case studies or use cases that highlight how to do corpus-based analyses by implementing procedures shown in other LADAL tutorials.\nAnalyzing learner language using R\nThis tutorial focuses on learner language and how to analyze differences between learners and L1 speakers of English using R.\nLexicography and Creating Dictionaries with R\nThis tutorial introduces lexicography with R and shows how to use R to create dictionaries and find synonyms through determining semantic similarity in R.\nVisualizing and Analyzing Questionnaire and Survey Data\nThis tutorial offers some advice on what to consider when creating surveys and questionnaires, provides tips on visualizing survey data, and exemplifies how survey and questionnaire data can be analyzed.\n\nCreating Vowel Charts in R\nThis tutorial exemplifies how to create a vowel chart with Praat and R.\nComputational Literary Stylistics with R\nThis tutorial focuses on computational literary stylistics (also digital literary stylistics) and shows how fictional texts can be analyzed by using computational means.\nPractical phylogenetic methods for linguistic typology\nThis tutorial shows how you can do phylogenetic analysis in R.\nReinforcement Learning and Text Summarization in R\nThis tutorial introduces the concept of Reinforcement Learning (RL), and how it can be applied in the domain of Natural Language Processing (NLP) and linguistics.\n\n\nHow-Tos\n\nConverting PDFs to txt\nThis tutorial shows how to extract text from one or more pdf-files using optical character recognition (OCR) and then saving the text(s) in txt-files on your computer.\nDownloading Texts from Project Gutenberg\nThis tutorial shows how to download and clean works from the Project Gutenberg archive using R. Project Gutenberg is a data base which contains roughly 60,000 texts for which the US copyright has expired.\nWebcrawling with R\nThis tutorial shows how to crawl and download web data using R, including link following and text extraction.\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "tutorials/clust/clust.html#underlying-concepts",
    "href": "tutorials/clust/clust.html#underlying-concepts",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Underlying Concepts",
    "text": "Underlying Concepts\nThe next section focuses on the basic idea that underlies all cluster analyses. WE will have a look at some very basic examples to highlight and discuss the principles that cluster analyses rely on.\nThe underlying idea of cluster analysis is very simple and rather intuitive as we ourselves perform cluster analyses every day in our lives. This is so because we group things together under certain labels and into concepts. The first example used to show this, deals with types of trees and how we group these types of trees based on their outward appearance.\nImagine you see six trees representing different types of trees: a pine tree, a fir tree, an oak tree, a beech tree, a phoenix palm tree, and a nikau palm tree. Now, you were asked to group these trees according to similarity. Have a look at the plot below and see whether you would have come up with a similar type of grouping.\n\nAn alternative way to group the trees would be the following.\n\nIn this display, conifers and broad-leaf trees are grouped together because there are more similar to each other compared to palm trees. This poses the question of what is meant by similarity. Consider the display below.\n\nAre the red and the blue line more similar because they have the same shape or are the red and the black line more similar because they are closer together? There is no single correct answer here. Rather the plot intends to raise awareness about the fact that how cluster analyses group data depends on how similarity is defined in the respective algorithm.\nLet’s consider another example to better understand how cluster analyses determine which data points should be merged when. Imagine you have five students and want to group them together based on their overall performance in school. The data that you rely on are their grades in math, music, and biology (with 1 being the best grade and 6 being the worst).\nStudentMathMusicBiologyStudentA232StudentB132StudentC121StudentD244StudentE343\nThe first step in determining the similarity among students is to create a distance matrix.\ndiststudents &lt;- dist(students, method = \"manhattan\") # create a distance matrix\nThe distance matrix below shows that Student A and Student B only differ by one grade. Student B and Student C differ by 2 grades. Student A and Student C differ by 3 grades and so on.\nStudentStudentAStudentBStudentCStudentDStudentB1StudentC32StudentD346StudentE3462\nBased on this distance matrix, we can now implement a cluster analysis in R."
  },
  {
    "objectID": "tutorials/clust/clust.html#cluster-analysis-on-numeric-data",
    "href": "tutorials/clust/clust.html#cluster-analysis-on-numeric-data",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Cluster Analysis on Numeric Data",
    "text": "Cluster Analysis on Numeric Data\nTo create a simple cluster object in R, we use the hclust function from the cluster package. The resulting object is then plotted to create a dendrogram which shows how students have been amalgamated (combined) by the clustering algorithm (which, in the present case, is called ward.D).\n# create hierarchical cluster object with ward.D as linkage method\nclusterstudents &lt;- hclust(diststudents, method = \"ward.D\")\n# plot result as dendrogram\nplot(clusterstudents, hang = 0)\n\nLet us have a look at how the clustering algorithm has amalgamated the students. The amalgamation process takes the distance matrix from above as a starting point and, in a first step, has merged Student A and Student B (because they were the most similar students in the data based on the distance matrix). After collapsing Student A and Student B, the resulting distance matrix looks like the distance matrix below (notice that Student A and Student B now form a cluster that is represented by the means of the grades of the two students).\n\nstudents2 &lt;- matrix(c(1.5, 3, 2, 1, 2, 1, 2, 4, 4, 3, 4, 3),\n  nrow = 4, byrow = T\n)\nstudents2 &lt;- as.data.frame(students2)\nrownames(students2) &lt;- c(\"Cluster1\", \"StudentC\", \"StudentD\", \"StudentE\")\ndiststudents2 &lt;- dist(students2, method = \"manhattan\")\n\nStudentCluster 1Student CStudent DStudent C2.5Student D3.56.0Student E3.56.02.0\nThe next lowest distance now is 2.0 between Student D and Student E which means that these two students are merged next. The resulting distance matrix is shown below.\n\nstudents3 &lt;- matrix(c(1.5, 3, 2, 1, 2, 1, 2.5, 4, 3.5),\n  nrow = 3, byrow = T\n)\nstudents3 &lt;- as.data.frame(students3)\nrownames(students3) &lt;- c(\"Cluster1\", \"StudentC\", \"Cluster2\")\ndiststudents3 &lt;- dist(students3,\n  method = \"manhattan\"\n)\n\n\n\nStudentCluster 1Student CStudent C2.5Cluster 23.56.0\n\n\nNow, the lowest distance value occurs between Cluster 1 and Student C. Thus, Student C and Cluster 1 are merged. In the final step, the Cluster 2 is merged with the new cluster encompassing Student C and Cluster 1. This amalgamation process can then be displayed visually as a dendrogram (see above).\nHow and which elements are merged depends on the what is understood as distance. Since “distance” is such an important concept in cluster analyses, we will briefly discuss this notion to understand why there are so many different types of clustering algorithms and this cluster analyses."
  },
  {
    "objectID": "tutorials/clust/clust.html#distance-and-similarity-measures",
    "href": "tutorials/clust/clust.html#distance-and-similarity-measures",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Distance and Similarity Measures",
    "text": "Distance and Similarity Measures\nTo understand how a cluster analysis determines to which cluster a given data point belongs, we need to understand what different distance measures represent. Have a look at the Figure below which visually represents three different ways to conceptualize distance.\n\n\n\n\n\n\n\n\n\nThe Figure above depicts three ways to measure distance: the euclidean distance represents the distance between points as the hypotenuse of the x- and y-axis distances while the “maximum distance” represents distance as the longer distance of either the distance on the x- or the y-axis. The manhatten distance (or block distance) is the sum of the distances on the x- and the y-axis.\nWe will now turn to another example in order to delve a little deeper into how clustering algorithms work. In this example, we will find cluster of varieties of English based on the relative frequency of selected non-standard features (such as the relative frequencies of cleft constructions and tag questions). As a first step, we generate some fictional data set for this analysis.\n\n# generate data\nIrishEnglish &lt;- round(sqrt((rnorm(10, 9.5, .5))^2), 3)\nScottishEnglish &lt;- round(sqrt((rnorm(10, 9.3, .4))^2), 3)\nBritishEnglish &lt;- round(sqrt((rnorm(10, 6.4, .7))^2), 3)\nAustralianEnglish &lt;- round(sqrt((rnorm(10, 6.6, .5))^2), 3)\nNewZealandEnglish &lt;- round(sqrt((rnorm(10, 6.5, .4))^2), 3)\nAmericanEnglish &lt;- round(sqrt((rnorm(10, 4.6, .8))^2), 3)\nCanadianEnglish &lt;- round(sqrt((rnorm(10, 4.5, .7))^2), 3)\nJamaicanEnglish &lt;- round(sqrt((rnorm(10, 1.4, .2))^2), 3)\nPhillipineEnglish &lt;- round(sqrt((rnorm(10, 1.5, .4))^2), 3)\nIndianEnglish &lt;- round(sqrt((rnorm(10, 1.3, .5))^2), 3)\nclus &lt;- data.frame(\n  IrishEnglish, ScottishEnglish, BritishEnglish,\n  AustralianEnglish, NewZealandEnglish, AmericanEnglish,\n  CanadianEnglish, JamaicanEnglish, PhillipineEnglish, IndianEnglish\n)\n# add row names\nrownames(clus) &lt;- c(\n  \"nae_neg\", \"like\", \"clefts\", \"tags\", \"youse\", \"soitwas\",\n  \"dt\", \"nsr\", \"invartag\", \"wh_cleft\"\n)\n\n\n\nFeatureIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishnae_neg9.3608.9776.0006.1746.6085.1534.2671.5921.7011.287like10.1709.0756.5586.2426.6263.3053.9171.7461.7411.522clefts9.3369.6896.2786.5937.0773.5525.6741.4372.2050.306tags9.4059.0076.0306.0706.4645.0174.8461.6971.4511.046youse9.3069.4356.9537.6516.7023.4804.2991.6481.1180.391soitwas9.7009.3976.6596.3446.3574.0963.9441.2762.0041.553dt9.0389.2156.7596.8866.8735.2473.9601.7332.1841.168nsr8.9119.2856.4156.6476.9045.0564.5371.1960.7681.294invartag8.7339.5806.2887.0036.9574.7334.9491.2491.3351.738wh_cleft9.0239.5655.7686.8086.1954.7305.9731.2791.1661.713\n\n\nAs a next step, we create a cluster object based on the data we have just generated.\n\n# clean data\nclusm &lt;- as.matrix(clus)\nclust &lt;- t(clusm) # transpose data\nclust &lt;- na.omit(clust) # remove missing values\nclusts &lt;- scale(clust) # standardize variables\nclusts &lt;- as.matrix(clusts) # convert into matrix\n\n\n\nVarietynae_neglikecleftstagsyousesoitwasdtnsrinvartagwh_cleftIrishEnglish9.36010.1709.3369.4059.3069.7009.0388.9118.7339.023ScottishEnglish8.9779.0759.6899.0079.4359.3979.2159.2859.5809.565BritishEnglish6.0006.5586.2786.0306.9536.6596.7596.4156.2885.768AustralianEnglish6.1746.2426.5936.0707.6516.3446.8866.6477.0036.808NewZealandEnglish6.6086.6267.0776.4646.7026.3576.8736.9046.9576.195AmericanEnglish5.1533.3053.5525.0173.4804.0965.2475.0564.7334.730CanadianEnglish4.2673.9175.6744.8464.2993.9443.9604.5374.9495.973JamaicanEnglish1.5921.7461.4371.6971.6481.2761.7331.1961.2491.279PhillipineEnglish1.7011.7412.2051.4511.1182.0042.1840.7681.3351.166IndianEnglish1.2871.5220.3061.0460.3911.5531.1681.2941.7381.713\n\n\nWe assess if data is “clusterable” by testing if the data contains non-randomness. To this end, we calculate the Hopkins statistic which indicates how similar the data is to a random distribution.\n\nA Hopkins value of 0.5 indicates that the data is random and that there are no inherent clusters.\nIf the Hopkins statistic is close to 1, then the data is highly clusterable.\nValues of 0 indicate that the data is uniform (Aggarwal, 158).\n\nThe n in the get_clust_tendency functions represents the maximum number of clusters to be tested which should be number of predictors in the data.\n\n# apply get_clust_tendency to cluster object\nclusttendency &lt;- get_clust_tendency(clusts,\n  # define number of points from sample space\n  n = 9,\n  gradient = list(\n    # define color for low values\n    low = \"steelblue\",\n    # define color for high values\n    high = \"white\"\n  )\n)\nclusttendency[1]\n\n$hopkins_stat\n[1] 0.7892281\n\n\nAs the Hopkins value is substantively higher than .5 (randomness) and closer to 1 (highly clusterable) than to .5, thus indicating that there is sufficient structure in the data to warrant a cluster analysis. As such, we can assume that there are actual clusters in the data and continue by generating a distance matrix using euclidean distances.\n\nclustd &lt;- dist(clusts, # create distance matrix\n  method = \"euclidean\"\n) # use euclidean (!) distance\n\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.000.563.072.852.765.064.908.067.978.34ScottishEnglish0.560.003.082.802.745.054.868.087.998.35BritishEnglish3.073.080.000.510.492.152.045.034.955.33AustralianEnglish2.852.800.510.000.452.392.205.325.255.61NewZealandEnglish2.762.740.490.450.002.392.245.355.285.65AmericanEnglish5.065.052.152.392.390.001.013.193.143.43CanadianEnglish4.904.862.042.202.241.010.003.343.293.62JamaicanEnglish8.068.085.035.325.353.193.340.000.440.65PhillipineEnglish7.977.994.955.255.283.143.290.440.000.81IndianEnglish8.348.355.335.615.653.433.620.650.810.00\n\n\nBelow are other methods to create distance matrices with some comments on when using which metric is appropriate.\n# create distance matrix (euclidean method: not good when dealing with many dimensions)\nclustd &lt;- dist(clusts, method = \"euclidean\")\n# create distance matrix (maximum method: here the difference between points dominates)\nclustd_maximum &lt;- round(dist(clusts, method = \"maximum\"), 2)\n# create distance matrix (manhattan method: most popular choice)\nclustd_manhatten &lt;- round(dist(clusts, method = \"manhattan\"), 2)\n# create distance matrix (canberra method: for count data only - focuses on small differences and neglects larger differences)\nclustd_canberra &lt;- round(dist(clusts, method = \"canberra\"), 2)\n# create distance matrix (binary method: for binary data only!)\nclustd_binary &lt;- round(dist(clusts, method = \"binary\"), 2)\n# create distance matrix (minkowski method: is not a true distance measure)\nclustd_minkowski &lt;- round(dist(clusts, method = \"minkowski\"), 2)\n# distance method for words: daisy (other possible distances are \"manhattan\" and \"gower\")\nclustd_daisy &lt;- round(daisy(clusts, metric = \"euclidean\"), 2)\nIf you call the individual distance matrices, you will see that depending on which distance measure is used, the distance matrices differ dramatically! Have a look at the distance matrix created using the manhatten metric and compare it to the distance matrix created using the euclidian metric (see above).\n\nclustd_maximum\n\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.000.351.161.261.142.212.012.752.712.82ScottishEnglish0.350.001.261.001.121.901.792.762.782.90BritishEnglish1.161.260.000.340.251.040.951.761.801.95AustralianEnglish1.261.000.340.000.281.241.001.911.942.15NewZealandEnglish1.141.120.250.280.001.090.991.891.962.10AmericanEnglish2.211.901.041.241.090.000.661.231.371.39CanadianEnglish2.011.790.951.000.990.660.001.551.591.66JamaicanEnglish2.752.761.761.911.891.231.550.000.240.37PhillipineEnglish2.712.781.801.941.961.371.590.240.000.59IndianEnglish2.822.901.952.152.101.391.660.370.590.00\n\n\nNext, we create a distance plot using the distplot function. If the distance plot shows different regions (non-random, non-uniform gray areas) then clustering the data is permittable as the data contains actual structures.\n\n# create distance plot\ndissplot(clustd)\n\n\n\n\n\n\n\n\nThe most common method for clustering is called ward.D or ward.D2. Both of these linkage functions seek to minimize variance. This means that they cluster in a way that the amount of variance is at a minimum (comparable to the regression line in an ordinary least squares (OLS) design).\n# create cluster object\ncd &lt;- hclust(clustd, method = \"ward.D2\")\n# display dendrogram\nplot(cd, hang = -1)\n\nWe will briefly go over some other, alternative linkage methods. Which linkage method is and should be used depends on various factors, for example, the type of variables (nominal versus numeric) or whether the focus should be placed on commonalities or differences.\n# single linkage: cluster with nearest data point\ncd_single &lt;- hclust(clustd, method = \"single\")\n# create cluster object (ward.D linkage)\ncd_wardd &lt;- hclust(clustd, method = \"ward.D\")\n# create cluster object (ward.D2 linkage):\n# cluster in a way to achieve minimum variance\ncd_wardd2 &lt;- hclust(clustd, method = \"ward.D2\")\n# average linkage: cluster with closest mean\ncd_average &lt;- hclust(clustd, method = \"average\")\n# mcquitty linkage\ncd_mcquitty &lt;- hclust(clustd, method = \"mcquitty\")\n# median linkage: cluster with closest median\ncd_median &lt;- hclust(clustd, method = \"median\")\n# centroid linkage: cluster with closest prototypical point of target cluster\ncd_centroid &lt;- hclust(clustd, method = \"centroid\")\n# complete linkage: cluster with nearest/furthest data point of target cluster\ncd_complete &lt;- hclust(clustd, method = \"complete\")\nNow, we determine the optimal number of clusters based on silhouette widths which shows the ratio of internal similarity of clusters against the similarity between clusters. If the silhouette widths have values lower than .2 then this indicates that clustering is not appropriate (Levshina, 311). The function below displays the silhouette width values of 2 to 8 clusters.\n\noptclus &lt;- sapply(2:8, function(x) summary(silhouette(cutree(cd, k = x), clustd))$avg.width)\noptclus # inspect results\n\n[1] 0.6053034 0.6706463 0.7489706 0.6359189 0.4726818 0.3113677 0.0966847\n\noptnclust &lt;- which(optclus == max(optclus)) # determine optimal number of clusters\ngroups &lt;- cutree(cd, k = optnclust) # cut tree into optimal number of clusters\n\nThe optimal number of clusters is the cluster solution with the highest silhouette width. We cut the tree into the optimal number of clusters and plot the result.\ngroups &lt;- cutree(cd, k = optnclust) # cut tree into optimal clusters\nplot(cd, hang = -1, cex = .75) # plot result as dendrogram\nrect.hclust(cd, k = optnclust, border = \"red\") # draw red borders around clusters\n\nIn a next step, we aim to determine which factors are particularly important for the clustering - this step is comparable to measuring the effect size in inferential designs.\n\n# which factors are particularly important\nceltic &lt;- clusts[c(1, 2), ]\nothers &lt;- clusts[-c(1, 2), ]\n# calculate column means\nceltic.cm &lt;- colMeans(celtic)\nothers.cm &lt;- colMeans(others)\n# calculate difference between celtic and other englishes\ndiff &lt;- celtic.cm - others.cm\nsort(diff, decreasing = F)\n\n   youse      nsr invartag       dt   clefts wh_cleft     tags  nae_neg \n1.584710 1.595632 1.615343 1.624963 1.663173 1.684370 1.732434 1.739142 \n soitwas     like \n1.803367 1.819887 \n\n\nplot(sort(diff), # y-values\n  1:length(diff), # x-values\n  type = \"n\", # plot type (empty)\n  cex.axis = .75, # axis font size\n  cex.lab = .75, # label font size\n  xlab = \"Prototypical for Non-Celtic Varieties (Cluster 2) &lt;-----&gt; Prototypical for Celtic Varieties (Cluster 1)\", # x-axis label\n  yaxt = \"n\", # no y-axis tick marks\n  ylab = \"\"\n) # no y-axis label\ntext(sort(diff), 1:length(diff), names(sort(diff)), cex = .75) # plot text into plot\n\n\nOuter &lt;- clusts[c(6:8), ] # data of outer circle varieties\nInner &lt;- clusts[-c(6:8), ] # data of inner circle varieties\nOuter.cm &lt;- colMeans(Outer) # column means for outer circle\nInner.cm &lt;- colMeans(Inner) # column means for inner circle\ndiff &lt;- Outer.cm - Inner.cm # difference between inner and outer circle\nsort(diff, decreasing = F) # order difference between inner and outer circle\n\n      like    soitwas      youse         dt   invartag     clefts    nae_neg \n-0.9640870 -0.9464393 -0.8291854 -0.8067919 -0.7634520 -0.7343231 -0.7061533 \n       nsr       tags   wh_cleft \n-0.6866729 -0.6032222 -0.5805237 \n\n\nplot( # start plot\n  sort(diff), # y-values\n  1:length(diff), # x-values\n  type = \"n\", # plot type (empty)\n  cex.axis = .75, # axis font size\n  cex.lab = .75, # label font size\n  xlab = \"Prototypical for Inner Circle Varieties (Cluster 2) &lt;-----&gt; Prototypical for Outer Circle Varieties (Cluster 1)\", # x-axis label\n  yaxt = \"n\", # no y-axis tick marks\n  ylab = \"\"\n) # no y-axis label\ntext(sort(diff), 1:length(diff), names(sort(diff)), cex = .75) # plot text into plot\n\nWe see that discourse like is typical for other varieties and that the use of youse as 2nd person plural pronoun and invariant tags are typical for Celtic Englishes.\nWe will now test whether the cluster is justified by validating the cluster solution using bootstrapping.\n\nres.pv &lt;- pvclust(clus, # apply pvclust method to clus data\n  method.dist = \"euclidean\", # use eucledian distance\n  method.hclust = \"ward.D2\", # use ward.d2 linkage\n  nboot = 100\n) # use 100 bootstrap runs\n\nBootstrap (r = 0.5)... Done.\nBootstrap (r = 0.6)... Done.\nBootstrap (r = 0.7)... Done.\nBootstrap (r = 0.8)... Done.\nBootstrap (r = 0.9)... Done.\nBootstrap (r = 1.0)... Done.\nBootstrap (r = 1.1)... Done.\nBootstrap (r = 1.2)... Done.\nBootstrap (r = 1.3)... Done.\nBootstrap (r = 1.4)... Done.\n\n\nThe clustering provides approximately unbiased p-values and bootstrap probability value (see Levshina, 316).\n\nplot(res.pv, cex = .75)\npvrect(res.pv)\n\n\n\n\n\n\n\n\nWe can also use other packages to customize the dendrograms.\nplot(as.phylo(cd), # plot cluster object\n  cex = 0.75, # .75 font size\n  label.offset = .5\n) # .5 label offset\n\nOne useful customization is to display an unrooted rather than a rooted tree diagram.\n# plot as unrooted tree\nplot(as.phylo(cd), # plot cluster object\n  type = \"unrooted\", # plot as unrooted tree\n  cex = .75, # .75 font size\n  label.offset = 1\n) # .5 label offset"
  },
  {
    "objectID": "tutorials/clust/clust.html#cluster-analysis-on-nominal-data",
    "href": "tutorials/clust/clust.html#cluster-analysis-on-nominal-data",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Cluster Analysis on Nominal Data",
    "text": "Cluster Analysis on Nominal Data\nSo far, all analyses were based on numeric data. However, especially when working with language data, the data is nominal or categorical rather than numeric. The following will thus show to implement a clustering method for nominal data.\nIn a first step, we will create a simple data set representing the presence and absence of features across varieties of English.\n\n# generate data\nIrishEnglish &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\nScottishEnglish &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\nBritishEnglish &lt;- c(0, 1, 1, 1, 0, 0, 1, 0, 1, 1)\nAustralianEnglish &lt;- c(0, 1, 1, 1, 0, 0, 1, 0, 1, 1)\nNewZealandEnglish &lt;- c(0, 1, 1, 1, 0, 0, 1, 0, 1, 1)\nAmericanEnglish &lt;- c(0, 1, 1, 1, 0, 0, 0, 0, 1, 0)\nCanadianEnglish &lt;- c(0, 1, 1, 1, 0, 0, 0, 0, 1, 0)\nJamaicanEnglish &lt;- c(0, 0, 1, 0, 0, 0, 0, 0, 1, 0)\nPhillipineEnglish &lt;- c(0, 0, 1, 0, 0, 0, 0, 0, 1, 0)\nIndianEnglish &lt;- c(0, 0, 1, 0, 0, 0, 0, 0, 1, 0)\nclus &lt;- data.frame(\n  IrishEnglish, ScottishEnglish, BritishEnglish,\n  AustralianEnglish, NewZealandEnglish, AmericanEnglish,\n  CanadianEnglish, JamaicanEnglish, PhillipineEnglish, IndianEnglish\n)\n# add row names\nrownames(clus) &lt;- c(\n  \"nae_neg\", \"like\", \"clefts\", \"tags\", \"youse\", \"soitwas\",\n  \"dt\", \"nsr\", \"invartag\", \"wh_cleft\"\n)\n# convert into factors\nclus &lt;- apply(clus, 1, function(x) {\n  x &lt;- as.factor(x)\n})\n\n\n\nVarietynae_neglikecleftstagsyousesoitwasdtnsrinvartagwh_cleftIrishEnglish1111111111ScottishEnglish1111111111BritishEnglish0111001011AustralianEnglish0111001011NewZealandEnglish0111001011AmericanEnglish0111000010CanadianEnglish0111000010JamaicanEnglish0010000010PhillipineEnglish0010000010IndianEnglish0010000010\n\n\nNow that we have our data, we will create a distance matrix but in contrast to previous methods, we will use a different distance measure that takes into account that we are dealing with nominal (or binary) data.\n\n# clean data\nclusts &lt;- as.matrix(clus)\n# create distance matrix\nclustd &lt;- dist(clusts, method = \"binary\") # create a distance object with binary (!) distance\n\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.00.00.400.400.400.600.600.800.800.80ScottishEnglish0.00.00.400.400.400.600.600.800.800.80BritishEnglish0.40.40.000.000.000.330.330.670.670.67AustralianEnglish0.40.40.000.000.000.330.330.670.670.67NewZealandEnglish0.40.40.000.000.000.330.330.670.670.67AmericanEnglish0.60.60.330.330.330.000.000.500.500.50CanadianEnglish0.60.60.330.330.330.000.000.500.500.50JamaicanEnglish0.80.80.670.670.670.500.500.000.000.00PhillipineEnglish0.80.80.670.670.670.500.500.000.000.00IndianEnglish0.80.80.670.670.670.500.500.000.000.00\n\n\nAs before, we can now use hierarchical clustering to display the results as a dendrogram\n\n# create cluster object (ward.D2 linkage)   : cluster in a way to achieve minimum variance\ncd &lt;- hclust(clustd, method = \"ward.D2\")\n# plot result as dendrogram\nplot(cd, hang = -1) # display dendogram\n\n\n\n\n\n\n\n\nIn a next step, we want to determine which features are particularly distinctive for one cluster (the “Celtic” cluster containing Irish and Scottish English).\n\n# create factor with celtic varieties on one hand and other varieties on other\ncluster &lt;- as.factor(ifelse(as.character(rownames(clusts)) == \"IrishEnglish\", \"1\",\n  ifelse(as.character(rownames(clusts)) == \"ScottishEnglish\", \"1\", \"0\")\n))\n# convert into data frame\nclsts.df &lt;- as.data.frame(clusts)\n# determine significance\nlibrary(exact2x2)\npfish &lt;- fisher.exact(table(cluster, clsts.df$youse))\npfish[[1]]\n\n[1] 0.02222222\n\n# determine effect size\nassocstats(table(cluster, clsts.df$youse))\n\n                    X^2 df  P(&gt; X^2)\nLikelihood Ratio 10.008  1 0.0015586\nPearson          10.000  1 0.0015654\n\nPhi-Coefficient   : 1 \nContingency Coeff.: 0.707 \nCramer's V        : 1 \n\nassocstats(table(cluster, clsts.df$like))\n\n                    X^2 df P(&gt; X^2)\nLikelihood Ratio 1.6323  1  0.20139\nPearson          1.0714  1  0.30062\n\nPhi-Coefficient   : 0.327 \nContingency Coeff.: 0.311 \nCramer's V        : 0.327 \n\n\nClustering is a highly complex topic and there many more complexities to it. However, this should have helped to get you started."
  },
  {
    "objectID": "tutorials/clust/clust.html#footnotes",
    "href": "tutorials/clust/clust.html#footnotes",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#types-of-hypotheses",
    "href": "tutorials/basicquant/basicquant.html#types-of-hypotheses",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Types of Hypotheses",
    "text": "Types of Hypotheses\nOn a very fundamental level, we can differentiate between null-hypotheses (H\\(_{0}\\)), that claim non-existence of either a state of being or a difference, and alternative or test-hypothesis (H\\(_{1}\\)) that claim or postulate the existence of of either a state of being or a difference. Among test-hypotheses, we can furthermore distinguish between non-directed hypotheses which claim that one sample is different from another sample, and directed hypotheses which claim that a feature of one sample is bigger, smaller, more frequent, or less frequent, etc. Thus, a hypothesis that group A will perform better in an exam is a directed test-hypothesis while an non-directed hypothesis would merely claim that they differ in their test results. In contrast, the null-hypothesis would claim that there is no difference between the groups in terms of their performance in that exam.\nAn additional distinction among hypotheses is the difference between deterministic and probabilistic hypotheses. While we are dealing with a deterministic hypotheses in (10) because it is a categorical claim, we are dealing with a probabilistic hypothesis in (11) because, here, the hypothesis simply claims that the likelihood of Y is higher if X is the case (but not necessarily categorically).\n\nIf the length of two words in an English phrase is different, then the shorter word will always proceed the longer word.\nIf the length of two words in an English phrase is different, then it is more likely for the shorter word to proceed the longer word than vice versa."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#why-test-the-h0",
    "href": "tutorials/basicquant/basicquant.html#why-test-the-h0",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Why test the H0?!",
    "text": "Why test the H0?!\nAlthough it is counter-intuitive, we do not actually test the test-hypothesis but we test the null-hypothesis. We will now have a closer look at how to formulate hypotheses and that formulating hypotheses is formulating expected outcomes/explanations in a formal description.\n\nNull hypothesis (H0) Groups A and B do not differ systematically! (\\(\\mu\\)A = \\(\\mu\\)B)\nTest hypothesis (H\\(_{1a}\\)) Groups A and B differ systematically! (\\(\\mu\\)A \\(\\neq\\) \\(\\mu\\)B; non-directed)\nTest hypothesis (H\\(_{1b}\\)) Group A has significantly better results/higher levels of x compared with group B. (\\(\\mu\\)A \\(&gt;\\) \\(\\mu\\)B; directed)\n\nWhat does that mean and what are we testing? In non-technical terms, we test how likely it is that the results came about by accident. If the probability is high (p &gt; .05) that the results happen to be random, then we do not discard the H0. If the likelihood is low (p &lt; .05) that the results came about randomly, then we discard the H0 and assume the H1 instead! To better understand this logic, we will discuss probabilities and their role in quantitative research.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nWhat four characteristics do hypotheses have?\n\n\n\nAnswer\n\nThey are are falsifiable (either true or false), they are testable, they are unambiguous, and they are inherently consistent (logically coherent).\n\n\nCome up with (a) three directed hypotheses and (b) three non-directed hypotheses.\n\n\n\nAnswer (examples)\n\nDirectedIn a 100 meter dash, Martin runs faster than Jack.In informal conversations, Martin uses fewer swearwords than Michael. Per minute, Martin can produce more words than Erich.Non-directedMartin and Michael differ in their use of swear words.Martin and Erich differ in their speech speed.Stefan and Martin differ in their need for sleep.\n\n\nOftentimes, it is not that easy to differentiate between hypotheses and other types of statements. Find a partner and come with statements that are not hypotheses and discuss why these statements are not hypotheses.\nFind a partner and come up with statements that can be classified as both hypotheses and non-hypotheses and be prepared to explain your reasoning to the group."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#research-designs",
    "href": "tutorials/basicquant/basicquant.html#research-designs",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Research Designs",
    "text": "Research Designs\nNow that the terms sample and population are clear, we can continue to think about how we could test hypotheses. In the empirical sciences, we try to answer questions we have by analyzing data. However, before we can attempt to answer a question and test if we are right with the answer we have in mind (our hypothesis), we have to decide on how to tackle the question. Thus, we have to come up with a plan about how to test our hypothesis. This plan is called a research design. The research design informs how to collect, process, visualize, and analyze the data. As such, the research design focuses on all aspects of research that are related to the data.\nThus, after comping up with a research question and a hypothesis, the next step typically consists in collecting the right type of data. There are different ways to collecting data and these different ways to collect data represent different research designs. We cannot go over all the different designs (and they can be combined, for instance, introspection is often a part of questionnaire designs), but we will focus and briefly discuss the most common ones.\nIn general, we can distinguish between experimental and observational designs. In contrast to observational designs (which can only confirm correlations, i.e. that X and Y occur together), experimental designs are more powerful in that they can determine causality (i.e., if X causes Y). We the most common research designs in the language sciences are\n\nexperimental\n\nexperimental designs (e.g., reaction times, eye-tracking)\n\nobservational\n\nbehavioral designs (e.g., corpora)\narchival/reviews (e.g., grammars, systematic literature reviews)\nintrospection (e.g., grammaticality judgements)\nquestionnaire designs (e.g., elicitation via surveys and questionnaires)\n\n\nThere are more designs(e.g. meta-studies) and also other classifications, but in the language sciences, I think that the ones listed above are the most frequent ones. Each of these designs has advantages and disadvantages which are briefly summarized in the table below.\n\ninstall.packages(\"e1071\")\ninstall.packages(\"flextable\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"ggpubr\")\ninstall.packages(\"graphics\")\ninstall.packages(\"grDevices\")\ninstall.packages(\"knitr\")\ninstall.packages(\"tidyverse\")\n\n\n\nTypeResearch designExampleDirectness of access to phenomenonCost/Labor intensityExternal validity (generalizability)Internal validityEase of data collection (e.g. access to participants)Experimental researchExperimentalReaction time measurementshigh/variablehigh/variablelow/variablehighlowObservational researchBehavioralCorpus studyhighlow/variablehighhighmediumArchival/ReviewSystematic literature reviewlowlowlowmediumhighIntrospectionGrammaticality judgementshighlowlow/variablehighlowQuestionnaireLanguage use surveymediumlowmediummediumhigh\n\n\nWe will now briefly focus on each design and discuss it.\n\nExperimental research\nExperimental designs can determine if there is a causal link between predictors and response which sets experimental designs apart from observational designs. Experiments typically take place in very well-controlled, but unnatural environments, .e.g, in laboratories.\nIn experimental designs, subjects are typically randomly assigned into test groups and control groups and the experimenter. If the subjects are not assigned randomly to test and control groups, the design is referred to as quasi-experimental (while experimental designs with randomized allocation of subjects are referred to as true experiments). The aim of experiments, in the most basic sense, is to determine if a result differs based on a change in some condition. The experimenter would change the condition only in the test group while not changing it in the control group while everything else remains constant. That way, it is possible to test if the change in the condition is responsible for the change in the outcome.\nDuring an experiment, subjects are shown stimuli (e.g., sentences, words, pictures) and the subjects (should) react to those stimuli showing a response (pressing buttons, providing judgments). The response does not have to be voluntary, meaning that responses can also simply be neuronal activity that the subject has no control over. In repeated measure designs, the same subjects are tested repeatedly while different subjects are tested in between-subjects designs.\nIn addition to the ability to establish causality, experimental designs allow a very direct access to a phenomenon and have high internal validity which means that experiments can test the phenomena in a very well-circumscribed and precise manner. Disadvantages of experimental designs are that they are typically rather costly and labor intensive and that the results are not necessarily generalizable due to the un-natural setting of most experiments (low external validity).\n\n\nQuasi-experimental research\nIn contract to experimental designs, subjects (or participants) in quasi-experimental designs are not randomly assigned to groups. Because of this, quasi-experimental designs can be affected by confounding (i.e., that changes in the dependent variable are caused by a variable that is not controlled for). As such, quasi-experimental designs are not experimental designs but can be considered observational. Examples for quasi-experimental designs are corpus-based variationist studies of the occurrence of linguistics variants in a variable context.\nQuasi-experimental designs encompass studies where subjects cannot be randomly assigned into test groups because either, this is not practically possible because the observation has already taken place or because the characteristic is inherent in the participants themselves. For instance, analyses of historical data can at best be quasi-experimental because the data have already been collected and the experimenter cannot assign participants to test groups. Also, some studies that analyze sex differences via the behavior of men and women cannot be experimental because participants cannot be randomly assigned to the male or female group.\n\n\nObservational designs\nIn the language sciences, observational designs encompass all forms of data acquisition and collection where the subjects are observed in their “natural habitat”. Hence, observational or behavioral designs are the most natural and thus have high external validity which means that we can assume that the results are transferable to the real world. The most common type of this design are corpus studies in which texts (which encompasses both written and transcribed spoken language) are searched for the occurrence of certain linguistic patterns and then correlated with some other feature, the use of a certain linguistic construction such as a specific suffix or a fixed expression like in order that in two different time periods or dialects.\nThe cost and labor intensity of corpus studies depends heavily upon if the corpus that you want to use has already been compiled - if the corpus is already available, the the costs and labor intensity are minimal but they can be high if the corpus has to be compiled first. However, even compiling a corpus can be comparatively easy and cheap, for example, if you compile a corpus consisting of a collection of texts that are available online but the costs can be substantive if you, for example, first need to record and transcribe spoken language in the field).\nThe problem with observational designs are that the contexts in which the data were collected were not controlled with means that the results are more likely to be affected by contextual or confounding factors that the researchers have no control over or don’t even know about.\n\n\nArchival research and reviews\nArchival research is more common in historical linguistics where researchers look for primary texts or records in archives. However, archival research also encompasses digital archives such as libraries which means that systematic literature reviews also fall under this research design.\nThe advantages of archival research designs are that the data collections is typically comparatively easy and associated with minimal costs. Also, depending on the phenomenon, the access to the phenomenon can be quite indirect, for example, entirely filtered through the eyes of eye witnesses or experts.\n\n\nIntrospection\nIntrospection has been the dominant form of “data collection” in the 20^th century and it is strongly connected to what is called armchair linguistics (linguists coming to insights about language by musing about what they can or would say and what they could or would not say). Introspection is, of course, by far the easiest and cheapest way to “collect data”. Unfortunately, introspection is subjective and can be heavily biased by the expectations and theoretical framework of subjects. Nonetheless, introspection can be and is extremely useful in particular regarding hypothesis generation and finding potential predictors for a certain linguistic behavior that can then be tested empirically.\nSince the 1990s and the rise of empirical linguistics, which is characterized by the employment of both experimental designs as well as corpus linguistics and increased use of questionnaire designs, introspection has been relegated to playing a part of other designs while only rarely being used as the sole source of data collection.\n\n\nQuestionnaire designs\nQuestionnaire designs represent any method of data collection where respondents are asked to provide (conscious) information to answers or ratings/evaluations to prompts (such as statements). The process to ask for information from respondents is called elicitation.\nIn addition to surveys which ask for socio-demographic information about speakers or about language use and language background information, questionnaires frequently ask respondents to rate their agreement with items, or the likelihood, frequency, or acceptability of items on Likert scales. A Likert scale, pronounced /lick.ert/ and named after the psychologist Rensis Likert, and often ranges from strongly disagree to strongly agree (or analogously from never to very frequently or extremely unlikely to extremely likely). Typically, respondents are given 5 or 7 options with the endpoints noted above representing the endpoints and respondents are asked to select the options that best matches their evaluation.\nA type of elicitation that is special in the language science is called discourse completion task or DCT. In DCTs, respondents are asked to imagine a certain discourse context or situation and are then asked how they would answer, respond, or express something. While DCTs have been shown to provide data that mirrors real-world behaviors, it is less reliable compared to corpus data as respondents cannot necessarily provide accurate information about their linguistic behavior.\nTo counter fatigue effects (respondents get tired or annoyed and do no longer answer to the best of their knowledge/ability), questionnaire items are often randomized so that their order differs between respondents. To counter what is called agreeability effects, researchers commonly have respondents rate test items (items that contain the phenomenon researchers are interested in) and fillers (items that do not contain what researchers are interested in). Fillers are included to disguise what the questionnaire aim to test so that respondents cannot (unconsciously) try to provide agreeable answers (the answers that researchers hope to get)."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#variable-types-scaling",
    "href": "tutorials/basicquant/basicquant.html#variable-types-scaling",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Variable Types (Scaling)",
    "text": "Variable Types (Scaling)\nIn the following, variable types (also referred to as scaling level or scaling) are discussed. It is very important to know which type of variables one is dealing with because the type of variable has affects many of the methods discussed, both in descriptive and in inferential statistics.\n\nNominal and Categorical Variables\nNominal and categorical variables only list the membership of a particular class. For nominal variables, there are exactly two occurrences (yes/no or on/off), while for categorical variables there can be several groups, e.g. the state in which someone was born.\n\n\nOrdinal Variables\nWith ordinal variables it is possible to rank the values, but the distances between the ranks can not be exactly quantified. An example of an ordinal variable is the ranking in a 100-meter dash. The 2nd in a 100-meter dash did not go twice as fast as the 4th. It is often the case that ordinal variables consist of integer, positive numbers (1, 2, 3, 4, etc.).\n\n\n(True) Numeric Variables\nThere are two basic types of numeric variables: interval-scaled variables and ratio-scaled variables. For interval scaled variables, the differences between levels are significant, but not the relationship between levels. For instance, 20 degree Celsius is not twice as hot as 10 degree Celsius.\nWith respect to ratio-scaled variables, both the differences and the relationship between the levels are significant. An example of this is the times in a 100-meter dash. For ratio-scaled variables, 10 is exactly twice as high as 5 and half as much as 20.\nIt is very important to keep in mind that both interval-scaled variables and ratio-scaled variables are numeric variables. This will play a role later because many tests can either only or never be applied to numeric variables.\n\n\nVariable TypeVariable LevelNominal Scaled / Categorical VariablesGender, Nationality, Part of SpeechOrdinal Scaled VariablesGraduation, School Grades, Military RankInterval Scaled VariablesTemperature, Acceptability JudgmentsRatio-scaled VariablesAge, Duration, Number of Syllables\n\n\nIt is enormously important to know variable types and levels, as the type of variable requires which tests are possible and which are not. For example, a \\(\\chi\\)2-test can only be applied to nominal or categorical variables, and a t-test to numeric variables only.\nIt is often necessary to translate variables into another type of variable. It should be noted that variables can only be transferred to variable types with less information content. The least informative variables are nominal, while the most informative variables are ratio scaled. The variable types thus form an implicit hierarchy:\nnominal/categorical &lt; ordinal &lt; interval/ratio\nHere is an example to illustrate this: let’s say you are investigating gender differences in the use of swear words in spoken Irish English and you find that you cannot use a linear regression or an ANOVA because too many speakers use no swear words (which violates the requirement that the errors must be normally distributed). In such a case, what one can do is to rank the speakers by their frequency of swear or curse words. Rank 1 would represent the speaker with the highest frequency of swear words, rank 2 would represent the speaker with the second highest frequency of swear words and so on. After you do this, you can, for example, use a Mann-Whitney U test to determine the relationship between the gender of speakers and their swear word ranking. You could also split the speakers into two groups (swear word users and non-swear-word users) and then perform a \\(\\chi\\)2-test of the frequencies of men and women in these groups. The important thing is that you cannot transform a categorical variable into a numeric variable as this would mean that you transform a less information-rich variable into a more information-rich variable.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nFor each of the variables listed below, consider how you could operationalize them and what kind of variables it would be.a. weather (cloudy, sunny, rainy, etc.)b. citizenshipc. Tense of matrix verbsd. Structural complexitye. word lengthf. Number of syllables in a wordg. The length of pauses in a sample of conversationsh. The appearance or non-appearance of finite verbs in a particular texti. Estimation of words on an acceptability scale from 1 to 5\n\n\n\nAnswer (examples)\n\nThere are other solutions so the following are just possible options!a. weather could, e.g., be operationalized as the number of sunny days or the amount of rain per year. Optimally, we would want weather to be operationalized as a numeric variable.b).citizenship would typically be operationalized as a categorical variable (e.g. German or Norwegian or Australian or other).c. This answer depends on the grammatical structure of the language in question. In English, we would probably operationalize the tense of matrix verbs as a nominal variable with the levels present and non-present.d. Structural complexity can be operationalized in many different ways, e.g., as the number of syntactic nodes, sentence length, number of phrases, average phrase length, etc. Optimally, we would want structural complexity to be operationalized as a numeric variable.e. Word length could,e.g. be operationalized as number of letters, number of phonemes, or the time it takes to produce that word. Thus, word length would typically be operationalized as a numerically scaled variable or an integer (which, in R, would be a type of numeric variable).f. The number of syllables per word would be operationalized as an integer (which, in R, would be a type of numeric variable).g. The length of pauses in a sample of conversations could be operationalized as the time of the pause (numeric) or even just as an ordered factor (short, middle, long). Optimally, however, we would want the length of pauses to be operationalized as a numeric variable.h. The appearance or non-appearance of finite verbs in a particular text would be a nominal variable (present versus absent).i. (The estimation of words on an acceptability scale from 1 to 5 represents judgements on a Likert scale which means that the resulting variable would represent an order factor (ordinal variable).\n\n\nAs you have seen from the above exercise, concepts can be operationalized differently. Find a partner and imagine that you are tasked with performing a study in which age of subjects is an important variable. Discuss with a partner how you would age. What advantages and disadvantages do the different operationalizations have?\nExample: When it rains, more people get wet than when it’s not raining. (If X, then Y)What is the dependent variable here and what is the independent variable?\n\n\n\nAnswer\n\nDependent variable: wet people Independent variable: rain\n\n\nWhich variable scales are time, rank, and name in the table below?\n\n\n\nNameRankTimeCarl Lewis19.86Ben Johnson29.97Steve Davis310.06\n\n\n\n\nAnswer\n\nName is categorical, Rank is ordinal, and Time is numeric.\n\n\nDiscuss with a partner: what obstacles might exist, so that a well operationalized variable has low extrinsic validity?\nConsider the following scenario: in a large representative study, shoe size is found to be an excellent predictor of intelligence. Given this result, find a partner and discuss if intrinsic validity is necessary?"
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#significance-levels",
    "href": "tutorials/basicquant/basicquant.html#significance-levels",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Significance Levels",
    "text": "Significance Levels\nBefore conducting a study, it is advisable to determine the so-called significance level or \\(\\alpha\\) level. This \\(\\alpha\\) level or level of significance indicates how high the p-value can be without having to assume that there is a significant relationship between the variables. It is customary to differentiate between three levels of significance (also called \\(\\alpha\\) levels):\n\np &lt; .001: highly significant - indicated by three stars (***)\np &lt; .01: very significant - indicated by two stars (**)\np &lt; .05: significant - indicated by one star (*)\n\nVariables with a p value that is smaller than .1 but larger than .05 are sometimes referred to as being marginally significant (+).\nVariables that are not significant are commonly referred to or labeled as n.s. (not significant). As we stated above, before we perform a test, we determine a value above which we reject the null hypothesis, the so-called significance level. It’s usually 5%. If the error probability is less than 5% (p &lt;. 05), we reject the null hypothesis. Conclusion: The relationship between the variables is statistically significant. It is important to note here that the H1 (or Test Hypothesis) is correct only because the null hypothesis can be rejected! Statistics can NEVER prove hypotheses but only reject Null Hypotheses which leads us to accept the H1 as preliminary accepted or not-yet-rejected. So all knowledge is preliminary in the empirical sciences."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#probability",
    "href": "tutorials/basicquant/basicquant.html#probability",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Probability",
    "text": "Probability\nIn the following, we will turn to probability and try to understand why probability is relevant for testing hypotheses. This is important at this point because statistics, and thus hypothesis testing, fundamentally builds upon probabilities and probability distributions. In order to understand how probability works, we will investigate what happens when we flip a coin. The first question that we will be addressing is “What is the probability of getting three Heads when flipping a coin three times?”.\nThe probability of getting three heads when flipping a coin three times is .5 to the power of 3: .53 = .5 times .5 times .5 = .125. The probability of getting Heads twice when flipping the coin three times is .375. How do we know?\nThe probability of getting 3 heads in tree tosses is 12.5 percent:\n.53 = .5 * .5 * .5 = .125\nThe probability of getting 2 heads in tree tosses is 37.5 percent:\n.125 + .125 + .125 = 0.375\nBut how do we know this? Well, have look at the table below.\n\n\n\nToss 1Toss 2Toss 3HeadsTailsProbabiltyHeadHeadHead300.125HeadHeadTails210.125HeadTailsHead210.125TailsHeadHead210.125HeadTailsTails120.125TailsHeadTails120.125TailsTailsHead120.125TailsTailsTails030.125\n\n\n\nGiven this table, we are in fact, in a position to calculate the probability of getting 100 heads in 100 coin tosses because we can simply fill in the numbers in the formulas used above: .5100 = 7.888609 * 10-31\nOkay, let us make a bet..\n\nIf head shows, I win a dollar.\nIf tails shows, you win a dollar.\n\nBut given that you know I am cheeky bastard, you do not trust me and claim that I will cheat. But how will you know that I cheat? At which point can you claim that the result is so unlikely that you are (scientifically backed) allowed to claim that I cheat and have manipulated the coin?\n\n\n\n\n\n\n\n\n\nSo before we actually start with the coin tossing, you operationalize your hypothesis:\n\nH0: The author (Martin) is not cheating (heads shows just as often as tails).\nH1: The author (Martin) is cheating (heads shows so often that the probability of the author not cheating is lower than 5 percent)\n\nWe now toss the coin and head shows twice. The question now is whether head showing twice is lower than 5 percent.\nWe toss the coin 3 times. Head shows twice. How likely is it that I do not cheat and head falls more than twice anyway? (In other words, what is the probability p that I win twice or more and not cheat?) If you set the significance level at .05, could you then accuse me of being a cheater?\nAs you can see in the fourth column, there are three options that lead to heads showing twice (rows 2, 3, and 4). If we add these up (0.125 + 0.125 + 0.125 = 0.375). Also, we need to add the case where head shows 3 times which is another .125 (0.375 + 0.125 = .5), then we find out that the probability of heads showing at least twice in three coin tosses is 50 percent and thus 10 times more than the 5-percent threshold that we set initially. Therefore, you cannot claim that I cheated.\n\n\n0 Heads1 Head2 Heads3 Heads0.1250.3750.3750.125\n\n\n\n\n\n\n\n\n\n\n\nWe can do the same and check the probabilities of having heads in 10 coin tosses - if we plot the resulting probabilities, we get the bar plot shown below.\n\n\n\n\n\n\n\n\n\nThe distribution looks more bell-shaped with very low probabilities of getting 0 or 1 as well as 9 or 10 times heads in the 10 coin tosses and a maximum at 5 times heads in 10 coin tosses. In fact, the probability of having 0 and 10 times head is .001 (or 1 in a 1000 attempts - one attempt is tossing a coin 10 times) - the probability of having heads once and 9 times is.01 or 1 in 100 attempts. In contrast, the probability of having 5 times head is .246 or about 25% (1 in 4 attempts will have 5 heads). Also, we can sum up probabilities: the probability of getting 8 or more heads in 10 coin tosses is the probability of getting 8, 9, and 10 times heads (.044 + .010 + 0.001 = .055 = 5.5 percent).\nCalculating the probabilities for three or even 10 coin tosses is still manageable manually but is there an easier way to calculate probabilities? A handier way is have a computer calculate probabilities and the code below shows how to do that in R - a very powerful and flexible programming environment that has been designed for quantitative analysis (but R can, in fact, do much more - this website, for instance, is programmed in R).\nThe code chunk below calculates the probabilities of having 0, 1, 2, and 3 times head in 3 tosses.\n\n# probabilities of  0, 1, 2 and 3 times head in 3 coin tosses\ndbinom(0:3, 3, 0.5)\n\n[1] 0.125 0.375 0.375 0.125\n\n\nThe code chunk below calculates the probabilities of having 2 or 3 times head in 3 tosses.\n\n# probabilities of  2 or 3 times head in 3 coin tosses\nsum(dbinom(2:3, 3, 0.5))\n\n[1] 0.5\n\n\nThe code chunk below calculates the probabilities of having 100 head in 100 tosses.\n\n# probability of  100 times head in 100 coin tosses\ndbinom(100, 100, 0.5)\n\n[1] 7.888609e-31\n\n\nThe code chunk below calculates the probabilities of having 58 or more times head in 100 tosses.\n\n# probability of  58 to a 100 times head in 100 coin tosses\nsum(dbinom(58:100, 100, 0.5))\n\n[1] 0.06660531\n\n\nThe code chunk below calculates the probabilities of having 59 or more times head in 100 tosses.\n\n# probability of  59 to a 100 times head in 100 coin tosses\nsum(dbinom(59:100, 100, 0.5))\n\n[1] 0.04431304\n\n\nThe code chunk below calculates the number of heads in 100 tosses where the probability of getting that number of heads or more sums up to 5 percent (0.05).\n\n# at which point does the probability of getting head\n# dip below 5 percent in 100 coin tosses?\nqbinom(0.05, 100, 0.5, lower.tail = FALSE)\n\n[1] 58\n\n\nLet’s go back to our example scenario. In our example scenario, we are dealing with a directed hypothesis and not with an un-directed/non-directed hypothesis because we claimed in our H1 that I was cheating and would get more heads than would be expected by chance (\\(\\mu_{Martin}\\) \\(&gt;\\) \\(\\mu_{NormalCoin}\\)). For this reason, the test we use is one-tailed. When dealing with un-directed hypotheses, you simply claim that the outcome is either higher or lower - in other words the test is two-tailed as you do not know in which direction the effect will manifest itself.\nTo understand this a more thoroughly, we will consider tossing a coin not merely 3 but 100 times. The Figure below shows the probabilities for the number of heads showing when we toss a coin 100 from 0 occurrences to 100 occurrences.\n\n\n\n\n\n\n\n\n\nThe next figure shows at which number of heads the cumulative probabilities dip below 5 percent for two-tailed hypotheses. According to the graph, if head shows up to 40 or more often than 60 times, the cumulative probability dips below 5 percent. Applied to our initial bet, you could thus claim that I cheated if head shows less than 41 times or more than 60 times (if out hypothesis were two-tailed - which it is not).\n\n\n\n\n\n\n\n\n\nThe Figure below shows at which point the probability of heads showing dips below 5 percent for one-tailed hypotheses. Thus, according to the figure below, if we toss a coin 100 times and head shows 59 or more often, then you are justified in claiming that I cheated.\n\n\n\n\n\n\n\n\n\nWhen comparing the two figures above, it is notable that the number at which you can claim I cheated differs according to whether the H1 as one- or two-tailed. When formulating a one-tailed hypothesis, then the number is lower compared with the the number at which you can reject the H0 if your H1 is two-tailed. This is actually the reason for why it is preferable to formulate more precise, one-tailed hypotheses (because then, it is easier for the data to be sufficient to reject the H0)."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#the-normal-distribution",
    "href": "tutorials/basicquant/basicquant.html#the-normal-distribution",
    "title": "Basic Concepts in Quantitative Research",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nIt is important to note here that the above described calculation of probabilities does not work for numeric variables that are interval-scaled. The reason for this is that it is not possible to calculate the probabilities for all possible outcomes of a reaction time experiment (where time is a continuous variable). In such cases, we rely on distributions in order to determine how likely or probable a certain outcome is.\nWhen relying on distributions, we determine whether a certain value falls within or outside of the area of a distribution that accounts for 5 percent of the entire area of the distribution - if it falls within the area that accounts for less than 5 percent of the total area, then the result is called statistically significant (see the normal distribution below) because the outcome is very unlikely.\nThe normal distribution (or Gaussian curve or Gaussian distribution) shown in the figure above has certain characteristics that can be derived mathematically. Some of these characteristics relate to the area of certain sections of that distribution. More generally, the normal distribution is a symmetric, bell-shaped probability distribution, used as the theoretical model for the distribution of physical and psychological variables. In fact, many variables in the real world are normally distributed: shoe sizes, IQs, sleep lengths, and many , many more.\nThe normal distribution is very important because it underlies many statistical procedures in one form or another. The normal distribution is a symmetrical, continuous distribution where\n\nthe arithmetic mean, the median, and the mode are identical and have a value of 0 (and are thus identical);\n\n50% of the area under the bell-shaped curve are smaller than the mean;\n\n50% of the area under the bell-shaped curve are bigger than the mean;\n\n50% of the area under the bell-shaped curve are within -0.675 and +0.675 standard deviations from the mean (0);\n\n95% of the area under the bell-shaped curve are within -1.96 and +1.96 standard deviations from the mean (0);\n\n99% of the area under the bell-shaped curve are within -2.576 and +2.576 standard deviations from the mean (0).\n\nThere is also a very interesting aspect to the normal distribution that relates to the means (averages) of samples drawn from any type of distribution: no matter what type of distribution we draw samples from, the distribution of the means will approximate a normal distribution. In other words, if we draw samples form a very weird looking distribution, the means of these samples will approximate a normal distribution. This fact is called the Central Limit Theorem. What makes the central limit theorem so remarkable is that this result holds no matter what shape the original population distribution may have been.\n\n\n\n\n\n\n\n\n\nAs shown above, 50 percent of the total area under the curve are to left and 50 percent of the right of the mean value. Furthermore, 68 percent of the area are within -1 and +1 standard deviations from the mean; 95 percent of the area lie between -2 and +2 standard deviations from the mean; 99.7 percent of the area lie between -3 and +3 standard deviations from the mean. Also, 5 percent of the area lie outside -1.96 and +1.96 standard deviations from the mean (if these areas are combined) (see the Figure below). This is important. because we can reject null hypotheses if the probability of them being true is lower than 5 percent. This means that we can use the normal distribution to reject null hypotheses if our dependent variable is normally distributed.\n\n\n\n\n\n\n\n\n\nFinally, 5 percent of the area lies beyond +1.68 standard deviations from the mean (see the Figure below).\n\n\n\n\n\n\n\n\n\nThese properties are extremely useful when determining the likelihood of values or outcomes that reflect certain interval-scaled variables.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCreate a table with the possible outcomes and probabilities of 4 coin tosses (you can consider the table showing the outcomes of three coin tosses above as a guideline).\n\n\n\nAnswer\n\n    {r message=FALSE, warning=FALSE}\n    sum(dbinom(4, 7, 0.5))\n\n\nHow likely is it for heads to show exactly 3 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n    {r message=FALSE, warning=FALSE}\n    sum(dbinom(3, 7, 0.5))\n\n\nHow likely is it for heads to show exactly 2 or 5 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n  {r message=FALSE, warning=FALSE}\n  sum(dbinom(c(2, 5), 7, 0.5))\n\n\nHow likely is it for heads to show 5 or more times when tossing a coin 7 times?\n\n\n\nAnswer\n\n    {r message=FALSE, warning=FALSE}\n    sum(dbinom(5:7, 7, 0.5))\n\n\nHow likely is it for heads to show between 3 and 6 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n    {r message=FALSE, warning=FALSE}\n    sum(dbinom(3:6, 7, 0.5))"
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#non-normality-skewness-and-kurtosis",
    "href": "tutorials/basicquant/basicquant.html#non-normality-skewness-and-kurtosis",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Non-normality: skewness and kurtosis",
    "text": "Non-normality: skewness and kurtosis\nDepending on the phenomenon you are investigating, the distribution of that phenomenon can be distributed non-normally. Two factors causing distributions to differ from the normal distribution (that is two factors causing distributions to be non-normal) are skewness and kurtosis.\n\nSkewness\nSkewed distributions are asymmetrical and they can be positively or negatively skewed. The tail of a negatively skewed distribution points towards negative values (to the left) which means that the distribution leans towards the right (towards positive values) while the tail of a positively skewed distribution points towards positive values (to the right) which means that the distribution leans towards the left (towards negative values).\nAnother characteristic of skewed distributions is that the mean and median of a distribution differ. If the median is greater than the mean, the distribution is negatively skewed (the long tail points towards negative values). If the median is smaller than the mean, the distribution is positively skewed (the long tail points towards positive values).\nNegatively skewed\n\nTail points towards negative values (to the left)\nMedian is greater than the mean\n\nPositively skewed\n\nTail points towards positive values (to the right)\nMedian is lower than the mean\n\n\n\n\n\n\n\n\n\n\nTo show how we can calculate skewness (or if a distribution is skewed), we generate a sample of values.\n\nSampleValues &lt;- sample(1:100, 50)\n# inspect data\nsummary(SampleValues)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   25.50   52.50   50.24   72.75   98.00 \n\n\nWe apply the skewness function from the e1071 package to the sample scores to calculated skewness. The skewness function allows to calculate skewness in three different ways:\nType 1 (This is the typical definition used in many older textbooks): \\[\\begin{equation}\ng_1 = m_3 / m_2^{(3/2)}.\n\\end{equation}\\]\nType 2 (Used in SAS and SPSS): \\[\\begin{equation}\nG_1 = g_1 * sqrt(n(n-1)) / (n-2)\n\\end{equation}\\]\nType 3 (Used in MINITAB and BMDP): \\[\\begin{equation}\nb_1 = m_3 / s^3 = g_1 ((n-1)/n)^{(3/2)}\n\\end{equation}\\]\nAll three formulas have in common that the more negative values are, the more strongly positively skewed are the data (distribution leans to the left) and the more positive the values are, the more strongly negatively skewed are the data (distribution leans to the right). Here we use the second formula (by setting type = 2) that is also used in SPSS.\n\nlibrary(e1071)\nskewness(SampleValues, type = 2)\n\n[1] -0.1925126\n\n\nIf the reported skewness value is negative, then the distribution is positively skewed. If the value is positive, then the distribution is negatively skewed. If the value is lower than -1 or greater than +1, then the distribution can be assumed to be substantively skewed (Hair et al.).\n\n\nKurtosis\nAnother way in which distributions can differ from the normal distribution relates to the thickness of the tails and the spikiness of the distribution. If distributions are bell-shaped like the normal distribution, they are called mesokurtic. If distributions are symmetrical but they are more spiky than the normal distribution, they are called leptokurtic. If symmetrical distributions are flatter and have bigger tails than the normal distribution, the distributions are called platykurtic.\n\n\n\n\n\n\n\n\n\nTo show how we can calculate kurtosis (or if a distribution suffers from kurtosis), we apply the kurtosis function from the e1071 package to the sample scores we generated above /when we calculated skewness).\n\nkurtosis(SampleValues)\n\n[1] -1.188218\n\n\nAs the kurtosis value is positive, the distribution is leptokurtic (if it were negative, the distribution would be platykurtic). As a rule of thumb, values greater than +1 indicate that the distribution is too peaked while values lower than –1 indicate that the distribution is substantively platykurtic (Hair et al., pp61).\nThe kurtosis score can thus be interpreted as follows:\n\nA values of 0 means that the distribution is perfectly mesokurtic with Values between -0.5 and 0.5 suggesting that the distribution is approximately mesokurtic\nValues between -0.5 and -1 mean that the distribution is moderately platykurtic with values smaller than -1 indicating that the distribution is platykurtic.\nValues between 0.5 and 1 mean that the distribution is moderately leptokurtic with values greater than 1 indicating that the distribution is leptokurtic"
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#the-binomial-distribution",
    "href": "tutorials/basicquant/basicquant.html#the-binomial-distribution",
    "title": "Basic Concepts in Quantitative Research",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\nA distribution which is very similar to the normal distribution is the “binomial distribution”. The binomial distribution displays the probability of binary outcomes. So, in fact, the distribution of the coin flips above represents a binomial distribution. However, the binomial distribution cane be (as above in case of the coin flips) but does not have to be symmetric (like the normal distribution).\nTo illustrate this, let us consider the following example: we are interested in how many people use discourse like in Australian English. We have analyzed a corpus, for instance the Australian component of the International Corpus of English and found that 10 percent of all speakers have used discourse like. In this example, the corpus encompasses 100 speakers (actually the the spoken section of the Australian component of the ICE represents more than 100 speakers but it will make things easier for us in this example).\nNow, given the result of our corpus analysis, we would like to know what percentage of speakers of Australian English use discourse like with a confidence of 95 percent. Given the corpus study results, we can plot the expected binomial distribution of discourse like users in Australian English.\n\n\n\n\n\n\n\n\n\nThe bar graph above shows the probability distribution of discourse like users in Australian English. The first thing we notice is that the binomial distribution is not symmetric as it is slightly left-skewed. If we would increase the number of draws or speakers in our example, the distribution would, however, approximate the normal distribution very quickly because the binomial distribution approximates the normal distribution for large N or if the probability is close to .5. Thus, it is common practice to use the normal distribution instead of the binomial distribution.\nThe bar graph below also shows the probability distribution of discourse like users in Australian English but, in addition, it is color-coded: bars within the 95 percent confidence interval are lightgray, the bars in red are outside the 95 percent confidence interval. In other words, if we repeated the corpus analysis, for instance 1000 times, on corpora which represent speakers from the same population, then 95 percent of samples would have between 1 and 8 discourse like users.\n\n\n\n\n\n\n\n\n\nIn addition to the normal distribution and the binomial distribution, there are many other distributions which underlay common procedures in quantitative analyses. For instance, the t- and the F-distribution or the Chi-distribution. However, we will not deal with these distributions here."
  }
]