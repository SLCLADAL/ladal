[
  {
    "objectID": "tutorials/postag/postag.html",
    "href": "tutorials/postag/postag.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\n\nThis tutorial introduces part-of-speech tagging and syntactic parsing using R. This tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to annotate textual data with part-of-speech (pos) tags and how to syntactically parse textual data using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with pos-tagging and syntactic parsing. Another highly recommendable tutorial on part-of-speech tagging in R with UDPipe is available here and another tutorial on pos-tagging and syntactic parsing by Andreas Niekler and Gregor Wiedemann can be found here (see Wiedemann and Niekler).\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.\n\n\n\nClick here to open an interactive Jupyter notebook that allows you execute, change, and edit the code as well as upload your own data.\n\n\n\n\n\n\n\n\n\nLADAL TOOL\n\n\n\n\n\nClick here to open an notebook-based tool  that allows you upload your own text(s), pos-tag them and download the resulting pos-tagged texts.\n\n\n\n\n\n\n\n\nPart-Of-Speech Tagging\nMany analyses of language data require that we distinguish different parts of speech. In order to determine the word class of a certain word, we use a procedure which is called part-of-speech tagging (commonly referred to as pos-, pos-, or PoS-tagging). pos-tagging is a common procedure when working with natural language data. Despite being used quite frequently, it is a rather complex issue that requires the application of statistical methods that are quite advanced. In the following, we will explore different options for pos-tagging and syntactic parsing.\nParts-of-speech, or word categories, refer to the grammatical nature or category of a lexical item, e.g. in the sentence Jane likes the girl each lexical item can be classified according to whether it belongs to the group of determiners, verbs, nouns, etc. pos-tagging refers to a (computation) process in which information is added to existing text. This process is also called annotation. Annotation can be very different depending on the task at hand. The most common type of annotation when it comes to language data is part-of-speech tagging where the word class is determined for each word in a text and the word class is then added to the word as a tag. However, there are many different ways to tag or annotate texts.\nPos–tagging assigns part-of-speech tags to character strings (these represent mostly words, of course, but also encompass punctuation marks and other elements). This means that pos–tagging is one specific type of annotation, i.e. adding information to data (either by directly adding information to the data itself or by storing information in e.g. a list which is linked to the data). It is important to note that annotation encompasses various types of information such as pauses, overlap, etc. pos–tagging is just one of these many ways in which corpus data can be enriched. Sentiment Analysis, for instance, also annotates texts or words with respect to its or their emotional value or polarity.\nAnnotation is required in many machine-learning contexts because annotated texts are commonly used as training sets on which machine learning or deep learning models are trained that then predict, for unknown words or texts, what values they would most likely be assigned if the annotation were done manually. Also, it should be mentioned that by many online services offer pos-tagging (e.g. here or here.\nWhen pos–tagged, the example sentence could look like the example below.\n\nJane/NNP likes/VBZ the/DT girl/NN\n\nIn the example above, NNP stands for proper noun (singular), VBZ stands for 3rd person singular present tense verb, DT for determiner, and NN for noun(singular or mass). The pos-tags used by the openNLPpackage are the Penn English Treebank pos-tags. A more elaborate description of the tags can be found here which is summarised below:\n\n\nTagDescriptionExamplesCCCoordinating conjunctionand, or, butCDCardinal numberone, two, threeDTDeterminera, theEXExistential thereThere/EX was a party in progressFWForeign wordpersona/FW non/FW grata/FWINPreposition or subordinating conuh, well, yesJJAdjectivegood, bad, uglyJJRAdjective, comparativebetter, nicerJJSAdjective, superlativebest, nicestLSList item markera., b., 1., 2.MDModalcan, would, willNNNoun, singular or masstree, chairNNSNoun, pluraltrees, chairsNNPProper noun, singularJohn, Paul, CIANNPSProper noun, pluralJohns, Pauls, CIAsPDTPredeterminerall/PDT this marble, many/PDT a soulPOSPossessive endingJohn/NNP 's/POS, the parentss/NNP '/POS distressPRPPersonal pronounI, you, hePRP$Possessive pronounmine, yoursRBAdverbevry, enough, notRBRAdverb, comparativelaterRBSAdverb, superlativelatestRPParticleRPSYMSymbolCO2TOtotoUHInterjectionuhm, uhVBVerb, base formgo, walkVBDVerb, past tensewalked, sawVBGVerb, gerund or present participwalking, seeingVBNVerb, past participlewalked, thoughtVBPVerb, non-3rd person singular prwalk, thinkVBZVerb, 3rd person singular presenwalks, thinksWDTWh-determinerwhich, thatWPWh-pronounwhat, who, whom (wh-pronoun)WP$Possessive wh-pronounwhose, who (wh-words)WRBWh-adverbhow, where, why (wh-adverb)\n\n\nAssigning these pos-tags to words appears to be rather straight forward. However, pos-tagging is quite complex and there are various ways by which a computer can be trained to assign pos-tags. For example, one could use orthographic or morphological information to devise rules such as. . .\n\nIf a word ends in ment, assign the pos-tag NN (for common noun)\nIf a word does not occur at the beginning of a sentence but is capitalized, assign the pos-tag NNP (for proper noun)\n\nUsing such rules has the disadvantage that pos-tags can only be assigned to a relatively small number of words as most words will be ambiguous – think of the similarity of the English plural (-(e)s) and the English 3rd person, present tense indicative morpheme (-(e)s), for instance, which are orthographically identical.Another option would be to use a dictionary in which each word is as-signed a certain pos-tag and a program could assign the pos-tag if the word occurs in a given text. This procedure has the disadvantage that most words belong to more than one word class and pos-tagging would thus have to rely on additional information.The problem of words that belong to more than one word class can partly be remedied by including contextual information such as. .\n\nIf the previous word is a determiner and the following word is a common noun, assign the pos-tag JJ (for a common adjective)\n\nThis procedure works quite well but there are still better options.The best way to pos-tag a text is to create a manually annotated training set which resembles the language variety at hand. Based on the frequency of the association between a given word and the pos-tags it is assigned in the training data, it is possible to tag a word with the pos-tag that is most often assigned to the given word in the training data.All of the above methods can and should be optimized by combining them and additionally including pos–n–grams, i.e. determining a pos-tag of an unknown word based on which sequence of pos-tags is most similar to the sequence at hand and also most common in the training data.This introduction is extremely superficial and only intends to scratch some of the basic procedures that pos-tagging relies on. The interested reader is referred to introductions on machine learning and pos-tagging such as e.g.https://class.coursera.org/nlp/lecture/149.\nThere are several different R packages that assist with pos-tagging texts (see Kumar and Paul). In this tutorial, we will use the udpipe (Wijffels 2021) and the openNLP packages (Hornik). Each of these has advantages and shortcomings and it is advantageous to try which result best matches one’s needs. That said, the udpipe package is really great as it is easy to use, covers a wide range of languages, is very flexible, and very accurate.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"udpipe\")\ninstall.packages(\"flextable\")\ninstall.packages(\"ggraph\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# load packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(udpipe)\nlibrary(flextable)\nlibrary(ggraph)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go.\n\n\nPOS-Tagging with UDPipe\nUDPipe was developed at the Charles University in Prague and the udpipe R package (Wijffels 2021) is an extremely interesting and really fantastic package as it provides a very easy and handy way for language-agnostic tokenization, pos-tagging, lemmatization and dependency parsing of raw text in R. It is particularly handy because it addresses and remedies major shortcomings that previous methods for pos-tagging had, namely\n\nit offers a wide range of language models (64 languages at this point)\nit does not rely on external software (like, e.g., TreeTagger, that had to be installed separately and could be challenging when using different operating systems)\nit is really easy to implement as one only need to install and load the udpipe package and download and activate the language model one is interested in\nit allows to train and tune one’s own models rather easily\n\nThe available pre-trained language models in UDPipe are:\n\n\nLanguagesModelsAfrikaansafrikaans-afriboomsAncient Greekancient_greek-perseus, ancient_greek-proielArabicarabic-padtArmenianarmenian-armtdpBasquebasque-bdtBelarusianbelarusian-hsebulgarian-btbbulgarian-btbBuryatburyat-bdtCatalancatalan-ancoraChinesechinese-gsd, chinese-gsdsimp, classical_chinese-kyotoCopticcoptic-scriptoriumCroatiancroatian-setCzechczech-cac, czech-cltt, czech-fictree, czech-pdtDanishdanish-ddtDutchdutch-alpino, dutch-lassysmallEnglishenglish-ewt, english-gum, english-lines, english-partutEstonianestonian-edt, estonian-ewtFinnishfinnish-ftb, finnish-tdtFrenchfrench-gsd, french-partut, french-sequoia, french-spokenGaliciangalician-ctg, galician-treegalGermangerman-gsd, german-hdtGothicgothic-proielGreekgreek-gdtHebrewhebrew-htbHindihindi-hdtbHungarianhungarian-szegedIndonesianindonesian-gsdIrish Gaelicirish-idtItalianitalian-isdt, italian-partut, italian-postwita, italian-twittiro, italian-vitJapanesejapanese-gsdKazakhkazakh-ktbKoreankorean-gsd, korean-kaistKurmanjikurmanji-mgLatinlatin-ittb, latin-perseus, latin-proielLatvianlatvian-lvtbLithuanianlithuanian-alksnis, lithuanian-hseMaltesemaltese-mudtMarathimarathi-ufalNorth Saminorth_sami-giellaNorwegiannorwegian-bokmaal, norwegian-nynorsk, norwegian-nynorskliaOld Church Slavonicold_church_slavonic-proielOld Frenchold_french-srcmfOld Russianold_russian-torotPersianpersian-serajiPolishpolish-lfg, polish-pdb, polish-szPortugeseportuguese-bosque, portuguese-br, portuguese-gsdRomanianromanian-nonstandard, romanian-rrtRussianrussian-gsd, russian-syntagrus, russian-taigaSanskritsanskrit-ufalScottish Gaelicscottish_gaelic-arcosgSerbianserbian-setSlovakslovak-snkSlovenianslovenian-ssj, slovenian-sstSpanishspanish-ancora, spanish-gsdSwedishswedish-lines, swedish-talbankenTamiltamil-ttbTelugutelugu-mtgTurkishturkish-imstUkrainianukrainian-iuUpper Sorbiaupper_sorbian-ufalUrduurdu-udtbUyghuruyghur-udtVietnamesevietnamese-vtbWolofwolof-wtb\n\n\n\n\n\nThe udpipe R package also allows you to easily train your own models, based on data in CONLL-U format, so that you can use these for your own commercial or non-commercial purposes. This is described in the other vignette of this package which you can view by the command  vignette(\"udpipe-train\", package = \"udpipe\")\n\n\n\n\n\n\nTo download any of these models, we can use the udpipe_download_model function. For example, to download the english-ewt model, we would use the call: m_eng   &lt;- udpipe::udpipe_download_model(language = \"english-ewt\").\nWe start by loading a text\n\n# load text\ntext &lt;- readLines(\"tutorials/postag/data/testcorpus/linguistics06.txt\", skipNul = T) %&gt;%\n    str_squish() %&gt;%\n    .[1]\n# inspect\ntext\n\n[1] \"Linguistics also deals with the social, cultural, historical and political factors that influence language, through which linguistic and language-based context is often determined. Research on language through the sub-branches of historical and evolutionary linguistics also focus on how languages change and grow, particularly over an extended period of time.\"\n\n\nNow that we have a text that we can work with, we will download a pre-trained language model.\n\n# download language model\nm_eng &lt;- udpipe::udpipe_download_model(language = \"english-ewt\")\n\nIf you have downloaded a model once, you can also load the model directly from the place where you stored it on your computer. In my case, I have stored the model in a folder called udpipemodels\n\n# load language model from your computer after you have downloaded it once\nm_eng &lt;- udpipe_load_model(file = here::here(\"udpipemodels\", \"english-ewt-ud-2.5-191206.udpipe\"))\n\nWe can now use the model to annotate out text.\n\n# tokenise, tag, dependency parsing\ntext_anndf &lt;- udpipe::udpipe_annotate(m_eng, x = text) %&gt;%\n    as.data.frame() %&gt;%\n    dplyr::select(-sentence)\n# inspect\nhead(text_anndf, 10)\n\n   doc_id paragraph_id sentence_id token_id       token      lemma  upos xpos\n1    doc1            1           1        1 Linguistics Linguistic  NOUN  NNS\n2    doc1            1           1        2        also       also   ADV   RB\n3    doc1            1           1        3       deals       deal  NOUN  NNS\n4    doc1            1           1        4        with       with   ADP   IN\n5    doc1            1           1        5         the        the   DET   DT\n6    doc1            1           1        6      social     social   ADJ   JJ\n7    doc1            1           1        7           ,          , PUNCT    ,\n8    doc1            1           1        8    cultural   cultural   ADJ   JJ\n9    doc1            1           1        9           ,          , PUNCT    ,\n10   doc1            1           1       10  historical historical   ADJ   JJ\n                       feats head_token_id  dep_rel deps          misc\n1                Number=Plur             3 compound &lt;NA&gt;          &lt;NA&gt;\n2                       &lt;NA&gt;             3   advmod &lt;NA&gt;          &lt;NA&gt;\n3                Number=Plur             0     root &lt;NA&gt;          &lt;NA&gt;\n4                       &lt;NA&gt;            13     case &lt;NA&gt;          &lt;NA&gt;\n5  Definite=Def|PronType=Art            13      det &lt;NA&gt;          &lt;NA&gt;\n6                 Degree=Pos            13     amod &lt;NA&gt; SpaceAfter=No\n7                       &lt;NA&gt;             8    punct &lt;NA&gt;          &lt;NA&gt;\n8                 Degree=Pos             6     conj &lt;NA&gt; SpaceAfter=No\n9                       &lt;NA&gt;            10    punct &lt;NA&gt;          &lt;NA&gt;\n10                Degree=Pos             6     conj &lt;NA&gt;          &lt;NA&gt;\n\n\nIt can be useful to extract only the words and their pos-tags and convert them back into a text format (rather than a tabular format).\n\ntagged_text &lt;- paste(text_anndf$token, \"/\", text_anndf$xpos, collapse = \" \", sep = \"\")\n# inspect tagged text\ntagged_text\n\n[1] \"Linguistics/NNS also/RB deals/NNS with/IN the/DT social/JJ ,/, cultural/JJ ,/, historical/JJ and/CC political/JJ factors/NNS that/WDT influence/VBP language/NN ,/, through/IN which/WDT linguistic/NN and/CC language/NN -/HYPH based/VBN context/NN is/VBZ often/RB determined/JJ ./. Research/VB on/IN language/NN through/IN the/DT sub-branches/NNS of/IN historical/JJ and/CC evolutionary/JJ linguistics/NNS also/RB focus/RB on/IN how/WRB languages/NNS change/VBP and/CC grow/VBP ,/, particularly/RB over/IN an/DT extended/JJ period/NN of/IN time/NN ./.\"\n\n\n\n\nPOS-Tagging non-English texts\nWe can apply the same method for annotating, e.g. adding pos-tags, to other languages. For this, we could train our own model, or, we can use one of the many pre-trained language models that udpipe provides.\nLet us explore how to do this by using example texts from different languages, here from German and Spanish (but we could also annotate texts from any of the wide variety of languages for which UDPipe provides pre-trained models.\nWe begin by loading a German and a Dutch text.\n\n# load texts\ngertext &lt;- readLines(\"tutorials/postag/data/german.txt\")\nduttext &lt;- readLines(\"tutorials/postag/data/dutch.txt\")\n# inspect texts\ngertext\n\n[1] \"Sprachwissenschaft untersucht in verschiedenen Herangehensweisen die menschliche Sprache.\"\n\nduttext\n\n[1] \"Taalkunde, ook wel taalwetenschap of linguïstiek, is de wetenschappelijke studie van de natuurlijke talen.\"\n\n\nNext, we install the pre-trained language models.\n\n# download language model\nm_ger &lt;- udpipe::udpipe_download_model(language = \"german-gsd\")\nm_dut &lt;- udpipe::udpipe_download_model(language = \"dutch-alpino\")\n\nOr we load them from our machine (if we have downloaded and saved them before).\n\n# load language model from your computer after you have downloaded it once\nm_ger &lt;- udpipe::udpipe_load_model(file = here::here(\"udpipemodels\", \"german-gsd-ud-2.5-191206.udpipe\"))\nm_dut &lt;- udpipe::udpipe_load_model(file = here::here(\"udpipemodels\", \"dutch-alpino-ud-2.5-191206.udpipe\"))\n\nNow, pos-tag the German text.\n\n# tokenise, tag, dependency parsing of german text\nger_pos &lt;- udpipe::udpipe_annotate(m_ger, x = gertext) %&gt;%\n    as.data.frame() %&gt;%\n    dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %&gt;%\n    dplyr::pull(unique(postxt))\n# inspect\nger_pos\n\n[1] \"Sprachwissenschaft/NN untersucht/VVFIN in/APPR verschiedenen/ADJA Herangehensweisen/NN die/ART menschliche/NN Sprache/NN ./$.\"\n\n\nAnd finally, we also pos-tag the Dutch text.\n\n# tokenise, tag, dependency parsing of german text\nnl_pos &lt;- udpipe::udpipe_annotate(m_dut, x = duttext) %&gt;%\n    as.data.frame() %&gt;%\n    dplyr::summarise(postxt = paste(token, \"/\", xpos, collapse = \" \", sep = \"\")) %&gt;%\n    dplyr::pull(unique(postxt))\n# inspect\nnl_pos\n\n[1] \"Taalkunde/N|soort|ev|basis|zijd|stan ,/LET ook/BW wel/BW taalwetenschap/N|soort|ev|basis|zijd|stan of/VG|neven linguïstiek/N|soort|ev|basis|zijd|stan ,/LET is/WW|pv|tgw|ev de/LID|bep|stan|rest wetenschappelijke/ADJ|prenom|basis|met-e|stan studie/N|soort|ev|basis|zijd|stan van/VZ|init de/LID|bep|stan|rest natuurlijke/ADJ|prenom|basis|met-e|stan talen/N|soort|mv|basis ./LET\"\n\n\n\n\nDependency Parsing Using UDPipe\nIn addition to pos-tagging, we can also generate plots showing the syntactic dependencies of the different constituents of a sentence. For this, we generate an object that contains a sentence (in this case, the sentence Linguistics is the scientific study of language), and we then plot (or visualize) the dependencies using the textplot_dependencyparser function.\n\n# parse text\nsent &lt;- udpipe::udpipe_annotate(m_eng, x = \"Linguistics is the scientific study of language\") %&gt;%\n    as.data.frame()\n# inspect\nhead(sent)\n\n  doc_id paragraph_id sentence_id\n1   doc1            1           1\n2   doc1            1           1\n3   doc1            1           1\n4   doc1            1           1\n5   doc1            1           1\n6   doc1            1           1\n                                         sentence token_id       token\n1 Linguistics is the scientific study of language        1 Linguistics\n2 Linguistics is the scientific study of language        2          is\n3 Linguistics is the scientific study of language        3         the\n4 Linguistics is the scientific study of language        4  scientific\n5 Linguistics is the scientific study of language        5       study\n6 Linguistics is the scientific study of language        6          of\n       lemma upos xpos                                                 feats\n1 Linguistic NOUN  NNS                                           Number=Plur\n2         be  AUX  VBZ Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n3        the  DET   DT                             Definite=Def|PronType=Art\n4 scientific  ADJ   JJ                                            Degree=Pos\n5      study NOUN   NN                                           Number=Sing\n6         of  ADP   IN                                                  &lt;NA&gt;\n  head_token_id dep_rel deps misc\n1             5   nsubj &lt;NA&gt; &lt;NA&gt;\n2             5     cop &lt;NA&gt; &lt;NA&gt;\n3             5     det &lt;NA&gt; &lt;NA&gt;\n4             5    amod &lt;NA&gt; &lt;NA&gt;\n5             0    root &lt;NA&gt; &lt;NA&gt;\n6             7    case &lt;NA&gt; &lt;NA&gt;\n\n\nWe now generate the plot.\n\n# generate dependency plot\ndplot &lt;- textplot::textplot_dependencyparser(sent, size = 3)\n# show plot\ndplot\n\n\n\n\n\n\n\n\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to annotate texts using language models and perform pos-tagging and dependency parsing of English texts as well as texts in other languages.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2023. Part-of-Speech Tagging and Dependency Parsing with R. Brisbane: The University of Queensland. url: https://ladal.edu.au/tutorials/postag.html (Version 2023.01.11).\n@manual{schweinberger2023postag,\n  author = {Schweinberger, Martin},\n  title = {Part-of-Speech Tagging and Dependency Parsing with R},\n  note = {tutorials/postag/postag.html},\n  year = {2023},\n  organization = \"The University of Queensland, School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2023.01.11}\n}\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggraph_2.2.1    ggplot2_3.5.1   udpipe_0.8.11   stringr_1.5.1  \n[5] dplyr_1.1.4     flextable_0.9.7\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6            xfun_0.49               htmlwidgets_1.6.4      \n [4] ggrepel_0.9.6           lattice_0.22-6          vctrs_0.6.5            \n [7] tools_4.4.1             generics_0.1.3          klippy_0.0.0.9500      \n[10] tibble_3.2.1            fansi_1.0.6             pkgconfig_2.0.3        \n[13] Matrix_1.7-1            data.table_1.16.2       assertthat_0.2.1       \n[16] uuid_1.2-1              lifecycle_1.0.4         compiler_4.4.1         \n[19] farver_2.1.2            textshaping_0.4.0       munsell_0.5.1          \n[22] ggforce_0.4.2           graphlayouts_1.2.0      fontquiver_0.2.1       \n[25] fontLiberation_0.1.0    htmltools_0.5.8.1       yaml_2.3.10            \n[28] pillar_1.9.0            tidyr_1.3.1             MASS_7.3-61            \n[31] textplot_0.2.2          openssl_2.2.2           cachem_1.1.0           \n[34] viridis_0.6.5           fontBitstreamVera_0.1.1 tidyselect_1.2.1       \n[37] zip_2.3.1               digest_0.6.37           stringi_1.8.4          \n[40] purrr_1.0.2             labeling_0.4.3          rprojroot_2.0.4        \n[43] polyclip_1.10-7         fastmap_1.2.0           grid_4.4.1             \n[46] here_1.0.1              colorspace_2.1-1        cli_3.6.3              \n[49] magrittr_2.0.3          tidygraph_1.3.1         utf8_1.2.4             \n[52] withr_3.0.2             gdtools_0.4.0           scales_1.3.0           \n[55] rmarkdown_2.28          officer_0.6.7           igraph_2.1.1           \n[58] gridExtra_2.3           askpass_1.2.1           ragg_1.3.3             \n[61] memoise_2.0.1           evaluate_1.0.1          knitr_1.48             \n[64] viridisLite_0.4.2       rlang_1.1.4             Rcpp_1.0.13            \n[67] glue_1.8.0              tweenr_2.0.3            xml2_1.3.6             \n[70] jsonlite_1.8.9          R6_2.5.1                systemfonts_1.1.0      \n\n\n\nBack to top\nBack to HOME\n\n\n\n\n\n\n\n\n\nReferences\n\nHornik, Kurt. “openNLP: Apache OpenNLP Tools Interface.” https://cran.r-project.org/web/packages/openNLP/index.html.\n\n\nKumar, Ashish, and Avinash Paul. Mastering Text Mining with r. Packt Publishing Ltd.\n\n\nWiedemann, Gregor, and Andreas Niekler. “Hands-on: A Five Day Text Mining Course for Humanists and Social Scientists in R.” In Proceedings of the Workshop on Teaching NLP for Digital Humanities (Teach4DHGSCL 2017), Berlin, Germany, September 12, 2017., 57–65. http://ceur-ws.org/Vol-1918/wiedemann.pdf.\n\n\nWijffels, Jan. 2021. https://CRAN.R-project.org/package = udpipe."
  },
  {
    "objectID": "tutorials/whyr/whyr.html#full-flexibility",
    "href": "tutorials/whyr/whyr.html#full-flexibility",
    "title": "Why R?",
    "section": "Full flexibility",
    "text": "Full flexibility\n\nR represents a fully-fledged programming environment that is extremely versatile and allows to perform a huge variety of things - from handling and processing data, performing text analytics, statistical analyses, generating very beautiful and even interactive data visualization,generating websites and apps, and creating questionnaires and surveys for gathering data online to for creating behavioral experiments (to name just a very few applications). As such, R is cool because it allows you to do really awesome, exciting, fancy things!\nSo, rather than having to learn how to use many different tools that are all very specific and limited in their applicability, R can do it all! And you only need to learn it once!\nInitially, it may seem like a better option to learn how to use specific tools for specific things but over time, having to learn more tools will turn out to be much more time consuming and much less efficient!"
  },
  {
    "objectID": "tutorials/whyr/whyr.html#great-for-learners",
    "href": "tutorials/whyr/whyr.html#great-for-learners",
    "title": "Why R?",
    "section": "Great for learners!",
    "text": "Great for learners!\n\nR is actually not that hard to learn - R is actually quite very easy to learn (once you have covered the basics). Particularly the tidyverse style or dialect of R - is commonly deemed to be very easy to learn especially for anyone who is new to programming. Although base R is probably easier to pick up for people who already have some programming experience and there are certain things where base R is easier than the tidyverse style (for example when it comes to generating plots), R is a really beginner-friendly, quite forgiving fully fledged programming environment. Indeed, R is so versatile while easy to get into that Google has recently completely switched to R in its Data Analytics certificate."
  },
  {
    "objectID": "tutorials/whyr/whyr.html#free-and-open-source",
    "href": "tutorials/whyr/whyr.html#free-and-open-source",
    "title": "Why R?",
    "section": "Free and Open Source",
    "text": "Free and Open Source\nR and RStudio are free and open source which means that everybody who has a computer, internet access, and is literate in a major world language can learn and work with R without any additional costs everywhere on this planet. This makes R the ultimate equity guarantee as one’s financial and even language language background do not bar anyone from exploring programming and data when using R - it’s like the green and environment-friendly gardening alternative to buying plastic wrapped tomatoes in the supermarket that have no taste anyway."
  },
  {
    "objectID": "tutorials/whyr/whyr.html#rstudio-and-ease-of-use",
    "href": "tutorials/whyr/whyr.html#rstudio-and-ease-of-use",
    "title": "Why R?",
    "section": "RStudio and ease of use",
    "text": "RStudio and ease of use\n\nWith RStudio, the use of R has become even simpler and user-friendly with RStudio allowing to have easy and incredibly efficient work flows that tie in to great version control and documentation options (by having easy access to Git and GitHub and containerization with renv. With RStudio, R is intuitive, and entire analyses can be performed in one single integrative development environment (IDE) - this is great because one can do everything in R and does not need to use many different tools in one project (which makes projects and work flows much for transparent and reproducible)."
  },
  {
    "objectID": "tutorials/whyr/whyr.html#r-is-a-community-effort",
    "href": "tutorials/whyr/whyr.html#r-is-a-community-effort",
    "title": "Why R?",
    "section": "R is a community effort",
    "text": "R is a community effort\n\nOn a related note, R is great because anyone can contribute and add to the multitude of packages, functions, and resources that are available when working with R. The community is really fantastic and super helpful with tips and tricks being publicized on many different channels and platforms - from YouTube tutorials, over online courses, help sites like StackOverflow, to facebook groups and Reddit channels.\nOne really great thing that is also a major achievement of RStudio and the RStudio community is that the R community is really welcoming, tolerant, and forgiving! You can post any question in a forum or on a discussion board and you will receive help and advice - this is really something that sets the R community apart from other programming and software communities (in a good way)."
  },
  {
    "objectID": "tutorials/whyr/whyr.html#reproducibilty-and-transparency",
    "href": "tutorials/whyr/whyr.html#reproducibilty-and-transparency",
    "title": "Why R?",
    "section": "Reproducibilty and Transparency",
    "text": "Reproducibilty and Transparency\n\nA really major advantage for using R when you are involved in research is that it is a fantastic - and maybe even the optimal - tool for sharing and documenting your analyses and making your research transparent, reproducible and replicable! When you generate a folder and create an Rproject with a project specific library using renv, use R notebooks to document what you have done, and enable version control with Git, and then connect that project to GitHub, your research is fully transparent and reproducible! Due to the in-built options in RStudio, this is easily done with a few mouse clicks if you have a GitHub account and installed Git on your machine.\nThis is really fantastic because is represents real or true reproducibility and transparency rather than just theoretical reproducibility that still requires researchers to manually re-perform analyses based on (often not ideal or accurate) descriptions of how data was handled in and by different tools - which is so time-consuming that it is almost never done (and that is why research done like this is only theoretically reproducible)."
  },
  {
    "objectID": "tutorials/whyr/whyr.html#tools-versus-scripts",
    "href": "tutorials/whyr/whyr.html#tools-versus-scripts",
    "title": "Why R?",
    "section": "Tools versus Scripts",
    "text": "Tools versus Scripts\n\nIt is perfectly fine to use tools for the analyses exemplified on LADAL. Almost anyone I know - including myself - started off with using fantastic tools like AntConc! However, the aim of LADAL is not primarily to show how to perform certain tasks or analyses but how to perform these tasks in a way that complies with practices that guarantee sustainable, transparent, reproducible research. As R code can be readily shared and optimally contains all the data extraction, processing, visualization, and analysis steps, using scripts is preferable over using (commercial) software.\nIn addition to being not as transparent and hindering reproduction of research, using tools can also lead to dependencies on third parties which does not arise when using open source software. Finally, the widespread use of R particularly among data scientists, engineers, and analysts reduces the risk of software errors as a very active community corrects flawed functions typically quite rapidly."
  },
  {
    "objectID": "tutorials/whyr/whyr.html#widely-used",
    "href": "tutorials/whyr/whyr.html#widely-used",
    "title": "Why R?",
    "section": "Widely used",
    "text": "Widely used\nR is a really widely used and it is becoming ever more popular - not only among data scientists. Ever more creative ways of working with data, discovering the use on ever more exciting topics, and filling niches are filled with applications of R- from sport analytics to improving communication with patients in hospitals to performing experiments and designing art!"
  },
  {
    "objectID": "tutorials/whyr/whyr.html#employability",
    "href": "tutorials/whyr/whyr.html#employability",
    "title": "Why R?",
    "section": "Employability",
    "text": "Employability\n\nR is extremely useful for many tasks as it is a fully fledged programming language or environment!. This means that you are flexible in what you can do and this offers you many options of what jobs you want to or can do.\nAlso, this makes you and your skills appealing and interesting for employers - not only because of its versatility but also because it allows you to do many different and complex things without having to buy expensive software packages (e.g. SPSS, Microsoft Excel, Stata, or MatLab) and you thus have an advantage over other candidates that may be able to do what you can do with R - but the employers would have to buy the software these candidates need in addition to paying a salary!"
  },
  {
    "objectID": "tutorials/whyr/whyr.html#what-about-python",
    "href": "tutorials/whyr/whyr.html#what-about-python",
    "title": "Why R?",
    "section": "What about Python?",
    "text": "What about Python?\nPython is a really great tool (at least I’m told so by Python users whenever they have the chance to let everyone know). Python has traditionally been stronger in Natural Language Processing (NLP) although recent developments have R at least on the same level.\nJokes aside, Python is really great for NLP and everything involving the web - when your focus is doing things like those, Python may be a better option for you. However, when it comes to data visualization and data analysis - particularly visualizations and statistical analyses as they are required, used, and performed in research, R is reeeeally hard to beat."
  },
  {
    "objectID": "contact.html#reporting-errors-via-email",
    "href": "contact.html#reporting-errors-via-email",
    "title": "CONTACT",
    "section": "Reporting errors via Email",
    "text": "Reporting errors via Email\nIf you simply want to report an error, you can contact the LADAL team via ladal@uq.edu.au or you can contact Martin via m.schweinberger@uq.edu.au.\nIt may take some time until we manage to check and fix the bug, but we try to be as swift as possible and we will get to it eventually."
  },
  {
    "objectID": "contact.html#fixing-bugs-directly-on-github",
    "href": "contact.html#fixing-bugs-directly-on-github",
    "title": "CONTACT",
    "section": "Fixing bugs directly on GitHub",
    "text": "Fixing bugs directly on GitHub\nAn even better option than reporting an error is to fix the bug via GitHub. That way, we receive an email that someone found an error and has submitted a fix which we can then review and accept (or decline). Below is a detailed description of how exactly you can do that.\nTo be able to correct errors via GitHub, you need a GitHub account (you can register and sign in here). We will create a tutorial in which we show you how to do this - in the mean time, we have to rely on your skills to try and find out how to do this yourself.\nServices {-} LADAL offers services to members of the School of Languages and Cultures of UQ as well as to external partners or customers including institutions, businesses, teams, or individual researchers and the public. These services include:\n\n\nAccess to resources\nWorkshops and training\n\nIf you are interested or would like to learn more, please contact the LADAL team via ladal@uq.edu.au."
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "TOOLS",
    "section": "",
    "text": "This page lists the tools provided by LADAL. To access a tool, simply click on the  badge or on the linked name of the tool, and the tool (an interactive Jupyter notebook) will launch, allowing you to perform the task.\n\nOverview\n\n\n\nTool\nContent\nLaunch Tool\n\n\n\n\nConcordancing Tool\nAllows users to create Keyword-in-Context displays of words or phrases in a single text or collection of texts and to download the resulting table.\n\n\n\nCollocation Tool\nTool that calculates various association measures (collocation statistics) based on textual input and allows users to download the resulting table.\n\n\n\nKeyword Tool\nTool that calculates various keyness measures based on textual input and allows users to download the resulting table.\n\n\n\nPos-tagging Tool\nTools that allows users to add part-of-speech tags to texts or collections of texts in more than 60 languages and allows users to download the resulting pos-tagged texts.\n\n\n\nCorpus-cleaning Tool\nTool that allows users to remove and replace words, tags, and other elements from the text data that you upload (of course you can then download the cleaned texts too).\n\n\n\nNetwork-Analysis Tool\nTool that allows users to generate and download network graphs based on their own data.\n\n\n\nTopic-Model Tool\nTool that allows users to generate their own custom topic models using unsupervised and supervised LDA and download the results in an MS Excel spreadsheet.\n\n\n\nSentiment-Analysis Tool\nTool that allows users to perform a sentiment analysis and download the results in an MS Excel spreadsheet.\n\n\n\n\n\n\nReporting Errors\nIf something’s amiss or if a tool isn’t cooperating, don’t hesitate to contact Martin at m.schweinberger@uq.edu.au. We’ll do our best to investigate and resolve the issue as quickly as we can. Thanks for bringing it to our attention!\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "events.html",
    "href": "events.html",
    "title": "EVENTS",
    "section": "",
    "text": "On this page you see an overview of events such as webinars, panels, and workshops that have been organised or hosted by LADAL."
  },
  {
    "objectID": "events.html#dcs-workshop-introduction-to-computational-text-analytics",
    "href": "events.html#dcs-workshop-introduction-to-computational-text-analytics",
    "title": "EVENTS",
    "section": "DCS workshop Introduction to Computational Text Analytics",
    "text": "DCS workshop Introduction to Computational Text Analytics\nDate: 23-24 May 2024\nContext: UQ Digital Cultures and Societies Hub\nDo you have more text than you know what to do with? Did you collect data including text for your project and now feel overwhelmed when you try to analyse? Is there too much? Are you doing the same thing over and over again and feeling like you’re not using your time efficiently? Are you worried about missing the forest for the trees (or the trees for the forest)? If any of these apply to you (or you’re just interested in learning more) this workshop is for This workshop will introduce the fundamentals of computational text analysis using LADAL. We’ll start with the key questions of why and where computational methods might be appropriate for your work before moving onto the how. For the how we will:\n\nOutline the necessary preparation and preconditions for using computational approaches to text.\nIntroduce you to the R programming language with Jupyter notebooks.\nGuide you through using a series of common methods and libraries in R so you can start asking the right questions for your own projects.\nTie everything back together with a discussion of the practical aspects of managing projects with computational components grounded in our research experiences."
  },
  {
    "objectID": "events.html#conference-workshop-text-analytics-for-humour-studies",
    "href": "events.html#conference-workshop-text-analytics-for-humour-studies",
    "title": "EVENTS",
    "section": "Conference workshop Text Analytics for Humour Studies",
    "text": "Conference workshop Text Analytics for Humour Studies\nDate: 9 February 2024\nContext: Australian Humour Network 2024 Conference\nThis workshop showcases the use of text analytics for humor studies using resources of the Language Technology and Data Analysis Laboratory (LADAL). We This workshop explores a variety of techniques and concepts and offer expert advice on data processing, analytics, and visualization,. Don’t miss this opportunity to discover the potential of text analytics to elevate your approach to humor studies."
  },
  {
    "objectID": "events.html#acqva-aurora-workshop-advanced-dimension-reduction-methods-and-using-online-computing-for-transparent-research",
    "href": "events.html#acqva-aurora-workshop-advanced-dimension-reduction-methods-and-using-online-computing-for-transparent-research",
    "title": "EVENTS",
    "section": "AcqVA Aurora Workshop Advanced Dimension-Reduction Methods and Using Online Computing for Transparent Research",
    "text": "AcqVA Aurora Workshop Advanced Dimension-Reduction Methods and Using Online Computing for Transparent Research\nDate: 16 January 2024\nContext: AcqVA Aurora workshop\nJoin this half-day workshop on advanced dimension reduction methods (t-SNE and UMAP) and creating interactive Jupyter notebooks.\nThis workshop will show you how to implement t-SNE and UMAP methods for reducing the number of dimensions in your data and how you can generate and make use of Jupyter notebooks from your R notebooks, and make use of GitHub and Binder to convert them into publicly accessible, interactive notebooks that allow others to reproduce your analysis by the push of a button. By combining Jupyter notebooks with Binder and GitHub, you can enhance transparency, reproducibility, and make use of online computing capabilities, allowing you to easily share and replicate analyses.\nThrough hands-on exercises and examples, participants will acquire the skills to\n\nPerform t-SNE and UMAP dimension reduction\ncreate reproducible Jupyter notebooks from their R notebooks\nconnect their RStudio projects to GitHub and push the contents of their project folders to GitHub\nUse Binder to convert a GitHub repository into interactive, publicly accessible notebooks that other can use to interactively reproduce analyses and even edit and change code to explore the data and analyses.\n\nNote: Basic knowledge of R and familiarity with Jupyter notebooks are recommended. Having a GitHub account is mandatory. Please bring your laptop with RStudio installed, along with relevant linguistic datasets for hands-on exercises."
  },
  {
    "objectID": "events.html#resbazqld-workshop-unlock-the-power-of-text-analytics-with-ladal",
    "href": "events.html#resbazqld-workshop-unlock-the-power-of-text-analytics-with-ladal",
    "title": "EVENTS",
    "section": "ResBazQld workshop Unlock the power of text analytics with LADAL",
    "text": "ResBazQld workshop Unlock the power of text analytics with LADAL\nDate: 22 November 2023\nContext: Research Bazaar Queensland, 2023\nJoin us for a practical workshop where you’ll explore the capabilities of text analytics using the resources provided by the Language Technology and Data Analysis Laboratory (LADAL). Through hands-on demonstrations in R, you’ll gain essential skills in natural language processing and learn how to extract valuable insights from text data. Discover diverse approaches and concepts that will empower your research. Plus, get practical advice on data processing, analytics, and visualisation to elevate your research with LADAL’s text analytics resources."
  },
  {
    "objectID": "events.html#acqva-aurora-workshop-dimension-reduction-methods-for-linguistic-data-analysis",
    "href": "events.html#acqva-aurora-workshop-dimension-reduction-methods-for-linguistic-data-analysis",
    "title": "EVENTS",
    "section": "AcqVA Aurora workshop Dimension Reduction Methods for Linguistic Data Analysis",
    "text": "AcqVA Aurora workshop Dimension Reduction Methods for Linguistic Data Analysis\nDate: 25 August 2023\nContext: AcqVA Aurora workshop\nResources: GitHub Repo, intercative Jupyter notebook\nJoin this half-day workshop on dimension reduction methods for linguistic data analysis to explore how Principal Component Analysis (PCA), Multidimensional Scaling (MDS), and Factor Analysis (FA), can enhance your understanding of linguistic data.\nIn this workshop, we will discuss basic concepts and principles behind dimension reduction methods and gain a basic understanding of their similarities and differences. In addition, we will explore practical applications of PCA, MDS, and FA, with a focus on their effectiveness in linguistic research, e.g., by enabling the identification of latent factors and dimensions that influence language variation and usage. Through interactive demonstrations and hands-on exercises, participants will gain practical experience in implementing dimension reduction methods. Learn how to interpret results, visualize reduced-dimensional representations, and effectively communicate findings.\nNote: Basic knowledge of statistics and basic familiarity with R is required. Participants are encouraged to bring their laptops and send through sample data sets beforehand."
  },
  {
    "objectID": "events.html#acqva-aurora-workshop-from-tables-to-forests",
    "href": "events.html#acqva-aurora-workshop-from-tables-to-forests",
    "title": "EVENTS",
    "section": "AcqVA Aurora workshop: From Tables to Forests",
    "text": "AcqVA Aurora workshop: From Tables to Forests\n\nDate: 17 June 2022\nThis workshop consists of two parts. The first part of the workshop focuses on loading and working with tabular data R. In this part of the workshop, we will learn how to load, inspect, process, and summarize tabulated data. The second part of the workshop focuses tree-based models in R, such as conditional inference trees, random forests, and Boruta, which have become more prominent in the language science over the past decade. This part will discuss the underlying logic of tree-based models, how they can be implemented in R, as well as their strengths and weaknesses.\nThe materials for this workshop can be accessed via this GitHub repo and the rendered script is available here."
  },
  {
    "objectID": "events.html#network-analysis-and-topic-modeling-on-twitter-data-using-r",
    "href": "events.html#network-analysis-and-topic-modeling-on-twitter-data-using-r",
    "title": "EVENTS",
    "section": "Network analysis and Topic Modeling on Twitter data using R",
    "text": "Network analysis and Topic Modeling on Twitter data using R\nThis online workshop is offered free to Australian researchers and research students and will cover:\n\nIntroduction to network theory.\n\nFundamental principles of using R for network analysis.\n\nTopic modeling of tweet text using R.\n\nThe data used for the workshop will be an open source dataset of Twitter data relating to the 2019 Federal Election.\nDate: May 18 2022 Time: 9:00AM - 12:00PM AEST Venue: Online\nRegistration is required and the workshop is brought to you by the teams at the Australian Digital Observatory (ADO) and the Australian Text Analytics Platform (ATAP) via the Australian Research Data Commons (ARDC)."
  },
  {
    "objectID": "events.html#monotreme-mania",
    "href": "events.html#monotreme-mania",
    "title": "EVENTS",
    "section": "Monotreme Mania!",
    "text": "Monotreme Mania!\n\nComparative text analytics on Twitter data\nThis online workshop is offered free to Australian researchers and research students and will cover:\n\nCollecting and transforming Twitter data using twarc.\n\nAnalysing the collected Twitter data using the R ecosystem for text analytics.\n\nRegister via Eventbrite\nUpdate - 09/03/2022 The workshop is now fully booked. If you missed out, do check back next week in case of cancellations, and let us know if you’re keen for us to run the workshop again later.\nBrought to you by the teams at the Australian Digital Observatory (ADO) and the Australian Text Analytics Platform (ATAP) via the Australian Research Data Commons (ARDC)."
  },
  {
    "objectID": "events.html#power-analysis-with-r",
    "href": "events.html#power-analysis-with-r",
    "title": "EVENTS",
    "section": "Power Analysis with R",
    "text": "Power Analysis with R\n\nTitle: Power Analysis with R (pwr and simr)\nDate: 1 February 2022\nEvent: AcqVA Aurora Lab Workshop\nLength: 4 hours\nFacilitators: Martin Schweinberger\nResources: GitHub Repo"
  },
  {
    "objectID": "events.html#data-visualization-with-r-ggplot2-and-likert",
    "href": "events.html#data-visualization-with-r-ggplot2-and-likert",
    "title": "EVENTS",
    "section": "Data Visualization with R (ggplot2 and likert)",
    "text": "Data Visualization with R (ggplot2 and likert)\n\nTitle: Data Visualization with R (ggplot2 and likert)\nDate: 25 January 2022\nEvent: AcqVA Aurora Lab Workshop\nLength: 4 hours\nFacilitators: Martin Schweinberger\nResources: GitHub Repo"
  },
  {
    "objectID": "events.html#an-introduction-to-jupyter-notebooks-for-text-analysis",
    "href": "events.html#an-introduction-to-jupyter-notebooks-for-text-analysis",
    "title": "EVENTS",
    "section": "An introduction to Jupyter notebooks for text analysis",
    "text": "An introduction to Jupyter notebooks for text analysis\n\nTitle: An introduction to Jupyter notebooks for text analysis - Virtual workshop for absolute beginners\nDate: 24 November 2021\nEvent: Digital Humanities Australasia 2021 Conference\nLength: 3 hours\nFacilitators: Sara King, Simon Musgrave"
  },
  {
    "objectID": "events.html#best-practices-in-corpus-linguistics",
    "href": "events.html#best-practices-in-corpus-linguistics",
    "title": "EVENTS",
    "section": "Best Practices in Corpus Linguistics",
    "text": "Best Practices in Corpus Linguistics\n\nTitle: Best Practices in Corpus Linguistics – What lessons should we take from the Replication Crisis and how can we guarantee high quality in our research?\nDate: 20–24 May 2020\nEvent: ICAME 41 (41th Meeting of the International Computer Archive of Modern and Medieval English). Heidelberg, Germany.\nSpeaker: Martin Schweinberger\nMaterials: slides, video"
  },
  {
    "objectID": "events.html#the-language-technology-and-data-analysis-laboratory-ladal",
    "href": "events.html#the-language-technology-and-data-analysis-laboratory-ladal",
    "title": "EVENTS",
    "section": "The Language Technology and Data Analysis Laboratory (LADAL)",
    "text": "The Language Technology and Data Analysis Laboratory (LADAL)\n\nTitle: Implementing school-based support infrastructure for digital humanities research at UQ - The Language Technology and Data Analysis Laboratory (LADAL)\nDate: 30 October 2019\nSpeakers: Michael Haugh & Martin Schweinberger\nPresentation at the Australian Research Data Commons (ARDC): The Australian eResearch Skilled Workforce Summit. Sydney, Australia, 29-30/7/2019.\nMaterials: slides"
  },
  {
    "objectID": "events.html#using-r-for-corpus-linguistics",
    "href": "events.html#using-r-for-corpus-linguistics",
    "title": "EVENTS",
    "section": "Using R for Corpus Linguistics",
    "text": "Using R for Corpus Linguistics\n\nTitle: Using R for Corpus Linguistics – an Introduction and Discussion Note on Sustainability and Replicability in Corpus Linguistics\nDate: 2 April 2019\nSpeaker: Martin Schweinberger\nPresentation at the Center of Excellence for the Dynamics of Language (CoEDL) Corpus Workshop. Melbourne, Australia, 2–3/4/2019.\nMaterials: slides"
  },
  {
    "objectID": "events.html#computational-thinking-in-the-humanities",
    "href": "events.html#computational-thinking-in-the-humanities",
    "title": "EVENTS",
    "section": "Computational Thinking in the Humanities",
    "text": "Computational Thinking in the Humanities\n\nThe workshop Computational Thinking in the Humanities was a 3-hour online workshop featuring two plenary talks, lightning presentations, as well as a panel discussion. The workshop is co-organized by the Australian Text Analytics Platform (ATAP), FIN-CLARIAH and its UEF representatives, and the Australian Digital Observatory.\nThe workshop has received financial supported from the Digital Cultures and Societies Hub at the University of Queensland.\nThe workshop took place\nThursday, Sep. 1, 2022, 5-8pm Queensland, 10am-1pm Finland, 8-11am UK.\nEvent Description\nComputers and software are now inescapable in our daily lives - while much of our research might now be enabled or mediated by these computers, that doesn’t mean that our research practices have fundamentally changed - a digital library is still recognisable as a library.\n\nWhat does computational thinking mean for the humanities?\n\nHow can the humanities embrace computing and computational approaches on their own terms, and avoid simply scavenging and repurposing approaches developed within other fields?\n\nHow do we resist having our “problems” “solved” for us?\n\nWhat are the methodological and epistemological foundations of a field that is both computational and of the humanities - does computation actually change the nature of what we do?\n\nThis international workshop will bring together speakers to address this topic, drawing on perspectives from several fields. We hope that this event will prompt reflection, introduce new ideas, spark discussions and plant the seeds for future collaborations.\nProgram\nPart I (90 min, start 17:00 Queensland, 10:00 Finland, 8:00 UK)\n\nOpening and Introductions\nPlenary 1: Krista Lagus: Bridging the impossible - How to avoid bringing technodystopia to the social sciences [Watch on YouTube]\nPlenary 2: Barbara McGillivray: Computational approaches and the Humanities: what might await us? [Watch on YouTube]\n\nBreak\nPart 2 (80 mins, start 18:30 Queensland, 11:30 Finland, 9:30 UK)\n\nLightning talks (10 mins + 2 mins time for changing presenters)\n\nMarissa Takahashi (QUT, Project Manager of the Australian Digital Observatory (ADO)) [Watch on YouTube]\nMartin Schweinberger (UQ, co-director of LADAL) [Watch on YouTube]\nEetu Mäkelä (technical director of FIN-CLARIAH) [Watch on YouTube]\nSteven Coats (University of Oulu) [Watch on YouTube]\n\nPanel discussion (20 min)\n\n\nKrista Lagus\nKrista is full professor at the University of Helsinki in the Centre for Social Data Science, CSDS. There, she applies quantitative and qualitative data analysis methods in order to understand various individual and social practices that affect the well-being of individuals. Recent topics of interest include loneliness, peer support, mindfulness practices and life-philosophical lecturing.\n\nBarbara McGillivray\n\nBarbara is lecturer in Digital Humanities and Cultural Computation at the Department of Digital Humanities of King’s College London and a Turing Research Fellow at The Alan Turing Institute. She is also Editor in Chief of the Journal of Open Humanities Data. Before going back to academia, Barbara was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature.\nBarbara has always been passionate about how the sciences and the humanities can meet. Her most recent book is Applying Language Technology in Humanities Research. Design, Application, and the Underlying Logic, co-authored with Gábor Mihály Tóth and published by Palgrave Macmillan in 2020."
  },
  {
    "objectID": "events.html#ladal-webinar-series-2022",
    "href": "events.html#ladal-webinar-series-2022",
    "title": "EVENTS",
    "section": "LADAL Webinar Series 2022",
    "text": "LADAL Webinar Series 2022\n\n\nThe LADAL Webinar Series 2022 consists of 6 webinars | online presentations from speakers with backgrounds in linguistics, data science, or computational humanities and it covers topics related to the computational handling of language data! All recordings of the webinar series are available on the LADAL YouTube channel.\nDetails about upcoming and past webinars that are part of the LADAL Webinar Series 2022 can be found below.\n\nAll events were announced on Twitter (@slcladal), via the UQ School of Languages and Cultures, and via our collaborators) - so please follow us if you like to catch up with the activities at LADAL. Below are links to recordings of past webinars on our YouTube channel.\nSpread of Lexical Innovations (Jack Grieve)\nArchives as Subject not Source (Cedric Courtois)\nAnalyzing Longitudinal Data (Dimitrios Vagenas)\nBayesian GLMMs with brms (Bodo Winter)\nBelow you will find the details of the webinars including abstracts, bioblurbs of the speakers, and additional resources.Please note that we have only included confirmed webinars | online presentations at the moment - so more webinars and online presentations will be added once they are confirmed! Stay put and check this space if you want to find out more.\n\nSpread of Lexical Innovations (Jack Grieve)\n\n\nThe Spread of Lexical Innovation is Constrained by Cultural Patterns\n\nThis talk was recorded March 7, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/sW70Y6XDiRA\nAbstract\nIn this talk I discuss the results of three studies on the geographical diffusion of lexical innovation, all of which are based on the analysis of multibillion word corpora of Twitter data collected between 2013 and 2014. In this first study, I track the spread of new words across the US. In the second study, I zoom in and look at how these same words spread out across New York City. In the third study, I consider how lexical items from Multicultural London English have diffused across the UK. In all cases, I show that the spread of lexical innovation is not only constrained by physical distance and population density, as predicted by the Wave and Gravity Models, but by cultural patterns and boundaries.\n\n\nAbout Jack\nJack Grieve is Professor of Corpus Linguistics at the University of Birmingham and Turing Fellow at the Alan Turing Institute. His research involves analysing large corpora of natural language to understand language variation and change. He is especially interested in grammatical and lexical variation in the English language across time, space and communicative context, as well as developing methods for quantitative linguistic analysis. Jack also conducts research on authorship analysis and sometimes consult on casework as a forensic linguist. You can get in touch with Jack at j.grieve@bham.ac.uk or via his Twitter handle @JWGrieve.\n\n\n\nArchives as Subject not Source (Cedric Courtois)\n\n\nThe Archive as Subject rather than Source: a Roadmap to Constructing and Disseminating a Digital Archive\n\nThis talk was recorded May 9, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/-dcwbKZuzDU\nAbstract\nArchives are valuable resources in historical media and communication research. Unfortunately, many collections are hard to find and access, and have not been properly digitised and described. In this paper we argue that computational methods are instrumental in engaging with and digitising archive materials, even though these methods introduce problems of their own. More specifically, we articulate critical thinking on the concept of ‘the archive’ with a discussion of the key decisions and practical hurdles encountered in the construction and digitisation of an archive. This is illustrated by a mid-sized project based on records of communications between the Pentagon and CIA Entertainment Liaison Offices (ELO) and audiovisual productions companies around the world, which are key in revealing influence of the US military on the entertainment industry. However, rather than discussing every step in the project in minute detail, we maximise the relevance for a broad readership by distilling a generic roadmap and key recommendations that cover all stages of strategic decision-making in archive construction and digitisation, as well as demonstrating the required generic technical implementations.\n\n\nAbout Cedric\nCedric Courtois is a senior lecturer in the School of Communication and Arts in the Faculty of Humanities and Social Sciences at the University of Queensland. He is both an audience researcher and a methodologist. His research interests include algorithmic impact in digital culture and data science applications in (digital) media and communication research (including text mining and image processing).\n\n\n\n\nAnalyzing Longitudinal Data (Dimitrios Vagenas)\n\n\nAnalyzing Longitudinal Data\n\nThis talk was recorded July 4, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/oKZHRHqU0YY\nZoom link: https://uqz.zoom.us/j/86849442143\nAbstract\nThe focus of the current seminar will be a (very) brief introduction to longitudinal data and their analysis focusing on regression. To start with we will look at longitudinal data and different designs for collecting such data. We will then look at some empirical observations and why they occur before turning our attention to simple linear regression and why it is generally not appropriate to use it for the analysis of such data. Understanding the above is very important but also a neglected aspect of longitudinal data. We will then, very briefly, introduce two methods for appropriately analysing such data: (i) mixed models and (ii) Generalised Estimating Equations.\n\n\nAbout Dimitrios\nDimitrios is a biostatistician and the head of the Research Methods Groups at the Queensland University of Technology. He is also the President of the QLD branch of the Statistical Society of Australia and a member of the Accreditation committee of the same society. He also holds the highest professional accreditation of the American Statistical Association and the Royal Statistical Society. He has also studied animal science, quantitative genetics, and digital biology. His main job is to help researchers do better research and he enjoys working in multidisciplinary teams.\n\n\n\nThe travels of Marco Polo (Andreas Niekler)\n\n\nThe travels of Marco Polo: Information extraction and visualization of historic travel literature\n\nThis talk was recorded September 26, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/oJFmw1Sy7iE\nAbstract\nMarco Polo was born into a wealthy Venetian merchant family in 1254 and at the age of 17 he embarked on an epic journey to Asia, as one of the first westerners to ever visit China. When he returned 24 years later he recorded his extensive travels in a book – publishing possibly the first travel guide ever – and introducing Europeans to Central Asia and China.\nIn our talk we show our preliminary work on the analysis of travel literature using Marco Polo’s travel diaries as an example. First, we show the annotation and automatic extraction of important landmarks. On the other hand, we propose an extraction procedure that can uncover possible movements and route segments from the text. This is complemented by a geo-visualization that does not only show a map, but rather works with the spatial situation and terrain models, so that the landscape context can provide complementary information for the text itself. With this work we hope to contribute to a better understanding of historical journeys.\n\n\nAbout Andreas\nAndreas Niekler is a research associate at the Institute of Computer Science at the University of Leipzig. He develops computer-based methods in the field of semantic properties in language and language-based AI. He develops computer-based algorithmic methods for computational social science and digital humanities research. Here he has contributed primarily text mining methods to political science, cultural studies, and communication research. Within several research projects he was also involved in the development of the interactive Leipzig Corpus Miner. An interactive and graphical tool for intuitive work with large text corpora and modern methods of text mining.\n\n\n\nBayesian GLMMs with brms (Bodo Winter)\n\n\nBayesian generalized linear mixed models with brms\n\nThis talk was recorded November 7, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/UwIgWD5pWzQ\nAbstract\nLinguistics is undergoing a rapid shift away from significance tests towards approaches emphasizing parameter estimation, such as linear mixed effects models. Alongside this shift, another revolution is underway: away from using p-values as part of a “null ritual” (Gigerenzer, 2004) towards Bayesian models. Both shifts can nicely be dealt with the ‘brms’ package (Bürkner, 2017). After briefly reviewing why we shouldn’t blindly follow the “null ritual” of significance testing, I will demonstrate how easy it is to fit quite complex models using this package. I will also talk about how mixed models are used in different subfields of linguistics (Winter & Grice, 2021), and why established practices such as dropping random slopes for non-converging models are a further reason to go Bayesian. Finally, I will briefly touch on issues relating to prior specification, especially the importance of weakly informative priors to prevent overfitting.\n\n\nAbout Bodo\nBodo Winter is a Senior Lecturer at the Department of Linguistics at the University of Birmingham, a UKRI Future Leaders Fellow, a Fellow of the Institute for Interdisciplinary Data Science and AI, and Editor-in-Chief at the journal Language and Cognition. Dr. Winter has received his PhD in Cognitive and Information Sciences from the University of California, Merced. His research focuses on multimodality, sound symbolism, gesture, and metaphor.\n\n\n\nFound in Translation (Jörg Tiedemann)\n\n\nFound in Translation - What can we learn from translations about languages and human communication\n\nThis talk was recorded December 5, 2022, as part of the LADAL Webinar Series 2022.\nRecording link: https://youtu.be/feF987aX5s8\nAbstract\nThe goal of language technology is to create computational models that can understand and generate language in a way humans can do. One of the strategies is to learn such communication abilities from real-world data and in that way somewhat resemble humans and their capability of picking up language skills through practical experience. However, the crucial question is what kind of experience is needed and what kind of tasks have to be practiced to build an understanding of human language signals. We are currently running a project that studies the use of translations that form natural semantic mirrors of original texts in other languages as a means of providing information about the underlying latent meaning that corresponds to the observable language string. The big question is what kind of abstractions can be learned from this cross-lingual signal and how much does that reflect our knowledge about linguistic properties on various levels. Part of this question is how much language diversity can be used to push abstraction levels even further. In this talk I will present some of our results and try to connect this kind of neural “black-box” NLP with questions in general linguistics and cognition.\n\n\nAbout Jörg\nJörg Tiedemann is professor of language technology at the Department of Digital Humanities at the University of Helsinki. He received his PhD in computational linguistics for work on bitext alignment and machine translation from Uppsala University before moving to the University of Groningen for 5 years of post-doctoral research on question answering and information extraction. His main research interests are connected with massively multilingual data sets and data-driven natural language processing and he currently runs an ERC-funded project on representation learning and natural language understanding.\nWebsite: https://blogs.helsinki.fi/tiedeman/\n\n\n\nGriffith Digital Humanities Webinar Series\n\nMartin Schweinberger and Michael Haugh presented about ATAP and LADAL at the Griffith Digital Humanities Webinar Series on Friday May 20, 2022. For more information about this webinar series and the Griffith Centre for Social and Cultural Research see here."
  },
  {
    "objectID": "events.html#ladal-webinar-series-2021-ladal-opening",
    "href": "events.html#ladal-webinar-series-2021-ladal-opening",
    "title": "EVENTS",
    "section": "LADAL Webinar Series 2021 (LADAL Opening)",
    "text": "LADAL Webinar Series 2021 (LADAL Opening)\n\n\nThe LADAL Opening Webinar Series 2021 consists of 25 webinars or online presentations from a wide range of voices with backgrounds in linguistics, data science, or computational humanities and it covers a variety of topics related to digital handling of language data! All recordings of the webinar series are available on the LADAL YouTube channel.\n\n\nAll events were announced on Twitter (@slcladal), via the UQ School of Languages and Cultures, and via our collaborators) - so please follow us if you like to catch up with the activities at LADAL. The LADAL Opening Webinar Series 2021 kicked off with a presentation by Stefan Th. Gries on MuPADRF (Multifactorial Prediction and Deviation Analysis Using Regression/Random Forests) on June 3, 2021, 5pm Brisbane time. See below for the full list of presentations that are part of the LADAL Opening Webinar Series 2021.\nRecordings on YouTube\nMuPADRF (S. Th. Gries)\nLADAL & ATAP (M. Schweinberger & M. Haugh)\nData Collection in the Field (F. Meakins)\nThe UZH Text Crunching Center (G. Schneider)\nCorpus-Based Media Linguistics (M. Bednarek)\nBayesian vs Frequentist (N. Levshina)\nOnline Data Collection (M. Vos)\nReproducible Research (A. Miotto & J. Toohey)\nNeurolinguistics of Bilingualism (V. DeLuca, T. Voits & J. Rothman)\nIntroducing Network Analysis (S. Musgrave)\nSpeech Recognition with Elpis (J. Wiles & B. Foley)\nSocietal Big Data (M. Laitinen)\nTackling Social Media Data (S. Hames)\nData-Driven Learning (P. Crosthwaite)\nText Classification & Hate Speech (G. Wiedemann)\nVARIENG (T. Nevalainen, T. Hiltunen & A. Liimatta)\nAntConc 4.0 (L. Anthony)\nDistributional Semantics (G. Desagulier)\nUsage-Based Language Learning (L. Janda)\nAnalyzing Historical Publications (T. Säily)\nGit and GitHub (S. Guillou)\nReplicability & Robustness (J. Flanagan)\nLinguistic Phylogenetics (J. Macklin-Cordes & E. Round)\nAnalyzing Emigrant Letters (C. P. Amador-Moreno)\nAARNet & CloudStor (S. King)\n\n\n\nUSER STORIESWe are currently looking for user stories (also known as testimonials) to see and show what people use LADAL resources for. If you have used LADAL resources - be it by simply copying some code, attending a workshop, learning about a method using a tutorial, or in any other way - we would be extremely grateful, if you would send us your user story! To submit your user story, simply write up a paragraph describing how you have used LADAL resources and what you have used them for and send it to ladal@uq.edu.au. We really appreciate any feedback from you about this!\n\n\n\n\n\nMuPADRF (S. Th. Gries)\n\n\nMuPADRF (Multifactorial Prediction and Deviation Analysis Using Regression/Random Forests)\n\nThis talk was recorded June 3, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording link: https://www.youtube.com/watch?v=cLocET9CC-E&t=253s\nAbstract\nIn this talk, Stefan gave a brief and relatively practical introduction to an approach called MuPDAR(F) (for Multifactorial Prediction and Deviation Analysis using Regressions/Random Forests) that he developed (see Gries and Deshors, Gries and Adelman for the first applications). The main part of the talk involved using a version of the data in Gries and Adelman to exemplify how this protocol works and how it can be done in R. Second, Stefan discussed a few recent extensions proposed in Gries and Deshors and Gries (n.d.), which have to do with\n\nhow to deal with situations with more than two linguistic choices,\nhow predictions are made, and\nhow deviations are quantified.\n\nFinally, he briefly comment on exploring individual variation among the target speakers (based on Gries and Wulff).\n\nAbout Stefan\nStefan Th. Gries is full professor at the University of California, Santa Barbara (UCSB), as well as Honorary Liebig-Professor and Chair of English Linguistics at the Justus-Liebig-Universität Giessen. Stefan has held several prestigious visiting professorships at top universities and, methodologically, he is a quantitative corpus linguist at the intersection of corpus linguistics, cognitive linguistics, and computational linguistics. Stefan has applied a variety of different statistical methods to investigate a wide range of linguistic topics and much of his work involves the open-source software R. Stefan has produced more than 200 publications (articles, chapters, books, and edited volumes), he is an active member of various editorial boards as well as academic societies.\n\n\n\n\nLADAL & ATAP (M. Schweinberger & M. Haugh)\n\n\nThe Australian Text Analytics Platform (ATAP) and the Language Technology and Data Analysis Laboratory (LADAL) - building computational humanities infrastructures: experiences, problems, and potentials\n\nThis talk was recorded June 10, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://www.youtube.com/watch?v=qGIGCubyJs0\nAbstract\nThis talk introduces the Language Technology and Data Analysis Laboratory (LADAL) which is a computational humanities resource infrastructure maintained by the School of Languages and Cultures at the University of Queensland. The talk will also provide information about its relations to the Australian Text Analytics Platform (ATAP) which represents an effort to promote text analytics in Australia and to make resources for using text analytics available to a wider community of researchers.\n\nAbout Martin\n\nMartin Schweinberger is a language data scientist with a PhD in English linguistics who has specialized in corpus linguistics and quantitative, computational analyses of language data. Martin is a Lecturer in Applied Linguistics at the University of Queensland, Australia where he has been establishing the Language Technology and Data Analysis Laboratory (LADAL) and he holds an additional part-time Associate Professorship in the AcqVA-Aurora Center at the Arctic University of Norway in Tromsø.\n\nAbout Michael\n\nMichael Haugh is Professor of Linguistics and a Fellow of the Australian Academy of the Humanities. His research interests lie primarily in the field of pragmatics, the science of language-in-use. He works with recordings and transcriptions of naturally occurring spoken interactions, as well as data from digitally-mediated forms of communication across a number of languages. An area of emerging importance in his view is the role that language corpora can play in the humanities and social sciences more broadly. He has been involved in the establishment of the Australian National Corpus and the Language Technology and Data Analytics Lab, and is currently leading the establishment of a national language data commons.\n\n\n\n\nData Collection in the Field (F. Meakins)\n\n\nField-based methods for collecting quantitative data\n\nThis talk was recorded June 18, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/BGa0gkkkWsc\nAbstract\nShana Poplack has set benchmarks for the development of corpora since the early 1980s. Poplack (2015, p. 921) maintains that the “gold standard remains the (standard sociolinguistic-style) … corpus”. The aim of producing corpora using these principles is to avoid the ‘cherry picking’ approach which dominates much of the theoretical literature. Poplack and her team have created the Ottawa-Hull Corpus which consists of 3.5 million words of informal speech data. This corpus is enormous and beyond the capabilities of a single linguist in a small language community. This talk offers suggestions for corpus development in the field that follow Poplack’s principles, but also shows where compromises can be made. I discuss the method developed during the Gurindji Kriol project called ‘peer elicitation’. It supplements Poplack’s gold standard of naturally occurring speech with semi-formal elicitation to ensure sufficient data for quantitative analyses.\n\nAbout Felicity\n\nFelicity Meakins is an ARC Future Fellow in Linguistics at the University of Queensland and a CI in the ARC Centre of Excellence for the Dynamics of Language. She is a field linguist who specialises in the documentation of Australian Indigenous languages in the Victoria River District of the Northern Territory and the effect of English on Indigenous languages. She has worked as a community linguist as well as an academic over the past 20 years, facilitating language revitalisation programs, consulting on Native Title claims and conducting research into Indigenous languages. She has compiled a number of dictionaries and grammars of traditional Indigenous languages and has written numerous papers on language change in Australia.\n\n\n\nThe UZH Text Crunching Center (G. Schneider)\n\n\nText Crunching Center (TCC): Data-driven methods for linguists, social science and digital humanities\n\nThis talk was recorded June 24, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/2_l3ViTzBOM\nAbstract\nThis talk introduces the Text Crunching Centre (TCC) which is a Computational Linguistics and Digital Humanities service hosted at the University of Zurich, and a collaboration partner of LADAL. We present a selection of our case studies using text analytics, from cognitive linguistics, social, political and historical studies. We show how stylistics, document classification, topic modelling, conceptual maps, distributional semantics and eye-tracking can offer new perspectives. Our case studies include language and age, learner language, the history of medicine, democratisation, religion, and attitudes to migration. We conclude with an outlook to the future of text analytics.\n\n\nAbout Gerold\nGerold Schneider is a Senior Lecturer, researcher and computing scientist at the department of Computational Linguistics at the University of Zurich, Switzerland. His doctoral degree is on large-scale dependency parsing, his habilitation on using computational models for corpus linguistics. His research interests include corpus linguistics, statistical approaches, Digital Humanities, text mining and language modeling. He has published over 100 articles on these topics. He has published a book on statistics for linguists (Schneider and Lauber), and a book on digital humanities is under way. His Google scholar page can be accessed here.\n\n\n\nCorpus-Based Media Linguistics (M. Bednarek)\n\n\nCorpus-based media linguistics: A case study of linguistic diversity in Australian television\n\nThis talk was recorded July 1, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/g54yLpYefbI\nAbstract\nIn this golden age of Indigenous television (Sebbens 2020, February 2), it is important to analyze Indigenous-authored drama series, so that we can move beyond ‘a deficit perspective’ (Charity Hudley, Mallison, and Bucholtz, 216) in relation to mediated linguistic diversity. This talk presents a corpus linguistic case study of Australian Aboriginal English (AAE) lexis as present in three such Indigenous-authored television series: Redfern Now, Cleverman, and Mystery Road. For television viewers, mediated AAE can be an important source of information, especially if they do not regularly interact with Aboriginal and/or Torres Strait Islander people. This would be the case for many Australians, and even more so for international viewers of Australian television series. All three analyzed series were exported overseas and thus have both Australian and international audiences. Using lexical profiling analysis [AntWordProfiler; Anthony] in combination with qualitative concordance analysis, the talk identifies and compares the use of AAE lexis across the three series. Analysis of frequency and distribution will pinpoint words that appear to be particularly significant lexical resources in mediated AAE. The talk will be framed through the notion of diversity, as conceptualised in relation to television series.\n\nAbout Monika\n\nMonika Bednarek is Professor of Linguistics at the University of Sydney and Director of the Sydney Corpus Lab. Her research uses corpus linguistic methodologies across a variety of fields, including media linguistics, discourse analysis and sociolinguistics. She has a particular interest in the linguistic expression of emotion and opinion, with a focus on English. Monika is the author or co-author of six books and two short volumes as well as numerous journal articles and book chapters. She has co-edited several edited volumes and special issues of journals, most recently Corpus approaches to telecinematic language (International Journal of Corpus Linguistics 26/1, 2021) and Corpus linguistics and Education in Australia (Australian Review of Applied Linguistics 43/2, 2020). She is on the steering committee of the Asia Pacific Corpus Linguistics Association and tweets @corpusling.\n\n\n\nBayesian vs Frequentist (N. Levshina)\n\n\nRecycle, relax, repeat: Advantages of Bayesian inference in comparison with frequentist methods\n\nThis talk was recorded July 8, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/vpsJj7Nkgw4\nResources\nThe slides for Natalia’s talk are available here. An Open Science Foundation (OSF) repository containing the R code and data used for the case study in this talk are available here. Here is a link to the GitHub repo of Shravan Vasishth (Professor for Psycholinguisics at the University of Potsdam) with additional resources on Bayesian statistics.\nAbstract\nBayesian inference is becoming increasingly popular in linguistic research. In this talk I will compare frequentist (maximum likelihood) and Bayesian approaches to generalized linear mixed-effects regression, which is de facto the standard method for testing linguistic hypotheses about linguistic variation. The main advantages of Bayesian inference include an opportunity to test the research hypothesis directly, instead of trying to reject the null hypothesis. One can also use information from previous research as priors for subsequent models, which helps to overcome the recent crisis of reproducibility. This also enables one to use smaller samples. It helps to solve such problems as overfitting, data separation and convergence issues, which often arise when one fits generalized mixed-effect models with complex structure. These advantages will be illustrated by a multifactorial case study of help + (to-)infinitive in US magazines, as in the example These simple tips will help you (to) survive the Zombie apocalypse.\n\nAbout Natalia\n\nNatalia Levshina is a linguist working at the Max Planck Institute for Psycholinguistics in Nijmegen. Her main research interests are cognitive and functional linguistics, pragmatics, typology, corpora and data science. She obtained her PhD at KU Leuven in 2011 and got her habilitation qualification at Leipzig University in 2019 with a thesis Towards a theory of communicative efficiency. In addition to papers on causatives, differential case marking, politeness, word order variation and other linguistic topics, Natalia is the author of a best-selling statistical manual How to Do Linguistics with R (Levshina).\n\n\n\nOnline Data Collection (M. Vos)\n\n\nGathering data and creating online experiments with jsPsych and JATOS\n\nThis talk was recorded July 15, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/7-9WRYpXEtE\nResources: The materials, including scripts, files, etc., can be accessed via and downloaded from this GitHub repository.\nAbstract\nWeb-based studies are an increasingly popular and attractive alternative to lab- and field-based studies, enabling remote data collection for a rapidly growing variety of (psycho-)linguistic methods ranging from acceptability judgment tasks and self-paced reading to interactive group designs and the Visual World paradigm. Most platforms for building and hosting online studies are proprietary and subscription-based (e.g., Gorilla, Pavlovia, and FindingFive), but there also exist various free, open-source tools for writing and managing studies on your own server. This talk gives a practical introduction to building and hosting studies using jsPsych (De Leeuw) and JATOS (Lange, Kühn, and Filevich), by demonstrating a speedrun of the entire process: from an empty file in a code editor, to distributing URLs to participants.\n\n\nAbout Myrte\nMyrte Vos (she/they) is a doctoral research fellow at the Arctic University of Norway (Tromsø). They’re supposed to be studying incremental processing of aspect and modality in English, but got sidetracked by figuring out how to do that using webcam-based eye tracking.\n\n\n\n\nReproducible Research (A. Miotto & J. Toohey)\n\n\nGoing down the Reproducible Research pathway: You have to begin somewhere, right?\n\nThis talk was recorded July 22, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/bANTr9RvnGg\nResources: The materials that are mentioned in this presentation can be accessed via the following url: https://www.qcif.edu.au/training/course-catalogue/\nAbstract\nThe idea that you can duplicate an experiment and get the same conclusion is the basis for all scientific discoveries. Reproducible research is data analysis that starts with the raw data and offers a transparent workflow to arrive at the same results and conclusions. However not all studies are replicable due to lack of information on the process. Therefore, reproducibility in research is extremely important.\nResearchers genuinely want to make their research more reproducible, but sometimes don’t know where to start and often don’t have the available time to investigate or establish methods on how reproducible research can speed up every day work. We aim for the philosophy “Be better than you were yesterday”. Reproducibility is a process, and we highlight there is no expectation to go from beginner to expert in a single workshop. Instead, we offer some steps you can take towards the reproducibility path following our Steps to Reproducible Research self paced program.\n\n\nAbout Amanda\nAmanda Miotto is an eResearch Analyst for Griffith University and QCIF. She started off in the field of Bioinformatics and learnt to appreciate the beauty of science before discovering the joys of coding. She is also heavily involved in Software Carpentry, Hacky Hours and ResBaz, and has developed on platforms around HPC and scientific portals.\n\n\nAbout Julie\nJulie Toohey is a Library Research Data Management Specialist at Griffith University Library. Julie has an extensive career in academic libraries and is passionate about research data management practices. Previously, Julie co-facilitated the Australian National Data Services 23 Things (research data) Health and Medical Data Community series and is currently a member of the QULOC Research Support Working Party. Julie works closely with Griffith eResearch Services delivering education awareness programs around managing research data, reproducible research and working with sensitive data. Julie has co-authored several research data related publications with Griffith researchers and eResearch Services partners.\nhttps://orcid.org/0000-0002-4249-8180\n\n\n\nNeurolinguistics of Bilingualism (DeLuca, Voits & Rothman)\n\n\nNeurocognitive effects of bilingual experience\n\nThis talk was recorded July 28, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/wVBkUSPQFUw\nAbstract\nMuch research over the past two decades shows that bilingualism affects brain structure, function, and potentially domain-general cognition (see e.g., Bialystok; Pliatsikas). The specificity of these effects, however, has become the subject of significant debate in recent years, in large part due to variability of findings across studies (see Leivada et al. for review). In this talk, we will introduce our research programs within the Psycholinguistics of Language Representation (PoLaR) lab that addresses the juxtaposition of data and argumentation. Our work is guided by the principle that although bilingual effects are existent, they are conditional. In other words, bilingualism per se is not a sufficient condition for relevant effects on neurocognition. We will review our work that is generally designed to test the hypothesis that specific experience-based factors (EBFs) variably affect neural activity and plasticity in brain regions and pathways implicated in language- and executive control across the lifespan. We present results from a series of MRI studies showing a specificity of neural adaptations to different EBFs (DeLuca, Rothman, and Pliatsikas; DeLuca et al., ) in younger adults. We will also present data from older adults, showing similar EBF effects in healthy cognitive ageing (Voits, Robson, and Pliatsikas submitted) and with mild cognitive impairment (Voits et al. in prep.). EBFs related to duration of bilingual language use correlate to neurocognitive adaptations suggesting increased efficiency in language control, whereas those related to extent of additional language use correlate with adaptations suggesting increased control demands. Considered together, these data suggest that the brain strives to be maximally effective and efficient in language processing and control, which in turn affects domain-general cognitive processes proportionally to degree of engagement with bilingual experiences. The work in older populations leads to the conclusion that degree of engagement with bilingualism is a catalyst for cognitive/brain reserve and thus has some real-world benefits in aging.\n\n\nAbout Vince\nVincent DeLuca is Associate Professor in the Neurocognition of Bilingualism and co-director of the Psycholinguistics of Language Representation (PoLaR) lab in the AcqVa Aurora Centre at UiT-The Arctic University of Norway. His research is focused on how different aspects of bilingual language experience variably impact brain structure, function, and several cognitive processes. His work focuses on how these neural and cognitive adaptations dynamically shift over time and with changes to patterns of language use.\n\nAbout Toms\nToms Voits is a Postdoctoral Researcher at UiT the Arctic University of Norway. He is affiliated with the Psycholinguistics of Language Representation (PoLaR) lab and the AcqVA Aurora Center in the Department of Language and Culture at UiT. His work is primarily focused on investigating the effects of bilingualism on neurocognition, with a particular interest in examining bilingualism as a contributing factor to cognitive and brain reserves in the later years of life.\n\nAbout Jason\nJason Rothman is Professor of Linguistics at UiT the Arctic University of Norway and Senior Researcher in Cognitive Science at Universidad Nebrija (Spain). He is deputy director of the AcqVA Aurora Center at UiT, where he co-leads the center’s theme/concentration on the Neurocognition of Bilingualism. Professor Rothman also co-directs the center’s Psycholinguistics of Language Representation (PoLaR) lab. A linguist by training, he has worked extensively on language acquisition, linguistic processing and language-associated links to domain general neurocognition across the lifespan of varioustypes of bi-/multilingual populations. He is founding editor of the journal Linguistic Approaches to Bilingualism and serves as executive editor of the book series Studies in Bilingualism.\n\n\n\nIntroducing Network Analysis (S. Musgrave)\n\n\nA gentle introduction to networks\n\nThis talk was recorded Aug. 2, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/QW0lVaHcNX4\nAbstract\nLinguists have adopted several methods from data science, but network analysis has been used rather less than others even though it is a useful tool. This presentation will introduce the basics of using network analysis, discussing the types of problems for which the method is useful, the kinds of data which are amenable to analysis, and the graphical outputs which can be achieved. These points will be illustrated with several examples from different areas of linguistic research, as well as with an example with data concerning a social network.\n\n\nAbout Simon\nSimon Musgrave was a lecturer in the Linguistics program at Monash University until the end of 2020. His research covers various areas in linguistics and sociolinguistics, linked by the themes of the use of computational tools in linguistic research and the relationship between Linguistics and Digital Humanities, an interest continued in his current work in the Linguistic Data Commons of Australia project.\n\n\n\nSpeech Recognition with Elpis (J. Wiles & B. Foley)\n\n\n(Semi-)Automated speech recognition using Elpis\n\nThis talk was recorded Aug. 12, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/9YjZH4lBco0\nResources: See here for a guide on how to use Elpis.\nAbstract\nSpeech recognition (ASR) technologies can be useful to accelerate transcription of language recordings, and to provide new insights into corpora. ASR tools are available now for hundreds of languages from commercial providers and can be trained for languages that are not commercially supported. However, there are significant hurdles to using ASR tools, from the preparation of data through to the training of the systems. This presentation introduces the motivations and process of co-designing Elpis, a speech recognition system built to be usable by ordinary working linguits. We discuss some examples of using Elpis with a range of low-resource language corpora, and how the co-design process can be used to benefit other language technologies.\n\n\nAbout Ben\nBen Foley is the project manager of CoEDL’s Transcription Acceleration Project (TAP). TAP brings cutting-edge language technology within reach of people working with some of the world’s oldest languages. A major focus of TAP is the development of user-friendly speech recognition tools. Ben’s previous experience with Aboriginal and Torres Strait Islander language resource development has resulted in apps and websites galore. Highlights include the Iltyem-iltyem sign language database and website, and the Gambay First Languages Map, showing the hundreds of languages in Australia.\n\nAbout Janet\nJanet Wiles is a Professor in Human Centred Computing at the University of Queensland and leads the Future Technologies Thread of the ARC Centre of Excellence for the Dynamics of Language (CoEDL). She has 30 years’ experience in cross-disciplinary research and teaching, including artificial intelligence, language technologies and social robotics, leading teams that span engineering, humanities, social sciences and neuroscience.\n\nBen and Janet currently teach a cross disciplinary course Voyages in Language Technologies”* that introduces computing students to the diversity of the world’s languages, and state-of-the-art tools for deep learning and other analysis techniques for working with language data.\n\n\nSocietal Big Data (M. Laitinen)\n\n\nAdding social information to societal big data?\n\nThis talk was recorded Aug. 19, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/kfb7xLC2nBs\nAbstract\nSocietal big data today provides a large source of language data in naturalistic settings. Such data have substantially enlarged pools of evidence in various fields in social sciences and the humanities. In linguistics, one direct impact has been the emergence of computational sociolinguistics, a field that intersects sociolinguistics with computational techniques.\nHowever, social big data have one major limitation, at least when it comes to computational sociolinguistics. This limitation, quite surprisingly, concerns the lack of social background information. Researchers have no direct access to background information (author’s gender, social layer, education, occupation, etc.), and it is difficult to combine evidence from social media with rich social information for the simple reason that such information is not available for proprietary reasons. If some social information is available, it is often self-reported and therefore prone to inaccuracies. Or, it is ethically unsustainable to link big language data with socio-cultural information (cf. the Cambridge Analytica scandal).\nThis talk introduces a method that is being developed in my SOCOS research group. The method builds on social network theory and utilizes freely available interaction data. These data can be quantified easily through a set of algorithms and used as proxies for social parameters. One downside is that it requires some degree of technical competence to extract these data, which can easily be accomplished through interdisciplinary partnerships in digital humanities environments.\n\n\nAbout Mikko\nMikko Laitinen is Professor of English Language at the University of Eastern Finland. He obtained PhD in 2007 and has been a member of the Academy of Finland Center of Excellence in Research Unit for Variation, Contacts and Change (VARIENG) since 2000. He previously worked as Professor of English at Linnaeus University, where he is one of the two founding members of the Center of Data Intensive Sciences and Applications (DISA), a multidisciplinary big data research consortium that consists of scholars in the humanities, computer scientists, mathematicians, and social scientists. His research focuses on the role social networks in language variation and change, computational sociolinguistics, and digital humanities.\n\n\nTackling Social Media Data (S. Hames)\n\n\nWorking with Social Media Data\n\nThis talk was recorded Aug. 26, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/QVwWPfxE93U\nAbstract\nSocial media platforms, built on the web and the internet, are now just part of life for most of us. The pervasive communication and connection the web and social media enables is a potentially rich source of data for research - but there are pitfalls for the unprepared. This talk provides a survey of the web as a source of data for research projects, including considerations of privacy, ethics and governance; the technical approaches to data collection; and an overview of approaches to analysing such data.\n\n\nAbout Sam\nSam is a developer/data scientist at Queensland University of Technology’s Digital Observatory (yes, the slash is important!). He has a PhD under examination in machine learning for medical image analysis and a commercial background in software development for text analytics algorithms and products. At the Digital Observatory, Sam helps researchers deal with the challenges of collecting, modelling and analysing digital data to address their research questions. He is particularly interested in both the web as a medium, and in the development of computationally assisted methods for bridging the qualitative and quantitative divide.\n\n\n\n\nData-Driven Learning (P. Crosthwaite)\n\n\nData-driven learning for younger learners: Boosting schoolgirls’ knowledge of passive voice constructions for STEM education\n\nThis talk was recorded Sep. 2, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/JlHchgpzO3o\nAbstract\nThis paper explores how corpus technology and DDL pedagogy can support secondary schoolgirls’ reporting of an observed science experiment through a written research report, focusing particularly on how corpora were used to develop receptive and productive knowledge of passive voice constructions. A pre-test of the grammaticality of passive constructions was conducted, alongside a diagnostic pre-instruction written report requiring the retelling of an observed science experiment were collected from 60 Year 9-10 girls at a high school in Australia. During a full 10-week term, students were given guided individual homework tasks and short in-class pair/group DDL activities focusing on passive voice constructions, using freely available online corpus applications such as SketchEngine. Following this treatment, a post-test was conducted while an additional written research report was collected. Questionnaire and interview data was also collected to determine the perceptions of younger female learners and their teachers regarding their engagement with corpora and DDL for improving knowledge and use of passive constructions over time. The data suggest that while the DDL treatment did not result in increased receptive knowledge of the grammaticality of passive voice constructions, their productive use of the passive was significantly improved. Moreover, when students were given the opportunity to use corpora to check their intuitions in the post-test, they produced accurate responses over 75% of the time. Qualitative stakeholder perceptions of improved disciplinary linguistic knowledge, increased data management skills, and positive engagement with “science” were also found in the survey/interview data, although a number of challenges at the technical and conceptual levels for DDL still remain.\n\n\nAbout Peter\nPeter Crosthwaite is Senior Lecturer in Applied Linguistics in the School of Languages and Cultures at the University of Queensland. Before he joined UQ, Peter was assistant professor at the Centre for Applied English Studies (CAES) at the University of Hong Kong. His areas of research and supervisory expertise include corpus linguistics and the use of corpora for language learning (known as data-driven learning), as well as English for General and Specific Academic Purposes. Peter is the author of the monograph Learning the language of Dentistry: Disciplinary corpora in the teaching of English for specific academic purposes which is part of Benjamins’ Studies in Corpus Linguistics series (with Lisa Cheung, published 2019), as well as the edited volumes Data Driven Learning for the Next Generation: Corpora and DDL for Pre-tertiary Learners (published 2019) and Referring in a second language: Reference to person in a multilingual world (with Jonathon Ryan, published 2020 with Routledge). Peter is also currently serving as the corpus linguistics section editor for Open Linguistics - an open access linguistics journal from De Gruyter, and he is on the editorial board of Applied Corpus Linguistics, a new journal covering the direct applications of corpora to teaching and learning.\n\n\n\n\nText Classification & Hate Speech (G. Wiedemann)\n\n\nText classification for automatic detection of hate speech, counter speech, and protest events\n\nThis talk was recorded Sep. 13, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/2oLJNYl_Ipw\nAdditional Resources\nText Mining Tutorials: https://tm4ss.github.io\nR package for active learning experiments: https://github.com/tm4ss/tmca.classify\nExperiment code for reproducing the experiments from the Journal Article Proportional Classification Revisited: https://github.com/tm4ss/tmca.classify-experiments\nAbstract\nSocial sciences have opened up to text mining, i.e., a set of methods to automatically identify semantic structures in large document collections. However, the methods have often been limited to a statistical analysis of textual data, strongly limiting the scope of possible research questions. The more complex concepts central to the social sciences such as arguments, frames, narratives and claims still are mainly studied using manual content analyses in which the knowledge needed to apply a category (i.e. to “code”) is verbally described in a codebook and implicit in the coder’s own background knowledge. Supervised machine learning provides an approach to scale-up this coding process to large datasets. Recent advantages in neural network-based natural language processing allow for pretraining language models that can transfer semantic knowledge from unsupervised text collections to specific automatic coding problems. With deep learning models such as BERT automatic coding of context-sensitive semantics with substantially lowered efforts in training data generation comes within reach to content analysis. The talk will introduce to the applied usage of these technologies along with two interdisciplinary research projects studying hate speech and counter speech in German Facebook postings, and information extraction for the analysis of the coverage of protest events in local news media.\n\n\nAbout Gregor\nGregor Wiedemann is working as Senior Researcher Computational Social Science at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI). Since September 2020, he heads the Media Research Methods Lab (MRML). His current work focuses on the development of methods and applications of natural language processing and text mining for empirical social and media research. Gregor Wiedemann studied political science and computer science in Leipzig and Miami, USA. In 2016 he received his doctorate from the Department of Computer Science at the University of Leipzig for his thesis on automation of discourse and content analysis using text mining and machine learning methods. Afterwards he worked as a postdoc in the NLP group of Computer Science Department at the University of Hamburg. Among other things, the resulting works are concerned with unsupervised information extraction to support investigative research in unknown document collections (see newsleak.io) and with the detection of hate and counter-speech in social media.\n\n\n\n\nVARIENG (Nevalainen, Hiltunen & Liimatta)\n\n\nIntroducing the VARIENG research unit in Helsinki \n\nThis talk was recorded Sep. 21, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/rDuYGQkAtQg\nAdditional Resources\nVARIENG website: https://www2.helsinki.fi/en/researchgroups/varieng\nCorpus Resource Database (CoRD): https://varieng.helsinki.fi/CoRD/\nStudies in Variation, Contacts and Change in English: https://varieng.helsinki.fi/series/\nAbstract\nOur presentation falls into three parts. It begins with Terttu’s brief introduction to the Research Unit for Variation, Contacts and Change in English (VARIENG), its past, present and future, and its current open-access resources. In addition, the talk presents two short cases studies touching on the methodology of corpus linguistics from different perspectives. The first one (by Turo) discusses how the availability of massive text archives may hold great promise for corpus linguistic work, but they may also present considerable methodological challenges for users (see e.g. Hiltunen, McVeigh, and Säily). In focus here are some specific problems related to the diachronic British Library Newspapers database, and how those problems might be addressed in the context of register analysis and the study of linguistic variation. The second case study (by Aatu) looks into the role of text length in register-internal variation by analyzing a big data sample of social media comments. Different registers seem to exhibit different kinds of internal functional variation by text length (as shown by the illustration).\n\n\nAbout Terttu\nTerttu Nevalainen is the Director of the VARIENG Research Unit. Her research interests include historical sociolinguistics, variation studies, corpus linguistics and digital humanities. She is one of the designers and compilers of the Helsinki Corpus of English Texts and of the Corpus of Early English Correspondence.\n\n\nAbout Turo\nTuro Hiltunen works as senior lecturer in English at the Department of Languages, University of Helsinki, Finland. His research interests include corpus linguistics and register analysis, and the grammatical and phraseological variation of scientific English past and present, compilation of specialised corpora, and democratisation in the context of parliamentary discourse.\n\n\nAbout Aatu\nAatu Liimatta is researching register variation on social media for his PhD. His interests include register and functional variation, computational methods, and big linguistic data.\n\n\n\nAntConc 4.0 (L. Anthony)\n\n\nAn Introduction to AntConc 4: Developing a general-purpose corpus toolkit for a broad user base\n\nThis talk was recorded Sep. 27, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/F_eHRWxCZnw\nAdditional Resources\nLaurence Anthony’s website (software subpage)\nAbstract\nAntConc is a widely used desktop corpus tool that has been downloaded over 2.5 million times since its first release in the early 2000s. Today, it is used by researchers, teachers, and learners in over 140 countries and its tutorial videos have been viewed over 500,000 times. AntConc has a relatively easy-to-use design and works especially well with small corpora of under a few million words. However, it begins to struggle when processing larger corpora and offers relatively few statistics for advanced analysis. In this talk, I will introduce a new version of AntConc (version 4.0) that has been built from the ground up to addresses these limitations. The new version includes features that allow the software to be easily extended, and it produces results in a way that allows for smooth data interoperability with other tools and scripts. I will also discuss various issues that need to be considered when developing software for a broad user base, which should be of interest to both users and developers of software tools.\n\nAbout Laurence\n\nLaurence Anthony is Professor of Applied Linguistics at the Faculty of Science and Engineering, Waseda University, Japan. He has a BSc degree (Mathematical Physics) from the University of Manchester, UK, and MA (TESL/TEFL) and PhD (Applied Linguistics) degrees from the University of Birmingham, UK. He is the current Director of the Center for English Language Education in Science and Engineering (CELESE), which runs discipline-specific language courses for the 10,000 students of the faculty. His main research interests are in corpus linguistics, educational technology, and English for Specific Purposes (ESP) program design and teaching methodologies. He received the National Prize of the Japan Association for English Corpus Studies (JAECS) in 2012 for his work in corpus software tools design.\n\n\n\n\nDistributional Semantics (G. Desagulier)\n\n\nDoing diachronic linguistics with distributional semantic models in R\n\nThis talk was recorded Sep. 30, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/Cof8lWdlRqI\nResources:\nLink to the Notebook\nLink to Guillaume’s blog Hypotheses\nLink to the latest issue of CogniTextes\nAbstract\nComputational linguistics offers promising tools for tracking language change in diachronic corpora. These tools exploit distributional semantic models, both old and new. DSMs tend to perform well at the level of lexical semantics but are more difficult to fine-tune when it comes to capturing grammatical meaning.\nI present ways in which the above can be improved. I start from well-trodden methodological paths implemented in diachronic construction grammar: changes in the collocational patterns of a linguistic unit reflect changes in meaning/function; distributional word representations can be supplemented with frequency-based methods. I move on to show that when meaning is apprehended with predictive models (e.g. word2vec), one can trace semantic shifts with greater explanatory power than with count models. Although this idea may sound outdated from the perspective of NLP, it actually goes great ways from the viewpoint of theory-informed corpus linguistics.\nI illustrate the above with several case studies, one of which involves complex locative prepositions in the Corpus of Historical American English. I conclude my talk by defending the idea that NLP, with its focus on computational efficiency, and corpus-linguistics, with its focus on tools that maximize data inspection, have much to gain from getting closer.\n\n\nAbout Guillaume\nGuillaume Desagulier is Associate Professor of Linguistics at Paris 8 University, France, and a researcher at the MoDyCo laboratory of the University of Paris Nanterre. He is the recipient of a 5-year honorary position at the Institut Universitaire de France, a division of the French Ministry of Higher Education that distinguishes university professors for their research excellence.\nGuillaume’s research interests are at the crossroads of cognitive linguistics and corpus linguistics. More specifically, he uses corpus-linguistics and statistical techniques to test usage-based hypotheses. He has published on modality, evidentiality, and intensification from a construction-grammar perspective.\nIn 2017 Guillaume published a reference textbook on corpus linguistics with R: Corpus Linguistics and Statistics with R (New York: Springer). The same year, he opened a research blog: Around the word, A corpus linguist’s notebook, where he has since recorded reflections and experiments on his practice as a usage-based corpus linguist (https://corpling.hypotheses.org/).\n\n\nUsage-Based Language Learning (L. Janda)\n\n\nStrategic targeting of rich inflectional morphology for linguistic analysis and L2 acquisition\n\nThis talk was recorded Oct. 7, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/FP8x1Euf7SI\nResources:\nSMARTool: SMARTool is a language-learning software tool assisting English-speaking learners of Russian.\nTROLLING is a repository of data, code, and other related materials used in linguistic research. The repository is open access, which means that all information is available to everyone. All postings are accompanied by searchable metadata that identify the researchers, the languages and linguistic phenomena involved, the statistical methods applied, and scholarly publications based on the data (where relevant).\nAbstract\nMany languages have rich inflectional morphology signaling grammatical categories such as case, number, tense, etc. Rich morphology presents a challenge for L2 learners because even a basic vocabulary of a few thousand words can entail mastery of over 100,000 word forms. However, only a handful of the potential forms of a given word occur frequently, while the remainder are rare. Access to digital corpora makes it possible to determine which forms of any given word are of highest frequency, as well as what grammatical and collocational contexts motivate those few frequent forms, facilitating strategically focused language learning tools. Corpus analysis of the frequency distributions of inflectional forms provide linguists with added insights into the function of languages. The results achieved primarily by using correspondence analysis of Russian material are potentially portable to any language with rich inflectional morphology.\n\n\nAbout Laura\nIn the Cold War era, Laura Janda combined study of Slavic linguistics at Princeton and UCLA with US-government-funded adventures as an exchange student behind the Iron Curtain in countries that have since changed their names: USSR, Czechoslovakia, and Yugoslavia. After over two decades at the University of Rochester and UNC-Chapel Hill, she moved to the University of Tromsø in 2008. Laura Janda was an early adopter of Cognitive Linguistics in the 1980s and has explored quantitative methods since 2007. Her research focuses primarily on the morphology of Slavic languages, with various admixtures (North Saami, conlangs, political discourse).\n\n\n\n\nAnalyzing Historical Publications (T. Säily)\n\n\nLanguage variation and change in eighteenth-century publications and publishing networks\n\nThis talk was recorded Oct. 15, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/B3qar4mYE4k\nResources:\nHelsinki Computational History Group\nResearch Unit for Variation, Contacts and Change in English\nProject website: https://blogs.helsinki.fi/c18-publishing/\nAbstract\nThis talk introduces our new project, Rise of Commercial Society and Eighteenth-Century Publishing, by discussing its premises and a pilot study. Combining historical sociolinguistics with intellectual history, book history and data science, the project studies the eighteenth-century print media and publishing networks that enabled the rise of commercial society and its conceptualization in the eighteenth-century Anglophone world. The chief focus is on Scottish, transatlantic and French influences on British print media.\nA key innovation in our project is that we connect the study of eighteenth-century publishing networks with that of language variation and change to gain results that are of interest to both linguists and historians. To do this, we link enriched bibliographic metadata with full-text sources for historical-sociolinguistic analysis. Our main interest is in charting the use and spread of new vocabulary in the networks to better understand how the discourse develops over time. Instead of focusing on well-known authors, we will identify influencers in a data-driven way; these can also be printers or publishers. By combining network analysis with text mining and social metadata on the actors, we are able to conduct large-scale analyses of how linguistic and stylistic changes spread across social groups in eighteenth-century public discourse.\n\n\nAbout Tanja\nTanja Säily is a tenure-track assistant professor in English language at the University of Helsinki. Her research interests include corpus linguistics, digital humanities, historical sociolinguistics, and linguistic productivity. She is also interested in the social embedding of language variation and change in general, including gendered styles in the history of English and extralinguistic factors influencing language change. Her overarching aim is to develop new ways of understanding language variation and change, often in collaboration with experts from other fields. Her current project combines historical sociolinguistics, intellectual history, book history and data science to analyse eighteenth-century publications and publishing networks.\n\n\n\n\nGit and GitHub (S. Guillou)\n\n\nGit and GitHub/Gitlab for versioning and collaborating\n\nThis talk was recorded Oct. 18, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/MbRNbJUY0aE\nResources: GitHub repo for this webinar\nAbstract\nGit is a tool for versioning of and collaborating on any text-based file. Widely used in software development, it is now adopted in many different settings, from document versioning to data analysis management. It is also at the centre of major platforms like GitHub and GitLab, used by millions to share and collaborate on code and documents.\nIn this workshop, you will learn about: * The main commands used in a git workflow\n\nHow to publish your work online\nHow to collaborate on a GitHub repository\n\nIf you would like to follow along, please do the following before attending:\n\nInstall Git on your computer (here are OS-specific instructions)\nCreate a GitHub account (or GitLab if you want an alternative)\n\n\n\nAbout Stéphane\nStéphane Guillou has worked for the last 10 years at the University of Queensland (UQ). After completing a master’s degree in plants science and ecology in France, he worked in research around the topic of sustainable agriculture. In 2018, a drastic move to a Technology Trainer position at the Library allowed him to share data analysis best practice skills, and promote Open Source tools for research. He is motivated by the principles of Open Science and the opportunities an increasingly collaborative research ecosystem offers.\n\n\n\nReplicability & Robustness (J. Flanagan)\n\n\nReproducibility, Replicability, and Robustness\n\nThis talk was recorded Oct. 28, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/1zaKr2B2YBU\nResources: Reproducible research: Strategies, tools, and workflows\nAbstract\nScientific progress has long rested on the often unstated assumptions that research is reproducible (i.e., independent analysts can recreate the results claimed by the original authors by using the original data and analysis techniques), replicable (the results claimed by analysts extends beyond the original data to some wider population or phenomenon), and robust (the findings reported by analysts are not ovely sensitive to assumptions in their model). Recently, however, there have been growing concerns about the extent to which current research practices can meet these assumptions. In this talk, I’ll present a high-level discussion of these issues, defining the key terms, demonstrating how and why questions have been raised about why work may not be as reproducible, replicable, or robust as we may wish, and offer some tentative suggestions for and examples of improving the reproducibility, replicability, and robustness of linguistic research in the future.\n\n\nAbout Joe\nJoseph Flanagan is a University Lecturer in Languages/English Studies at the University of Helsinki where he is also supervisor for the doctoral program in Philosophy, Arts, and Society. Joe’s research interests primarily center on issues related to English phonetics and phonology, reproducible research, and the digital humanities. Teaching-wise, he is especially interested in exploring how digital technology can enhance student learning.\nJoe has presented, given talks, and co-organized several workshops on reproducibility and replicability in (corpus) linguistics at international conferences such as the 6th Meeting of the International Society for the Linguistics of English (ISLE6) and ICAME42.\n\n\n\n\nLinguistic Phylogenetics (J. Macklin-Cordes & E. Round)\n\n\nPhylogenetic comparative methods: What all the fuss is about, and how to use them in everyday research\n\nThis talk was recorded Nov. 4, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/yJeQ6AGDLK0\nResources: LADAL tutorial on Phylogentics for language data\nAbstract\nIn this talk, we attempt two tasks. Firstly, we dispel some of the mystery around phylogenetic comparative methods and highlight their fundamental relationship to matters of enduring concern in linguistic typology. Secondly, we aim to show how linguists can carry out essential tasks using phylogenetic methods, easily.\nWe begin by laying out the historical commonalities between typology and comparative biology, and the breakthrough insight that makes phylogenetic comparative methods distinct. This is followed by an overview of some fundamental phylogenetic concepts and tools. Finally, we illustrate these with a typological case study from the Pama-Nyungan languages. Our talk is accompanied by online interactive materials, demonstrating how phylogenetic comparative methods can be incorporated into everyday typological workflows.\n\n\nAbout Jayden\nJayden Macklin-Cordes is a postdoctoral researcher in linguistics at the CNRS Dynamics of Language Lab, Lumière University Lyon 2. He recently completed his PhD at The University of Queensland as a member of the Ancient Language Lab. Jayden specialises in historical linguistics and typology, particularly using phylogenetic and quantitative methods.\n\n\nAbout Erich\nErich Round is British Academy Global Professor of linguistics at the Surrey Morphology Group, University of Surrey, UK, and director of the Ancient Language Lab at the University of Queensland. His research focuses on linguistic evolution and typology, especially in the domains of morphology and phonology. Erich also specialises in the Tangkic languages of Queensland and the phonologies of Australian Indigenous languages.\n\n\n\nAnalyzing Emigrant Letters (C. P. Amador-Moreno)\n\n\nTracking Irish English through a corpus of emigrants’ letters\n\nThis talk was recorded Nov. 11, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/YBC_y12bcF8\nAbstract\nIn recent years, linguists have focused on reconstructing earlier regional and social varieties of English through the quantitative analysis of emigrant letters and other written documents. As demonstrated in these studies this type of material can provide important, quantifiable data on certain features proper to a particular variety, thus allowing for the reconstruction of some of the features proper to that variety prior to the collection of spoken data.\nThe aim of this talk is to present the Corpus of Irish English Correspondence (CORIECOR), a corpus of personal letters written between 1750-1940. The corpus contains some 4700 texts (approx. 3 million words), of which 4100 (2.5m words) are correspondence maintained between Irish emigrants and their relatives, friends and contacts. The letters were sent mainly between Ireland and other countries such as the United States, Canada, Great Britain, New Zealand, Argentina and Australia, and therefore provide an empirical base for studies of historical change in Irish English and its contribution to other major overseas varieties. In order to explore the use of a corpus like this for the study of Irish English, the talk will show some recent findings.\n\n\nAbout Carolina\nCarolina P. Amador-Moreno is Professor of English Linguistics at the University of Bergen. She has held different teaching positions at the University of Extremadura (Department of English), the University of Limerick (Department of Languages and Cultural Studies), and University College Dublin (English Department). Her research interests centre on the English spoken in Ireland and include historical linguistics, stylistics, discourse analysis, corpus linguistics, sociolinguistics, and pragmatics. Her publications include articles and chapters dealing with these topics. She is the author, among others, of Orality in written texts: Using historical corpora to investigate Irish English (1700-1900), Routledge (2019); An Introduction to Irish English, Equinox (2010); the co-edited volumes Irish Identities: Sociolinguistic Perspectives (Mouton de Gruyter, 2020); Voice and Discourse in the Irish Context (Palgrave-Macmillan, 2017); Pragmatic Markers in Irish English (John Benjamins, 2015). She’s an associate member of CALS (Centre for Applied Language Studies), IVACS (Inter-Variational Applied Corpus Linguistics network), both at the University of Limerick, and LINGLAP (the Research Institute for Linguistics and Applied Languages), at the University of Extremadura, which she was Director of until August 2020.\n\n\n\nAARNet & CloudStor (S. King)\n\n\nAARNet, CloudStor and SWAN: easy storing and sharing of research data and resources\n\nThis talk was recorded Nov. 18, 2021, as part of the LADAL Opening Webinar Series 2021.\nRecording on YouTube: https://youtu.be/YfiV4QWzS6I\nResources: https://jupyterbook.org/intro.html\nAbstract\nThis webinar introduces AARNet, Australia’s Academic Research Network, and CloudStor, a research-specific sync, store and share platform. Discover more about the CloudStor interface and its associated tools and services for managing active research data. Learn how to organise, maintain, store and analyse active data, and understand safe and secure ways of sharing and storing data. This session will also introduce SWAN, the Service for Web-based Analysis, and Jupyter Notebooks, a digital tool that has exploded in popularity in recent years for those working with data. This will be an introductory session for those who are brand new, have little or no knowledge of coding and computational methods in research, but would like to know more about how to get started.\n\n\nAbout Sara\nSara King is the Training and Engagement Lead at Australia’s academic and research network provider, AARNet. She has extensive experience in engagement and training, with expertise in research data and technologies in the Humanities and Social Science (HASS) research areas. Prior to eResearch she worked for almost a decade at the National Archives of Australia and a few years in a public library. She has a PhD in Migration Studies and is a little bit obsessed with the idea of knitting as a form of coding.\n\n\nBack to top\nBack to LADAL home"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Language Technology and Data Analysis Laboratory (LADAL)",
    "section": "",
    "text": "WELCOME\n\nWelcomes to the website of the Language Technology and Data Analysis Laboratory (LADAL). LADAL (pronounced lah’dahl) is a free, open-source, collaborative support infrastructure for digital and computational humanities established 2019 by the School of Languages and Cultures at the University of Queensland. LADAL aims at assisting anyone interested in working with language data in matters relating to data processing, visualization, and analysis and offers guidance on matters relating to language technology and digital research tools. To this end, LADAL offers introductions to topics and concepts related to digital and computational humanities, online tutorials, interactive Jupyter notebooks, and events including workshops and webinar series.\n\nLADAL is part of the Language Data Commons of Australia (LDaCAATAP) and the Australian Text Analytics Platform (ATAP). The aim of LDaCA and ATAP is to provide researchers with a Notebook environment – in other words a tool set - that is more powerful and customisable than standard packages, while being accessible to a large number of researchers who do not have strong coding skills.\nThe Language Data Commons of Australia (LDaCAATAP) and the Australian Text Analytics Platform (ATAP) projects received investment (https://doi.org/10.47486/PL074) from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS).\n\n\nThe Australian Text Analytics Platform (ATAP) projects received investment (https://doi.org/10.47486/PL074) from the Australian Research Data Commons (ARDC). The ARDC is funded by the National Collaborative Research Infrastructure Strategy (NCRIS).\n\n\n\n\nSince going public January 1, 2021, LADAL has received almost 1,100,000 page views of more than 500,000 active users in nearly 750,000 engaged sessions! The majority of LADAL users access the LADAL website from the USA (app. 20%), Great Britain and Germany (app. 6%), Australia and China (app. 5%), India (4%), and the Netherlands (3%). The highest number of users per day was May 18, 2022 with 981 users.\n\n\n\n\n\n\n\n\n\nGet in Touch!\nTo get in touch with us here at LADAL, maybe because you are interested in becoming a contributor or because you found an error on our site, you can simply write an email to ladal@uq.edu.au, reach out to us on Twitter (@slcladal), send us a message on Facebook (see our Facebook page or you can sign up to the LADAL email list. To subscribe to our email list, simply write an email to ladal@uq.edu.au with the subject email list.\n\n\nGoals\nThe LADAL aims to help develop computational and digital skills by providing information and practical, hands-on tutorials on data and text analytics as well as on statistical methods relevant for language research. In addition, the LADAL provides self-guided study materials relevant for computational Natural Language Processing. In order to be attractive to both beginners and people with advanced skills, the LADAL website covers topics and introduces methods relevant for people coming with different degrees of prior knowledge and experience - ranging from introductions to concepts of quantitative reasoning to step-by-step guides on advanced statistical modeling.\nSince the primary concern of the LADAL is to introduce computational methods that are relevant to research involving natural language, the focus of this website is placed on linguistic data and methods relevant for text analytics. As such, the LADAL provides resources for (computational) text analytics and offers introductions to quantitative reasoning, research designs, and computational methods including data visualization and statistics. The areas covered on the LADAL website are\n\n\n\nintroductions to quantitative reasoning and basic concepts in empirical language studies.\nintroductions to R as programming environment for processing natural language data.\ntutorials on data visualization and data analytics (statistics and machine learning).\ntutorials on text analysis, text mining, distant reading, and corpus linguistics.\n\n\nThe resources and events offered by LADAL also aim at promoting and informing about Best Practices in data handling such as transparency, reproducibility and the FAIR principles (data should be findable, accessible, interoperable, and reusable).\n\n\n\nUser Stories\nBelow are selected user stories of people that have used LADAL resources in their research, training, or teaching.\n\n\n\nUSER STORIESWe are currently looking for user stories (also known as testimonials) to see and show what people use LADAL resources for. If you have used LADAL resources - be it by simply copying some code, attending a workshop, learning about a method using a tutorial, or in any other way - we would be extremely grateful, if you would send us your user story! To submit your user story, simply write up a paragraph describing how you have used LADAL resources and what you have used them for and send it to ladal@uq.edu.au. We really appreciate any feedback from you about this!\n\n\n\n\n\nPaula Rautionaho (University researcher, University of Eastern Finland)\nI learned about the LADAL website at a conference and since then I’ve been going through the contents bit by bit, starting with R and Data science basics and the many tutorials available. The website is great for acquiring basic knowledge, and I’ve also used it to find information on specific methods that I need for my research. The way the code is explained in detail and exemplified through actual studies, and the fact that the code is downloadable from the site, are extremely useful and helpful. What I usually do is download the string of code, go through each line to understand what’s going on and then modify it to my needs. The website also helps in understanding the output of statistical analyses, which for me is what sets this resource apart from many others.\n\n\n\n\nLaura Janda (Professor of Russian, The Arctic University of Norway, Tromsø)\nLADAL is a tremendously valuable resource that I recommend to all my students in my Quantitative Methods in Linguistics course. Given the broad portfolio of various courses that I teach plus my numerous other commitments, combined with the rapid pace of developments in both R itself and its application to linguistic analyses, it is not possible for me to keep apace with all of the developments all of the time. It is very important to have an authoritative and comprehensive resource that represents current best practices in the field, and that is exactly what LADAL is.\n\n\n\n\nRobert Daugs (Postdoctoral Researcher, University of Kiel)\nIt’s great to see how the content has evolved and it seems that whenever I come across a method I hear about in another talk or read in a paper and wish to implement in my own research (or just try it out), a corresponding script with meaty instructions is already available at LADAL. The tutorial I probably came back to more than once actually covers mixed-effects regression modeling. Given this method’s value in corpus-based, variationist linguistics and elsewhere, I think it’s great that this tutorial made the cut and I’m looking forward to any further updates the LADAL crowd might have planned for this.\nThanks for such a wonderful, open access resource!\n\n\n\n\nTouba Warsi (Analyst - School of Medicine, University of California San Diego)\nI was very happy to find the text analysis tutorial (https://slcladal.github.io/textanalysis.html) for text analysis in R. Up-to-date and super helpful! I use it for mining open-ended student survey comments with the hope of identifying themes, glean feedback without resorting to human coding of the comments.\nPlease keep up the great work!\n\n\n\n\nSimone Griesser (Senior Research and Teaching Fellow, FHNW University of Applied Sciences and Arts Northwestern Switzerland)\nDear LADAL Team\nI hope you are well. Thanks for your wonderful website. I find it tremendously useful. Following your call for user stories I am e-mailing you.\nI have used LADAL’s POS-tagging in German for a project which aims to develop language patterns to recognise brand personality in LinkedIn and Facebook posts. The project is for a social media company advising SME’s on their social media strategy and activities. The rationale for using word types as a language characteristic is that active and dynamic brand personalities should use more verbs, especially active verbs. Community-focused brands should use more personal and possessive pronouns. Brand personalities focussing on competency with facts and figures should use more adjectives.\nThe language patterns will include other language characteristics such as language arousal, dominance, concreteness as well as key words specific to the different brand personality types. If feasible I also like to include tenses in German.\n\n\n\n\nAudience\nThe LADAL resources are aimed at researchers in HASS (Humanities, Arts, and the Social Sciences) and we aspire to attract complete novices as well as expert users. And, while the focus of the LADAL website is placed on handling data that represents natural language, anyone who has an interest in quantitative methods, data visualization, statistics, or R is welcome to explore this webpage.\n\n\nAt LADAL, we aim to reach out and make our resources available to the research community and anyone interested Language Technology, Data Analysis, and using computational means to extract, process, visualize and analyze language data. To this end, we offer workshops, give presentations and talks, organize webinars (see, e.g., the LADAL Webinar Series 2021).\n\nIn addition, we provide resources on the LADAL website and on the LADAL YouTube channel, we announce updates on Twitter (@slcladal) as well as on our NEWS site and via our Facebook page. To get in touch, you can contact us on Twitter or send us an email via ladal@uq.edu.au.\n\n\n\n\nProgramming\n\nThe LADAL primarily uses the programming language R because R is extremely flexible, relatively easy to learn, free and open source, and R has a substantive and very friendly user community. R is not merely a software package but a fully-fledged programming environment which allows complex Natural Language Processing, statistics and data visualizations and it can also be used to create websites or apps, and has direct pipelines for version control (Git). This website as well as the self-guided study materials offered by the LADAL use are written in R-markdown - a way to combine R-code with text. The flexibility of R makes it a sensible choice for researchers that strive for high quality and extreme flexibility while following best practices that enable complete replicability and full transparency. If you want to learn more about R and why we use it, please check out our Why R? page.\n\nAs computation is becoming ever more prevalent across disciplines as well as in both the social and economic domains, the LADAL offers a resource space for R that make it accessible to lay users as well as expert programmers. That said, we will expand the resources provided by the LADAL to other tools and environments and include tutorials based on Python in the future.\n\n\n\nLicensing\nThe LADAL website was created by Martin Schweinberger. It was freely released under GNU General Public License, Version 3, June 2007.\n\n\n\nCitation\nIf you use (parts of) LADAL tutorials for your own research or in your teaching materials, please cite the individual subpages as shown at the bottom of each page or reference it as:\nSchweinberger, Martin. `2023. The Language Technology and Data Analysis Laboratory (LADAL). Brisbane: The University of Queensland, School of Languages and Cultures. url: https://slcladal.github.io/index.html (Version 2023.09.14).\n@manual{uqslc2023ladal,\n  author = {Schweinberger, Martin},\n  title = {The Language Technology and Data Analysis Laboratory (LADAL)},\n  note = {https://ladal.edu.au},\n  year = {2023},\n  organization = {The University of Queensland, School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2023.09.14}\n}\n\nDisclaimer\n\nThe content of this website is free and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute the content of LADAL resources given you adhere to the licensing. The content of this website is distributed under the terms of the GNU General Public License, Version 3, June 2007.\n\n\nShare and Enjoy!\n\nBack to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "ABOUT LADAL",
    "section": "",
    "text": "The LADAL is a collaborative effort that is sponsored by the School of Languages and Cultures at the University of Queensland. If you are interested in becoming an affiliate member or even a contributor contact the LADAL team via email (ladal@uq.edu.au).\n\n\n\nUSER STORIESWe are currently looking for user stories (also known as testimonials) to see and show what people use LADAL resources for. If you have used LADAL resources - be it by simply copying some code, attending a workshop, learning about a method using a tutorial, or in any other way - we would be extremely grateful, if you would send us your user story! To submit your user story, simply write up a paragraph describing how you have used LADAL resources and what you have used them for and send it to ladal@uq.edu.au. We really appreciate any feedback from you about this!\n\n\n\n\nDIRECTORS\nMartin Schweinberger\n\nMartin is Lecturer in Applied Linguistics at the School of Languages and Cultures at the University of Queensland, Australia as well as Associate Professor and Lab Director at the AcqVA-Aurora Center at Arctic University of Norway in Tromsø .\nMartin has initiated and established LADAL and is its leading proponent. His role at LADAL encompasses creating content, supervising team members, and directing the activities at LADAL. He is a language data scientist with a PhD in English linguistics specialized in corpus linguistics, computational analyses and visualization of linguistic data.\n\nMichael Haugh\n\nMichael is Full Professor of Linguistics and Applied Linguistics at School of Languages and Cultures, a Fellow of the Australian Academy of the Humanities, and project lead of the Australian Text Analytics Platform (ATAP) as well as the Language Data Commons of Australia (LDaCA)\nMichael is supervising and managing the activities of LADAL and overseeing its promotion. He is a long-standing proponent of Digital Humanities in Australia and globally with a special focus on data management and the accessibility and usability of language data\n\n\n\nCONTRIBUTORS | MEMBERS\nContributors are actively engaged in the LADAL and assist in the development of LADAL infrastructure or resources.\nLaurence Anthony\n\nLaurence Anthony is full professor in the Faculty of Science and Engineering, Waseda University, Japan.\nHis research is focused on language data science, educational technology, corpus linguistics, and science communication. He is also the CEO of AntLab Solutions, a startup company that provides educational software solutions and consulting services for individual researchers, academic institutions, small venture businesses, as well as national and multinational corporations.\nLaurence has written code in various languages with Python and Javascript being his languages of choice.\nBen Foley\n\nBen Foley was the project manager of CoEDL’s Transcription Acceleration Project (TAP).\nBen has specialized on speech recognition and the development of user-friendly speech recognition tools.\nBen’s previous experience with Aboriginal and Torres Strait Islander language resource development has resulted in apps and websites galore. Highlights include the Iltyem-iltyem sign language database and website, and the Gambay First Languages Map, showing the hundreds of languages in Australia.\nStefan Th. Gries\n\nStefan is full professor of linguistics in the Department of Linguistics at the University of California, Santa Barbara (UCSB), Honorary Liebig-Professor and Chair of English Linguistics at the Justus-Liebig-Universität Giessen, Germany.\nHis research is mainly situated at the intersection of quantitative corpus linguistics, cognitive linguistics, computational linguistics, and L1/Ln acquisition. Stefan’s research also uses experimental methods.\nStefan is a major proponent of using the open source software R in language data science.\nAndreas Niekler\n\nAndreas Niekler is a research associate in Computer Science at the University of Leipzig and he develops computer-based methods in the field of semantic properties in language and language-based AI.\nHe develops computer-based algorithmic methods for computational social science, including the Postdemocracy and Neoliberalism project and the interactive analysis platform Leipzig Corpus Miner (iLCM).\nThe focus is on machine learning methods and data management. This includes processing of unstructured data for knowledge and document management.\nGregor Wiedemann\n\nGregor is a Senior Researcher in Computational Social Science at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI).\nTogether with Sascha Hölig he heads the Media Research Methods Lab (MRML) where he focuses on the development of methods and applications of natural language processing and text mining for empirical social and media research.\nAfter studying in Leipzig and Miami, USA, Gregor worked at the Computer Science departments of the universities of Leipzig and Hamburg.\n\nErich Round\n\nErich is a British Academy Global Professor in linguistics at the University of Surrey, UK.\nHis research is in phonology and morphology, especially of Australian Indigenous languages, and the modeling of language evolution and diversification.\nHe is an active creator and analyzer of large scale cross-linguistic data sets, for investigating the nature and origins of linguistic diversity.\n Stephane Guillou\n\nStephane has worked for the last 10 years at the University of Queensland (UQ) and he is the leading proponent of computational up-skilling at the UQ library\nAfter completing a master’s degree in plants science and ecology in France, he worked in research but moved to a Technology Trainer position at the UQ Library in 2018 that allowed him to share data analysis best practice skills, and promote Open Source tools for research.\nHe has extensive experience in several programming environments and teaching courses on computation at various levels of competence\nJoeseph Flanagan\n\n\nJoe is a Senior Lecturer and computational linguist at the University of Helsinki\nHis research interests primarily center on issues related to English phonetics and phonology, reproducible research, and the digital humanities.\nTeaching-wise, he is especially interested in exploring how digital technology can enhance student learning. (Joe’s GitHub repo)\n\n\n\n\nAFFILIATE MEMBERS\nAffiliate members support the LADAL and are informed about events, workshops, and training opportunities at LADAL.\nGerold Schneider (University of Zurich, Switzerland)\nMonika Bednarek (University of Sydney, Australia)\nLaurence Anthony (Waseda University, Japan)\nPeter Crosthwaite (The University of Queensland, Australia)\nSimon Musgave (Monash University, Australia)\n\n\n\nCOLLABORATIONS\nCollaborating institutions and organizations support LADAL and share information or resources with LADAL.\n\n\nThe School of Languages and Cultures at the University of Queensland\n\n\n\nThe Text Crunching Center at the University of Zurich (UZH)\n\n\n\nThe Sydney Corpus Lab at The University of Sydney\n\n\n\nVARIENG at the University of Helsinki\n\n\n\nThe AcqVA Aurora Lab in the UiT Aurora Center for Language Acquisition, Variation & Attrition at The Arctic University of Norway in Tromsø. (AcqVA Aurora Lab’s GitHub repo)\n\n\n\nThe Media Research Methods Lab (MRML) at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI). .\n\n\n\n\nFORMER MEMBERS\nFormer members were engaged with LADAL but have taken up new positions, changed affiliations, or moved institutions resulting in parting trajectories.\nKatherine Dallaston\nRestuadi Restuadi\nKaty McHugh\nAlex Trueman\nDattatreya Majumdar\nStephen Kennedy-Clark\nLiam Crowhurst\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "TUTORIALS",
    "section": "",
    "text": "This page presents the tutorials provided by LADAL. To access a tutorial, click on its title, and you will be directed to the respective website.\n\nData Science Basics\n\nWorking with Computers\nThis tutorial provides advice and general tips on how to keep your computer clean and running smoothly, how to organize files and folders, and how to store your data safely and orderly.\nData Management and Reproducibility\nThis tutorial introduces basic data management techniques, version control measures, and issues relating to reproducible research.\nIntroduction to Quantitative Reasoning\nThis tutorial takes a philosophical or history-of-ideas approach and introduces the logical and cognitive underpinnings of the the scientific method.\nBasic Concepts in Quantitative Research\nThis tutorial introduces basic concepts of data analysis and quantitative research.\n\n\nR Basics\n\nWhy R?\nThis site provides our reasoning for focusing (almost exclusively) on R in LADAL.\nGetting started\nThis tutorial shows how to get started with R and it specifically focuses on R for analyzing language data but it offers valuable information for anyone who wants to get started with R.\nLoading and saving data\nThis tutorial shows how you can load and save different types of data when working with R.\nString Processing\nThis tutorial introduces string processing and this can be used when working with language data.\nRegular Expressions\nThis tutorial introduces regular expressions and how they can be used when working with language data.\nHandling tables in R\nThis tutorial shows how to work with tables and how to tabulate data in R.\n\n\nData Visualization\n\nIntroduction to Data Viz\nThis tutorial introduces data visualization using R and shows how to modify different types of visualizations in the ggplot framework in R.\nData Visualization with R\nThis tutorial introduces different types of data visualization and how to prepare your data for different plot types.\nIntroduction to Geospatial Data Visualization\nThis tutorial introduces geo-spatial data visualization in R.\nInteractive Charts\nThis tutorial shows how to generate interactive data visualizations in R.\n\n\nStatistics\n\nDescriptive Statistics\nThis tutorial focuses on how to describe and summarize data in R.\nBasic Inferential Statistics\nThis tutorial introduces basic inferential procedures for null-hypothesis hypothesis testing.\nRegression Analysis\nThis tutorial introduces regression analyses (also called regression modeling) using R. Regression models are among the most widely used quantitative methods in the language sciences to assess if and how predictors (variables or interactions between variables) correlate with a certain response.\n\nTree-Based Models\nThis tutorial focuses on tree-based models and their implementation in R.\nCluster and Correspondence Analysis\nThis tutorial introduces classification and clustering using R. Cluster analyses fall within the domain of classification methods which are used to find groups or patterns in data or to predict group membership.\n\n\nIntroduction to Lexical Similarity\nThis tutorial introduces Text Similarity, i.e. how close or similar two pieces of text are with respect to either their use of words or characters (lexical similarity) or in terms of meaning (semantic similarity).\nSemantic Vector Space Models\nThis tutorial introduces Semantic Vector Space (SVM) modeling R. SVMs are used to find groups or patterns in data or to predict group membership.\nDimension Reduction Methods\nThis tutorial introduces selected dimension reduction methods (Principal Component Analysis, Factor Analysis, and Multidimensional Scaling) which allow to detect and evaluate structures, called components, latent variables, or factors, underlying observed variables.\nPower Analysis\nThis tutorial introduces power analysis using R. Power analysis is a method primarily used to determine the appropriate sample size for empirical studies.\n\n\nText Analytics\n\nIntroduction to Text Analysis\nThis tutorial introduces Text Analysis, i.e. computer-based analysis of language data or the (semi-)automated extraction of information from text.\nPractical Overview of Selected Text Analytics Methods\nThis tutorial showcases some basic but useful methods for text analysis and serves as a practical overview or introduction to Text analytics and distant reading.\nConcordancing (keywords-in-context)\nThis tutorial introduces how to find words or phrases in text and display concordances, a so-called keyword-in-context (KWIC) display, with R.\nCollocation and N-gram Analysis\nThis tutorial introduces collocation analysis and identifying N-grams with R and shows how to extract and visualize semantic links between words.\nKeyness and Keyword Analysis\nThis tutorial introduces keyness analysis and identifying keywords with R and shows how to visualize keywords.\n\nNetwork Analysis\nThis tutorial introduces network analysis using R. Network analysis is a method for visualization that can be used to represent various types of data.\nTopic Modeling\nThis tutorial introduces topic modeling using R.\nSentiment Analysis\nThis tutorial introduces sentiment analysis (SA) and shows how to perform a SA in R.\nTagging and Parsing\nThis tutorial introduces part-of-speech tagging and syntactic parsing using R.\nAutomated Text Summarization\nThis tutorial shows how to summarize texts automatically using R by extracting the most prototypical sentences.\nSpell Checking\nThis tutorial shows how to implement and use spell checking in R when working with text data.\n\n\nCase Studies\n\nClassifying American Speeches\nThis tutorial shows how to perform document classification using R. It was created by Gerold Schneider and Max Lauber for the Australian Text Analytics Platform (ATAP).\nCorpus Linguistics with R\nThis section presents different case studies or use cases that highlight how to do corpus-based analyses by implementing procedures shown in other LADAL tutorials.\nAnalyzing learner language using R\nThis tutorial focuses on learner language and how to analyze differences between learners and L1 speakers of English using R.\nLexicography and Creating Dictionaries with R\nThis tutorial introduces lexicography with R and shows how to use R to create dictionaries and find synonyms through determining semantic similarity in R.\nVisualizing and Analyzing Questionnaire and Survey Data\nThis tutorial offers some advice on what to consider when creating surveys and questionnaires, provides tips on visualizing survey data, and exemplifies how survey and questionnaire data can be analyzed.\n\nCreating Vowel Charts in R\nThis tutorial exemplifies how to create a vowel chart with Praat and R.\nComputational Literary Stylistics with R\nThis tutorial focuses on computational literary stylistics (also digital literary stylistics) and shows how fictional texts can be analyzed by using computational means.\nPractical phylogenetic methods for linguistic typology\nThis tutorial shows how you can do phylogenetic analysis in R.\nReinforcement Learning and Text Summarization in R\nThis tutorial introduces the concept of Reinforcement Learning (RL), and how it can be applied in the domain of Natural Language Processing (NLP) and linguistics.\n\n\nHow-Tos\n\nConverting PDFs to txt\nThis tutorial shows how to extract text from one or more pdf-files using optical character recognition (OCR) and then saving the text(s) in txt-files on your computer.\nDownloading Texts from Project Gutenberg\nThis tutorial shows how to download and clean works from the Project Gutenberg archive using R. Project Gutenberg is a data base which contains roughly 60,000 texts for which the US copyright has expired.\nWebcrawling with R\nThis tutorial shows how to crawl and download web data using R, including link following and text extraction.\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#types-of-hypotheses",
    "href": "tutorials/basicquant/basicquant.html#types-of-hypotheses",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Types of Hypotheses",
    "text": "Types of Hypotheses\nOn a very fundamental level, we can differentiate between null-hypotheses (H\\(_{0}\\)), that claim non-existence of either a state of being or a difference, and alternative or test-hypothesis (H\\(_{1}\\)) that claim or postulate the existence of of either a state of being or a difference. Among test-hypotheses, we can furthermore distinguish between non-directed hypotheses which claim that one sample is different from another sample, and directed hypotheses which claim that a feature of one sample is bigger, smaller, more frequent, or less frequent, etc. Thus, a hypothesis that group A will perform better in an exam is a directed test-hypothesis while an non-directed hypothesis would merely claim that they differ in their test results. In contrast, the null-hypothesis would claim that there is no difference between the groups in terms of their performance in that exam.\nAn additional distinction among hypotheses is the difference between deterministic and probabilistic hypotheses. While we are dealing with a deterministic hypotheses in (10) because it is a categorical claim, we are dealing with a probabilistic hypothesis in (11) because, here, the hypothesis simply claims that the likelihood of Y is higher if X is the case (but not necessarily categorically).\n\nIf the length of two words in an English phrase is different, then the shorter word will always proceed the longer word.\nIf the length of two words in an English phrase is different, then it is more likely for the shorter word to proceed the longer word than vice versa."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#why-test-the-h0",
    "href": "tutorials/basicquant/basicquant.html#why-test-the-h0",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Why test the H0?!",
    "text": "Why test the H0?!\nAlthough it is counter-intuitive, we do not actually test the test-hypothesis but we test the null-hypothesis. We will now have a closer look at how to formulate hypotheses and that formulating hypotheses is formulating expected outcomes/explanations in a formal description.\n\nNull hypothesis (H0) Groups A and B do not differ systematically! (\\(\\mu\\)A = \\(\\mu\\)B)\nTest hypothesis (H\\(_{1a}\\)) Groups A and B differ systematically! (\\(\\mu\\)A \\(\\neq\\) \\(\\mu\\)B; non-directed)\nTest hypothesis (H\\(_{1b}\\)) Group A has significantly better results/higher levels of x compared with group B. (\\(\\mu\\)A \\(&gt;\\) \\(\\mu\\)B; directed)\n\nWhat does that mean and what are we testing? In non-technical terms, we test how likely it is that the results came about by accident. If the probability is high (p &gt; .05) that the results happen to be random, then we do not discard the H0. If the likelihood is low (p &lt; .05) that the results came about randomly, then we discard the H0 and assume the H1 instead! To better understand this logic, we will discuss probabilities and their role in quantitative research.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nWhat four characteristics do hypotheses have?\n\n\n\nAnswer\n\nThey are are falsifiable (either true or false), they are testable, they are unambiguous, and they are inherently consistent (logically coherent).\n\n\nCome up with (a) three directed hypotheses and (b) three non-directed hypotheses.\n\n\n\nAnswer (examples)\n\nDirectedIn a 100 meter dash, Martin runs faster than Jack.In informal conversations, Martin uses fewer swearwords than Michael. Per minute, Martin can produce more words than Erich.Non-directedMartin and Michael differ in their use of swear words.Martin and Erich differ in their speech speed.Stefan and Martin differ in their need for sleep.\n\n\nOftentimes, it is not that easy to differentiate between hypotheses and other types of statements. Find a partner and come with statements that are not hypotheses and discuss why these statements are not hypotheses.\nFind a partner and come up with statements that can be classified as both hypotheses and non-hypotheses and be prepared to explain your reasoning to the group."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#research-designs",
    "href": "tutorials/basicquant/basicquant.html#research-designs",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Research Designs",
    "text": "Research Designs\nNow that the terms sample and population are clear, we can continue to think about how we could test hypotheses. In the empirical sciences, we try to answer questions we have by analyzing data. However, before we can attempt to answer a question and test if we are right with the answer we have in mind (our hypothesis), we have to decide on how to tackle the question. Thus, we have to come up with a plan about how to test our hypothesis. This plan is called a research design. The research design informs how to collect, process, visualize, and analyze the data. As such, the research design focuses on all aspects of research that are related to the data.\nThus, after comping up with a research question and a hypothesis, the next step typically consists in collecting the right type of data. There are different ways to collecting data and these different ways to collect data represent different research designs. We cannot go over all the different designs (and they can be combined, for instance, introspection is often a part of questionnaire designs), but we will focus and briefly discuss the most common ones.\nIn general, we can distinguish between experimental and observational designs. In contrast to observational designs (which can only confirm correlations, i.e. that X and Y occur together), experimental designs are more powerful in that they can determine causality (i.e., if X causes Y). We the most common research designs in the language sciences are\n\nexperimental\n\nexperimental designs (e.g., reaction times, eye-tracking)\n\nobservational\n\nbehavioral designs (e.g., corpora)\narchival/reviews (e.g., grammars, systematic literature reviews)\nintrospection (e.g., grammaticality judgements)\nquestionnaire designs (e.g., elicitation via surveys and questionnaires)\n\n\nThere are more designs(e.g. meta-studies) and also other classifications, but in the language sciences, I think that the ones listed above are the most frequent ones. Each of these designs has advantages and disadvantages which are briefly summarized in the table below.\n\ninstall.packages(\"e1071\")\ninstall.packages(\"flextable\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"ggpubr\")\ninstall.packages(\"graphics\")\ninstall.packages(\"grDevices\")\ninstall.packages(\"knitr\")\ninstall.packages(\"tidyverse\")\n\n\n\nTypeResearch designExampleDirectness of access to phenomenonCost/Labor intensityExternal validity (generalizability)Internal validityEase of data collection (e.g. access to participants)Experimental researchExperimentalReaction time measurementshigh/variablehigh/variablelow/variablehighlowObservational researchBehavioralCorpus studyhighlow/variablehighhighmediumArchival/ReviewSystematic literature reviewlowlowlowmediumhighIntrospectionGrammaticality judgementshighlowlow/variablehighlowQuestionnaireLanguage use surveymediumlowmediummediumhigh\n\n\nWe will now briefly focus on each design and discuss it.\n\nExperimental research\nExperimental designs can determine if there is a causal link between predictors and response which sets experimental designs apart from observational designs. Experiments typically take place in very well-controlled, but unnatural environments, .e.g, in laboratories.\nIn experimental designs, subjects are typically randomly assigned into test groups and control groups and the experimenter. If the subjects are not assigned randomly to test and control groups, the design is referred to as quasi-experimental (while experimental designs with randomized allocation of subjects are referred to as true experiments). The aim of experiments, in the most basic sense, is to determine if a result differs based on a change in some condition. The experimenter would change the condition only in the test group while not changing it in the control group while everything else remains constant. That way, it is possible to test if the change in the condition is responsible for the change in the outcome.\nDuring an experiment, subjects are shown stimuli (e.g., sentences, words, pictures) and the subjects (should) react to those stimuli showing a response (pressing buttons, providing judgments). The response does not have to be voluntary, meaning that responses can also simply be neuronal activity that the subject has no control over. In repeated measure designs, the same subjects are tested repeatedly while different subjects are tested in between-subjects designs.\nIn addition to the ability to establish causality, experimental designs allow a very direct access to a phenomenon and have high internal validity which means that experiments can test the phenomena in a very well-circumscribed and precise manner. Disadvantages of experimental designs are that they are typically rather costly and labor intensive and that the results are not necessarily generalizable due to the un-natural setting of most experiments (low external validity).\n\n\nQuasi-experimental research\nIn contract to experimental designs, subjects (or participants) in quasi-experimental designs are not randomly assigned to groups. Because of this, quasi-experimental designs can be affected by confounding (i.e., that changes in the dependent variable are caused by a variable that is not controlled for). As such, quasi-experimental designs are not experimental designs but can be considered observational. Examples for quasi-experimental designs are corpus-based variationist studies of the occurrence of linguistics variants in a variable context.\nQuasi-experimental designs encompass studies where subjects cannot be randomly assigned into test groups because either, this is not practically possible because the observation has already taken place or because the characteristic is inherent in the participants themselves. For instance, analyses of historical data can at best be quasi-experimental because the data have already been collected and the experimenter cannot assign participants to test groups. Also, some studies that analyze sex differences via the behavior of men and women cannot be experimental because participants cannot be randomly assigned to the male or female group.\n\n\nObservational designs\nIn the language sciences, observational designs encompass all forms of data acquisition and collection where the subjects are observed in their “natural habitat”. Hence, observational or behavioral designs are the most natural and thus have high external validity which means that we can assume that the results are transferable to the real world. The most common type of this design are corpus studies in which texts (which encompasses both written and transcribed spoken language) are searched for the occurrence of certain linguistic patterns and then correlated with some other feature, the use of a certain linguistic construction such as a specific suffix or a fixed expression like in order that in two different time periods or dialects.\nThe cost and labor intensity of corpus studies depends heavily upon if the corpus that you want to use has already been compiled - if the corpus is already available, the the costs and labor intensity are minimal but they can be high if the corpus has to be compiled first. However, even compiling a corpus can be comparatively easy and cheap, for example, if you compile a corpus consisting of a collection of texts that are available online but the costs can be substantive if you, for example, first need to record and transcribe spoken language in the field).\nThe problem with observational designs are that the contexts in which the data were collected were not controlled with means that the results are more likely to be affected by contextual or confounding factors that the researchers have no control over or don’t even know about.\n\n\nArchival research and reviews\nArchival research is more common in historical linguistics where researchers look for primary texts or records in archives. However, archival research also encompasses digital archives such as libraries which means that systematic literature reviews also fall under this research design.\nThe advantages of archival research designs are that the data collections is typically comparatively easy and associated with minimal costs. Also, depending on the phenomenon, the access to the phenomenon can be quite indirect, for example, entirely filtered through the eyes of eye witnesses or experts.\n\n\nIntrospection\nIntrospection has been the dominant form of “data collection” in the 20^th century and it is strongly connected to what is called armchair linguistics (linguists coming to insights about language by musing about what they can or would say and what they could or would not say). Introspection is, of course, by far the easiest and cheapest way to “collect data”. Unfortunately, introspection is subjective and can be heavily biased by the expectations and theoretical framework of subjects. Nonetheless, introspection can be and is extremely useful in particular regarding hypothesis generation and finding potential predictors for a certain linguistic behavior that can then be tested empirically.\nSince the 1990s and the rise of empirical linguistics, which is characterized by the employment of both experimental designs as well as corpus linguistics and increased use of questionnaire designs, introspection has been relegated to playing a part of other designs while only rarely being used as the sole source of data collection.\n\n\nQuestionnaire designs\nQuestionnaire designs represent any method of data collection where respondents are asked to provide (conscious) information to answers or ratings/evaluations to prompts (such as statements). The process to ask for information from respondents is called elicitation.\nIn addition to surveys which ask for socio-demographic information about speakers or about language use and language background information, questionnaires frequently ask respondents to rate their agreement with items, or the likelihood, frequency, or acceptability of items on Likert scales. A Likert scale, pronounced /lick.ert/ and named after the psychologist Rensis Likert, and often ranges from strongly disagree to strongly agree (or analogously from never to very frequently or extremely unlikely to extremely likely). Typically, respondents are given 5 or 7 options with the endpoints noted above representing the endpoints and respondents are asked to select the options that best matches their evaluation.\nA type of elicitation that is special in the language science is called discourse completion task or DCT. In DCTs, respondents are asked to imagine a certain discourse context or situation and are then asked how they would answer, respond, or express something. While DCTs have been shown to provide data that mirrors real-world behaviors, it is less reliable compared to corpus data as respondents cannot necessarily provide accurate information about their linguistic behavior.\nTo counter fatigue effects (respondents get tired or annoyed and do no longer answer to the best of their knowledge/ability), questionnaire items are often randomized so that their order differs between respondents. To counter what is called agreeability effects, researchers commonly have respondents rate test items (items that contain the phenomenon researchers are interested in) and fillers (items that do not contain what researchers are interested in). Fillers are included to disguise what the questionnaire aim to test so that respondents cannot (unconsciously) try to provide agreeable answers (the answers that researchers hope to get)."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#variable-types-scaling",
    "href": "tutorials/basicquant/basicquant.html#variable-types-scaling",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Variable Types (Scaling)",
    "text": "Variable Types (Scaling)\nIn the following, variable types (also referred to as scaling level or scaling) are discussed. It is very important to know which type of variables one is dealing with because the type of variable has affects many of the methods discussed, both in descriptive and in inferential statistics.\n\nNominal and Categorical Variables\nNominal and categorical variables only list the membership of a particular class. For nominal variables, there are exactly two occurrences (yes/no or on/off), while for categorical variables there can be several groups, e.g. the state in which someone was born.\n\n\nOrdinal Variables\nWith ordinal variables it is possible to rank the values, but the distances between the ranks can not be exactly quantified. An example of an ordinal variable is the ranking in a 100-meter dash. The 2nd in a 100-meter dash did not go twice as fast as the 4th. It is often the case that ordinal variables consist of integer, positive numbers (1, 2, 3, 4, etc.).\n\n\n(True) Numeric Variables\nThere are two basic types of numeric variables: interval-scaled variables and ratio-scaled variables. For interval scaled variables, the differences between levels are significant, but not the relationship between levels. For instance, 20 degree Celsius is not twice as hot as 10 degree Celsius.\nWith respect to ratio-scaled variables, both the differences and the relationship between the levels are significant. An example of this is the times in a 100-meter dash. For ratio-scaled variables, 10 is exactly twice as high as 5 and half as much as 20.\nIt is very important to keep in mind that both interval-scaled variables and ratio-scaled variables are numeric variables. This will play a role later because many tests can either only or never be applied to numeric variables.\n\n\nVariable TypeVariable LevelNominal Scaled / Categorical VariablesGender, Nationality, Part of SpeechOrdinal Scaled VariablesGraduation, School Grades, Military RankInterval Scaled VariablesTemperature, Acceptability JudgmentsRatio-scaled VariablesAge, Duration, Number of Syllables\n\n\nIt is enormously important to know variable types and levels, as the type of variable requires which tests are possible and which are not. For example, a \\(\\chi\\)2-test can only be applied to nominal or categorical variables, and a t-test to numeric variables only.\nIt is often necessary to translate variables into another type of variable. It should be noted that variables can only be transferred to variable types with less information content. The least informative variables are nominal, while the most informative variables are ratio scaled. The variable types thus form an implicit hierarchy:\nnominal/categorical &lt; ordinal &lt; interval/ratio\nHere is an example to illustrate this: let’s say you are investigating gender differences in the use of swear words in spoken Irish English and you find that you cannot use a linear regression or an ANOVA because too many speakers use no swear words (which violates the requirement that the errors must be normally distributed). In such a case, what one can do is to rank the speakers by their frequency of swear or curse words. Rank 1 would represent the speaker with the highest frequency of swear words, rank 2 would represent the speaker with the second highest frequency of swear words and so on. After you do this, you can, for example, use a Mann-Whitney U test to determine the relationship between the gender of speakers and their swear word ranking. You could also split the speakers into two groups (swear word users and non-swear-word users) and then perform a \\(\\chi\\)2-test of the frequencies of men and women in these groups. The important thing is that you cannot transform a categorical variable into a numeric variable as this would mean that you transform a less information-rich variable into a more information-rich variable.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nFor each of the variables listed below, consider how you could operationalize them and what kind of variables it would be.a. weather (cloudy, sunny, rainy, etc.)b. citizenshipc. Tense of matrix verbsd. Structural complexitye. word lengthf. Number of syllables in a wordg. The length of pauses in a sample of conversationsh. The appearance or non-appearance of finite verbs in a particular texti. Estimation of words on an acceptability scale from 1 to 5\n\n\n\nAnswer (examples)\n\nThere are other solutions so the following are just possible options!a. weather could, e.g., be operationalized as the number of sunny days or the amount of rain per year. Optimally, we would want weather to be operationalized as a numeric variable.b).citizenship would typically be operationalized as a categorical variable (e.g. German or Norwegian or Australian or other).c. This answer depends on the grammatical structure of the language in question. In English, we would probably operationalize the tense of matrix verbs as a nominal variable with the levels present and non-present.d. Structural complexity can be operationalized in many different ways, e.g., as the number of syntactic nodes, sentence length, number of phrases, average phrase length, etc. Optimally, we would want structural complexity to be operationalized as a numeric variable.e. Word length could,e.g. be operationalized as number of letters, number of phonemes, or the time it takes to produce that word. Thus, word length would typically be operationalized as a numerically scaled variable or an integer (which, in R, would be a type of numeric variable).f. The number of syllables per word would be operationalized as an integer (which, in R, would be a type of numeric variable).g. The length of pauses in a sample of conversations could be operationalized as the time of the pause (numeric) or even just as an ordered factor (short, middle, long). Optimally, however, we would want the length of pauses to be operationalized as a numeric variable.h. The appearance or non-appearance of finite verbs in a particular text would be a nominal variable (present versus absent).i. (The estimation of words on an acceptability scale from 1 to 5 represents judgements on a Likert scale which means that the resulting variable would represent an order factor (ordinal variable).\n\n\nAs you have seen from the above exercise, concepts can be operationalized differently. Find a partner and imagine that you are tasked with performing a study in which age of subjects is an important variable. Discuss with a partner how you would age. What advantages and disadvantages do the different operationalizations have?\nExample: When it rains, more people get wet than when it’s not raining. (If X, then Y)What is the dependent variable here and what is the independent variable?\n\n\n\nAnswer\n\nDependent variable: wet people Independent variable: rain\n\n\nWhich variable scales are time, rank, and name in the table below?\n\n\n\nNameRankTimeCarl Lewis19.86Ben Johnson29.97Steve Davis310.06\n\n\n\n\nAnswer\n\nName is categorical, Rank is ordinal, and Time is numeric.\n\n\nDiscuss with a partner: what obstacles might exist, so that a well operationalized variable has low extrinsic validity?\nConsider the following scenario: in a large representative study, shoe size is found to be an excellent predictor of intelligence. Given this result, find a partner and discuss if intrinsic validity is necessary?"
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#significance-levels",
    "href": "tutorials/basicquant/basicquant.html#significance-levels",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Significance Levels",
    "text": "Significance Levels\nBefore conducting a study, it is advisable to determine the so-called significance level or \\(\\alpha\\) level. This \\(\\alpha\\) level or level of significance indicates how high the p-value can be without having to assume that there is a significant relationship between the variables. It is customary to differentiate between three levels of significance (also called \\(\\alpha\\) levels):\n\np &lt; .001: highly significant - indicated by three stars (***)\np &lt; .01: very significant - indicated by two stars (**)\np &lt; .05: significant - indicated by one star (*)\n\nVariables with a p value that is smaller than .1 but larger than .05 are sometimes referred to as being marginally significant (+).\nVariables that are not significant are commonly referred to or labeled as n.s. (not significant). As we stated above, before we perform a test, we determine a value above which we reject the null hypothesis, the so-called significance level. It’s usually 5%. If the error probability is less than 5% (p &lt;. 05), we reject the null hypothesis. Conclusion: The relationship between the variables is statistically significant. It is important to note here that the H1 (or Test Hypothesis) is correct only because the null hypothesis can be rejected! Statistics can NEVER prove hypotheses but only reject Null Hypotheses which leads us to accept the H1 as preliminary accepted or not-yet-rejected. So all knowledge is preliminary in the empirical sciences."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#probability",
    "href": "tutorials/basicquant/basicquant.html#probability",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Probability",
    "text": "Probability\nIn the following, we will turn to probability and try to understand why probability is relevant for testing hypotheses. This is important at this point because statistics, and thus hypothesis testing, fundamentally builds upon probabilities and probability distributions. In order to understand how probability works, we will investigate what happens when we flip a coin. The first question that we will be addressing is “What is the probability of getting three Heads when flipping a coin three times?”.\nThe probability of getting three heads when flipping a coin three times is .5 to the power of 3: .53 = .5 times .5 times .5 = .125. The probability of getting Heads twice when flipping the coin three times is .375. How do we know?\nThe probability of getting 3 heads in tree tosses is 12.5 percent:\n.53 = .5 * .5 * .5 = .125\nThe probability of getting 2 heads in tree tosses is 37.5 percent:\n.125 + .125 + .125 = 0.375\nBut how do we know this? Well, have look at the table below.\n\n\n\nToss 1Toss 2Toss 3HeadsTailsProbabiltyHeadHeadHead300.125HeadHeadTails210.125HeadTailsHead210.125TailsHeadHead210.125HeadTailsTails120.125TailsHeadTails120.125TailsTailsHead120.125TailsTailsTails030.125\n\n\n\nGiven this table, we are in fact, in a position to calculate the probability of getting 100 heads in 100 coin tosses because we can simply fill in the numbers in the formulas used above: .5100 = 7.888609 * 10-31\nOkay, let us make a bet..\n\nIf head shows, I win a dollar.\nIf tails shows, you win a dollar.\n\nBut given that you know I am cheeky bastard, you do not trust me and claim that I will cheat. But how will you know that I cheat? At which point can you claim that the result is so unlikely that you are (scientifically backed) allowed to claim that I cheat and have manipulated the coin?\n\n\n\n\n\n\n\n\n\nSo before we actually start with the coin tossing, you operationalize your hypothesis:\n\nH0: The author (Martin) is not cheating (heads shows just as often as tails).\nH1: The author (Martin) is cheating (heads shows so often that the probability of the author not cheating is lower than 5 percent)\n\nWe now toss the coin and head shows twice. The question now is whether head showing twice is lower than 5 percent.\nWe toss the coin 3 times. Head shows twice. How likely is it that I do not cheat and head falls more than twice anyway? (In other words, what is the probability p that I win twice or more and not cheat?) If you set the significance level at .05, could you then accuse me of being a cheater?\nAs you can see in the fourth column, there are three options that lead to heads showing twice (rows 2, 3, and 4). If we add these up (0.125 + 0.125 + 0.125 = 0.375). Also, we need to add the case where head shows 3 times which is another .125 (0.375 + 0.125 = .5), then we find out that the probability of heads showing at least twice in three coin tosses is 50 percent and thus 10 times more than the 5-percent threshold that we set initially. Therefore, you cannot claim that I cheated.\n\n\n0 Heads1 Head2 Heads3 Heads0.1250.3750.3750.125\n\n\n\n\n\n\n\n\n\n\n\nWe can do the same and check the probabilities of having heads in 10 coin tosses - if we plot the resulting probabilities, we get the bar plot shown below.\n\n\n\n\n\n\n\n\n\nThe distribution looks more bell-shaped with very low probabilities of getting 0 or 1 as well as 9 or 10 times heads in the 10 coin tosses and a maximum at 5 times heads in 10 coin tosses. In fact, the probability of having 0 and 10 times head is .001 (or 1 in a 1000 attempts - one attempt is tossing a coin 10 times) - the probability of having heads once and 9 times is.01 or 1 in 100 attempts. In contrast, the probability of having 5 times head is .246 or about 25% (1 in 4 attempts will have 5 heads). Also, we can sum up probabilities: the probability of getting 8 or more heads in 10 coin tosses is the probability of getting 8, 9, and 10 times heads (.044 + .010 + 0.001 = .055 = 5.5 percent).\nCalculating the probabilities for three or even 10 coin tosses is still manageable manually but is there an easier way to calculate probabilities? A handier way is have a computer calculate probabilities and the code below shows how to do that in R - a very powerful and flexible programming environment that has been designed for quantitative analysis (but R can, in fact, do much more - this website, for instance, is programmed in R).\nThe code chunk below calculates the probabilities of having 0, 1, 2, and 3 times head in 3 tosses.\n\n# probabilities of  0, 1, 2 and 3 times head in 3 coin tosses\ndbinom(0:3, 3, 0.5)\n\n[1] 0.125 0.375 0.375 0.125\n\n\nThe code chunk below calculates the probabilities of having 2 or 3 times head in 3 tosses.\n\n# probabilities of  2 or 3 times head in 3 coin tosses\nsum(dbinom(2:3, 3, 0.5))\n\n[1] 0.5\n\n\nThe code chunk below calculates the probabilities of having 100 head in 100 tosses.\n\n# probability of  100 times head in 100 coin tosses\ndbinom(100, 100, 0.5)\n\n[1] 7.888609e-31\n\n\nThe code chunk below calculates the probabilities of having 58 or more times head in 100 tosses.\n\n# probability of  58 to a 100 times head in 100 coin tosses\nsum(dbinom(58:100, 100, 0.5))\n\n[1] 0.06660531\n\n\nThe code chunk below calculates the probabilities of having 59 or more times head in 100 tosses.\n\n# probability of  59 to a 100 times head in 100 coin tosses\nsum(dbinom(59:100, 100, 0.5))\n\n[1] 0.04431304\n\n\nThe code chunk below calculates the number of heads in 100 tosses where the probability of getting that number of heads or more sums up to 5 percent (0.05).\n\n# at which point does the probability of getting head\n# dip below 5 percent in 100 coin tosses?\nqbinom(0.05, 100, 0.5, lower.tail = FALSE)\n\n[1] 58\n\n\nLet’s go back to our example scenario. In our example scenario, we are dealing with a directed hypothesis and not with an un-directed/non-directed hypothesis because we claimed in our H1 that I was cheating and would get more heads than would be expected by chance (\\(\\mu_{Martin}\\) \\(&gt;\\) \\(\\mu_{NormalCoin}\\)). For this reason, the test we use is one-tailed. When dealing with un-directed hypotheses, you simply claim that the outcome is either higher or lower - in other words the test is two-tailed as you do not know in which direction the effect will manifest itself.\nTo understand this a more thoroughly, we will consider tossing a coin not merely 3 but 100 times. The Figure below shows the probabilities for the number of heads showing when we toss a coin 100 from 0 occurrences to 100 occurrences.\n\n\n\n\n\n\n\n\n\nThe next figure shows at which number of heads the cumulative probabilities dip below 5 percent for two-tailed hypotheses. According to the graph, if head shows up to 40 or more often than 60 times, the cumulative probability dips below 5 percent. Applied to our initial bet, you could thus claim that I cheated if head shows less than 41 times or more than 60 times (if out hypothesis were two-tailed - which it is not).\n\n\n\n\n\n\n\n\n\nThe Figure below shows at which point the probability of heads showing dips below 5 percent for one-tailed hypotheses. Thus, according to the figure below, if we toss a coin 100 times and head shows 59 or more often, then you are justified in claiming that I cheated.\n\n\n\n\n\n\n\n\n\nWhen comparing the two figures above, it is notable that the number at which you can claim I cheated differs according to whether the H1 as one- or two-tailed. When formulating a one-tailed hypothesis, then the number is lower compared with the the number at which you can reject the H0 if your H1 is two-tailed. This is actually the reason for why it is preferable to formulate more precise, one-tailed hypotheses (because then, it is easier for the data to be sufficient to reject the H0)."
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#the-normal-distribution",
    "href": "tutorials/basicquant/basicquant.html#the-normal-distribution",
    "title": "Basic Concepts in Quantitative Research",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nIt is important to note here that the above described calculation of probabilities does not work for numeric variables that are interval-scaled. The reason for this is that it is not possible to calculate the probabilities for all possible outcomes of a reaction time experiment (where time is a continuous variable). In such cases, we rely on distributions in order to determine how likely or probable a certain outcome is.\nWhen relying on distributions, we determine whether a certain value falls within or outside of the area of a distribution that accounts for 5 percent of the entire area of the distribution - if it falls within the area that accounts for less than 5 percent of the total area, then the result is called statistically significant (see the normal distribution below) because the outcome is very unlikely.\nThe normal distribution (or Gaussian curve or Gaussian distribution) shown in the figure above has certain characteristics that can be derived mathematically. Some of these characteristics relate to the area of certain sections of that distribution. More generally, the normal distribution is a symmetric, bell-shaped probability distribution, used as the theoretical model for the distribution of physical and psychological variables. In fact, many variables in the real world are normally distributed: shoe sizes, IQs, sleep lengths, and many , many more.\nThe normal distribution is very important because it underlies many statistical procedures in one form or another. The normal distribution is a symmetrical, continuous distribution where\n\nthe arithmetic mean, the median, and the mode are identical and have a value of 0 (and are thus identical);\n\n50% of the area under the bell-shaped curve are smaller than the mean;\n\n50% of the area under the bell-shaped curve are bigger than the mean;\n\n50% of the area under the bell-shaped curve are within -0.675 and +0.675 standard deviations from the mean (0);\n\n95% of the area under the bell-shaped curve are within -1.96 and +1.96 standard deviations from the mean (0);\n\n99% of the area under the bell-shaped curve are within -2.576 and +2.576 standard deviations from the mean (0).\n\nThere is also a very interesting aspect to the normal distribution that relates to the means (averages) of samples drawn from any type of distribution: no matter what type of distribution we draw samples from, the distribution of the means will approximate a normal distribution. In other words, if we draw samples form a very weird looking distribution, the means of these samples will approximate a normal distribution. This fact is called the Central Limit Theorem. What makes the central limit theorem so remarkable is that this result holds no matter what shape the original population distribution may have been.\n\n\n\n\n\n\n\n\n\nAs shown above, 50 percent of the total area under the curve are to left and 50 percent of the right of the mean value. Furthermore, 68 percent of the area are within -1 and +1 standard deviations from the mean; 95 percent of the area lie between -2 and +2 standard deviations from the mean; 99.7 percent of the area lie between -3 and +3 standard deviations from the mean. Also, 5 percent of the area lie outside -1.96 and +1.96 standard deviations from the mean (if these areas are combined) (see the Figure below). This is important. because we can reject null hypotheses if the probability of them being true is lower than 5 percent. This means that we can use the normal distribution to reject null hypotheses if our dependent variable is normally distributed.\n\n\n\n\n\n\n\n\n\nFinally, 5 percent of the area lies beyond +1.68 standard deviations from the mean (see the Figure below).\n\n\n\n\n\n\n\n\n\nThese properties are extremely useful when determining the likelihood of values or outcomes that reflect certain interval-scaled variables.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCreate a table with the possible outcomes and probabilities of 4 coin tosses (you can consider the table showing the outcomes of three coin tosses above as a guideline).\n\n\n\nAnswer\n\n    {r message=FALSE, warning=FALSE}\n    sum(dbinom(4, 7, 0.5))\n\n\nHow likely is it for heads to show exactly 3 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n    {r message=FALSE, warning=FALSE}\n    sum(dbinom(3, 7, 0.5))\n\n\nHow likely is it for heads to show exactly 2 or 5 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n  {r message=FALSE, warning=FALSE}\n  sum(dbinom(c(2, 5), 7, 0.5))\n\n\nHow likely is it for heads to show 5 or more times when tossing a coin 7 times?\n\n\n\nAnswer\n\n    {r message=FALSE, warning=FALSE}\n    sum(dbinom(5:7, 7, 0.5))\n\n\nHow likely is it for heads to show between 3 and 6 times when tossing a coin 7 times?\n\n\n\nAnswer\n\n    {r message=FALSE, warning=FALSE}\n    sum(dbinom(3:6, 7, 0.5))"
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#non-normality-skewness-and-kurtosis",
    "href": "tutorials/basicquant/basicquant.html#non-normality-skewness-and-kurtosis",
    "title": "Basic Concepts in Quantitative Research",
    "section": "Non-normality: skewness and kurtosis",
    "text": "Non-normality: skewness and kurtosis\nDepending on the phenomenon you are investigating, the distribution of that phenomenon can be distributed non-normally. Two factors causing distributions to differ from the normal distribution (that is two factors causing distributions to be non-normal) are skewness and kurtosis.\n\nSkewness\nSkewed distributions are asymmetrical and they can be positively or negatively skewed. The tail of a negatively skewed distribution points towards negative values (to the left) which means that the distribution leans towards the right (towards positive values) while the tail of a positively skewed distribution points towards positive values (to the right) which means that the distribution leans towards the left (towards negative values).\nAnother characteristic of skewed distributions is that the mean and median of a distribution differ. If the median is greater than the mean, the distribution is negatively skewed (the long tail points towards negative values). If the median is smaller than the mean, the distribution is positively skewed (the long tail points towards positive values).\nNegatively skewed\n\nTail points towards negative values (to the left)\nMedian is greater than the mean\n\nPositively skewed\n\nTail points towards positive values (to the right)\nMedian is lower than the mean\n\n\n\n\n\n\n\n\n\n\nTo show how we can calculate skewness (or if a distribution is skewed), we generate a sample of values.\n\nSampleValues &lt;- sample(1:100, 50)\n# inspect data\nsummary(SampleValues)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   23.25   45.00   48.36   75.50   98.00 \n\n\nWe apply the skewness function from the e1071 package to the sample scores to calculated skewness. The skewness function allows to calculate skewness in three different ways:\nType 1 (This is the typical definition used in many older textbooks): \\[\\begin{equation}\ng_1 = m_3 / m_2^{(3/2)}.\n\\end{equation}\\]\nType 2 (Used in SAS and SPSS): \\[\\begin{equation}\nG_1 = g_1 * sqrt(n(n-1)) / (n-2)\n\\end{equation}\\]\nType 3 (Used in MINITAB and BMDP): \\[\\begin{equation}\nb_1 = m_3 / s^3 = g_1 ((n-1)/n)^{(3/2)}\n\\end{equation}\\]\nAll three formulas have in common that the more negative values are, the more strongly positively skewed are the data (distribution leans to the left) and the more positive the values are, the more strongly negatively skewed are the data (distribution leans to the right). Here we use the second formula (by setting type = 2) that is also used in SPSS.\n\nlibrary(e1071)\nskewness(SampleValues, type = 2)\n\n[1] 0.1054117\n\n\nIf the reported skewness value is negative, then the distribution is positively skewed. If the value is positive, then the distribution is negatively skewed. If the value is lower than -1 or greater than +1, then the distribution can be assumed to be substantively skewed (Hair et al.).\n\n\nKurtosis\nAnother way in which distributions can differ from the normal distribution relates to the thickness of the tails and the spikiness of the distribution. If distributions are bell-shaped like the normal distribution, they are called mesokurtic. If distributions are symmetrical but they are more spiky than the normal distribution, they are called leptokurtic. If symmetrical distributions are flatter and have bigger tails than the normal distribution, the distributions are called platykurtic.\n\n\n\n\n\n\n\n\n\nTo show how we can calculate kurtosis (or if a distribution suffers from kurtosis), we apply the kurtosis function from the e1071 package to the sample scores we generated above /when we calculated skewness).\n\nkurtosis(SampleValues)\n\n[1] -1.291724\n\n\nAs the kurtosis value is positive, the distribution is leptokurtic (if it were negative, the distribution would be platykurtic). As a rule of thumb, values greater than +1 indicate that the distribution is too peaked while values lower than –1 indicate that the distribution is substantively platykurtic (Hair et al., pp61).\nThe kurtosis score can thus be interpreted as follows:\n\nA values of 0 means that the distribution is perfectly mesokurtic with Values between -0.5 and 0.5 suggesting that the distribution is approximately mesokurtic\nValues between -0.5 and -1 mean that the distribution is moderately platykurtic with values smaller than -1 indicating that the distribution is platykurtic.\nValues between 0.5 and 1 mean that the distribution is moderately leptokurtic with values greater than 1 indicating that the distribution is leptokurtic"
  },
  {
    "objectID": "tutorials/basicquant/basicquant.html#the-binomial-distribution",
    "href": "tutorials/basicquant/basicquant.html#the-binomial-distribution",
    "title": "Basic Concepts in Quantitative Research",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\nA distribution which is very similar to the normal distribution is the “binomial distribution”. The binomial distribution displays the probability of binary outcomes. So, in fact, the distribution of the coin flips above represents a binomial distribution. However, the binomial distribution cane be (as above in case of the coin flips) but does not have to be symmetric (like the normal distribution).\nTo illustrate this, let us consider the following example: we are interested in how many people use discourse like in Australian English. We have analyzed a corpus, for instance the Australian component of the International Corpus of English and found that 10 percent of all speakers have used discourse like. In this example, the corpus encompasses 100 speakers (actually the the spoken section of the Australian component of the ICE represents more than 100 speakers but it will make things easier for us in this example).\nNow, given the result of our corpus analysis, we would like to know what percentage of speakers of Australian English use discourse like with a confidence of 95 percent. Given the corpus study results, we can plot the expected binomial distribution of discourse like users in Australian English.\n\n\n\n\n\n\n\n\n\nThe bar graph above shows the probability distribution of discourse like users in Australian English. The first thing we notice is that the binomial distribution is not symmetric as it is slightly left-skewed. If we would increase the number of draws or speakers in our example, the distribution would, however, approximate the normal distribution very quickly because the binomial distribution approximates the normal distribution for large N or if the probability is close to .5. Thus, it is common practice to use the normal distribution instead of the binomial distribution.\nThe bar graph below also shows the probability distribution of discourse like users in Australian English but, in addition, it is color-coded: bars within the 95 percent confidence interval are lightgray, the bars in red are outside the 95 percent confidence interval. In other words, if we repeated the corpus analysis, for instance 1000 times, on corpora which represent speakers from the same population, then 95 percent of samples would have between 1 and 8 discourse like users.\n\n\n\n\n\n\n\n\n\nIn addition to the normal distribution and the binomial distribution, there are many other distributions which underlay common procedures in quantitative analyses. For instance, the t- and the F-distribution or the Chi-distribution. However, we will not deal with these distributions here."
  },
  {
    "objectID": "tutorials/tree/tree.html#advantages",
    "href": "tutorials/tree/tree.html#advantages",
    "title": "Tree-Based Models in R",
    "section": "Advantages",
    "text": "Advantages\nSeveral advantages have been associated with using tree-based models:\n\nTree-structure models are very useful because they are extremely flexible as they can deal with different types of variables and provide a very good understanding of the structure in the data.\nTree-structure models have been deemed particularly interesting for linguists because they can handle moderate sample sizes and many high-order interactions better then regression models.\nTree-structure models are (supposedly) better at detecting non-linear or non-monotonic relationships between predictors and dependent variables. This also means that they are better at finding and displaying interaction sinvolving many predictors.\nTree-structure models are easy to implement in R and do not require the model selection, validation, and diagnostics associated with regression models.\nTree-structure models can be used as variable-selection procedure which informs about which variables have any sort of significant relationship with the dependent variable and can thereby inform model fitting."
  },
  {
    "objectID": "tutorials/tree/tree.html#problems",
    "href": "tutorials/tree/tree.html#problems",
    "title": "Tree-Based Models in R",
    "section": "Problems",
    "text": "Problems\nDespite these potential advantages, a word of warning is in order: Gries admits that tree-based models can be very useful but there are some issues that but some serious short-comings of tree-structure models remain under-explored. For instance,\n\nForest-models (Random Forests and Boruta) only inform about the importance of a variable but not if the variable is important as a main effect or as part of interactions (or both)! The importance only shows that there is some important connection between the predictor and the dependent variable. While partial dependence plots (see here for more information) offer a remedy for this shortcoming to a certain degree, regression models are still much better at dealing with this issue.\nSimple tree-structure models have been shown to fail in detecting the correct predictors if the variance is solely determined by a single interaction (Gries, chap. 7.3). This failure is caused by the fact that the predictor used in the first split of a tree is selected as the one with the strongest main effect (Boulesteix et al., 344). This issue can, however, be avoided by hard-coding the interactions as predictors plus using ensemble methods such as random forests rather than individual trees (see Gries, chap. 7.3).\nAnother shortcoming is that tree-structure models partition the data (rather than “fitting a line” through the data which can lead to more coarse-grained predictions compared to regression models when dealing with numeric dependent variables (again, see Gries, chap. 7.3).\nBoulesteix et al. (341) state that high correlations between predictors can hinder the detection of interactions when using small data sets. However, regression do not fare better here as they are even more strongly affected by (multi-)collinearity (see Gries, chap. 7.3).\nTree-structure models are bad a detecting interactions when the variables have strong main effects which is, unfortunately, common when dealing with linguistic data (Wright, Ziegler, and König).\nTree-structure models cannot handle factorial variables with many levels (more than 53 levels) which is very common in linguistics where individual speakers or items are variables.\nForest-models (Random Forests and Boruta) have been deemed to be better at dealing with small data sets. However, this is only because the analysis is based on permutations of the original small data set. As such, forest-based models only appear to be better at handling small data sets because they “blow up” the data set rather than really being better at analyzing the original data.\n\nBefore we implement a conditional inference tree in R, we will have a look at how decision trees work. We will do this in more detail here as random forests and Boruta analyses are extensions of inference trees and are therefore based on the same concepts."
  },
  {
    "objectID": "tutorials/tree/tree.html#classification-and-regression-trees",
    "href": "tutorials/tree/tree.html#classification-and-regression-trees",
    "title": "Tree-Based Models in R",
    "section": "Classification And Regression Trees",
    "text": "Classification And Regression Trees\nBelow is an example of a decision tree which shows what what response to expect - in this case whether a speaker uses discourse like or not. Decision trees, like all CARTs and CITs, answer a simple question, namely How do we best classify elements based on the given predictors?. The answer that decision trees provide is the classification of the elements based on the levels of the predictors. In simple decision trees, all predictors, even those that are not significant are included in the decision tree. The decision tree shows that the best (or most important) predictor for the use of discourse like is age as it is the highest node. Among young speakers, those with high status use like more compared with speakers of lower social status. Among old speakers, women use discourse like more than men.\n\ninstall.packages(\"tree\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"grid\")\n\n\n\n\n\n\n\n\n\n\nThe yes and no at the bottom show if the speaker should be classified as a user of discourse like (yes or no). Each split can be read as true to the left and false to the right. So that, at the first split, if the person is between the ages of 15 and 40, we need to follow the branch to the left while we need to follow to the right if the person is not 15 to 40.\nBefore going through how this conditional decision tree is generated, let us first go over some basic concepts. The top of the decision tree is called root or root node, the categories at the end of branches are called leaves or leaf nodes. Nodes that are in-between the root and leaves are called internal nodes or just nodes. The root node has only arrows or lines pointing away from it, internal nodes have lines going to and from them, while leaf nodes only have lines pointing towards them.\nHow to prune and evaluate the accuracy of decision trees is not shown here. If you are interested in this, please check out chapter 7 of Gries which is a highly recommendable resource that provide a lot of additional information about decision trees and CARTs."
  },
  {
    "objectID": "tutorials/tree/tree.html#how-tree-based-methods-work",
    "href": "tutorials/tree/tree.html#how-tree-based-methods-work",
    "title": "Tree-Based Models in R",
    "section": "How Tree-Based Methods Work",
    "text": "How Tree-Based Methods Work\nLet us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse like given their age, gender, and social status.\nLet us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse like given their age, gender, and social status.\nIn a first step, we load and inspect the data that we will use in this tutorial. As tree-based models require either numeric or factorized data, we factorize the “character” variables in our data.\n\n# load data\ncitdata &lt;- read.delim(\"tutorials/tree/data/treedata.txt\", header = T, sep = \"\\t\") %&gt;%\n    dplyr::mutate_if(is.character, factor)\n\n\n\nAgeGenderStatusLikeUser15-40femalehighno15-40femalehighno15-40malehighno41-80femalelowyes41-80malehighno41-80malelowno41-80femalelowyes15-40malehighno41-80malelowno41-80malelowno\n\n\nThe data now consists of factors which two levels each.\nThe first step in generating a decision tree consists in determining, what the root of the decision tree should be. This means that we have to determine which of the variables represents the root node. In order to do so, we tabulate for each variable level, how many speakers of that level have used discourse like (LikeUsers) and how many have not used discourse like (NonLikeUsers).\n\n# tabulate data\ntable(citdata$LikeUser, citdata$Gender)\n\n     \n      female male\n  no      43   75\n  yes     91   42\n\ntable(citdata$LikeUser, citdata$Age)\n\n     \n      15-40 41-80\n  no     34    84\n  yes    92    41\n\ntable(citdata$LikeUser, citdata$Status)\n\n     \n      high low\n  no    33  85\n  yes   73  60\n\n\nNone of the predictors is perfect (the predictors are therefore referred to as impure). To determine which variable is the root, we will calculate the degree of “impurity” for each variable - the variable which has the lowest impurity value will be the root.\nThe most common measure of impurity in the context of conditional inference trees is called Gini (an alternative that is common when generating regression trees is the deviance). The Gini value or gini index was introduced by Corrado Gini as a measure for income inequality. In our case we seek to maximize inequality of distributions of leave nodes which is why the gini index is useful for tree based models. For each level we apply the following equation to determine the gini impurity value:\n\\[\\begin{equation}\n\nG_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}\n\n\\end{equation}\\]\nFor the node for men, this would mean the following:\n\\[\\begin{equation}\n\nG_{men} = 1-(\\frac{42} {42+75})^{2} - (\\frac{75} {42+75})^{2} = 0.4602235\n\n\\end{equation}\\]\nFor women, we calculate G or Gini as follows:\n\\[\\begin{equation}\n\nG_{women} = 1-(\\frac{91} {91+43})^{2} - (\\frac{43} {91+43})^{2} = 0.4358432\n\n\\end{equation}\\]\nTo calculate the Gini value of Gender, we need to calculate the weighted average leaf node impurity (weighted because the number of speakers is different in each group). We calculate the weighted average leaf node impurity using the equation below.\n\\[\\begin{equation}\n\nG_{Gender} = \\frac{N_{men}} {N_{Total}} \\times G_{men} +  \\frac{N_{women}} {N_{Total}} \\times G_{women}\n\nG_{Gender} = \\frac{159} {303} \\times 0.4602235 +  \\frac{144} {303} \\times 0.4358432 = 0.4611915\n\n\\end{equation}\\]\nWe will now perform the gini-calculation for gender (see below).\n\n# calculate Gini for men\ngini_men &lt;- 1 - (42 / (42 + 75))^2 - (75 / (42 + 75))^2\n# calculate Gini for women\ngini_women &lt;- 1 - (91 / (91 + 43))^2 - (43 / (91 + 43))^2\n# calculate weighted average of Gini for Gender\ngini_gender &lt;- 42 / (42 + 75) * gini_men + 91 / (91 + 43) * gini_women\ngini_gender\n\n[1] 0.4611915\n\n\nThe gini for gender is 0.4612. In a next step, we revisit the age distribution and we continue to calculate the gini value for age.\n\n# calculate Gini for age groups\ngini_young &lt;- 1 - (92 / (92 + 34))^2 - (34 / (92 + 34))^2 # Gini: young\ngini_old &lt;- 1 - (41 / (41 + 84))^2 - (84 / (41 + 84))^2 # Gini: old\n# calculate weighted average of Gini for Age\ngini_age &lt;- 92 / (92 + 34) * gini_young + 41 / (41 + 84) * gini_old\ngini_age\n\n[1] 0.4323148\n\n\nThe gini for age is .4323 and we continue by revisiting the status distribution and we continue to calculate the gini value for status.\n\ngini_high &lt;- 1 - (73 / (33 + 73))^2 - (33 / (33 + 73))^2 # Gini: high\ngini_low &lt;- 1 - (60 / (60 + 85))^2 - (85 / (60 + 85))^2 # Gini: low\n# calculate weighted average of Gini for Status\ngini_status &lt;- 73 / (33 + 73) * gini_high + 60 / (60 + 85) * gini_low\ngini_status\n\n[1] 0.4960521\n\n\nThe gini for status is .4961 and we can now compare the gini values for age, gender, and status.\n\n# compare age, gender, and status ginis\ngini_age\n\n[1] 0.4323148\n\ngini_gender\n\n[1] 0.4611915\n\ngini_status\n\n[1] 0.4960521\n\n\nSince age has the lowest gini (impurity) value, our first split is by age and age, thus, represents our root node. Our manually calculated conditional inference tree right now looks as below.\n\n\n\n\n\n\n\n\n\nIn a next step, we need to find out which of the remaining variables best separates the speakers who use discourse like from those that do not under the first node. In order to do so, we calculate the Gini values for Gender and SocialStatus for the 15-40 node.\nWe thus move on and test if and how to split this branch.\n\n# 5TH NODE\n# split data according to first split (only young data)\nyoung &lt;- citdata %&gt;%\n    dplyr::filter(Age == \"15-40\")\n# inspect distribution\ntbyounggender &lt;- table(young$LikeUser, young$Gender)\ntbyounggender\n\n     \n      female male\n  no      17   17\n  yes     58   34\n\n\n\n# calculate Gini for Gender\n# calculate Gini for men\ngini_youngmen &lt;- 1 - (tbyounggender[2, 2] / sum(tbyounggender[, 2]))^2 - (tbyounggender[1, 2] / sum(tbyounggender[, 2]))^2\n# calculate Gini for women\ngini_youngwomen &lt;- 1 - (tbyounggender[2, 1] / sum(tbyounggender[, 1]))^2 - (tbyounggender[1, 1] / sum(tbyounggender[, 1]))^2\n# # calculate weighted average of Gini for Gender\ngini_younggender &lt;- sum(tbyounggender[, 2]) / sum(tbyounggender) * gini_youngmen + sum(tbyounggender[, 1]) / sum(tbyounggender) * gini_youngwomen\ngini_younggender\n\n[1] 0.3885714\n\n\nThe gini value for gender among young speakers is 0.3886.\nWe continue by inspecting the status distribution.\n\n# calculate Gini for Status\n# inspect distribution\ntbyoungstatus &lt;- table(young$LikeUser, young$Status)\ntbyoungstatus\n\n     \n      high low\n  no    11  23\n  yes   57  35\n\n\nWe now calculate the gini value for status.\n\n# calculate Gini for status\n# calculate Gini for low\ngini_younglow &lt;- 1 - (tbyoungstatus[2, 2] / sum(tbyoungstatus[, 2]))^2 - (tbyoungstatus[1, 2] / sum(tbyoungstatus[, 2]))^2\n# calculate Gini for high\ngini_younghigh &lt;- 1 - (tbyoungstatus[2, 1] / sum(tbyoungstatus[, 1]))^2 - (tbyoungstatus[1, 1] / sum(tbyoungstatus[, 1]))^2\n# # calculate weighted average of Gini for status\ngini_youngstatus &lt;- sum(tbyoungstatus[, 2]) / sum(tbyoungstatus) * gini_younglow + sum(tbyoungstatus[, 1]) / sum(tbyoungstatus) * gini_younghigh\ngini_youngstatus\n\n[1] 0.3666651\n\n\nSince the gini value for status (0.3667) is lower than the gini value for gender (0.3886), we split by status.\nWe would continue to calculate the gini values and always split at the lowest gini levels until we reach a leaf node. Then, we would continue doing the same for the remaining branches until the entire data is binned into different leaf nodes.\nIn addition to plotting the decision tree, we can also check its accuracy. To do so, we predict the use of like based on the decision tree and compare them to the observed uses of like. Then we use the confusionMatrix function from the caret package to get an overview of the accuracy statistics.\n\ndtreeprediction &lt;- as.factor(ifelse(predict(dtree)[, 2] &gt; .5, \"yes\", \"no\"))\nconfusionMatrix(dtreeprediction, citdata$LikeUser)\n\nThe conditional inference tree has an accuracy of 72.9 percent which is significantly better than the base-line accuracy of 53.0 percent (No Information Rate \\(*\\) 100). To understand what the other statistics refer to and how they are calculated, run the command ?confusionMatrix."
  },
  {
    "objectID": "tutorials/tree/tree.html#splitting-numeric-ordinal-and-true-categorical-variables",
    "href": "tutorials/tree/tree.html#splitting-numeric-ordinal-and-true-categorical-variables",
    "title": "Tree-Based Models in R",
    "section": "Splitting numeric, ordinal, and true categorical variables",
    "text": "Splitting numeric, ordinal, and true categorical variables\nWhile it is rather straight forward to calculate the Gini values for categorical variables, it may not seem quite as apparent how to calculate splits for numeric or ordinal variables. To illustrate how the algorithm works on such variables, consider the example data set shown below.\n\n\nAgeLikeUser15yes37no63no42yes22yes27yes\n\n\nIn a first step, we order the numeric variable so that we arrive at the following table.\n\n\nAgeLikeUser15yes22yes27yes37no42yes63no\n\n\nNext, we calculate the means for each level of “Age”.\n\n\nAgeLikeUser15.0yes18.522.0yes24.527.0yes32.037.0no39.542.0yes52.5\n\n\nNow, we calculate the Gini values for each average level of age. How this is done is shown below for the first split.\n\\[\\begin{equation}\n\nG_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}\n\n\\end{equation}\\]\nFor an age smaller than 18.5 this would mean:\n\\[\\begin{equation}\n\nG_{youngerthan18.5} = 1-(\\frac{1} {1+0})^{2} - (\\frac{0} {1+0})^{2} = 0.0\n\n\\end{equation}\\]\nFor an age greater than 18.5, we calculate G or Gini as follows:\n\\[\\begin{equation}\n\nG_{olerthan18.5} = 1-(\\frac{2} {2+3})^{2} - (\\frac{3} {2+3})^{2} = 0.48\n\n\\end{equation}\\]\nNow, we calculate the Gini for that split as we have done above.\n\\[\\begin{equation}\n\nG_{split18.5} = \\frac{N_{youngerthan18.5}} {N_{Total}} \\times G_{youngerthan18.5} +  \\frac{N_{olderthan18.5}} {N_{Total}} \\times G_{olderthan18.5}\n\nG_{split18.5} = \\frac{1} {6} \\times 0.0 +  \\frac{5} {6} \\times 0.48 = 0.4\n\n\\end{equation}\\]\nWe then have to calculate the gini values for all possible age splits which yields the following results:\n\n# 18.5\n1 - (1 / (1 + 0))^2 - (0 / (1 + 0))^2\n1 - (2 / (2 + 3))^2 - (3 / (2 + 3))^2\n1 / 6 * 0.0 + 5 / 6 * 0.48\n# 24.4\n1 - (2 / (2 + 0))^2 - (0 / (2 + 0))^2\n1 - (3 / (3 + 1))^2 - (2 / (3 + 1))^2\n2 / 6 * 0.0 + 4 / 6 * 0.1875\n# 32\n1 - (3 / (3 + 0))^2 - (0 / (3 + 0))^2\n1 - (1 / (1 + 2))^2 - (2 / (1 + 2))^2\n3 / 6 * 0.0 + 3 / 6 * 0.4444444\n# 39.5\n1 - (3 / (3 + 1))^2 - (1 / (3 + 1))^2\n1 - (1 / (1 + 1))^2 - (1 / (1 + 1))^2\n4 / 6 * 0.375 + 2 / 6 * 0.5\n# 52.5\n1 - (4 / (4 + 1))^2 - (1 / (4 + 1))^2\n1 - (0 / (0 + 1))^2 - (1 / (0 + 1))^2\n5 / 6 * 0.32 + 1 / 6 * 0.0\n\n\n\nAgeSplitGini18.50.40024.50.50032.00.44439.50.41052.50.267\n\n\nThe split at 52.5 years of age has the lowest Gini value. Accordingly, we would split the data between speakers who are younger than 52.5 and speakers who are older than 52.5 years of age. The lowest Gini value for any age split would also be the Gini value that would be compared to other variables.\nThe same procedure that we have used to determine potential splits for a numeric variable would apply to an ordinal variable with only two differences:\n\nThe Gini values are calculated for the actual levels and not the means between variable levels.\nThe Gini value is nor calculated for the lowest and highest level as the calculation of the Gini values is impossible for extreme values. Extreme levels can, therefore, not serve as a potential split location.\n\nWhen dealing with categorical variables with more than two levels, the situation is slightly more complex as we would also have to calculate the Gini values for combinations of variable levels. While the calculations are, in principle, analogous to the ones performed for binary of nominal categorical variables, we would also have to check if combinations would lead to improved splits. For instance, imagine we have a variable with categories A, B, and C. In such cases we would not only have to calculate the Gini scores for A, B, and C but also for A plus B, A plus C, and B plus C. Note that we ignore the combination A plus B plus C as this combination would include all other potential combinations."
  },
  {
    "objectID": "tutorials/tree/tree.html#prettifying-your-cit-tree",
    "href": "tutorials/tree/tree.html#prettifying-your-cit-tree",
    "title": "Tree-Based Models in R",
    "section": "Prettifying your CIT tree",
    "text": "Prettifying your CIT tree\nThe easiest and most common way to visualize CITs is to simply use the plot function from base R. However, using this function does not allow to adapt and customize the visualization except for some very basic parameters. The ggparty function allows to use the ggplot syntax to customize CITs which allows more adjustments and is more aesthetically pleasing.\nTo generate this customized CIT, we activate the ggparty package and extract the significant p-values from the CIT object. We then plot the CIT and define the nodes, edges, and text elements as shown below.\n\n# extract p-values\npvals &lt;- unlist(nodeapply(citd.ctree, ids = nodeids(citd.ctree), function(n) info_node(n)$p.value))\npvals &lt;- pvals[pvals &lt; .05]\n# plotting\nggparty(citd.ctree) +\n    geom_edge() +\n    geom_edge_label() +\n    geom_node_label(\n        line_list = list(\n            aes(label = splitvar),\n            aes(\n                label = paste0(\n                    \"N=\", nodesize, \", p\",\n                    ifelse(pvals &lt; .001, \"&lt;.001\", paste0(\"=\", round(pvals, 3)))\n                ),\n                size = 10\n            )\n        ),\n        line_gpar = list(\n            list(size = 13),\n            list(size = 10)\n        ),\n        ids = \"inner\"\n    ) +\n    geom_node_label(aes(label = paste0(\"Node \", id, \", N = \", nodesize)),\n        ids = \"terminal\", nudge_y = -0.0, nudge_x = 0.01\n    ) +\n    geom_node_plot(\n        gglist = list(\n            geom_bar(aes(x = \"\", fill = LikeUser),\n                position = position_fill(), color = \"black\"\n            ),\n            theme_minimal(),\n            scale_fill_manual(values = c(\"gray50\", \"gray80\"), guide = FALSE),\n            scale_y_continuous(breaks = c(0, 1)),\n            xlab(\"\"),\n            ylab(\"Probability\"),\n            geom_text(\n                aes(\n                    x = \"\", group = LikeUser,\n                    label = stat(count)\n                ),\n                stat = \"count\", position = position_fill(), vjust = 1.1\n            )\n        ),\n        shared_axis_labels = TRUE\n    )\n\n\n\n\n\n\n\n\nWe can also use position_dodge (instead of position_fill) to display frequencies rather than probabilities as shown below.\n\n# plotting\nggparty(citd.ctree) +\n    geom_edge() +\n    geom_edge_label() +\n    geom_node_label(\n        line_list = list(\n            aes(label = splitvar),\n            aes(\n                label = paste0(\n                    \"N=\", nodesize, \", p\",\n                    ifelse(pvals &lt; .001, \"&lt;.001\", paste0(\"=\", round(pvals, 3)))\n                ),\n                size = 10\n            )\n        ),\n        line_gpar = list(\n            list(size = 13),\n            list(size = 10)\n        ),\n        ids = \"inner\"\n    ) +\n    geom_node_label(aes(label = paste0(\"Node \", id, \", N = \", nodesize)),\n        ids = \"terminal\", nudge_y = 0.01, nudge_x = 0.01\n    ) +\n    geom_node_plot(\n        gglist = list(\n            geom_bar(aes(x = \"\", fill = LikeUser),\n                position = position_dodge(), color = \"black\"\n            ),\n            theme_minimal(),\n            theme(\n                panel.grid.major = element_blank(),\n                panel.grid.minor = element_blank()\n            ),\n            scale_fill_manual(values = c(\"gray50\", \"gray80\"), guide = FALSE),\n            scale_y_continuous(\n                breaks = seq(0, 100, 20),\n                limits = c(0, 100)\n            ),\n            xlab(\"\"),\n            ylab(\"Frequency\"),\n            geom_text(\n                aes(\n                    x = \"\", group = LikeUser,\n                    label = stat(count)\n                ),\n                stat = \"count\",\n                position = position_dodge(0.9), vjust = -0.7\n            )\n        ),\n        shared_axis_labels = TRUE\n    )"
  },
  {
    "objectID": "tutorials/tree/tree.html#problems-of-conditional-inference-trees",
    "href": "tutorials/tree/tree.html#problems-of-conditional-inference-trees",
    "title": "Tree-Based Models in R",
    "section": "Problems of Conditional Inference Trees",
    "text": "Problems of Conditional Inference Trees\nLike other tree-based methods, CITs are very intuitive, multivariate, non-parametric, they do not require large data sets, and they are easy to implement. Despite these obvious advantages, they have at least one major short coming compared to other, more sophisticated tree-structure models (in addition to the general issues that tree-structure models exhibit as discussed above: they are prone to overfitting which means that they fit the observed data very well but preform much worse when being applied to new data.\nAn extension which remedies this problem is to use a so-called ensemble method which grows many varied trees. The most common ensemble method is called a Random Forest Analysis and will have a look at how Random Forests work and how to implement them in R in the next section."
  },
  {
    "objectID": "tutorials/tree/tree.html#random-forests-in-r",
    "href": "tutorials/tree/tree.html#random-forests-in-r",
    "title": "Tree-Based Models in R",
    "section": "Random Forests in R",
    "text": "Random Forests in R\nThis section shows how a Random Forest Analysis can be implemented in R. Ina first step, we load and inspect the data.\n\n# load random forest data\nrfdata &lt;- read.delim(\"tutorials/tree/data/mblrdata.txt\", header = T, sep = \"\\t\")\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-061$CWomenYoungMixedGenderNoPrime0S1A-023$BWomenYoungMixedGenderNoPrime0S1A-054$AWomenYoungSameGenderNoPrime0S1A-090$BWomenYoungMixedGenderNoPrime0S1A-009$BWomenOldSameGenderPrime0S1A-085$EMenYoungMixedGenderPrime1S1A-003$CWomenYoungMixedGenderNoPrime1S1A-084$CWomenYoungSameGenderNoPrime0S1A-076$AWomenYoungSameGenderNoPrime0S1A-083$DMenOldMixedGenderNoPrime1\n\n\nThe data consists of four categorical variables (Gender, Age, ConversationType, and SUFlike). Our dependent variable is SUFlike which stands for speech-unit final like (a pragmatic marker that is common in Irish English and is used as in A wee girl of her age, like). While Age and Gender are pretty straight forward what they are called, ConversationType encodes whether a conversation has taken place between interlocutors of the same or of different genders.\nBefore going any further, we need to factorize the variables as tree-based models require factors instead of character variables (but they can, of course, handle numeric and ordinal variables). In addition, we will check if the data contains missing values (NAs; NA stands for not available).\n\n# factorize variables (rf require factors instead of character vectors)\nrfdata &lt;- rfdata %&gt;%\n    dplyr::mutate_if(is.character, factor) %&gt;%\n    dplyr::select(-ID)\n\n\n\nGenderAgeConversationTypePrimingSUFlikeWomenYoungMixedGenderNoPrime0WomenYoungMixedGenderNoPrime0WomenYoungSameGenderNoPrime0WomenYoungMixedGenderNoPrime0WomenOldSameGenderPrime0MenYoungMixedGenderPrime1WomenYoungMixedGenderNoPrime1WomenYoungSameGenderNoPrime0WomenYoungSameGenderNoPrime0MenOldMixedGenderNoPrime1\n\n\nWe now check if the data contains missing values and remove those (if necessary).\n\n# check for NAs\nnatest &lt;- rfdata %&gt;%\n    na.omit()\nnrow(natest) # no NAs present in data (same number of rows with NAs omitted)\n\n[1] 2000\n\n\nIn our case, the data does not contain missing values. Random Forests offer a very nice way to deal with missing data though. If NAs are present, they can either be deleted OR their values for any missing values can be imputed using proximities. In this way, such data points do not have to be removed which can be problematic especially when dealing with relatively small data sets. For imputing values, you could run the code below but as our data does not have NAs, we will skip this step and just show it here so you can have a look at how it is done.\n\n# replacing NAs with estimates\ndata.imputed &lt;- rfImpute(SUFlike ~ ., data = rfdata, iter = 6)\n\nThe argument iter refers to the number of iterations to run. According to (Breiman), 4 to 6 iterations is usually good enough. With this data set (if it had NAs) and when we were to execute the code, the resulting OOB-error rates lie somewhere around 17 and 18 percent. When we were to set iter to 20, we get values a little better and a little worse, so doing more iterations doesn’t improve the situation.\nAlso, if you want to customize the rfImpute function, you can change the number of trees it uses (the default is 300) and the number of variables that it will consider at each step.\nWe will now generate a first random forest object and inspect its model fit. As random forests rely on re-sampling, we set a seed so that we arrive at the same estimations.\n\n# set.seed\nset.seed(2019120204)\n# create initial model\nrfmodel1 &lt;- cforest(SUFlike ~ .,\n    data = rfdata,\n    controls = cforest_unbiased(ntree = 50, mtry = 3)\n)\n# evaluate random forest (model diagnostics)\nrfmodel1_pred &lt;- unlist(party::treeresponse(rfmodel1)) # [c(FALSE,TRUE)]\nsomers2(rfmodel1_pred, as.numeric(rfdata$SUFlike))\n\n           C          Dxy            n      Missing \n   0.7112131    0.4224262 2000.0000000    0.0000000 \n\n\nThe model parameters are good but not excellent: remember that if the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity (Baayen, 204). Somers’ Dxy is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ Dxy values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction (Baayen, 204).\nIn a next step, we extract the variable importance conditional=T adjusts for correlations between predictors).\n\n# extract variable importance based on mean decrease in accuracy\nrfmodel1_varimp &lt;- varimp(rfmodel1, conditional = T)\n# show variable importance\nrfmodel1_varimp\n\n          Gender              Age ConversationType          Priming \n     0.003770260      0.000542920      0.002520164      0.022095496 \n\n\nWe can also calculate more robust variable importance using the varimpAUC function from the party package which calculates importance statistics that are corrected towards class imbalance, i.e. differences in the number of instances per category. The variable importance is easily visualized using the dotplot function from base R.\n\n# extract more robust variable importance\nrfmodel1_robustvarimp &lt;- party::varimp(rfmodel1)\n# plot result\ndotchart(sort(rfmodel1_robustvarimp), pch = 20, main = \"Conditional importance of variables\")\n\n\n\n\n\n\n\n\nThe plot shows that Age is the most important predictor and that Priming is not really important as a predictor for speech-unit final like. Gender and ConversationType are equally important but both much less so than Age.\nWe will now use an alternative way to calculate RFs which allows us to use different diagnostics and pruning techniques by using the randomForest rather than the cforest function.\nA few words on the parameters of the randomForest function: if the thing we’re trying to predict is a numeric variable, the randomForest function will set mtry (the number of variables considered at each step) to the total number of variables divided by 3 (rounded down), or to 1 if the division results in a value less than 1. If the thing we’re trying to predict is a “factor” (i.e. either “yes/no” or “ranked”), then randomForest() will set mtry to the square root of the number of variables (rounded down to the next integer value).Again, we start by setting a seed to store random numbers and thus make results reproducible.\n\n# set.seed\nset.seed(2019120205)\nrfmodel2 &lt;- randomForest::randomForest(SUFlike ~ .,\n    data = rfdata,\n    mtry = 2,\n    proximity = TRUE\n)\n# inspect model\nrfmodel2\n\n\nCall:\n randomForest(formula = SUFlike ~ ., data = rfdata, mtry = 2,      proximity = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 0.1277716\n                    % Var explained: 10.28\n\n\nThe output tells us that the model explains less than 15 percent of the variance. It is recommendable to check if changing parameters causes and increase in the amount of variance that is explained by a model (which is desirable). In this case, we can try different values for mtry and for ntree as shown below and then compare the performance of the random forest models by inspecting the amount of variance that they explain. Again, we begin by setting a seed and then continue by specifying the random forest model.\n\n# set.seed (to store random numbers and thus make results reproducible)\nset.seed(2019120206)\n# create a new model with fewer trees and that takes 2 variables at a time\nrfmodel3 &lt;- randomForest(SUFlike ~ ., data = rfdata, ntree = 30, mtry = 4, proximity = TRUE)\n# inspect model\nrfmodel3\n\n\nCall:\n randomForest(formula = SUFlike ~ ., data = rfdata, ntree = 30,      mtry = 4, proximity = TRUE) \n               Type of random forest: regression\n                     Number of trees: 30\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 0.1283899\n                    % Var explained: 9.85\n\n\nDespite optimization, the results have not changed but it may be very useful for other data. To evaluate the tree, we create a confusion matrix.\n\n# save what the model predicted in a new variable\nrfdata$Probability &lt;- predict(rfmodel3, rfdata)\nrfdata$Prediction &lt;- ifelse(rfdata$Probability &gt;= .5, 1, 0)\n# create confusion matrix to check accuracy\nconfusionMatrix(as.factor(rfdata$Prediction), as.factor(rfdata$SUFlike))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1622  297\n         1   34   47\n                                             \n               Accuracy : 0.8345             \n                 95% CI : (0.8175, 0.8505)   \n    No Information Rate : 0.828              \n    P-Value [Acc &gt; NIR] : 0.2303             \n                                             \n                  Kappa : 0.1665             \n                                             \n Mcnemar's Test P-Value : &lt;0.0000000000000002\n                                             \n            Sensitivity : 0.9795             \n            Specificity : 0.1366             \n         Pos Pred Value : 0.8452             \n         Neg Pred Value : 0.5802             \n             Prevalence : 0.8280             \n         Detection Rate : 0.8110             \n   Detection Prevalence : 0.9595             \n      Balanced Accuracy : 0.5580             \n                                             \n       'Positive' Class : 0                  \n                                             \n\n\nThe RF performs significantly better than a no-information base-line model but the base-line model already predicts 78.18 percent of cases correctly (compared to the RF with a prediction accuracy of 82.5 percent).\nUnfortunately, we cannot easily compute robust variable importance for RF models nor C or Somers’ Dxy which is why it is advisable to create analogous models using both the cforest and the randomForest functions. In a last step, we can now visualize the results of the optimized RF.\n\n# plot variable importance\nvarImpPlot(rfmodel3, main = \"\", pch = 20)\n\n\n\n\n\n\n\n\nHere is an alternative way of plotting variable importance using the vip function from the vip package.\n\n# generate vip plot\nvip::vip(rfmodel3, geom = \"point\", horizontal = FALSE)\n\n\n\n\n\n\n\n\nUsing the vip package, you can also generate variable importance barplots.\n\n# generate vip plot\nvip::vip(rfmodel3, horizontal = FALSE)\n\n\n\n\n\n\n\n\nA second type of visualization that can provide insights in the partial function from the pdp package which shows the effects of individual predictors - but remember that this still does not provide information about the way that the predictor interacts with other predictors.\n\n# extract importance of individual variables\nrfmodel3 %&gt;% # the %&gt;% operator is read as \"and then\"\n    partial(pred.var = \"Age\") %&gt;%\n    autoplot(smooth = TRUE, ylab = expression(f(Age))) +\n    theme_light() +\n    ggtitle(\"Partial Depencence Plot: Age\") +\n    ylim(-20, 0)\n\n\n\n\n\n\n\n\nYou can however use the partial function to show how the effect of predictors interacts with other predictors (see below).\n\npartial(rfmodel3, pred.var = c(\"Age\", \"Gender\"), plot = TRUE, plot.engine = \"ggplot2\")\n\n\n\n\n\n\n\n\nAnother common way to evaluate the performance of RFs is to split the data into a test and a training set. the model is then fit to the training set and, after that, applied to the test set. This allows us to evaluate how well the RF performs on data that it was not trained on. This approach is particularly common in machine learning contexts."
  },
  {
    "objectID": "tutorials/tree/tree.html#advantages-1",
    "href": "tutorials/tree/tree.html#advantages-1",
    "title": "Tree-Based Models in R",
    "section": "Advantages",
    "text": "Advantages\nBoruta outperforms random forest analyses because:\n\nBoruta does not provide merely a single value for each predictor but a distribution of values leading to higher reliability.\nBoruta provides definitive cut-off points for variables that have no meaningful relationship with the dependent variable. This is a crucial difference between RF and Boruta that make Boruta particularly interesting from a variable selection point of view."
  },
  {
    "objectID": "tutorials/tree/tree.html#procedure",
    "href": "tutorials/tree/tree.html#procedure",
    "title": "Tree-Based Models in R",
    "section": "Procedure",
    "text": "Procedure\nThe Boruta procedure consists out of five steps.\n\nIn a first step, the Boruta algorithm copies the data set and adds randomness to the data by (re-)shuffling data points and thereby creating randomized variables. These randomized variables are referred to as shadow features.\nSecondly, a random forest classifier is trained on the extended data set.\nIn a third step, a feature importance measure (Mean Decrease Accuracy represented by z-scores) is calculated to determine the relative importance of all predictors (both original or real variables and the randomized shadow features).\nIn the next step, it is checked at each iteration of the process whether a real predictor has a higher importance compared with the best shadow feature. The algorithm keeps track of the performance of the original variables by storing whether they outperformed the best shadow feature or not in a vector.\nIn the fifth step, predictors that did not outperform the best shadow feature are removed and the process continues without them. After a set number of iterations, or if all the variables have been either confirmed as outperforming the best shadow feature, the algorithm stops.\n\nDespite its obvious advantages of Boruta over random forest analyses and regression modeling, it can neither handle multicollinearity not hierarchical data structures where data points are nested or grouped by a given predictor (as is the case in the present analysis as data points are grouped by adjective type). As Boruta is a variable selection procedure, it is also limited in the sense that it provides information on which predictors to include and how good these predictors are (compared to the shadow variables) while it is neither able to take hierarchical data structure into account, nor does it provide information about how one level of a factor compares to other factors. In other words, Boruta shows that a predictor is relevant and how strong it is but it does not provide information on how the likelihood of an outcome being used differs between variable levels, for instance between men and women."
  },
  {
    "objectID": "tutorials/tree/tree.html#boruta-in-r",
    "href": "tutorials/tree/tree.html#boruta-in-r",
    "title": "Tree-Based Models in R",
    "section": "Boruta in R",
    "text": "Boruta in R\nWe begin by loading and inspecting the data.\n\n# load data\nborutadata &lt;- read.delim(\"tutorials/tree/data/ampaus05_statz.txt\", header = T, sep = \"\\t\")\n\n\n\nAgeAdjectiveFileSpeakerFunctionPrimingGenderOccupationConversationTypeAudienceSizeveryreallyFreqGradabiltySemanticCategoryEmotionality26-40good&lt;S1A-001:1$B&gt;AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional26-40good&lt;S1A-001:1$B&gt;AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional26-40good&lt;S1A-001:1$B&gt;PredicativeNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional17-25nice&lt;S1A-003:1$B&gt;AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexDyad107.29282NotGradableHumanPropensityNonEmotional41-80other&lt;S1A-003:1$A&gt;PredicativeNoPrimeMenAcademicManagerialProfessionalsSameSexDyad000.61728NotGradableValueNonEmotional41-80other&lt;S1A-004:1$C&gt;PredicativeNoPrimeMenMixedSexMultipleInterlocutors102.46914NotGradableValuePositiveEmotional41-80good&lt;S1A-004:1$B&gt;AttributiveNoPrimeWomenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors0020.98765NotGradableValuePositiveEmotional41-80other&lt;S1A-005:1$B&gt;PredicativeNoPrimeWomenMixedSexMultipleInterlocutors100.61728GradabilityUndeterminedHumanPropensityNegativeEmotional17-25other&lt;S1A-006:1$B&gt;AttributiveNoPrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors014.64088GradabilityUndeterminedDimensionNonEmotional17-25other&lt;S1A-006:1$B&gt;AttributivePrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors010.44199NotGradablePhysicalPropertyNonEmotional\n\n\nAs the data contains non-factorized character variables, we convert those into factors.\n\n# factorize variables (boruta - like rf - require factors instead of character vectors)\nborutadata &lt;- borutadata %&gt;%\n    dplyr::filter(complete.cases(.)) %&gt;%\n    dplyr::mutate_if(is.character, factor)\n\n\n\nAgeAdjectiveFileSpeakerFunctionPrimingGenderOccupationConversationTypeAudienceSizeveryreallyFreqGradabiltySemanticCategoryEmotionality26-40good&lt;S1A-001:1$B&gt;AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional26-40good&lt;S1A-001:1$B&gt;AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional26-40good&lt;S1A-001:1$B&gt;PredicativeNoPrimeMenAcademicManagerialProfessionalsSameSexMultipleInterlocutors0027.84810NotGradableValuePositiveEmotional17-25nice&lt;S1A-003:1$B&gt;AttributiveNoPrimeMenAcademicManagerialProfessionalsSameSexDyad107.29282NotGradableHumanPropensityNonEmotional41-80other&lt;S1A-003:1$A&gt;PredicativeNoPrimeMenAcademicManagerialProfessionalsSameSexDyad000.61728NotGradableValueNonEmotional41-80good&lt;S1A-004:1$B&gt;AttributiveNoPrimeWomenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors0020.98765NotGradableValuePositiveEmotional17-25other&lt;S1A-006:1$B&gt;AttributiveNoPrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors014.64088GradabilityUndeterminedDimensionNonEmotional17-25other&lt;S1A-006:1$B&gt;AttributivePrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors010.44199NotGradablePhysicalPropertyNonEmotional17-25other&lt;S1A-006:1$B&gt;PredicativePrimeMenAcademicManagerialProfessionalsMixedSexMultipleInterlocutors010.44199NotGradablePhysicalPropertyNonEmotional17-25nice&lt;S1A-007:1$A&gt;AttributiveNoPrimeWomenAcademicManagerialProfessionalsSameSexDyad017.29282NotGradableHumanPropensityNonEmotional\n\n\nWe can now create our initial Boruta model and set a seed for reproducibility.\n\n# set.seed\nset.seed(2019120207)\n# initial run\nboruta1 &lt;- Boruta(really ~ ., data = borutadata)\nprint(boruta1)\n\nBoruta performed 99 iterations in 2.143655 secs.\n 8 attributes confirmed important: Adjective, AudienceSize,\nConversationType, Emotionality, FileSpeaker and 3 more;\n 5 attributes confirmed unimportant: Age, Gender, Occupation, Priming,\nSemanticCategory;\n 1 tentative attributes left: Gradabilty;\n\n# extract decision\ngetConfirmedFormula(boruta1)\n\nreally ~ Adjective + FileSpeaker + Function + ConversationType + \n    AudienceSize + very + Freq + Emotionality\n&lt;environment: 0x1660e8eb8&gt;\n\n\nIn a next step, we inspect the history to check if any of the variables shows drastic fluctuations in their importance assessment.\n\nplotImpHistory(boruta1)\n\n\n\n\n\n\n\n\nThe fluctuations are do not show clear upward or downward trends (which what we want). If predictors do perform worse than the shadow variables, then these variables should be excluded and the Boruta analysis should be re-run on the data set that does no longer contain the superfluous variables. Tentative variables can remain but they are unlikely to have any substantial effect. We thus continue by removing variables that were confirmed as being unimportant, then setting a new seed, re-running the Boruta on the reduced data set, and again inspecting the decisions.\n\n# remove irrelevant variables\nrejected &lt;- names(boruta1$finalDecision)[which(boruta1$finalDecision == \"Rejected\")]\n# update data for boruta\nborutadata &lt;- borutadata %&gt;%\n    dplyr::select(-rejected)\n# set.seed (to store random numbers and thus make results reproducible)\nset.seed(2019120208)\n# 2nd run\nboruta2 &lt;- Boruta(really ~ ., data = borutadata)\nprint(boruta2)\n\nBoruta performed 99 iterations in 2.216482 secs.\n 8 attributes confirmed important: Adjective, AudienceSize,\nConversationType, Emotionality, FileSpeaker and 3 more;\n No attributes deemed unimportant.\n 1 tentative attributes left: Gradabilty;\n\n# extract decision\ngetConfirmedFormula(boruta2)\n\nreally ~ Adjective + FileSpeaker + Function + ConversationType + \n    AudienceSize + very + Freq + Emotionality\n&lt;environment: 0x11e62eb10&gt;\n\n\nOnly adjective frequency and adjective type are confirmed as being important while all other variables are considered tentative. However, no more variables need to be removed as all remaining variables are not considered unimportant. In a last step, we visualize the results of the Boruta analysis.\n\nborutadf &lt;- as.data.frame(boruta2$ImpHistory) %&gt;%\n    tidyr::gather(Variable, Importance, Adjective:shadowMin) %&gt;%\n    dplyr::mutate(Type = ifelse(str_detect(Variable, \"shadow\"), \"Control\", \"Predictor\")) %&gt;%\n    dplyr::mutate(\n        Type = factor(Type),\n        Variable = factor(Variable)\n    )\nggplot(borutadf, aes(x = reorder(Variable, Importance, mean), y = Importance, fill = Type)) +\n    geom_boxplot() +\n    geom_vline(xintercept = 3.5, linetype = \"dashed\", color = \"black\") +\n    scale_fill_manual(values = c(\"gray80\", \"gray40\")) +\n    theme_bw() +\n    theme(\n        legend.position = \"top\",\n        axis.text.x = element_text(angle = 90)\n    ) +\n    labs(x = \"Variable\")\n\n\n\n\n\n\n\n\nOf the remaining variables, adjective frequency and adjective type have the strongest effect and are confirmed as being important while syntactic function fails to perform better than the best shadow variable. All other variables have only a marginal effect on the use of really as an adjective amplifier."
  },
  {
    "objectID": "tutorials/introviz/introviz.html#basics-of-data-visualization",
    "href": "tutorials/introviz/introviz.html#basics-of-data-visualization",
    "title": "Introduction to Data Visualization in R",
    "section": "Basics of data visualization",
    "text": "Basics of data visualization\nOn a very general level, graphs should be used to inform the reader about properties and relationships between variables. This implies that…\n\ngraphs, including axes, must be labeled properly to allow the reader to understand the visualization with ease.\nthere should not be more dimensions in the visualization than there are in the data.\nall elements within a graph should be unambiguous.\nvariable scales should be portrayed accurately (for instance, lines - which imply continuity - should not be used for categorically scaled variables).\ngraphs should be as intuitive as possible and should not mislead the reader."
  },
  {
    "objectID": "tutorials/introviz/introviz.html#graphics-philosophies",
    "href": "tutorials/introviz/introviz.html#graphics-philosophies",
    "title": "Introduction to Data Visualization in R",
    "section": "Graphics philosophies",
    "text": "Graphics philosophies\nThe three main frameworks in which to create graphics are basic framework, the lattice framework, and the ggplot or tidyverse framework. These frameworks reflect the changing nature of R as a programming language (or as a programming environment). The so-called base R consists of about 30 packages that are always loaded automatically when you open R - it is, so to say - the default version of using R when nothing else is loaded. The base R framework is the oldest way to generate visualizations in R that was used when other packages did not exists yet. However, base R can and is still used to create visualizations although most visualizations are now generated using the ggplot or tidyverse framework. The lattice framework followed the base R framework and offered some advantages such as handy ways to split up visualizations. However, lattice was replaced by the ggplot or tidyverse framework because the latter are much more flexible, offer full control, and follow an easy to understand syntax.\nWe will briefly elaborate on these three frameworks before moving on.\n\nThe base R framework\nThe base R framework is the oldest of the three and is included in what is called the base R - a collection of about 30 packages that are automatically activated/loaded when you start R. The idea behind the “base” environment is that the creation of graphics is seen in analogy to a painter who paints on an empty canvass. Each line or element is added to the graph consecutively which oftentimes leads to code that is very comprehensible but also very long.\n\n\nThe lattice framework\nThe lattice environment was a follow-up to the base framework and it complements it insofar as it made it much easier to display various variables and variable levels simultaneously. The philosophy of the lattice-package is quite different from the philosophy of base: whereas everything had to be specified in base, the graphs created in the lattice environment require only very little code but are therefore very easily created when one is satisfied with the design but very labor intensive when it comes to customizing graphs. However, lattice is very handy when summarizing relationships between multiple variable and variable levels.\n\n\nThe ggplot framework\nThe ggplot environment was written by Hadley Wickham and it combines the positive aspects of both the base and the lattice package. It was first publicized in the gplot and ggplot1 packages but the latter was soon repackaged and improved in the now most widely used package for data visualization: the ggplot2 package. The ggplot environment implements a philosophy of graphic design described in builds on The Grammar of Graphics by Leland Wilkinson (Wilkinson).\nThe philosophy of ggplot2 is to consider graphics as consisting out of basic elements (called aesthetics and they include, for instance, the data set to be plotted and the axes) and layers that overlaid onto the aesthetics. The idea of the ggplot2 package can be summarized as taking “care of many of the fiddly details that make plotting a hassle (like drawing legends) as well as providing a powerful model of graphics that makes it easy to produce complex multi-layered graphics.”\nThus, ggplots typically start with the function call (ggplot) followed by the specification of the data, then the aesthetics (aes), and then a specification of the type of plot that is created (geom_line for line graphs, geom_box for box plots, geom_bar for bar graphs, geom_text for text, etc.). In addition, ggplot allows to specify all elements that the graph consists of (e.g. the theme and axes). The underlying principle is that a visualization is build up by adding layers as shown below.\n\n\n\n\n\n\n\n\n\nAs the ggplot framework has become the dominant way to create visualizations in R, we will only focus on this framework in the following practical examples."
  },
  {
    "objectID": "tutorials/introviz/introviz.html#preparation-and-session-set-up",
    "href": "tutorials/introviz/introviz.html#preparation-and-session-set-up",
    "title": "Introduction to Data Visualization in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"vcd\")\ninstall.packages(\"SnowballC\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"gridExtra\")\ninstall.packages(\"flextable\")\ninstall.packages(\"RColorBrewer\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\n\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# activate packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(flextable)\nlibrary(RColorBrewer)\n\nlibrary(gridExtra)\n\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/introviz/introviz.html#footnotes",
    "href": "tutorials/introviz/introviz.html#footnotes",
    "title": "Introduction to Data Visualization in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/clust/clust.html#underlying-concepts",
    "href": "tutorials/clust/clust.html#underlying-concepts",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Underlying Concepts",
    "text": "Underlying Concepts\nThe next section focuses on the basic idea that underlies all cluster analyses. WE will have a look at some very basic examples to highlight and discuss the principles that cluster analyses rely on.\nThe underlying idea of cluster analysis is very simple and rather intuitive as we ourselves perform cluster analyses every day in our lives. This is so because we group things together under certain labels and into concepts. The first example used to show this, deals with types of trees and how we group these types of trees based on their outward appearance.\nImagine you see six trees representing different types of trees: a pine tree, a fir tree, an oak tree, a beech tree, a phoenix palm tree, and a nikau palm tree. Now, you were asked to group these trees according to similarity. Have a look at the plot below and see whether you would have come up with a similar type of grouping.\n\nAn alternative way to group the trees would be the following.\n\nIn this display, conifers and broad-leaf trees are grouped together because there are more similar to each other compared to palm trees. This poses the question of what is meant by similarity. Consider the display below.\n\nAre the red and the blue line more similar because they have the same shape or are the red and the black line more similar because they are closer together? There is no single correct answer here. Rather the plot intends to raise awareness about the fact that how cluster analyses group data depends on how similarity is defined in the respective algorithm.\nLet’s consider another example to better understand how cluster analyses determine which data points should be merged when. Imagine you have five students and want to group them together based on their overall performance in school. The data that you rely on are their grades in math, music, and biology (with 1 being the best grade and 6 being the worst).\nStudentMathMusicBiologyStudentA232StudentB132StudentC121StudentD244StudentE343\nThe first step in determining the similarity among students is to create a distance matrix.\ndiststudents &lt;- dist(students, method = \"manhattan\") # create a distance matrix\nThe distance matrix below shows that Student A and Student B only differ by one grade. Student B and Student C differ by 2 grades. Student A and Student C differ by 3 grades and so on.\nStudentStudentAStudentBStudentCStudentDStudentB1StudentC32StudentD346StudentE3462\nBased on this distance matrix, we can now implement a cluster analysis in R."
  },
  {
    "objectID": "tutorials/clust/clust.html#cluster-analysis-on-numeric-data",
    "href": "tutorials/clust/clust.html#cluster-analysis-on-numeric-data",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Cluster Analysis on Numeric Data",
    "text": "Cluster Analysis on Numeric Data\nTo create a simple cluster object in R, we use the hclust function from the cluster package. The resulting object is then plotted to create a dendrogram which shows how students have been amalgamated (combined) by the clustering algorithm (which, in the present case, is called ward.D).\n# create hierarchical cluster object with ward.D as linkage method\nclusterstudents &lt;- hclust(diststudents, method = \"ward.D\")\n# plot result as dendrogram\nplot(clusterstudents, hang = 0)\n\nLet us have a look at how the clustering algorithm has amalgamated the students. The amalgamation process takes the distance matrix from above as a starting point and, in a first step, has merged Student A and Student B (because they were the most similar students in the data based on the distance matrix). After collapsing Student A and Student B, the resulting distance matrix looks like the distance matrix below (notice that Student A and Student B now form a cluster that is represented by the means of the grades of the two students).\n\nstudents2 &lt;- matrix(c(1.5, 3, 2, 1, 2, 1, 2, 4, 4, 3, 4, 3),\n  nrow = 4, byrow = T\n)\nstudents2 &lt;- as.data.frame(students2)\nrownames(students2) &lt;- c(\"Cluster1\", \"StudentC\", \"StudentD\", \"StudentE\")\ndiststudents2 &lt;- dist(students2, method = \"manhattan\")\n\nStudentCluster 1Student CStudent DStudent C2.5Student D3.56.0Student E3.56.02.0\nThe next lowest distance now is 2.0 between Student D and Student E which means that these two students are merged next. The resulting distance matrix is shown below.\n\nstudents3 &lt;- matrix(c(1.5, 3, 2, 1, 2, 1, 2.5, 4, 3.5),\n  nrow = 3, byrow = T\n)\nstudents3 &lt;- as.data.frame(students3)\nrownames(students3) &lt;- c(\"Cluster1\", \"StudentC\", \"Cluster2\")\ndiststudents3 &lt;- dist(students3,\n  method = \"manhattan\"\n)\n\n\n\nStudentCluster 1Student CStudent C2.5Cluster 23.56.0\n\n\nNow, the lowest distance value occurs between Cluster 1 and Student C. Thus, Student C and Cluster 1 are merged. In the final step, the Cluster 2 is merged with the new cluster encompassing Student C and Cluster 1. This amalgamation process can then be displayed visually as a dendrogram (see above).\nHow and which elements are merged depends on the what is understood as distance. Since “distance” is such an important concept in cluster analyses, we will briefly discuss this notion to understand why there are so many different types of clustering algorithms and this cluster analyses."
  },
  {
    "objectID": "tutorials/clust/clust.html#distance-and-similarity-measures",
    "href": "tutorials/clust/clust.html#distance-and-similarity-measures",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Distance and Similarity Measures",
    "text": "Distance and Similarity Measures\nTo understand how a cluster analysis determines to which cluster a given data point belongs, we need to understand what different distance measures represent. Have a look at the Figure below which visually represents three different ways to conceptualize distance.\n\n\n\n\n\n\n\n\n\nThe Figure above depicts three ways to measure distance: the euclidean distance represents the distance between points as the hypotenuse of the x- and y-axis distances while the “maximum distance” represents distance as the longer distance of either the distance on the x- or the y-axis. The manhatten distance (or block distance) is the sum of the distances on the x- and the y-axis.\nWe will now turn to another example in order to delve a little deeper into how clustering algorithms work. In this example, we will find cluster of varieties of English based on the relative frequency of selected non-standard features (such as the relative frequencies of cleft constructions and tag questions). As a first step, we generate some fictional data set for this analysis.\n\n# generate data\nIrishEnglish &lt;- round(sqrt((rnorm(10, 9.5, .5))^2), 3)\nScottishEnglish &lt;- round(sqrt((rnorm(10, 9.3, .4))^2), 3)\nBritishEnglish &lt;- round(sqrt((rnorm(10, 6.4, .7))^2), 3)\nAustralianEnglish &lt;- round(sqrt((rnorm(10, 6.6, .5))^2), 3)\nNewZealandEnglish &lt;- round(sqrt((rnorm(10, 6.5, .4))^2), 3)\nAmericanEnglish &lt;- round(sqrt((rnorm(10, 4.6, .8))^2), 3)\nCanadianEnglish &lt;- round(sqrt((rnorm(10, 4.5, .7))^2), 3)\nJamaicanEnglish &lt;- round(sqrt((rnorm(10, 1.4, .2))^2), 3)\nPhillipineEnglish &lt;- round(sqrt((rnorm(10, 1.5, .4))^2), 3)\nIndianEnglish &lt;- round(sqrt((rnorm(10, 1.3, .5))^2), 3)\nclus &lt;- data.frame(\n  IrishEnglish, ScottishEnglish, BritishEnglish,\n  AustralianEnglish, NewZealandEnglish, AmericanEnglish,\n  CanadianEnglish, JamaicanEnglish, PhillipineEnglish, IndianEnglish\n)\n# add row names\nrownames(clus) &lt;- c(\n  \"nae_neg\", \"like\", \"clefts\", \"tags\", \"youse\", \"soitwas\",\n  \"dt\", \"nsr\", \"invartag\", \"wh_cleft\"\n)\n\n\n\nFeatureIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishnae_neg9.7719.1377.1946.3076.7053.3444.2681.4492.1991.063like9.3199.9385.7866.3786.5844.7585.2971.8321.4011.365clefts9.0399.2715.9176.4316.5435.8681.8621.1631.2370.839tags9.8419.2596.2897.0426.0254.6883.8851.3781.1831.000youse9.7808.9636.7726.4166.3614.0572.9941.5011.8242.115soitwas9.2409.2756.2776.9386.2154.5814.9571.2952.1890.947dt9.5109.3647.4746.6985.8645.4934.6551.3831.2260.647nsr9.8779.5816.0557.1906.4385.4614.4691.6741.5901.334invartag9.6789.2487.0496.9386.5362.9624.4581.1771.9261.168wh_cleft9.6098.9446.3216.9545.8635.8784.0391.0971.1761.087\n\n\nAs a next step, we create a cluster object based on the data we have just generated.\n\n# clean data\nclusm &lt;- as.matrix(clus)\nclust &lt;- t(clusm) # transpose data\nclust &lt;- na.omit(clust) # remove missing values\nclusts &lt;- scale(clust) # standardize variables\nclusts &lt;- as.matrix(clusts) # convert into matrix\n\n\n\nVarietynae_neglikecleftstagsyousesoitwasdtnsrinvartagwh_cleftIrishEnglish9.7719.3199.0399.8419.7809.2409.5109.8779.6789.609ScottishEnglish9.1379.9389.2719.2598.9639.2759.3649.5819.2488.944BritishEnglish7.1945.7865.9176.2896.7726.2777.4746.0557.0496.321AustralianEnglish6.3076.3786.4317.0426.4166.9386.6987.1906.9386.954NewZealandEnglish6.7056.5846.5436.0256.3616.2155.8646.4386.5365.863AmericanEnglish3.3444.7585.8684.6884.0574.5815.4935.4612.9625.878CanadianEnglish4.2685.2971.8623.8852.9944.9574.6554.4694.4584.039JamaicanEnglish1.4491.8321.1631.3781.5011.2951.3831.6741.1771.097PhillipineEnglish2.1991.4011.2371.1831.8242.1891.2261.5901.9261.176IndianEnglish1.0631.3650.8391.0002.1150.9470.6471.3341.1681.087\n\n\nWe assess if data is “clusterable” by testing if the data contains non-randomness. To this end, we calculate the Hopkins statistic which indicates how similar the data is to a random distribution.\n\nA Hopkins value of 0.5 indicates that the data is random and that there are no inherent clusters.\nIf the Hopkins statistic is close to 1, then the data is highly clusterable.\nValues of 0 indicate that the data is uniform (Aggarwal, 158).\n\nThe n in the get_clust_tendency functions represents the maximum number of clusters to be tested which should be number of predictors in the data.\n\n# apply get_clust_tendency to cluster object\nclusttendency &lt;- get_clust_tendency(clusts,\n  # define number of points from sample space\n  n = 9,\n  gradient = list(\n    # define color for low values\n    low = \"steelblue\",\n    # define color for high values\n    high = \"white\"\n  )\n)\nclusttendency[1]\n\n$hopkins_stat\n[1] 0.7425154\n\n\nAs the Hopkins value is substantively higher than .5 (randomness) and closer to 1 (highly clusterable) than to .5, thus indicating that there is sufficient structure in the data to warrant a cluster analysis. As such, we can assume that there are actual clusters in the data and continue by generating a distance matrix using euclidean distances.\n\nclustd &lt;- dist(clusts, # create distance matrix\n  method = \"euclidean\"\n) # use euclidean (!) distance\n\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.000.523.122.883.305.035.588.238.038.46ScottishEnglish0.520.002.902.633.024.745.307.967.768.20BritishEnglish3.122.900.000.700.682.252.645.184.975.42AustralianEnglish2.882.630.700.000.662.252.795.385.185.62NewZealandEnglish3.303.020.680.660.001.992.454.964.765.20AmericanEnglish5.034.742.252.251.990.001.593.463.363.73CanadianEnglish5.585.302.642.792.451.590.002.862.683.15JamaicanEnglish8.237.965.185.384.963.462.860.000.490.42PhillipineEnglish8.037.764.975.184.763.362.680.490.000.65IndianEnglish8.468.205.425.625.203.733.150.420.650.00\n\n\nBelow are other methods to create distance matrices with some comments on when using which metric is appropriate.\n# create distance matrix (euclidean method: not good when dealing with many dimensions)\nclustd &lt;- dist(clusts, method = \"euclidean\")\n# create distance matrix (maximum method: here the difference between points dominates)\nclustd_maximum &lt;- round(dist(clusts, method = \"maximum\"), 2)\n# create distance matrix (manhattan method: most popular choice)\nclustd_manhatten &lt;- round(dist(clusts, method = \"manhattan\"), 2)\n# create distance matrix (canberra method: for count data only - focuses on small differences and neglects larger differences)\nclustd_canberra &lt;- round(dist(clusts, method = \"canberra\"), 2)\n# create distance matrix (binary method: for binary data only!)\nclustd_binary &lt;- round(dist(clusts, method = \"binary\"), 2)\n# create distance matrix (minkowski method: is not a true distance measure)\nclustd_minkowski &lt;- round(dist(clusts, method = \"minkowski\"), 2)\n# distance method for words: daisy (other possible distances are \"manhattan\" and \"gower\")\nclustd_daisy &lt;- round(daisy(clusts, metric = \"euclidean\"), 2)\nIf you call the individual distance matrices, you will see that depending on which distance measure is used, the distance matrices differ dramatically! Have a look at the distance matrix created using the manhatten metric and compare it to the distance matrix created using the euclidian metric (see above).\n\nclustd_maximum\n\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.000.271.221.121.192.092.262.762.692.78ScottishEnglish0.270.001.361.171.101.952.272.662.802.81BritishEnglish1.221.360.000.360.491.271.261.871.922.10AustralianEnglish1.121.170.360.000.351.231.401.881.832.00NewZealandEnglish1.191.100.490.350.001.111.431.681.701.80AmericanEnglish2.091.951.271.231.110.001.221.511.491.54CanadianEnglish2.262.271.261.401.431.220.001.221.281.34JamaicanEnglish2.762.661.871.881.681.511.220.000.300.23PhillipineEnglish2.692.801.921.831.701.491.280.300.000.41IndianEnglish2.782.812.102.001.801.541.340.230.410.00\n\n\nNext, we create a distance plot using the distplot function. If the distance plot shows different regions (non-random, non-uniform gray areas) then clustering the data is permittable as the data contains actual structures.\n\n# create distance plot\ndissplot(clustd)\n\n\n\n\n\n\n\n\nThe most common method for clustering is called ward.D or ward.D2. Both of these linkage functions seek to minimize variance. This means that they cluster in a way that the amount of variance is at a minimum (comparable to the regression line in an ordinary least squares (OLS) design).\n# create cluster object\ncd &lt;- hclust(clustd, method = \"ward.D2\")\n# display dendrogram\nplot(cd, hang = -1)\n\nWe will briefly go over some other, alternative linkage methods. Which linkage method is and should be used depends on various factors, for example, the type of variables (nominal versus numeric) or whether the focus should be placed on commonalities or differences.\n# single linkage: cluster with nearest data point\ncd_single &lt;- hclust(clustd, method = \"single\")\n# create cluster object (ward.D linkage)\ncd_wardd &lt;- hclust(clustd, method = \"ward.D\")\n# create cluster object (ward.D2 linkage):\n# cluster in a way to achieve minimum variance\ncd_wardd2 &lt;- hclust(clustd, method = \"ward.D2\")\n# average linkage: cluster with closest mean\ncd_average &lt;- hclust(clustd, method = \"average\")\n# mcquitty linkage\ncd_mcquitty &lt;- hclust(clustd, method = \"mcquitty\")\n# median linkage: cluster with closest median\ncd_median &lt;- hclust(clustd, method = \"median\")\n# centroid linkage: cluster with closest prototypical point of target cluster\ncd_centroid &lt;- hclust(clustd, method = \"centroid\")\n# complete linkage: cluster with nearest/furthest data point of target cluster\ncd_complete &lt;- hclust(clustd, method = \"complete\")\nNow, we determine the optimal number of clusters based on silhouette widths which shows the ratio of internal similarity of clusters against the similarity between clusters. If the silhouette widths have values lower than .2 then this indicates that clustering is not appropriate (Levshina, 311). The function below displays the silhouette width values of 2 to 8 clusters.\n\noptclus &lt;- sapply(2:8, function(x) summary(silhouette(cutree(cd, k = x), clustd))$avg.width)\noptclus # inspect results\n\n[1] 0.5741389 0.6425794 0.6961735 0.6161859 0.4186355 0.4080729 0.2104184\n\noptnclust &lt;- which(optclus == max(optclus)) # determine optimal number of clusters\ngroups &lt;- cutree(cd, k = optnclust) # cut tree into optimal number of clusters\n\nThe optimal number of clusters is the cluster solution with the highest silhouette width. We cut the tree into the optimal number of clusters and plot the result.\ngroups &lt;- cutree(cd, k = optnclust) # cut tree into optimal clusters\nplot(cd, hang = -1, cex = .75) # plot result as dendrogram\nrect.hclust(cd, k = optnclust, border = \"red\") # draw red borders around clusters\n\nIn a next step, we aim to determine which factors are particularly important for the clustering - this step is comparable to measuring the effect size in inferential designs.\n\n# which factors are particularly important\nceltic &lt;- clusts[c(1, 2), ]\nothers &lt;- clusts[-c(1, 2), ]\n# calculate column means\nceltic.cm &lt;- colMeans(celtic)\nothers.cm &lt;- colMeans(others)\n# calculate difference between celtic and other englishes\ndiff &lt;- celtic.cm - others.cm\nsort(diff, decreasing = F)\n\n      dt wh_cleft   clefts invartag  soitwas  nae_neg     tags      nsr \n1.616065 1.653324 1.657821 1.688336 1.694390 1.719798 1.742143 1.743499 \n    like    youse \n1.788169 1.788212 \n\n\nplot(sort(diff), # y-values\n  1:length(diff), # x-values\n  type = \"n\", # plot type (empty)\n  cex.axis = .75, # axis font size\n  cex.lab = .75, # label font size\n  xlab = \"Prototypical for Non-Celtic Varieties (Cluster 2) &lt;-----&gt; Prototypical for Celtic Varieties (Cluster 1)\", # x-axis label\n  yaxt = \"n\", # no y-axis tick marks\n  ylab = \"\"\n) # no y-axis label\ntext(sort(diff), 1:length(diff), names(sort(diff)), cex = .75) # plot text into plot\n\n\nOuter &lt;- clusts[c(6:8), ] # data of outer circle varieties\nInner &lt;- clusts[-c(6:8), ] # data of inner circle varieties\nOuter.cm &lt;- colMeans(Outer) # column means for outer circle\nInner.cm &lt;- colMeans(Inner) # column means for inner circle\ndiff &lt;- Outer.cm - Inner.cm # difference between inner and outer circle\nsort(diff, decreasing = F) # order difference between inner and outer circle\n\n     youse   invartag    nae_neg     clefts       tags    soitwas        nsr \n-1.0604093 -0.9975212 -0.9682485 -0.8091655 -0.7722906 -0.7526520 -0.6846847 \n  wh_cleft       like         dt \n-0.6444093 -0.6105837 -0.6094369 \n\n\nplot( # start plot\n  sort(diff), # y-values\n  1:length(diff), # x-values\n  type = \"n\", # plot type (empty)\n  cex.axis = .75, # axis font size\n  cex.lab = .75, # label font size\n  xlab = \"Prototypical for Inner Circle Varieties (Cluster 2) &lt;-----&gt; Prototypical for Outer Circle Varieties (Cluster 1)\", # x-axis label\n  yaxt = \"n\", # no y-axis tick marks\n  ylab = \"\"\n) # no y-axis label\ntext(sort(diff), 1:length(diff), names(sort(diff)), cex = .75) # plot text into plot\n\nWe see that discourse like is typical for other varieties and that the use of youse as 2nd person plural pronoun and invariant tags are typical for Celtic Englishes.\nWe will now test whether the cluster is justified by validating the cluster solution using bootstrapping.\n\nres.pv &lt;- pvclust(clus, # apply pvclust method to clus data\n  method.dist = \"euclidean\", # use eucledian distance\n  method.hclust = \"ward.D2\", # use ward.d2 linkage\n  nboot = 100\n) # use 100 bootstrap runs\n\nBootstrap (r = 0.5)... Done.\nBootstrap (r = 0.6)... Done.\nBootstrap (r = 0.7)... Done.\nBootstrap (r = 0.8)... Done.\nBootstrap (r = 0.9)... Done.\nBootstrap (r = 1.0)... Done.\nBootstrap (r = 1.1)... Done.\nBootstrap (r = 1.2)... Done.\nBootstrap (r = 1.3)... Done.\nBootstrap (r = 1.4)... Done.\n\n\nThe clustering provides approximately unbiased p-values and bootstrap probability value (see Levshina, 316).\n\nplot(res.pv, cex = .75)\npvrect(res.pv)\n\n\n\n\n\n\n\n\nWe can also use other packages to customize the dendrograms.\nplot(as.phylo(cd), # plot cluster object\n  cex = 0.75, # .75 font size\n  label.offset = .5\n) # .5 label offset\n\nOne useful customization is to display an unrooted rather than a rooted tree diagram.\n# plot as unrooted tree\nplot(as.phylo(cd), # plot cluster object\n  type = \"unrooted\", # plot as unrooted tree\n  cex = .75, # .75 font size\n  label.offset = 1\n) # .5 label offset"
  },
  {
    "objectID": "tutorials/clust/clust.html#cluster-analysis-on-nominal-data",
    "href": "tutorials/clust/clust.html#cluster-analysis-on-nominal-data",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Cluster Analysis on Nominal Data",
    "text": "Cluster Analysis on Nominal Data\nSo far, all analyses were based on numeric data. However, especially when working with language data, the data is nominal or categorical rather than numeric. The following will thus show to implement a clustering method for nominal data.\nIn a first step, we will create a simple data set representing the presence and absence of features across varieties of English.\n\n# generate data\nIrishEnglish &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\nScottishEnglish &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\nBritishEnglish &lt;- c(0, 1, 1, 1, 0, 0, 1, 0, 1, 1)\nAustralianEnglish &lt;- c(0, 1, 1, 1, 0, 0, 1, 0, 1, 1)\nNewZealandEnglish &lt;- c(0, 1, 1, 1, 0, 0, 1, 0, 1, 1)\nAmericanEnglish &lt;- c(0, 1, 1, 1, 0, 0, 0, 0, 1, 0)\nCanadianEnglish &lt;- c(0, 1, 1, 1, 0, 0, 0, 0, 1, 0)\nJamaicanEnglish &lt;- c(0, 0, 1, 0, 0, 0, 0, 0, 1, 0)\nPhillipineEnglish &lt;- c(0, 0, 1, 0, 0, 0, 0, 0, 1, 0)\nIndianEnglish &lt;- c(0, 0, 1, 0, 0, 0, 0, 0, 1, 0)\nclus &lt;- data.frame(\n  IrishEnglish, ScottishEnglish, BritishEnglish,\n  AustralianEnglish, NewZealandEnglish, AmericanEnglish,\n  CanadianEnglish, JamaicanEnglish, PhillipineEnglish, IndianEnglish\n)\n# add row names\nrownames(clus) &lt;- c(\n  \"nae_neg\", \"like\", \"clefts\", \"tags\", \"youse\", \"soitwas\",\n  \"dt\", \"nsr\", \"invartag\", \"wh_cleft\"\n)\n# convert into factors\nclus &lt;- apply(clus, 1, function(x) {\n  x &lt;- as.factor(x)\n})\n\n\n\nVarietynae_neglikecleftstagsyousesoitwasdtnsrinvartagwh_cleftIrishEnglish1111111111ScottishEnglish1111111111BritishEnglish0111001011AustralianEnglish0111001011NewZealandEnglish0111001011AmericanEnglish0111000010CanadianEnglish0111000010JamaicanEnglish0010000010PhillipineEnglish0010000010IndianEnglish0010000010\n\n\nNow that we have our data, we will create a distance matrix but in contrast to previous methods, we will use a different distance measure that takes into account that we are dealing with nominal (or binary) data.\n\n# clean data\nclusts &lt;- as.matrix(clus)\n# create distance matrix\nclustd &lt;- dist(clusts, method = \"binary\") # create a distance object with binary (!) distance\n\n\n\nVarietyIrishEnglishScottishEnglishBritishEnglishAustralianEnglishNewZealandEnglishAmericanEnglishCanadianEnglishJamaicanEnglishPhillipineEnglishIndianEnglishIrishEnglish0.00.00.400.400.400.600.600.800.800.80ScottishEnglish0.00.00.400.400.400.600.600.800.800.80BritishEnglish0.40.40.000.000.000.330.330.670.670.67AustralianEnglish0.40.40.000.000.000.330.330.670.670.67NewZealandEnglish0.40.40.000.000.000.330.330.670.670.67AmericanEnglish0.60.60.330.330.330.000.000.500.500.50CanadianEnglish0.60.60.330.330.330.000.000.500.500.50JamaicanEnglish0.80.80.670.670.670.500.500.000.000.00PhillipineEnglish0.80.80.670.670.670.500.500.000.000.00IndianEnglish0.80.80.670.670.670.500.500.000.000.00\n\n\nAs before, we can now use hierarchical clustering to display the results as a dendrogram\n\n# create cluster object (ward.D2 linkage)   : cluster in a way to achieve minimum variance\ncd &lt;- hclust(clustd, method = \"ward.D2\")\n# plot result as dendrogram\nplot(cd, hang = -1) # display dendogram\n\n\n\n\n\n\n\n\nIn a next step, we want to determine which features are particularly distinctive for one cluster (the “Celtic” cluster containing Irish and Scottish English).\n\n# create factor with celtic varieties on one hand and other varieties on other\ncluster &lt;- as.factor(ifelse(as.character(rownames(clusts)) == \"IrishEnglish\", \"1\",\n  ifelse(as.character(rownames(clusts)) == \"ScottishEnglish\", \"1\", \"0\")\n))\n# convert into data frame\nclsts.df &lt;- as.data.frame(clusts)\n# determine significance\nlibrary(exact2x2)\npfish &lt;- fisher.exact(table(cluster, clsts.df$youse))\npfish[[1]]\n\n[1] 0.02222222\n\n# determine effect size\nassocstats(table(cluster, clsts.df$youse))\n\n                    X^2 df  P(&gt; X^2)\nLikelihood Ratio 10.008  1 0.0015586\nPearson          10.000  1 0.0015654\n\nPhi-Coefficient   : 1 \nContingency Coeff.: 0.707 \nCramer's V        : 1 \n\nassocstats(table(cluster, clsts.df$like))\n\n                    X^2 df P(&gt; X^2)\nLikelihood Ratio 1.6323  1  0.20139\nPearson          1.0714  1  0.30062\n\nPhi-Coefficient   : 0.327 \nContingency Coeff.: 0.311 \nCramer's V        : 0.327 \n\n\nClustering is a highly complex topic and there many more complexities to it. However, this should have helped to get you started."
  },
  {
    "objectID": "tutorials/clust/clust.html#footnotes",
    "href": "tutorials/clust/clust.html#footnotes",
    "title": "Cluster and Correspondence Analysis in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [bibliography file](/assets/bibliography.bib and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/net/net.html#creating-a-matrix",
    "href": "tutorials/net/net.html#creating-a-matrix",
    "title": "Network Analysis using R",
    "section": "Creating a matrix",
    "text": "Creating a matrix\nWe start by loading the data which represents a table that contains the personas that are present during a sub-scene as well as how many contributions they make and how often they occur.\n\n# load data\nnet_dat &lt;- read.delim(\"tutorials/net/data/romeo_tidy.txt\", sep = \"\\t\")\n\n\n\nactscenepersoncontriboccurrencesACT I_SCENE IBENVOLIO247ACT I_SCENE ICAPULET29ACT I_SCENE IFIRST CITIZEN12ACT I_SCENE ILADY CAPULET110ACT I_SCENE IMONTAGUE63ACT I_SCENE IPRINCE13ACT I_SCENE IROMEO1614ACT I_SCENE ITYBALT23ACT I_SCENE IIBENVOLIO57ACT I_SCENE IICAPULET39ACT I_SCENE IIPARIS25ACT I_SCENE IIROMEO1114ACT I_SCENE IISERVANT83ACT I_SCENE IIIJULIET511ACT I_SCENE IIILADY CAPULET1110\n\n\nWe now transform that table into a co-occurrence matrix.\n\nnet_cmx &lt;- crossprod(table(net_dat[1:2]))\ndiag(net_cmx) &lt;- 0\nnet_df &lt;- as.data.frame(net_cmx)\n\n\n\nPersonaBALTHASARBENVOLIOCAPULETFIRST CITIZENFIRST SERVANTBALTHASAR00100BENVOLIO00321CAPULET13012FIRST CITIZEN02100FIRST SERVANT01200\n\n\nThe data shows how often a character has appeared with each other character in the play - only Friar Lawrence and Friar John were excluded because they only appear in one scene where they talk to each other."
  },
  {
    "objectID": "tutorials/net/net.html#quanteda-networks",
    "href": "tutorials/net/net.html#quanteda-networks",
    "title": "Network Analysis using R",
    "section": "Quanteda Networks",
    "text": "Quanteda Networks\nThe quanteda package contains many very useful functions for analyzing texts. Among these functions is the textplot_network function which provides a very handy way to display networks. The advantage of the network plots provided by or generated with the quanteda package is that you can create them with very little code. However, this comes at a cost as these visualizations cannot be modified easily (which means that their design is not very flexible compared to other methods for generating network visualizations).\nIn a first step, we transform the text vectors of the romeo data into a document-feature matrix using the dfm function.\n\n# create a document feature matrix\nnet_dfm &lt;- quanteda::as.dfm(net_df)\n# create feature co-occurrence matrix\nnet_fcm &lt;- quanteda::fcm(net_dfm, tri = F)\n# inspect data\nhead(net_fcm)\n\nFeature co-occurrence matrix of: 6 by 18 features.\n                features\nfeatures         BALTHASAR BENVOLIO CAPULET FIRST CITIZEN FIRST SERVANT\n  BALTHASAR              1       25      31            11             6\n  BENVOLIO              25       39      93            39            27\n  CAPULET               31       93      65            42            39\n  FIRST CITIZEN         11       39      42             6            10\n  FIRST SERVANT          6       27      39            10             3\n  FRIAR LAWRENCE        20       53      74            18            17\n                features\nfeatures         FRIAR LAWRENCE JULIET LADY CAPULET MERCUTIO MONTAGUE\n  BALTHASAR                  20     26           31       11       17\n  BENVOLIO                   53     87           99       42       55\n  CAPULET                    74    131          117       52       65\n  FIRST CITIZEN              18     32           36       24       29\n  FIRST SERVANT              17     40           42       12       15\n  FRIAR LAWRENCE             15     61           72       23       32\n[ reached max_nfeat ... 8 more features ]\n\n\nThis feature-co-occurrence matrix can then serve as the input for the textplot_network function which already generates a nice network graph.\nNow we generate a network graph using the textplot_network function from the quanteda.textplots package. This function has the following arguments:\n\nx: a fcm or dfm object\n\nmin_freq: a frequency count threshold or proportion for co-occurrence frequencies of features to be included (default = 0.5),\n\nomit_isolated: if TRUE, features do not occur more frequent than min_freq will be omitted (default = TRUE),\n\nedge_color: color of edges that connect vertices (default = “#1F78B4”),\nedge_alpha: opacity of edges ranging from 0 to 1.0 (default = 0.5),\nedge_size: size of edges for most frequent co-occurrence (default = 2),\nvertex_color: color of vertices (default = “#4D4D4D”),\nvertex_size: size of vertices (default = 2),\nvertex_labelcolor: color of texts. Defaults to the same as vertex_color,\nvertex_labelfont: font-family of texts,\nvertex_labelsize: size of vertex labels in mm. Defaults to size 5. Supports both integer values and vector values (default = 5),\noffset: if NULL (default), the distance between vertices and texts are determined automatically,\n\n\nquanteda.textplots::textplot_network(\n    x = net_fcm, # a fcm or dfm object\n    min_freq = 0.5, # frequency count threshold or proportion for co-occurrence frequencies (default = 0.5)\n    edge_alpha = 0.5, # opacity of edges ranging from 0 to 1.0 (default = 0.5)\n    edge_color = \"gray\", # color of edges that connect vertices (default = \"#1F78B4\")\n    edge_size = 2, # size of edges for most frequent co-occurrence (default = 2)\n    # calculate the size of vertex labels for the network plot\n    vertex_labelsize = net_dfm %&gt;%\n        # convert the dfm object to a data frame\n        quanteda::convert(to = \"data.frame\") %&gt;%\n        # exclude the 'doc_id' column\n        dplyr::select(-doc_id) %&gt;%\n        # calculate the sum of row values for each row\n        rowSums() %&gt;%\n        # apply the natural logarithm to the resulting sums\n        log(),\n    vertex_color = \"#4D4D4D\", # color of vertices (default = \"#4D4D4D\")\n    vertex_size = 2 # size of vertices (default = 2)\n)\n\n\n\n\n\n\n\n\nWe now turn to generating tidy networks with is more complex but also offers more flexibility and options for customization."
  },
  {
    "objectID": "tutorials/net/net.html#tidy-networks",
    "href": "tutorials/net/net.html#tidy-networks",
    "title": "Network Analysis using R",
    "section": "Tidy Networks",
    "text": "Tidy Networks\nWe now turn to a different method for generating networks that is extremely flexible.\nFirst, we define the nodes and we can also add information about the nodes that we can use later on (such as frequency information).\n\n# create a new data frame 'va' using the 'net_dat' data\nnet_dat %&gt;%\n    # rename the 'person' column to 'node' and 'occurrences' column to 'n'\n    dplyr::rename(\n        node = person,\n        n = occurrences\n    ) %&gt;%\n    # group the data by the 'node' column\n    dplyr::group_by(node) %&gt;%\n    # summarize the data, calculating the total occurrences ('n') for each 'node'\n    dplyr::summarise(n = sum(n)) -&gt; va\n\n\n\nnodenBALTHASAR4BENVOLIO49CAPULET81FIRST CITIZEN4FIRST SERVANT4FRIAR LAWRENCE49JULIET121LADY CAPULET100MERCUTIO16MONTAGUE9NURSE121PARIS25PETER4PRINCE9ROMEO196SECOND SERVANT9SERVANT9TYBALT9\n\n\nThe next part is optional but it can help highlight important information. We add a column with additional information to our nodes table.\n\n# define family\nmon &lt;- c(\"ABRAM\", \"BALTHASAR\", \"BENVOLIO\", \"LADY MONTAGUE\", \"MONTAGUE\", \"ROMEO\")\ncap &lt;- c(\"CAPULET\", \"CAPULET’S COUSIN\", \"FIRST SERVANT\", \"GREGORY\", \"JULIET\", \"LADY CAPULET\", \"NURSE\", \"PETER\", \"SAMPSON\", \"TYBALT\")\noth &lt;- c(\"APOTHECARY\", \"CHORUS\", \"FIRST CITIZEN\", \"FIRST MUSICIAN\", \"FIRST WATCH\", \"FRIAR JOHN\", \"FRIAR LAWRENCE\", \"MERCUTIO\", \"PAGE\", \"PARIS\", \"PRINCE\", \"SECOND MUSICIAN\", \"SECOND SERVANT\", \"SECOND WATCH\", \"SERVANT\", \"THIRD MUSICIAN\")\n# create color vectors\nva &lt;- va %&gt;%\n    dplyr::mutate(type = dplyr::case_when(\n        node %in% mon ~ \"MONTAGUE\",\n        node %in% cap ~ \"CAPULET\",\n        TRUE ~ \"Other\"\n    ))\n# inspect updates nodes table\nva\n\n# A tibble: 18 × 3\n   node               n type    \n   &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;   \n 1 BALTHASAR          4 MONTAGUE\n 2 BENVOLIO          49 MONTAGUE\n 3 CAPULET           81 CAPULET \n 4 FIRST CITIZEN      4 Other   \n 5 FIRST SERVANT      4 CAPULET \n 6 FRIAR LAWRENCE    49 Other   \n 7 JULIET           121 CAPULET \n 8 LADY CAPULET     100 CAPULET \n 9 MERCUTIO          16 Other   \n10 MONTAGUE           9 MONTAGUE\n11 NURSE            121 CAPULET \n12 PARIS             25 Other   \n13 PETER              4 CAPULET \n14 PRINCE             9 Other   \n15 ROMEO            196 MONTAGUE\n16 SECOND SERVANT     9 Other   \n17 SERVANT            9 Other   \n18 TYBALT             9 CAPULET \n\n\n\n\nnodentypeBALTHASAR4MONTAGUEBENVOLIO49MONTAGUECAPULET81CAPULETFIRST CITIZEN4OtherFIRST SERVANT4CAPULETFRIAR LAWRENCE49OtherJULIET121CAPULETLADY CAPULET100CAPULETMERCUTIO16OtherMONTAGUE9MONTAGUENURSE121CAPULETPARIS25OtherPETER4CAPULETPRINCE9OtherROMEO196MONTAGUE\n\n\nNow, we define the edges, i.e., the connections between nodes and, again, we can add information in separate variables that we can use later on.\n\n# create a new data frame 'ed' using the 'dat' data\ned &lt;- net_df %&gt;%\n    # add a new column 'from' with row names\n    dplyr::mutate(from = rownames(.)) %&gt;%\n    # reshape the data from wide to long format using 'gather'\n    tidyr::gather(to, n, BALTHASAR:TYBALT) %&gt;%\n    # remove zero frequencies\n    dplyr::filter(n != 0)\n\n\n\nfromtonCAPULETBALTHASAR1FRIAR LAWRENCEBALTHASAR1JULIETBALTHASAR1LADY CAPULETBALTHASAR1MONTAGUEBALTHASAR1PARISBALTHASAR1PRINCEBALTHASAR1ROMEOBALTHASAR2CAPULETBENVOLIO3FIRST CITIZENBENVOLIO2FIRST SERVANTBENVOLIO1JULIETBENVOLIO1LADY CAPULETBENVOLIO2MERCUTIOBENVOLIO4MONTAGUEBENVOLIO2\n\n\nNow that we have generated tables for the edges and the nodes, we can generate a graph object.\n\nig &lt;- igraph::graph_from_data_frame(d = ed, vertices = va, directed = FALSE)\n\nWe will also add labels to the nodes as follows:\n\ntg &lt;- tidygraph::as_tbl_graph(ig) %&gt;%\n    tidygraph::activate(nodes) %&gt;%\n    dplyr::mutate(label = name)\n\nWhen we now plot our network, it looks as shown below.\n\n# set seed (so that the exact same network graph is created every time)\nset.seed(12345)\n\n# create a graph using the 'tg' data frame with the Fruchterman-Reingold layout\ntg %&gt;%\n    ggraph::ggraph(layout = \"fr\") +\n\n    # add arcs for edges with various aesthetics\n    geom_edge_arc(\n        colour = \"gray50\",\n        lineend = \"round\",\n        strength = .1,\n        aes(\n            edge_width = ed$n,\n            alpha = ed$n\n        )\n    ) +\n\n    # add points for nodes with size based on log-transformed 'v.size' and color based on 'va$Family'\n    geom_node_point(\n        size = log(va$n) * 2,\n        aes(color = va$type)\n    ) +\n\n    # add text labels for nodes with various aesthetics\n    geom_node_text(aes(label = name),\n        repel = TRUE,\n        point.padding = unit(0.2, \"lines\"),\n        size = sqrt(va$n),\n        colour = \"gray10\"\n    ) +\n\n    # adjust edge width and alpha scales\n    scale_edge_width(range = c(0, 2.5)) +\n    scale_edge_alpha(range = c(0, .3)) +\n\n    # set graph background color to white\n    theme_graph(background = \"white\") +\n\n    # adjust legend position to the top\n    theme(\n        legend.position = \"top\",\n        # suppress legend title\n        legend.title = element_blank()\n    ) +\n\n    # remove edge width and alpha guides from the legend\n    guides(\n        edge_width = FALSE,\n        edge_alpha = FALSE\n    )"
  },
  {
    "objectID": "tutorials/net/net.html#degree-centrality",
    "href": "tutorials/net/net.html#degree-centrality",
    "title": "Network Analysis using R",
    "section": "Degree centrality",
    "text": "Degree centrality\nWe now generate an edge list from the dg object and then extract the degree centrality. The degree centrality reflects how many edges each node has with the most central node having the highest value.\n\ndgg &lt;- graph.edgelist(as.matrix(dg), directed = T)\n# extract degree centrality\nigraph::degree(dgg) %&gt;%\n    as.data.frame() %&gt;%\n    tibble::rownames_to_column(\"node\") %&gt;%\n    dplyr::rename(`degree centrality` = 2) %&gt;%\n    dplyr::arrange(-`degree centrality`) -&gt; dc_tbl\n\n\n\nnodedegree centralityROMEO108CAPULET92LADY CAPULET90NURSE76JULIET72BENVOLIO68MONTAGUE44PRINCE44TYBALT44PARIS42FRIAR LAWRENCE40SECOND SERVANT32MERCUTIO30SERVANT30FIRST CITIZEN28"
  },
  {
    "objectID": "tutorials/net/net.html#central-node",
    "href": "tutorials/net/net.html#central-node",
    "title": "Network Analysis using R",
    "section": "Central node",
    "text": "Central node\nNext, we extract the most central node.\n\nnames(igraph::degree(dgg))[which(igraph::degree(dgg) == max(igraph::degree(dgg)))]\n\n[1] \"ROMEO\""
  },
  {
    "objectID": "tutorials/net/net.html#betweenness-centrality",
    "href": "tutorials/net/net.html#betweenness-centrality",
    "title": "Network Analysis using R",
    "section": "Betweenness centrality",
    "text": "Betweenness centrality\nWe now extract the betweenness centrality. Betweenness centrality provides a measure of how important nodes are for information flow between nodes in a network. The node with the highest betweenness centrality creates the shortest paths in the network. The higher a node’s betweenness centrality, the more important it is for the efficient flow of goods in a network.\n\nigraph::betweenness(dgg) %&gt;%\n    as.data.frame() %&gt;%\n    tibble::rownames_to_column(\"node\") %&gt;%\n    dplyr::rename(`betweenness centrality` = 2) %&gt;%\n    dplyr::arrange(-`betweenness centrality`) -&gt; bc_tbl\n\n\n\nnodebetweenness centralityROMEO27.62437026LADY CAPULET16.27686423CAPULET15.62321868BENVOLIO9.61512099NURSE7.40145363JULIET5.55471008TYBALT3.19940849MONTAGUE2.18220323PRINCE2.18220323PARIS1.85942942FRIAR LAWRENCE1.09118044MERCUTIO0.84421390PETER0.26841707SERVANT0.23874480FIRST CITIZEN0.03846154\n\n\nWe now extract the node with the highest betweenness centrality.\n\nnames(igraph::betweenness(dgg))[which(igraph::betweenness(dgg) == max(igraph::betweenness(dgg)))]\n\n[1] \"ROMEO\""
  },
  {
    "objectID": "tutorials/net/net.html#closeness",
    "href": "tutorials/net/net.html#closeness",
    "title": "Network Analysis using R",
    "section": "Closeness",
    "text": "Closeness\nIn addition, we extract the closeness statistic of all edges in the dg object by using the closeness function from the igraph package. Closeness centrality refers to the shortest paths between nodes. The distance between two nodes represents the length of the shortest path between them. The closeness of a node is the average distance from that node to all other nodes.\n\nigraph::closeness(dgg) %&gt;%\n    as.data.frame() %&gt;%\n    tibble::rownames_to_column(\"node\") %&gt;%\n    dplyr::rename(closeness = 2) %&gt;%\n    dplyr::arrange(-closeness) -&gt; c_tbl\n\n\n\nnodeclosenessLADY CAPULET0.05882353ROMEO0.05882353CAPULET0.05555556BENVOLIO0.05263158JULIET0.05000000NURSE0.04761905TYBALT0.04761905MONTAGUE0.04545455PARIS0.04545455PRINCE0.04545455FRIAR LAWRENCE0.04166667SERVANT0.04166667FIRST SERVANT0.04000000MERCUTIO0.04000000SECOND SERVANT0.04000000\n\n\nWe now extract the node with the highest closeness.\n\nnames(igraph::closeness(dgg))[which(igraph::closeness(dgg) == max(igraph::closeness(dgg)))]\n\n[1] \"LADY CAPULET\" \"ROMEO\"       \n\n\nWe have reached the end of this tutorial and you now know how to create and modify networks in R and how you can highlight aspects of your data."
  },
  {
    "objectID": "tutorials/lex/lex.html#preparation-and-session-set-up",
    "href": "tutorials/lex/lex.html#preparation-and-session-set-up",
    "title": "Lexicography with R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"udpipe\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"coop\")\ninstall.packages(\"cluster\")\ninstall.packages(\"flextable\")\ninstall.packages(\"textdata\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nIn a next step, we load the packages.\n\n# load packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(udpipe)\nlibrary(tidytext)\nlibrary(coop)\nlibrary(cluster)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and once you have initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/lex/lex.html#correcting-and-extending-dictionaries",
    "href": "tutorials/lex/lex.html#correcting-and-extending-dictionaries",
    "title": "Lexicography with R",
    "section": "Correcting and Extending Dictionaries",
    "text": "Correcting and Extending Dictionaries\nFortunately, it is very easy in R to correct entries, i.e., changing lemmas or part-of-speech tags, and to extend entries, i.e., adding additional layers of information such as urls or examples.\nWe will begin to extend our dictionary by adding an additional column (called annotation) in which we will add information.\n\n# generate dictionary\ntext_dict_ext &lt;- text_dict %&gt;%\n  # removing an entry\n  dplyr::filter(!(lemma == \"a\" & upos == \"NOUN\")) %&gt;%\n  # editing entries\n  dplyr::mutate(upos = ifelse(lemma == \"aback\" & upos == \"NOUN\", \"PREP\", upos)) %&gt;%\n  # adding comments\n  dplyr::mutate(comment = dplyr::case_when(\n    lemma == \"a\" ~ \"also an before vowels\",\n    lemma == \"Aaronson\" ~ \"Name of someone.\",\n    T ~ \"\"\n  ))\n# inspect\nhead(text_dict_ext, 10)\n\n# A tibble: 10 × 5\n# Groups:   token, lemma [10]\n   token     lemma     upos  frequency comment                \n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;                  \n 1 A         a         DET         107 \"also an before vowels\"\n 2 AND       and       CCONJ         2 \"\"                     \n 3 Aaronson  Aaronson  PROPN         8 \"Name of someone.\"     \n 4 About     about     ADV           4 \"\"                     \n 5 Above     above     ADP           2 \"\"                     \n 6 Abruptly  abruptly  ADV           2 \"\"                     \n 7 Actually  actually  ADV          13 \"\"                     \n 8 Adam      Adam      PROPN         1 \"\"                     \n 9 Admission admission NOUN          1 \"\"                     \n10 Africa    Africa    PROPN        10 \"\"                     \n\n\nTo make it a bit more interesting but also keep this tutorial simple and straight-forward, we will add information about the polarity and emotionally of the words in our dictionary. We can do this by performing a sentiment analysis on the lemmas using the tidytext package.\nThe tidytext package contains three sentiment dictionaries (nrc, bing, and afinn). For the present purpose, we use the ncrdictionary which represents the Word-Emotion Association Lexicon (Mohammad and Turney). The Word-Emotion Association Lexicon which comprises 10,170 terms, and in which lexical elements are assigned scores based on ratings gathered through the crowd-sourced Amazon Mechanical Turk service. For the Word-Emotion Association Lexicon raters were asked whether a given word was associated with one of eight emotions. The resulting associations between terms and emotions are based on 38,726 ratings from 2,216 raters who answered a sequence of questions for each word which were then fed into the emotion association rating (cf. Mohammad and Turney). Each term was rated 5 times. For 85 percent of words, at least 4 raters provided identical ratings. For instance, the word cry or tragedy are more readily associated with SADNESS while words such as happy or beautiful are indicative of JOY and words like fit or burst may indicate ANGER. This means that the sentiment analysis here allows us to investigate the expression of certain core emotions rather than merely classifying statements along the lines of a crude positive-negative distinction.\nTo be able to use the Word-Emotion Association Lexicon we need to add another column to our data frame called word which simply contains the lemmatized word. The reason is that the lexicon expects this column and only works if it finds a word column in the data. The code below shows how to add the emotion and polarity entries to our dictionary.\n\n# generate dictionary\ntext_dict_snt &lt;- text_dict_ext %&gt;%\n  dplyr::mutate(word = lemma) %&gt;%\n  dplyr::left_join(get_sentiments(\"bing\")) %&gt;%\n  dplyr::group_by(token, lemma, upos, comment) %&gt;%\n  dplyr::summarise(sentiment = paste0(sentiment, collapse = \", \"))\n# inspect\nhead(text_dict_snt, 10)\n\n# A tibble: 10 × 5\n# Groups:   token, lemma, upos [10]\n   token     lemma     upos  comment                 sentiment\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;                   &lt;chr&gt;    \n 1 A         a         DET   \"also an before vowels\" NA       \n 2 AND       and       CCONJ \"\"                      NA       \n 3 Aaronson  Aaronson  PROPN \"Name of someone.\"      NA       \n 4 About     about     ADV   \"\"                      NA       \n 5 Above     above     ADP   \"\"                      NA       \n 6 Abruptly  abruptly  ADV   \"\"                      negative \n 7 Actually  actually  ADV   \"\"                      NA       \n 8 Adam      Adam      PROPN \"\"                      NA       \n 9 Admission admission NOUN  \"\"                      NA       \n10 Africa    Africa    PROPN \"\"                      NA       \n\n\nThe resulting extended dictionary now contains not only the token, the lemma, and the pos-tag but also the sentiment from the Word-Emotion Association Lexicon."
  },
  {
    "objectID": "tutorials/lex/lex.html#generating-dictionaries-for-other-languages",
    "href": "tutorials/lex/lex.html#generating-dictionaries-for-other-languages",
    "title": "Lexicography with R",
    "section": "Generating dictionaries for other languages",
    "text": "Generating dictionaries for other languages\nAs mentioned above, the procedure for generating dictionaries can easily be applied to languages other than English. If you want to follow exactly the procedure described above, then the language set of the TreeTagger is the limiting factors as its R implementation only supports English, German, French, Italian, Spanish, and Dutch. fa part-of-speech tagged text in another language is already available to you, and you do not require the TreeTagger for the part-of-speech tagging, then you can skip the code chunk that is related to the tagging and you can modify the procedure described above to virtually any language.\nWe will now briefly create a German dictionary based on a subsection of the fairy tales collected by the brothers Grimm to show how the above procedure can be applied to a language other than English. In a first step, we load a German text into R.\n\ngrimm &lt;- readLines(\"tutorials/lex/data/GrimmsFairytales.txt\",\n  encoding = \"latin1\"\n) %&gt;%\n  paste0(collapse = \" \")\n# show the first 500 characters of the text\nsubstr(grimm, start = 1, stop = 200)\n\n[1] \"Der Froschkönig oder der eiserne Heinrich  Ein Märchen der Brüder Grimm Brüder Grimm  In den alten Zeiten, wo das Wünschen noch geholfen hat, lebte ein König, dessen Töchter waren alle schön; aber die\"\n\n\nNext, we download a udpipe language model. In this case, we download a udpipe language model for German, but you can download udpipe language models for more than 60 languages.\n\n# download language model\nudpipe::udpipe_download_model(language = \"german-hdt\")\n\nIn my case, I have stored this model in a folder called udpipemodels and you can load it (if you have also save the model in a folder called udpipemodels within your Rproj folder as shown below).\n\n# load language model from your computer after you have downloaded it once\nm_ger &lt;- udpipe_load_model(file = here::here(\n  \"udpipemodels\",\n  # \"german-hdt-ud-2.5-191206.udpipe\"))\n  \"german-gsd-ud-2.5-191206.udpipe\"\n))\n\nIn a next step, we generating the dictionary based on the brothers’ Grimm fairy tales. We go through the same steps as for the English dictionary and collapse all the steps into a single code block.\n\n# tokenise, tag, dependency parsing\ngrimm_ann &lt;- udpipe::udpipe_annotate(m_ger, x = grimm) %&gt;%\n  # convert into a data frame\n  as.data.frame() %&gt;%\n  # remove non-words\n  dplyr::filter(!stringr::str_detect(token, \"\\\\W\")) %&gt;%\n  # filter out numbers\n  dplyr::filter(!stringr::str_detect(token, \"[0-9]\")) %&gt;%\n  dplyr::group_by(token, lemma, upos) %&gt;%\n  dplyr::summarise(frequency = dplyr::n()) %&gt;%\n  dplyr::arrange(lemma)\n# inspect\nhead(grimm_ann, 10)\n\n# A tibble: 10 × 4\n# Groups:   token, lemma [10]\n   token           lemma           upos  frequency\n   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;     &lt;int&gt;\n 1 A               A               NOUN          1\n 2 Abend           Abend           NOUN          3\n 3 Abschied        Abschied        NOUN          1\n 4 Ade             Ade             NOUN          2\n 5 Allergnädigster Allergnädigster ADJ           1\n 6 Alte            Alte            NOUN          1\n 7 Angst           Angst           NOUN          1\n 8 Antwort         Antwort         NOUN          1\n 9 Anwesenden      Anwesende       NOUN          1\n10 Anzahl          Anzahl          NOUN          1\n\n\nAs with the English dictionary, we have created a customized German dictionary based of a subsample of the brothers’ Grimm fairy tales holding the word form(token), the part-of-speech tag (tag), the lemmatized word type (lemma), the general word class (wclass), ad the frequency with which a word form occurs as a part-of-speech in the data (frequency)."
  },
  {
    "objectID": "tutorials/dimred/dimred.html#footnotes",
    "href": "tutorials/dimred/dimred.html#footnotes",
    "title": "Introduction to Dimension Reduction Methods with R - AcqVA Aurora workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/gutenberg/gutenberg.html#preparation-and-session-set-up",
    "href": "tutorials/gutenberg/gutenberg.html#preparation-and-session-set-up",
    "title": "Downloading Texts from Project Gutenberg using R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install libraries\ninstall.packages(\"tidyverse\")\ninstall.packages(\"gutenbergr\")\ninstall.packages(\"DT\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# activate packages\nlibrary(tidyverse)\nlibrary(gutenbergr)\nlibrary(DT)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/gutenberg/gutenberg.html#footnotes",
    "href": "tutorials/gutenberg/gutenberg.html#footnotes",
    "title": "Downloading Texts from Project Gutenberg using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI would like to thank Max Lauber for pointing out that I wrongly stated that both works were written by Jane Austen in an earlier version of this tutorial.↩︎"
  },
  {
    "objectID": "tutorials/spellcheck/spellcheck.html#checking-individual-words",
    "href": "tutorials/spellcheck/spellcheck.html#checking-individual-words",
    "title": "Spell checking text data with R",
    "section": "Checking individual words",
    "text": "Checking individual words\nWe start by checking a vector of individual words.\n\nwords &lt;- c(\"analyze\", \"langauge\", \"data\")\ncorrect &lt;- hunspell_check(words)\nprint(correct)\n\n[1]  TRUE FALSE  TRUE\n\n\nThe output shows that the second word was not found in the dictionary, i.e., it is identified as being incorrect. Next, we can ask for suggestions, i.e. the correct form of the word.\n\n\n[[1]]\n[1] \"language\" \"melange\" \n\n\nIn this case, there are two words that are deemed as potential alternatives."
  },
  {
    "objectID": "tutorials/spellcheck/spellcheck.html#checking-documents",
    "href": "tutorials/spellcheck/spellcheck.html#checking-documents",
    "title": "Spell checking text data with R",
    "section": "Checking documents",
    "text": "Checking documents\nSince we rarely want to check individual words, we will now focus on spell checking full texts rather than individual vectors with words.\nFirst, we load a text (in this case an explanation of what grammer is that is taken from Wikipedia).\n\n# read in text\nexampletext &lt;- base::readRDS(\"tutorials/spellcheck/data/tx1.rda\", \"rb\")\n# inspect\nexampletext\n\n[1] \"Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\"\n\n\nWe now use the hunspell function to find incorrect words (or, more precisely, words that are not in the default dictionary). The output shows that organisation is considered an error as it follows British spelling conventions (more about this in the section on setting a language).\n\nerrors &lt;- hunspell(exampletext)\nerrors[[1]]\n\n[1] \"componential\" \"organisation\" \"Noam\"        \n\n\nWe check what suggestions we get for these words.\n\nhunspell_suggest(errors[[1]])\n\n[[1]]\n[1] \"component\"   \"exponential\" \"continental\"\n\n[[2]]\n[1] \"organization\" \"organist\"     \"sanitation\"  \n\n[[3]]\n[1] \"Nam\"   \"Roam\"  \"Loam\"  \"Noah\"  \"Foam\"  \"No am\" \"No-am\" \"Moan\"  \"Norm\" \n\n\nIn our case, the words are not errors but simply terms that do not occur in the dictionary, either because they are specialist terminology (componential), they follow British spelling conventions (organisation), or they represent an uncommon proper name (Noam). Thus, so we will not replace them."
  },
  {
    "objectID": "tutorials/spellcheck/spellcheck.html#stemming-words",
    "href": "tutorials/spellcheck/spellcheck.html#stemming-words",
    "title": "Spell checking text data with R",
    "section": "Stemming Words",
    "text": "Stemming Words\nThe hunspell_stem looks up words from the dictionary which match the root of the given word (sometimes multiple items are returned if there are multiple matches in the dictionary).\n\ntoks &lt;- c(\"love\", \"loving\", \"lovingly\", \"loved\", \"lover\", \"lovely\")\nhunspell_stem(toks)\n\n[[1]]\n[1] \"love\"\n\n[[2]]\n[1] \"loving\" \"love\"  \n\n[[3]]\n[1] \"loving\"\n\n[[4]]\n[1] \"loved\" \"love\" \n\n[[5]]\n[1] \"lover\" \"love\" \n\n[[6]]\n[1] \"lovely\" \"love\"  \n\n\nThe hunspell_stem function can be very useful when trying to find the stems of words in a corpusto see, e.g., how many word types a text contains."
  },
  {
    "objectID": "tutorials/spellcheck/spellcheck.html#working-with-texts",
    "href": "tutorials/spellcheck/spellcheck.html#working-with-texts",
    "title": "Spell checking text data with R",
    "section": "Working with texts",
    "text": "Working with texts\nIt is quite common that we work with texts rather than individual word vectors. As such, in the following, we will go through a workflow that resembles what one might use spell checking for in their research.\n\ntexttable &lt;- quanteda::tokens(\"Noam said he loves to analyze langauge and collors.\") %&gt;%\n    unlist() %&gt;%\n    as.data.frame() %&gt;%\n    dplyr::rename(words = 1) %&gt;%\n    dplyr::mutate(\n        id = 1:nrow(.),\n        error = hunspell::hunspell_check(words)\n    ) %&gt;%\n    dplyr::relocate(id)\n\n\n\nidwordserror1NoamFALSE2saidTRUE3heTRUE4lovesTRUE5toTRUE6analyzeTRUE7langaugeFALSE8andTRUE9collorsFALSE10.TRUE\n\n\nThe output shows the original word and if it was identified as an errors (i.e., it did not occur in the dictionary). Next, we extract suggestions for the words that were identified as errors.\n\ntexttable2 &lt;- texttable %&gt;%\n    # add suggestions\n    dplyr::mutate(\n        suggestions = ifelse(error == F,\n            paste0(hunspell_suggest(words), \", \"),\n            \"\"\n        ),\n        # clean suggestions\n        suggestions = stringr::str_remove_all(suggestions, fixed(\"c(\")),\n        suggestions = stringr::str_remove_all(suggestions, fixed(\")\")),\n        suggestions = stringr::str_remove_all(suggestions, \", {0,1}$\")\n    )\n\n\n\nidwordserrorsuggestions1NoamFALSE\"Nam\", \"Roam\", \"Loam\", \"Noah\", \"Foam\", \"No am\", \"No-am\", \"Moan\", \"Norm\"2saidTRUE3heTRUE4lovesTRUE5toTRUE6analyzeTRUE7langaugeFALSE\"language\", \"melange\"8andTRUE9collorsFALSE\"colors\", \"collars\", \"collators\", \"color's\", \"color\"10.TRUE\n\n\nWe now want to replace the errors with the correct words but aso retain words that are erroneously regraded as errors.\n\ntexttable3 &lt;- texttable2 %&gt;%\n    # replace errors with the first suggestion\n    dplyr::mutate(corrected = dplyr::case_when(\n        error == T ~ words,\n        error == F ~ stringr::str_remove_all(suggestions, '\\\\\", .*'),\n        T ~ words\n    )) %&gt;%\n    # clean the corrected words\n    dplyr::mutate(corrected = stringr::str_remove_all(corrected, '^\\\\\"')) %&gt;%\n    # insert words where we do not want the suggestions but the original word\n    dplyr::mutate(corrected = dplyr::case_when(\n        words == \"Noam\" ~ words,\n        T ~ corrected\n    ))\n\n\n\nidwordserrorsuggestionscorrected1NoamFALSE\"Nam\", \"Roam\", \"Loam\", \"Noah\", \"Foam\", \"No am\", \"No-am\", \"Moan\", \"Norm\"Noam2saidTRUEsaid3heTRUEhe4lovesTRUEloves5toTRUEto6analyzeTRUEanalyze7langaugeFALSE\"language\", \"melange\"language8andTRUEand9collorsFALSE\"colors\", \"collars\", \"collators\", \"color's\", \"color\"colors10.TRUE.\n\n\nWe can now check how many errors there are in our text.\n\ntexttable4 &lt;- texttable3 %&gt;%\n    dplyr::summarise(\n        tokens = nrow(.),\n        types = length(names(table(tolower(words)))),\n        errors_n = sum(ifelse(corrected == words, 0, 1)),\n        errors_percent = errors_n / tokens * 100\n    )\n\n\n\ntokenstypeserrors_nerrors_percent1010220\n\n\nFinally, we can put the corrected text back together.\n\ncorrectedtext &lt;- paste0(texttable3$corrected, collapse = \" \") %&gt;%\n    stringr::str_replace_all(\" \\\\.\", \".\")\ncorrectedtext\n\n[1] \"Noam said he loves to analyze language and colors.\""
  },
  {
    "objectID": "tutorials/spellcheck/spellcheck.html#setting-a-language",
    "href": "tutorials/spellcheck/spellcheck.html#setting-a-language",
    "title": "Spell checking text data with R",
    "section": "Setting a Language",
    "text": "Setting a Language\nBy default, the hunspell package includes dictionaries for en_US and en_GB which means that you can easily switch from US American to British English spelling. As a default, hunspell uses the en_US dictionary but we can easily switch to British English instead as shown below.\nFirst, we use the default en_US dictionary which returns LADAL as well as the British spellings of analyse and colours as errors.\n\nhunspell(\"At LADAL we like to analyse language and colours.\")\n\n[[1]]\n[1] \"LADAL\"   \"analyse\" \"colours\"\n\n\nIf we switch to the en_GB dictionary, only LADAL is identified as not occuring in the dictionary.\n\nhunspell(\"At LADAL we like to analyse language and colours.\", dict = \"en_GB\")\n\n[[1]]\n[1] \"LADAL\"\n\n\n\nIf you want to use another language you need to make sure that the dictionary is available in your system so that RStudio can access the dictionary. You can install dictionaries very easily in RStudio. Simply go to Tools &gt; Global options &gt; Spelling and then under Main dictionary language select Install More Languages from the drop-down menu. Once the additional languages are installed, their dictionaries are available to the hunspell package in RStudio.\nHowever, you may want to install dictionaries directly, e.g., into your working directory so that you can use the dictionary when working with text data either on your computer or in a cloud environment. In this case, you can go to the wooorm dictionary GitHub repository, which has dict and aff files (i.e., the files that are needed to create a dictionary) for many different languages, and install the dict and aff files manually. In our case, I downloaded the dict and aff files from the German dictionary, stored them together with the other dictionary files in the hunspell package library, and renamed the files as de.dict and de.aff.\nIf you then want to use the dictionary, you simply specify the dict argument as shown below for a German sentence.\n\nhunspell(\"Im LADAL analysieren wir Sprachen und Farben.\", dict = \"de\")\n\n[[1]]\n[1] \"LADAL\"\n\n\nThis is the end of this short tutorial on spell checking with R. If you want to go deeper, please check out the documentation site of the hunspell package)[https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html] (Ooms)."
  },
  {
    "objectID": "tutorials/introta/introta.html#word",
    "href": "tutorials/introta/introta.html#word",
    "title": "Introduction to Text Analysis",
    "section": "Word",
    "text": "Word\nWhat a word is is actually very tricky. For instance, How many words are there in this sentence?\n\nThe cat sat on the mat.\n\nOne answer is that there are six words; that is, there are six groups of characters which are separated according to typographical convention.\nBut there is another answer: There are five words, that is five distinct sequences of characters and one of those sequences (the) occurs twice.\nThe terms commonly used to make this distinction are type and token. Tokens are instances of types, therefore if we count tokens, we count without considering repetition, while if we count types, we do consider repetition. In our example, there are five types (the, cat, sat, on, mat) but six tokens, because there are two tokens of one of the types (the).\nThere is a further distinction we may need to make which we can see if we consider another question: are cat and cats the same word? They are distinct types, and therefore must also be distinct as tokens. But we have an intuition that at some level they are related, that there is some more abstract item which underlies both of them. This concept is usually referred to as a lemma."
  },
  {
    "objectID": "tutorials/introta/introta.html#concordancing",
    "href": "tutorials/introta/introta.html#concordancing",
    "title": "Introduction to Text Analysis",
    "section": "Concordancing",
    "text": "Concordancing\nIn Text Analysis, concordancing refers to the extraction of words from a given text or texts (Lindquist). Commonly, concordances are displayed in the form of key-word in contexts (KWIC) where the search term is shown with some preceding and following context. Thus, such displays are referred to as key word in context concordances. A more elaborate tutorial on how to perform concordancing with R is available here. If you do not want to use coding to extract concordances, a highly recommendable tool for extracting concordances (and many other TA tasks) is AntConc.\nConcordancing is helpful for seeing how the term is used in the data, for inspecting how often a given word occurs in a text or a collection of texts, for extracting examples, and it also represents a basic procedure and often the first step in more sophisticated analyses of language data."
  },
  {
    "objectID": "tutorials/introta/introta.html#corpus-pl.-corpora",
    "href": "tutorials/introta/introta.html#corpus-pl.-corpora",
    "title": "Introduction to Text Analysis",
    "section": "Corpus (pl. Corpora)",
    "text": "Corpus (pl. Corpora)\nA corpus is a machine readable and electronically stored collection of natural language texts representing writing or speech chosen to be characteristic of a variety or the state of a language (see Sinclair 1991). Corpora are great for extracting examples of natural examples and testing research hypotheses as it is easy to obtain information on frequencies, grammatical patterns, or collocations and they are commonly publicly available so the research results can be contrasted, compared and repeated.\nThere are four main types of corpora:\n\nMonitor corpora: large collections of texts from different genres/modes that aim at representing a language or language variety, e.g., International Corpus of English (ICE), Corpus of Contemporary Corpus of American English (COCA), that are, e.g., used to analyse the use of certain linguistic phenomena or to investigate collocations of certain words/topics\nLearner corpora: Contain data from language learners - these can be either L1 learners, e.g., Child Language Data Exchange System (CHILDES), and/or L2 learners, e.g., the International Corpus of Learner English (ICLE)) - to study, e.g., how L1 and/or L2 speakers learn/acquire (aspects of) a language and to see how learners differ from native speakers.\nHistorical or diachronic corpora: Contain data from different points in time that allow to analyse the development of a language or language variety (e.g., Penn Parsed Corpora of Historical English,The Helsinki Corpus of English Texts) to study, e.g., how language changes or how genres develop over time.\nSpecialized corpora: Contain data representing a specific genre/text type (e.g., British Academic Written English Corpus (BAWE)) to study, e.g., (linguistic) features of a genre (e.g. academic writing) or language in class rooms."
  },
  {
    "objectID": "tutorials/introta/introta.html#collocations",
    "href": "tutorials/introta/introta.html#collocations",
    "title": "Introduction to Text Analysis",
    "section": "Collocations",
    "text": "Collocations\nCollocations are words that are attracted to each other (and that co-occur or co-locate together), e.g., Merry Christmas, Good Morning, No worries, or Fuck off. Collocations are important because any word in any given language has collocations, i.e., others words that are attracted to that word or words that that word is attracted to allow us to anticipate what word comes next and collocations are context/text type specific. It is important to note that collocations to not have to appear/occur right next to each other but that other words can be in between. There are various different statistical measures are used to define the strength of the collocations, like the Mutual Information (MI) score and log-likelihood (see here for an over view of different association strengths measures)."
  },
  {
    "objectID": "tutorials/introta/introta.html#document-classification",
    "href": "tutorials/introta/introta.html#document-classification",
    "title": "Introduction to Text Analysis",
    "section": "Document Classification",
    "text": "Document Classification\nDocument or Text Classification (also referred to as text categorization) generally refers to process of grouping texts or documents based on similarity. This similarity can be based on word frequencies or other linguistics features but also on text external features such as genre labels or polarity scores."
  },
  {
    "objectID": "tutorials/introta/introta.html#document-term-matrix",
    "href": "tutorials/introta/introta.html#document-term-matrix",
    "title": "Introduction to Text Analysis",
    "section": "Document-Term Matrix",
    "text": "Document-Term Matrix\nDocument-Term Matrices (DTM) and Term- Document Matrices (TDM) contain the frequencies of words per document. DTM and TDM differ in whether the words or the documents are represented as rows. Thus, the words (terms) are listed as row names and the documents represent the column names while the matrix itself contains the frequencies of the words in the documents."
  },
  {
    "objectID": "tutorials/introta/introta.html#frequency-analysis",
    "href": "tutorials/introta/introta.html#frequency-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Frequency Analysis",
    "text": "Frequency Analysis\nFrequency Analysis is a suit of methods which extract and compare frequencies of different words (tokens and/or types), collocations, phrases, sentences, etc. These frequencies are the often tabulated to show lists of words, phrases, etc. descending by frequency, visualized to show distributions, and/or compared and analyzed statistically to find differences between texts or collections fo texts."
  },
  {
    "objectID": "tutorials/introta/introta.html#keyword-analysis",
    "href": "tutorials/introta/introta.html#keyword-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Keyword Analysis",
    "text": "Keyword Analysis\nKeyword Analysis refers to a suit of methods that allow to detect words that are characteristic of on text or collection of texts compared to another text/collection of texts. There are various keyness measures such as Log-Likelihood or the term frequency–inverse document frequency (tf-idf)."
  },
  {
    "objectID": "tutorials/introta/introta.html#lemma-lemmatization",
    "href": "tutorials/introta/introta.html#lemma-lemmatization",
    "title": "Introduction to Text Analysis",
    "section": "Lemma (Lemmatization)",
    "text": "Lemma (Lemmatization)\nLemma refers to the base form of a word (example: walk, walked, and walking are word forms of the lemma WALK). Lemmatization refers to a annotation process in which word forms are associated with their base form (lemma). Lemmatization is a very common and sometimes useful processing step for further analyses. In contrast to stemming - which is a related process - lemmatization also takes into account semantic differences (differences in the word meaning), while stemming only takes the orthography of words into consideration."
  },
  {
    "objectID": "tutorials/introta/introta.html#n-gram",
    "href": "tutorials/introta/introta.html#n-gram",
    "title": "Introduction to Text Analysis",
    "section": "N-Gram",
    "text": "N-Gram\nN-grams are combinations/sequences of words, e.g. the sentence I really like pizza! has the bi-grams (2-grams): I really, really like, and like pizza and the tri-grams (3-grams) I really like and really like pizza. N-grams play an important part in natural language processing (e.g. part-of-speech tagging), language learning, psycholinguistics models of language production, and genre analysis."
  },
  {
    "objectID": "tutorials/introta/introta.html#natural-language-processing",
    "href": "tutorials/introta/introta.html#natural-language-processing",
    "title": "Introduction to Text Analysis",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nNatural Language Processing (NLP) is an interdisciplinary field in computer science that has specialized on processing natural language data using computational and mathematical methods. Many methods used in Text Analysis have been developed in NLP."
  },
  {
    "objectID": "tutorials/introta/introta.html#network-analysis",
    "href": "tutorials/introta/introta.html#network-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Network Analysis",
    "text": "Network Analysis\nNetwork Analysis is the most common way to visualize relationships between entities. Networks, also called graphs, consist of nodes (typically represented as dots) and edges (typically represented as lines) and they can be directed or undirected networks.\nIn directed networks, the direction of edges is captured. For instance, the exports of countries. In such cases the lines are directed and typically have arrows to indicate direction. The thickness of lines can also be utilized to encode information such as frequency of contact."
  },
  {
    "objectID": "tutorials/introta/introta.html#part-of-speech-tagging",
    "href": "tutorials/introta/introta.html#part-of-speech-tagging",
    "title": "Introduction to Text Analysis",
    "section": "Part-of-Speech Tagging",
    "text": "Part-of-Speech Tagging\nPart-of-Speech (PoS) Tagging identifies the word classes of words (e.g., noun, adjective, verb, etc.) in a text and adds part-of-speech tags to each word. There are various part-of-speech tagsets, e.g. the Penn Treebank is the most frequently used tagset used for English. A more detailed tutorial on how to perform part-of-speech tagging in R can be found here."
  },
  {
    "objectID": "tutorials/introta/introta.html#project-gutenberg",
    "href": "tutorials/introta/introta.html#project-gutenberg",
    "title": "Introduction to Text Analysis",
    "section": "Project Gutenberg",
    "text": "Project Gutenberg\nThe Project Gutenberg is a excellent resource for accessing digitized literary texts. The Project Gutenberg library contains over 60,000 ebooks that are out of copyright in the US. A tutorial on how to download texts form the Project Gutenberg library using the GutenbergR package can be found here."
  },
  {
    "objectID": "tutorials/introta/introta.html#regular-expression",
    "href": "tutorials/introta/introta.html#regular-expression",
    "title": "Introduction to Text Analysis",
    "section": "Regular Expression",
    "text": "Regular Expression\nRegular Expressions - often simply referred to as regex - are symbols or sequence of symbols utilized to search for patterns in textual data. Regular Expressions are very useful and widely used in Text Analysis and often different programming languages will have very similar but slightly different Regular Expressions. A tutorial on how to use regular expression in R can be found here and here is a link to a regex in R cheat sheet."
  },
  {
    "objectID": "tutorials/introta/introta.html#semantic-analysis",
    "href": "tutorials/introta/introta.html#semantic-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Semantic Analysis",
    "text": "Semantic Analysis\nSemantic Analysis refers to a suit of methods that allow to analyze the semantic (semantics) fo texts. Such analyses often rely on semantic tagsets that are based on word meaning or meaning families/categories. Two examples of such semantic tagsets are the URCEL tagset and the Historical Thesaurus Semantic Tagger (Alexander and Wattam 2015) developed at the University of Glasgow."
  },
  {
    "objectID": "tutorials/introta/introta.html#sentiment-analysis",
    "href": "tutorials/introta/introta.html#sentiment-analysis",
    "title": "Introduction to Text Analysis",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nSentiment Analysis is a computational approach to determine if words or texts are associated with (positive or negative) polarity or emotions.Commonly, sentiments analyses are based on sentiment dictionaries (words are annotated based on whether they occur in a list of words associated with, e.g., positive polarity or emotion, e.g., fear, anger, or joy. A tutorial on how to perform sentiment analysis in R can be found here."
  },
  {
    "objectID": "tutorials/introta/introta.html#string",
    "href": "tutorials/introta/introta.html#string",
    "title": "Introduction to Text Analysis",
    "section": "String",
    "text": "String\nIn computational approaches, a string is a specific type of data that represents text and is often encoded in specific format, e.g., Latin1 or UTF8. Strings may also be present in other data types such as lists or data frames. A tutorial on how to work with strings in R can be found here."
  },
  {
    "objectID": "tutorials/introta/introta.html#term-frequencyinverse-document-frequency-tf-idf",
    "href": "tutorials/introta/introta.html#term-frequencyinverse-document-frequency-tf-idf",
    "title": "Introduction to Text Analysis",
    "section": "Term Frequency–Inverse Document Frequency (tf-idf)",
    "text": "Term Frequency–Inverse Document Frequency (tf-idf)\nTerm Frequency–Inverse Document Frequency is a statistical measure of keyness which reflects how characteristic a word is of a specific text. Term Frequency–Inverse Document Frequency is based on the frequencies of words in a text compared to the frequency of documents in which it occurs"
  },
  {
    "objectID": "tutorials/introta/introta.html#topic-modeling",
    "href": "tutorials/introta/introta.html#topic-modeling",
    "title": "Introduction to Text Analysis",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nTopic modelling is a machine learning method seeks to answer the question: given a collection of documents, can we identify what they are about?\nTopic model algorithms look for patterns of co-occurrences of words in documents. We assume that, if a document is about a certain topic, one would expect words that are related to that topic to appear in the document more often than in documents that deal with other topics. Topic model commonly use Latent Dirichlet Allocation (LDA) to find topics in textual data.\nThere are two basic types of Topic models\n\nsupervised or seeded topics models where the researchers provides seed terms around which the LDS looks for topics (collections of correlating terms)\nunsupervised or unseeded topic models which try to find a predefined number of topics (collections of correlating terms)\n\nA tutorial on how to work with strings in R can be found here."
  },
  {
    "objectID": "tutorials/motion/motion.html#preparation-and-session-set-up",
    "href": "tutorials/motion/motion.html#preparation-and-session-set-up",
    "title": "Creating Interactive Visualizations in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"googleVis\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"DT\")\ninstall.packages(\"flextable\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"gganimate\")\ninstall.packages(\"gapminder\")\ninstall.packages(\"maptools\")\ninstall.packages(\"plotly\")\ninstall.packages(\"leaflet\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# Warning: the following option adaptation requires re-setting during session outro!\nop &lt;- options(gvis.plot.tag = \"chart\") # set gViz options\n# activate packages\nlibrary(googleVis)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(flextable)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gapminder)\nlibrary(maptools)\nlibrary(plotly)\nlibrary(leaflet)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/motion/motion.html#getting-started",
    "href": "tutorials/motion/motion.html#getting-started",
    "title": "Creating Interactive Visualizations in R",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started with motion charts, we load the googleVis package for the visualizations, the tidyverse package for data processing, and we load a data set called coocdata. The coocdata contains information about how often adjectives were amplified by a degree adverb across time (see below).\n\n# load data\ncoocdata &lt;- base::readRDS(\"tutorials/motion/data/coo.rda\", \"rb\")\n\n\n\nDecadeAmpAdjectiveN.Amp.N.Adj.OBSEXPProbabilityCollStrengthOddsRatiopattrsig1,870prettyamiable7400.00.00001repeln.s.1,870prettyamusing7400.00.00001repeln.s.1,870prettyangry7800.00.00001repeln.s.1,870prettyannoyed7100.00.00001repeln.s.1,870prettybad7700.00.00001repeln.s.1,870prettybeautiful71600.00.00001repeln.s.1,870prettybusy710800.10.02001repeln.s.1,870prettycharming7700.00.00001repeln.s.1,870prettyclever7400.00.00001repeln.s.1,870prettycomfortable7700.00.00001repeln.s.1,870prettycontemptible7200.00.00001repeln.s.1,870prettyconvenient71300.00.00001repeln.s.1,870prettydangerous74400.10.01001repeln.s.1,870prettydark71100.00.00001repeln.s.1,870prettydifferent720200.20.03001repeln.s.\n\n\nThe coocdata is rather complex and requires some processing. First, we rename the columns to render their naming more meaningful. In this context we rename the OBS column Frequency and the Amp column Amplifier. As we are only interested if an adjective was amplified by very, we collapse all amplifiers that are not very in a bin category called other. We then calculate the frequency of the adjective within each time period and also the frequency with which each adjective is amplified by either very or other amplifiers. Then, we calculate the percentage with which each adjective is amplified by very.\n\n# process data\ncoocs &lt;- coocdata %&gt;%\n  dplyr::select(Decade, Amp, Adjective, OBS) %&gt;%\n  dplyr::rename(\n    Frequency = OBS,\n    Amplifier = Amp\n  ) %&gt;%\n  dplyr::mutate(Amplifier = ifelse(Amplifier == \"very\", \"very\", \"other\")) %&gt;%\n  dplyr::group_by(Decade, Adjective, Amplifier) %&gt;%\n  dplyr::summarise(Frequency = sum(Frequency)) %&gt;%\n  dplyr::ungroup() %&gt;%\n  tidyr::spread(Amplifier, Frequency) %&gt;%\n  dplyr::group_by(Decade, Adjective) %&gt;%\n  dplyr::mutate(\n    Frequency_Adjective = sum(other + very),\n    Percent_very = round(very / (other + very) * 100, 2)\n  ) %&gt;%\n  dplyr::mutate(\n    Percent_very = ifelse(is.na(Percent_very), 0, Percent_very),\n    Adjective = factor(Adjective)\n  )\n\n\n\nDecadeAdjectiveotherveryFrequency_AdjectivePercent_very1,870amiable00001,870amusing00001,870angry00001,870annoyed00001,870bad00001,870beautiful00001,870busy10101,870charming00001,870clever00001,870comfortable0000\n\n\nWe now have a data set that we can use to generate interactive visualization."
  },
  {
    "objectID": "tutorials/motion/motion.html#scatter-plots",
    "href": "tutorials/motion/motion.html#scatter-plots",
    "title": "Creating Interactive Visualizations in R",
    "section": "Scatter Plots",
    "text": "Scatter Plots\nScatter plots show the relationship between two numeric variables if you have more than one observation per variable level (if the data is not grouped by another variable). This means that you can use scatter plots to display data when you have, e.g. more than one observation for each data in your data set. If you only have a single observation, you could also use a line graph (which we will turn to below).\n\nscdat &lt;- coocs %&gt;%\n  dplyr::group_by(Decade) %&gt;%\n  dplyr::summarise(Precent_very = mean(Percent_very))\n# create scatter plot\nSC &lt;- gvisScatterChart(scdat,\n  options = list(\n    title = \"Interactive Scatter Plot\",\n    legend = \"none\",\n    pointSize = 5\n  )\n)\n\nIf you want to display the visualization in a Notebook environment, you can use the plot function as shown below.\n\nplot(SC)\n\nHowever, if you want to display the visualization on a website, you must use the print function rather than the plot function and specify that you want to print a chart.\nprint(SC, \"chart\")"
  },
  {
    "objectID": "tutorials/motion/motion.html#line-graphs",
    "href": "tutorials/motion/motion.html#line-graphs",
    "title": "Creating Interactive Visualizations in R",
    "section": "Line Graphs",
    "text": "Line Graphs\nTo create an interactive line chart, we use the gvisLineChart function as shown below.\n\n# create scatter plot\nSC &lt;- gvisLineChart(scdat,\n  options = list(\n    title = \"Interactive Scatter Plot\",\n    legend = \"none\"\n  )\n)\n\nIf you want to display the visualization in a Notebook environment, you can use the plot function. For website, you must use the print function and specify that you want to print a chart.\nprint(SC, \"chart\")"
  },
  {
    "objectID": "tutorials/motion/motion.html#bar-plots",
    "href": "tutorials/motion/motion.html#bar-plots",
    "title": "Creating Interactive Visualizations in R",
    "section": "Bar Plots",
    "text": "Bar Plots\nTo create an interactive bar chart, we use the gvisBarChart function as shown below.\n\n# create scatter plot\nSC &lt;- gvisBarChart(scdat,\n  options = list(\n    title = \"Interactive Bar chart\",\n    legend = \"right\",\n    pointSize = 10\n  )\n)\n\nNormally, you would use the plot function to display the interactive chart but you must use the print function with the chart argument if you want to display the result on a website.\nprint(SC, \"chart\")"
  },
  {
    "objectID": "tutorials/motion/motion.html#session-outro",
    "href": "tutorials/motion/motion.html#session-outro",
    "title": "Creating Interactive Visualizations in R",
    "section": "Session Outro",
    "text": "Session Outro\nWhen generating interactive maps, it is important that you reset the default graphics parameters which had to be adapted in the session set-up. Therefore, in a final step, we restore the default graphics options.\n\n## Set options back to original options\noptions(op)"
  },
  {
    "objectID": "tutorials/key/key.html#dimensions-of-keyness",
    "href": "tutorials/key/key.html#dimensions-of-keyness",
    "title": "Keyness and Keyword Analysis in R",
    "section": "Dimensions of keyness",
    "text": "Dimensions of keyness\nBefore we start with the practical part of this tutorial, it is important to talk about the different dimensions of keyness (see Sönning 2023).\nKeyness analysis identifies typical items in a discourse domain, where typicalness traditionally relates to frequency of occurrence. The emphasis is on items used more frequently in the target corpus compared to a reference corpus. Egbert and Biber (2019) expanded this notion, highlighting two criteria for typicalness: content-distinctiveness and content-generalizability.\n\nContent-distinctiveness refers to an item’s association with the domain and its topical relevance.\nContent-generalizability pertains to an item’s widespread usage across various texts within the domain.\n\nThese criteria bridge traditional keyness approaches with broader linguistic perspectives, emphasizing both the distinctiveness and generalizability of key items within a corpus.\nFollowing Sönning (2023), we adopt Egbert and Biber (2019) keyness criteria, distinguishing between frequency-oriented and dispersion-oriented approaches to assess keyness. These perspectives capture distinct, linguistically meaningful attributes of typicalness. We also differentiate between keyness features inherent to the target variety and those that emerge from comparing it to a reference variety. This four-way classification, detailed in the table below, links methodological choices to the linguistic meaning conveyed by quantitative measures. Typical items exhibit a sufficiently high occurrence rate to be discernible in the target variety, with discernibility measured solely within the target corpus. Key items are also distinct, being used more frequently than in reference domains of language use. While discernibility and distinctiveness both rely on frequency, they measure different aspects of typicalness.\n\n\nAnalysisFrequency.orientedDispersion.orientedTarget variety in isolationDiscernibility of item in the target varietyGenerality across texts in the target varietyComparison to reference varietyDistinctiveness relative to the reference varietyComparative generality relative to the reference variety\n\n\nThe second aspect of keyness involves an item’s dispersion across texts in the target domain, indicating its widespread use. A typical item should appear evenly across various texts within the target domain, reflecting its generality. This breadth of usage can be compared to its occurrence in the reference domain, termed as comparative generality. Therefore, a key item should exhibit greater prevalence across target texts compared to those in the reference domain."
  },
  {
    "objectID": "tutorials/key/key.html#identifying-keywords",
    "href": "tutorials/key/key.html#identifying-keywords",
    "title": "Keyness and Keyword Analysis in R",
    "section": "Identifying keywords",
    "text": "Identifying keywords\nHere, we focus on a frequency-based approach that assesses distinctiveness relative to the reference variety. To identify these keywords, we can follow the procedure we have used to identify collocations using kwics - the idea is essentially identical: we compare the use of a word in a target corpus A to its use in a reference corpus.\nTo determine if a token is a keyword and if it occurs significantly more frequently in a target corpus compared to a reference corpus, we use the following information (that is provided by the table above):\n\nO11 = Number of times wordx occurs in target corpus\nO12 = Number of times wordx occurs in reference corpus (without target corpus)\nO21 = Number of times other words occur in target corpus\nO22 = Number of times other words occur in reference corpus\n\nExample:\n\n\n\n\ntarget corpus\nreference corpus\n\n\n\n\n\ntoken\nO11\nO12\n= R1\n\n\nother tokens\nO21\nO22\n= R2\n\n\n\n= C1\n= C2\n= N\n\n\n\nWe begin with loading two texts (text1 is our target and text2 is our reference).\n\n# load data\ntext1 &lt;- base::readRDS(\"tutorials/key/data/orwell.rda\", \"rb\") %&gt;%\n    paste0(collapse = \" \")\ntext2 &lt;- base::readRDS(\"tutorials/key/data/melville.rda\", \"rb\") %&gt;%\n    paste0(collapse = \" \")\n\n\n\n.1984 George Orwell Part 1, Chapter 1 It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, sli\n\n\nAs you can see, text1 is George Orwell’s 1984.\n\n\n.MOBY-DICK; or, THE WHALE. By Herman Melville CHAPTER 1. Loomings. Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interes\n\n\nThe table shows that text2 is Herman Melville’s Moby Dick.\nAfter loading the two texts, we create a frequency table of first text.\n\ntext1_words &lt;- text1 %&gt;%\n    # remove non-word characters\n    stringr::str_remove_all(\"[^[:alpha:] ]\") %&gt;%\n    # convert to lower\n    tolower() %&gt;%\n    # tokenize the corpus files\n    quanteda::tokens(\n        remove_punct = T,\n        remove_symbols = T,\n        remove_numbers = T\n    ) %&gt;%\n    # unlist the tokens to create a data frame\n    unlist() %&gt;%\n    as.data.frame() %&gt;%\n    # rename the column to 'token'\n    dplyr::rename(token = 1) %&gt;%\n    # group by 'token' and count the occurrences\n    dplyr::group_by(token) %&gt;%\n    dplyr::summarise(n = n()) %&gt;%\n    # add column stating where the frequency list is 'from'\n    dplyr::mutate(type = \"text1\")\n\nNow, we create a frequency table of second text.\n\ntext2_words &lt;- text2 %&gt;%\n    # remove non-word characters\n    stringr::str_remove_all(\"[^[:alpha:] ]\") %&gt;%\n    # convert to lower\n    tolower() %&gt;%\n    # tokenize the corpus files\n    quanteda::tokens(\n        remove_punct = T,\n        remove_symbols = T,\n        remove_numbers = T\n    ) %&gt;%\n    # unlist the tokens to create a data frame\n    unlist() %&gt;%\n    as.data.frame() %&gt;%\n    # rename the column to 'token'\n    dplyr::rename(token = 1) %&gt;%\n    # group by 'token' and count the occurrences\n    dplyr::group_by(token) %&gt;%\n    dplyr::summarise(n = n()) %&gt;%\n    # add column stating where the frequency list is 'from'\n    dplyr::mutate(type = \"text2\")\n\nIn a next step, we combine the tables.\n\ntexts_df &lt;- dplyr::left_join(text1_words, text2_words, by = c(\"token\")) %&gt;%\n    # rename columns and select relevant columns\n    dplyr::rename(\n        text1 = n.x,\n        text2 = n.y\n    ) %&gt;%\n    dplyr::select(-type.x, -type.y) %&gt;%\n    # replace NA values with 0 in 'corpus' and 'kwic' columns\n    tidyr::replace_na(list(text1 = 0, text2 = 0))\n\n\n\ntokentext1text2a2,3904,536aaronson80aback22abandon33abandoned47abashed12abbreviated10abiding11ability11abject30\n\n\nWe now calculate the frequencies of the observed and expected frequencies as well as the row and column totals.\n\ntexts_df %&gt;%\n    dplyr::mutate(\n        text1 = as.numeric(text1),\n        text2 = as.numeric(text2)\n    ) %&gt;%\n    dplyr::mutate(\n        C1 = sum(text1),\n        C2 = sum(text2),\n        N = C1 + C2\n    ) %&gt;%\n    dplyr::rowwise() %&gt;%\n    dplyr::mutate(\n        R1 = text1 + text2,\n        R2 = N - R1,\n        O11 = text1,\n        O11 = ifelse(O11 == 0, O11 + 0.1, O11),\n        O12 = R1 - O11,\n        O21 = C1 - O11,\n        O22 = C2 - O12\n    ) %&gt;%\n    dplyr::mutate(\n        E11 = (R1 * C1) / N,\n        E12 = (R1 * C2) / N,\n        E21 = (R2 * C1) / N,\n        E22 = (R2 * C2) / N\n    ) %&gt;%\n    dplyr::select(-text1, -text2) -&gt; stats_tb2\n\n\n\ntokenC1C2NR1R2O11O12O21O22E11E12E21E22a94,677169,163263,8406,926256,9142,3904,53692,287164,6272,485.34301854,440.656981592,191.66164,722.3aaronson94,677169,163263,8408263,8328094,669169,1632.87073985.129260294,674.13169,157.9aback94,677169,163263,8404263,8362294,675169,1611.43536992.564630194,675.56169,160.4abandon94,677169,163263,8406263,8343394,674169,1602.15305493.846945194,674.85169,159.2abandoned94,677169,163263,84011263,8294794,673169,1563.94726737.052732794,673.05169,155.9abashed94,677169,163263,8403263,8371294,676169,1611.07652741.923472694,675.92169,161.1abbreviated94,677169,163263,8401263,8391094,676169,1630.35884250.641157594,676.64169,162.4abiding94,677169,163263,8402263,8381194,676169,1620.71768501.282315094,676.28169,161.7ability94,677169,163263,8402263,8381194,676169,1620.71768501.282315094,676.28169,161.7abject94,677169,163263,8403263,8373094,674169,1631.07652741.923472694,675.92169,161.1\n\n\nWe can now calculate the keyness measures.\n\nstats_tb2 %&gt;%\n    # determine number of rows\n    dplyr::mutate(Rws = nrow(.)) %&gt;%\n    # work row-wise\n    dplyr::rowwise() %&gt;%\n    # calculate fishers' exact test\n    dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22),\n        ncol = 2, byrow = T\n    ))[1]))) %&gt;%\n    # extract descriptives\n    dplyr::mutate(\n        ptw_target = O11 / C1 * 1000,\n        ptw_ref = O12 / C2 * 1000\n    ) %&gt;%\n    # extract x2 statistics\n    dplyr::mutate(X2 = (O11 - E11)^2 / E11 + (O12 - E12)^2 / E12 + (O21 - E21)^2 / E21 + (O22 - E22)^2 / E22) %&gt;%\n    # extract keyness measures\n    dplyr::mutate(\n        phi = sqrt((X2 / N)),\n        MI = log2(O11 / E11),\n        t.score = (O11 - E11) / sqrt(O11),\n        PMI = log2((O11 / N) / ((O11 + O12) / N) *\n            ((O11 + O21) / N)),\n        DeltaP = (O11 / R1) - (O21 / R2),\n        LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ((O12 + 0.5) * (O21 + 0.5))),\n        G2 = 2 * ((O11 + 0.001) * log((O11 + 0.001) / E11) + (O12 + 0.001) * log((O12 + 0.001) / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22)),\n\n        # traditional keyness measures\n        RateRatio = ((O11 + 0.001) / (C1 * 1000)) / ((O12 + 0.001) / (C2 * 1000)),\n        RateDifference = (O11 / (C1 * 1000)) - (O12 / (C2 * 1000)),\n        DifferenceCoefficient = RateDifference / sum((O11 / (C1 * 1000)), (O12 / (C2 * 1000))),\n        OddsRatio = ((O11 + 0.5) * (O22 + 0.5)) / ((O12 + 0.5) * (O21 + 0.5)),\n        LLR = 2 * (O11 * (log((O11 / E11)))),\n        RDF = abs((O11 / C1) - (O12 / C2)),\n        PDiff = abs(ptw_target - ptw_ref) / ((ptw_target + ptw_ref) / 2) * 100,\n        SignedDKL = sum(ifelse(O11 &gt; 0, O11 * log(O11 / ((O11 + O12) / 2)), 0) - ifelse(O12 &gt; 0, O12 * log(O12 / ((O11 + O12) / 2)), 0))\n    ) %&gt;%\n    # determine Bonferroni corrected significance\n    dplyr::mutate(Sig_corrected = dplyr::case_when(\n        p / Rws &gt; .05 ~ \"n.s.\",\n        p / Rws &gt; .01 ~ \"p &lt; .05*\",\n        p / Rws &gt; .001 ~ \"p &lt; .01**\",\n        p / Rws &lt;= .001 ~ \"p &lt; .001***\",\n        T ~ \"N.A.\"\n    )) %&gt;%\n    # round p-value\n    dplyr::mutate(\n        p = round(p, 5),\n        type = ifelse(E11 &gt; O11, \"antitype\", \"type\"),\n        phi = ifelse(E11 &gt; O11, -phi, phi),\n        G2 = ifelse(E11 &gt; O11, -G2, G2)\n    ) %&gt;%\n    # filter out non significant results\n    dplyr::filter(Sig_corrected != \"n.s.\") %&gt;%\n    # arrange by G2\n    dplyr::arrange(-G2) %&gt;%\n    # remove superfluous columns\n    dplyr::select(-any_of(c(\n        \"TermCoocFreq\", \"AllFreq\", \"NRows\",\n        \"R1\", \"R2\", \"C1\", \"C2\", \"E12\", \"E21\",\n        \"E22\", \"upp\", \"low\", \"op\", \"t.score\", \"z.score\", \"Rws\"\n    ))) %&gt;%\n    dplyr::relocate(any_of(c(\n        \"token\", \"type\", \"Sig_corrected\", \"O11\", \"O12\",\n        \"ptw_target\", \"ptw_ref\", \"G2\", \"RDF\", \"RateRatio\",\n        \"RateDifference\", \"DifferenceCoefficient\", \"LLR\", \"SignedDKL\",\n        \"PDiff\", \"LogOddsRatio\", \"MI\", \"PMI\", \"phi\", \"X2\",\n        \"OddsRatio\", \"DeltaP\", \"p\", \"E11\", \"O21\", \"O22\"\n    ))) -&gt; assoc_tb3\n\n\n\ntokentypeSig_correctedO11O12ptw_targetptw_refG2RDFRateRatioRateDifferenceDifferenceCoefficientLLRSignedDKLPDiffLogOddsRatioMIPMIphiX2OddsRatioDeltaPpE11O21O22Nwinstontypep &lt; .001***44004.64738000.00000000903.17990.0046473800786,166.5363600.00000464738001.0000000901.8871304.98476200.000007.36610511.4785774-1.4785770.05463223787.47801,581.4621940.64222850157.8906994,237169,163263,840wastypep &lt; .001***2,1461,61822.66653999.56473933703.97430.01310180062.3698020.00001310180060.40649331,987.1753526.2580781.298670.87604460.6679609-2.2891940.05299452740.97332.4013830.214353701,350.6831092,531167,545263,840hadtypep &lt; .001***1,26876513.39290434.52226551591.36770.00887063882.9615460.00000887063880.49514681,401.9010497.7710199.029361.09440130.7975219-2.1596330.04865982624.71462.9873940.26692310729.5267693,409168,398263,840partytypep &lt; .001***25092.64055680.05320312442.68710.002587353749.6262970.00000258735370.9604990494.7523188.44312192.099803.85514731.4275534-1.5296010.03962998414.369947.2355730.6070044092.9402094,427169,154263,840hetypep &lt; .001***1,8891,72919.952047510.22091119406.28060.00973113631.9520810.00000973113630.32251181,416.7423159.9478264.502370.67874940.5410077-2.4161470.04013583425.01581.9714110.165539201,298.2920992,788167,434263,840obrientypep &lt; .001***17801.88007650.00000000365.04990.0018800765318,041.1627220.00000188007651.0000000364.8543123.38020200.000006.46000691.4785774-1.4785770.03473095318.2541639.0654920.6415904063.8739694,499169,163263,840shetypep &lt; .001***3781103.99252190.65026040352.40950.00334226156.1398420.00000334226150.7198833581.7046253.09608143.976661.81493991.1100825-1.8470720.03731102367.29496.1407070.41651810175.1151394,299169,053263,840youtypep &lt; .001***95085110.03411605.03065091214.02510.00500346511.9945960.00000500346510.3321303731.949298.9501066.426050.69541940.5557786-2.4013760.02914781224.15722.0045500.16980130646.2753193,727168,312263,840couldtypep &lt; .001***3782113.99252191.24731768194.31440.00274520433.2008800.00000274520430.5239100439.4929164.70635104.782001.16513280.8386960-2.1184590.02790018205.37833.2063490.28355620211.3582294,299168,952263,840telescreentypep &lt; .001***9000.95060050.00000000184.51390.0009506005160,808.2127970.00000095060051.0000000184.476962.38325200.000005.77983741.4785774-1.4785770.02469195160.8613323.7065520.6413763032.2958294,587169,163263,840\n\n\nThe above table shows the keywords for text1, which is George Orwell’s Nineteeneightyfour. The table starts with token (word type), followed by type, which indicates whether the token is a keyword in the target data (type) or a keyword in the reference data (antitype). Next is the Bonferroni corrected significance (Sig_corrected), which accounts for repeated testing. This is followed by O11, representing the observed frequency of the token, and Exp which represents the expected frequency of the token if it were distributed evenly across the target and reference data. After this, the table provides different keyness statistics, which are explained below:\nDelta P (ΔP) is a measure of association that indicates the difference in conditional probabilities. It measures the strength and direction of association between two binary variables.\n\n(P(A|B) = P(A|B) - P(A|B))\n(P(B|A) = P(B|A) - P(B|A))\n\nWhere (P(A|B)) is the probability of A given B and (P(A|B)) is the probability of A given not B.\nThe Log Odds Ratio measures the strength of association between two binary variables. It is the natural logarithm of the odds ratio and provides a symmetric measure.\n[ = () ]\nMutual Information (MI) quantifies the amount of information obtained about one random variable through the other random variable. It measures the mutual dependence between the variables.\n[ I(X;Y) = {x X} {y Y} P(x, y) () ]\nWhere (P(x, y)) is the joint probability distribution and (P(x)) and (P(y)) are the marginal probability distributions.\nPointwise Mutual Information (PMI) measures the association between a specific event and another specific event. It is a pointwise measure of mutual information.\n[ (x, y) = () ]\nThe Phi (φ) Coefficient is a measure of association for two binary variables. It is a specific case of the Pearson correlation coefficient for binary data.\n[ = ]\nWhere (n_{ij}) represents the count of observations where the first variable is (i) and the second variable is (j).\nThe Chi-Square (χ²) statistic measures the independence between two categorical variables. It assesses whether observed frequencies differ from expected frequencies.\n[ ^2 = ]\nWhere (O_i) is the observed frequency and (E_i) is the expected frequency.\nThe Likelihood Ratio (G²) compares the fit of two models: one under the null hypothesis and one under the alternative hypothesis. It measures how much better the data fits one model over the other.\n[ G^2 = 2 O_i () ]\nWhere (O_i) is the observed frequency and (E_i) is the expected frequency.\nThe Rate Ratio compares the rate of events between two groups. It is commonly used in epidemiology.\n[ = ]\nThe Rate Difference measures the absolute difference in event rates between two groups.\n[ = - ]\nThe Difference Coefficient (also known as the Difference Score) measures the difference between the observed and expected values, standardized by the expected values.\n[ D = ]\nWhere (O) is the observed frequency and (E) is the expected frequency.\nThe Odds Ratio quantifies the strength of association between two events. It compares the odds of an event occurring in one group to the odds of it occurring in another group.\n[ = ]\nWhere (P(A|B)) is the probability of A given B, (P(A|B)) is the probability of not A given B, (P(A|B)) is the probability of A given not B, and (P(A|B)) is the probability of not A given not B.\nThese measures help analyze the association strength, and significance of the the attraction or likelihood of a token to surface in the target rather than the reference data."
  },
  {
    "objectID": "tutorials/key/key.html#visualising-keywords",
    "href": "tutorials/key/key.html#visualising-keywords",
    "title": "Keyness and Keyword Analysis in R",
    "section": "Visualising keywords",
    "text": "Visualising keywords\nWe can now visualize the keyness strengths in a dotplot as shown in the code chunk below.\n\n# sort the assoc_tb3 data frame in descending order based on the 'G2' column\nassoc_tb3 %&gt;%\n    dplyr::arrange(-G2) %&gt;%\n    # select the top 20 rows after sorting\n    head(20) %&gt;%\n    # create a ggplot with 'token' on the x-axis (reordered by 'G2') and 'G2' on the y-axis\n    ggplot(aes(x = reorder(token, G2, mean), y = G2)) +\n    # add a scatter plot with points representing the 'G2' values\n    geom_point() +\n    # flip the coordinates to have horizontal points\n    coord_flip() +\n    # set the theme to a basic white and black theme\n    theme_bw() +\n    # set the x-axis label to \"Token\" and y-axis label to \"Keyness (G2)\"\n    labs(x = \"Token\", y = \"Keyness (G2)\")\n\n\n\n\n\n\n\n\nAnother option to visualize keyness is a barplot as shown below.\n\n# get top 10 keywords for text 1\ntop &lt;- assoc_tb3 %&gt;%\n    dplyr::ungroup() %&gt;%\n    dplyr::slice_head(n = 12)\n# get top 10 keywords for text 2\nbot &lt;- assoc_tb3 %&gt;%\n    dplyr::ungroup() %&gt;%\n    dplyr::slice_tail(n = 12)\n# combine into table\nrbind(top, bot) %&gt;%\n    # create a ggplot\n    ggplot(aes(x = reorder(token, G2, mean), y = G2, label = G2, fill = type)) +\n    # add a bar plot using the 'phi' values\n    geom_bar(stat = \"identity\") +\n    # add text labels above the bars with rounded 'phi' values\n    geom_text(aes(\n        y = ifelse(G2 &gt; 0, G2 - 50, G2 + 50),\n        label = round(G2, 1)\n    ), color = \"white\", size = 3) +\n    # flip the coordinates to have horizontal bars\n    coord_flip() +\n    # set the theme to a basic white and black theme\n    theme_bw() +\n    # remove legend\n    theme(legend.position = \"none\") +\n    # define colors\n    scale_fill_manual(values = c(\"orange\", \"darkgray\")) +\n    # set the x-axis label to \"Token\" and y-axis label to \"Keyness (G2)\"\n    labs(title = \"Top 10 keywords for text1 and text 2\", x = \"Keyword\", y = \"Keyness (G2)\")"
  },
  {
    "objectID": "tutorials/key/key.html#comparative-wordclouds",
    "href": "tutorials/key/key.html#comparative-wordclouds",
    "title": "Keyness and Keyword Analysis in R",
    "section": "Comparative wordclouds",
    "text": "Comparative wordclouds\nAnother form of word clouds, known as comparison clouds, is helpful in discerning disparities between texts. The problem compared to previous, more informative methods for identifying keywords is that comparison clouds use a very basic and not very sophisticated methods for identifying keywords. Nonetheless, comparison clouds are very useful visualization tools during initial steps on an analysis.\nIn a first step, we generate a corpus object from the texts and create a variable with the author name.\n\ncorp_dom &lt;- quanteda::corpus(c(text1, text2))\nattr(corp_dom, \"docvars\")$Author &lt;- c(\"Orwell\", \"Melville\")\n\nNow, we can remove so-called stopwords (non-lexical function words) and punctuation and generate the comparison cloud.\n\n# create a comparison word cloud for a corpus\ncorp_dom %&gt;%\n    # tokenize the corpus, removing punctuation, symbols, and numbers\n    quanteda::tokens(\n        remove_punct = TRUE,\n        remove_symbols = TRUE,\n        remove_numbers = TRUE\n    ) %&gt;%\n    # remove English stopwords\n    quanteda::tokens_remove(stopwords(\"english\")) %&gt;%\n    # create a Document-Feature Matrix (DFM)\n    quanteda::dfm() %&gt;%\n    # group the DFM by the 'Author' column from 'corp_dom'\n    quanteda::dfm_group(groups = corp_dom$Author) %&gt;%\n    # trim the DFM, keeping terms that occur at least 10 times\n    quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE) %&gt;%\n    # generate a comparison word cloud\n    quanteda.textplots::textplot_wordcloud(\n        # create a comparison word cloud\n        comparison = TRUE,\n        # set colors for different groups\n        color = c(\"darkgray\", \"orange\"),\n        # define the maximum number of words to display in the word cloud\n        max_words = 150\n    )"
  },
  {
    "objectID": "tutorials/key/key.html#footnotes",
    "href": "tutorials/key/key.html#footnotes",
    "title": "Keyness and Keyword Analysis in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m extremely grateful to Joseph Flanagan who provided very helpful feedback and pointed out errors in previous versions of this tutorial. All remaining errors are, of course, my own.↩︎\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/topic/topic.html#human-in-the-loop-topic-modelling",
    "href": "tutorials/topic/topic.html#human-in-the-loop-topic-modelling",
    "title": "Topic Modeling with R",
    "section": "Human-in-the-loop Topic Modelling",
    "text": "Human-in-the-loop Topic Modelling\nIn this human-in-the-loop approach to topic modelling which mainly uses and combines the quanteda package (Benoit et al.), the topicmodels package (Grün and Hornik 2024, 2011), and the seededlda package (Watanabe and Xuan-Hieu 2024). Now that we have cleaned the data, we can perform the topic modelling. This consists of two steps:\n\nFirst, we perform an unsupervised LDA. We do this to check what topics are in our corpus.\nThen, we perform a supervised LDA (based on the results of the unsupervised LDA) to identify meaningful topics in our data. For the supervised LDA, we define so-called seed terms that help in generating coherent topics.\n\n\nLoading and preparing data\nWhen preparing the data for analysis, we employ several preprocessing steps to ensure its cleanliness and readiness for analysis. Initially, we load the data and then remove punctuation, symbols, and numerical characters. Additionally, we eliminate common stop words, such as the and and, which can introduce noise and hinder the topic modeling process. To standardize the text, we convert it to lowercase and, lastly, we apply stemming to reduce words to their base form.\n\n# load data\ntxts &lt;- base::readRDS(\"tutorials/topic/data/sotu_paragraphs.rda\", \"rb\")\ntxts$text %&gt;%\n  # tokenise\n  quanteda::tokens(\n    remove_punct = TRUE, # remove punctuation\n    remove_symbols = TRUE, # remove symbols\n    remove_number = TRUE\n  ) %&gt;% # remove numbers\n  # remove stop words\n  quanteda::tokens_select(pattern = stopwords(\"en\"), selection = \"remove\") %&gt;%\n  # stemming\n  quanteda::tokens_wordstem() %&gt;%\n  # convert to document-frequency matrix\n  quanteda::dfm(tolower = T) -&gt; ctxts\n# add docvars\ndocvars(ctxts, \"president\") &lt;- txts$president\ndocvars(ctxts, \"date\") &lt;- txts$date\ndocvars(ctxts, \"speechid\") &lt;- txts$speech_doc_id\ndocvars(ctxts, \"docid\") &lt;- txts$doc_id\n# clean data\nctxts &lt;- dfm_subset(ctxts, ntoken(ctxts) &gt; 0)\n# inspect data\nctxts[1:5, 1:5]\n\nDocument-feature matrix of: 5 documents, 5 features (80.00% sparse) and 4 docvars.\n       features\ndocs    fellow-citizen senat hous repres embrac\n  text1              1     1    1      1      0\n  text2              0     0    0      0      1\n  text3              0     0    0      0      0\n  text4              0     0    0      0      0\n  text5              0     0    0      0      0\n\n\n\n\nInitial unsupervised topic model\nNow that we have loaded and prepared the data for analysis, we will follow a two-step approach.\n\nFirst, we perform an unsupervised topic model using Latent Dirichlet Allocation (LDA) to identify the topics present in our data. This initial step helps us understand the broad themes and structure within the data set.\nThen, based on the results of the unsupervised topic model, we conduct a supervised topic model using LDA to refine and identify more meaningful topics in our data.\n\nThis combined approach allows us to leverage both data-driven insights and expert supervision to enhance the accuracy and interpretability of the topics.\nIn the initial step that implements a unsupervised, data-driven topic model, we vary the number of topics the LDA algorithm looks for until we identify coherent topics in the data. We use the LDA function from the topicmodels package instead of the textmodel_lda function from the seededlda package because the former allows us to include a seed. Including a seed ensures that the results of this unsupervised topic model are reproducible, which is not the case if we do not seed the model, as each model will produce different results without setting a seed.\n\n# generate model: change k to different numbers, e.g. 10 or 20 and look for consistencies in the keywords for the topics below.\ntopicmodels::LDA(ctxts, k = 15, control = list(seed = 1234)) -&gt; ddlda\n\nNow that we have generated an initial data-driven model, the next step is to inspect it to evaluate its performance and understand the topics it has identified. To do this, we need to examine the terms associated with each detected topic. By analyzing these terms, we can gain insights into the themes represented by each topic and assess the coherence and relevance of the model’s output.\n\n# define number of topics\nntopics &lt;- 15\n# define number of terms\nnterms &lt;- 10\n# generate table\ntidytext::tidy(ddlda, matrix = \"beta\") %&gt;%\n  dplyr::group_by(topic) %&gt;%\n  dplyr::slice_max(beta, n = nterms) %&gt;%\n  dplyr::ungroup() %&gt;%\n  dplyr::arrange(topic, -beta) %&gt;%\n  dplyr::mutate(\n    term = paste(term, \" (\", round(beta, 3), \")\", sep = \"\"),\n    topic = paste(\"topic\", topic),\n    topic = factor(topic, levels = c(paste(\"topic\", 1:ntopics))),\n    top = rep(paste(\"top\", 1:nterms), nrow(.) / nterms),\n    top = factor(top, levels = c(paste(\"top\", 1:nterms)))\n  ) %&gt;%\n  dplyr::select(-beta) %&gt;%\n  tidyr::spread(topic, term) -&gt; ddlda_top_terms\nddlda_top_terms\n\n# A tibble: 10 × 16\n   top    `topic 1`  `topic 2` `topic 3` `topic 4` `topic 5` `topic 6` `topic 7`\n   &lt;fct&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    \n 1 top 1  state (0.… countri … state (0… govern (… state (0… state (0… state (0…\n 2 top 2  countri (… upon (0.… unite (0… will (0.… countri … will (0.… upon (0.…\n 3 top 3  will (0.0… present … congress… year (0.… will (0.… unite (0… will (0.…\n 4 top 4  congress … war (0.0… may (0.0… unite (0… congress… govern (… congress…\n 5 top 5  nation (0… can (0.0… treati (… law (0.0… public (… power (0… may (0.0…\n 6 top 6  can (0.00… unite (0… citizen … may (0.0… year (0.… law (0.0… govern (…\n 7 top 7  subject (… nation (… nation (… upon (0.… nation (… peopl (0… citizen …\n 8 top 8  govern (0… author (… great (0… act (0.0… can (0.0… last (0.… nation (…\n 9 top 9  land (0.0… may (0.0… territor… public (… law (0.0… duti (0.… import (…\n10 top 10 made (0.0… subject … made (0.… last (0.… import (… part (0.… great (0…\n# ℹ 8 more variables: `topic 8` &lt;chr&gt;, `topic 9` &lt;chr&gt;, `topic 10` &lt;chr&gt;,\n#   `topic 11` &lt;chr&gt;, `topic 12` &lt;chr&gt;, `topic 13` &lt;chr&gt;, `topic 14` &lt;chr&gt;,\n#   `topic 15` &lt;chr&gt;\n\n\nIn a real analysis, we would re-run the unsupervised model multiple times, adjusting the number of topics that the Latent Dirichlet Allocation (LDA) algorithm “looks for.” For each iteration, we would inspect the key terms associated with the identified topics to check their thematic consistency. This evaluation helps us determine whether the results of the topic model make sense and accurately reflect the themes present in the data. By varying the number of topics and examining the corresponding key terms, we can identify the optimal number of topics that best represent the underlying themes in our data set. However, we will skip re-running the model here, as this is just a tutorial intended to showcase the process rather than a comprehensive analysis.\nTo obtain a comprehensive table of terms and their association strengths with topics (the beta values), follow the steps outlined below. This table can help verify if the data contains thematically distinct topics. Additionally, visualizations and statistical modeling can be employed to compare the distinctness of topics and determine the ideal number of topics. However, I strongly recommend not solely relying on statistical measures when identifying the optimal number of topics. In my experience, human intuition is still essential for evaluating topic coherence and consistency.\n\n# extract topics\nddlda_topics &lt;- tidy(ddlda, matrix = \"beta\")\n# inspect\nhead(ddlda_topics, 20)\n\n# A tibble: 20 × 3\n   topic term                beta\n   &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     1 fellow-citizen 0.000249 \n 2     2 fellow-citizen 0.000351 \n 3     3 fellow-citizen 0.000416 \n 4     4 fellow-citizen 0.0000333\n 5     5 fellow-citizen 0.0000797\n 6     6 fellow-citizen 0.000183 \n 7     7 fellow-citizen 0.000445 \n 8     8 fellow-citizen 0.000306 \n 9     9 fellow-citizen 0.000381 \n10    10 fellow-citizen 0.000332 \n11    11 fellow-citizen 0.000187 \n12    12 fellow-citizen 0.000147 \n13    13 fellow-citizen 0.000278 \n14    14 fellow-citizen 0.000336 \n15    15 fellow-citizen 0.000205 \n16     1 senat          0.000708 \n17     2 senat          0.000477 \n18     3 senat          0.00263  \n19     4 senat          0.00118  \n20     5 senat          0.000436 \n\n\nThe purpose of this initial step, in which we generate data-driven unsupervised topic models, is to identify the number of coherent topics present in the data and to determine the key terms associated with these topics. These key terms will then be used as seed terms in the next step: the supervised, seeded topic model. This approach ensures that the supervised model is grounded in the actual thematic structure of the data set, enhancing the accuracy and relevance of the identified topics.\n\n\nSupervised, seeded topic model\nTo implement the supervised, seeded topic model, we start by creating a dictionary containing the seed terms we have identified in the first step.\nTo check terms (to see if ), you can use the following code chunk:\n\nddlda_topics %&gt;%\n  select(term) %&gt;%\n  unique() %&gt;%\n  filter(str_detect(term, \"agri\"))\n\n# A tibble: 3 × 1\n  term               \n  &lt;chr&gt;              \n1 agricultur         \n2 agriculturist      \n3 agricultural-colleg\n\n\n\n# semisupervised LDA\ndict &lt;- dictionary(list(\n  military = c(\"armi\", \"war\", \"militari\", \"conflict\"),\n  liberty = c(\"freedom\", \"liberti\", \"free\"),\n  nation = c(\"nation\", \"countri\", \"citizen\"),\n  law = c(\"law\", \"court\", \"prison\"),\n  treaty = c(\"claim\", \"treati\", \"negoti\"),\n  indian = c(\"indian\", \"tribe\", \"territori\"),\n  labor = c(\"labor\", \"work\", \"condit\"),\n  money = c(\"bank\", \"silver\", \"gold\", \"currenc\", \"money\"),\n  finance = c(\"debt\", \"invest\", \"financ\"),\n  wealth = c(\"prosper\", \"peac\", \"wealth\"),\n  industry = c(\"produc\", \"industri\", \"manufactur\"),\n  navy = c(\"navi\", \"ship\", \"vessel\", \"naval\"),\n  consitution = c(\"constitut\", \"power\", \"state\"),\n  agriculture = c(\"agricultur\", \"grow\", \"land\"),\n  office = c(\"office\", \"serv\", \"duti\")\n))\ntmod_slda &lt;- seededlda::textmodel_seededlda(ctxts,\n  dict,\n  residual = TRUE,\n  min_termfreq = 2\n)\n# inspect\nseededlda::terms(tmod_slda)\n\n      military   liberty   nation     law       treaty    indian     \n [1,] \"war\"      \"free\"    \"countri\"  \"law\"     \"treati\"  \"territori\"\n [2,] \"militari\" \"peopl\"   \"nation\"   \"court\"   \"claim\"   \"indian\"   \n [3,] \"armi\"     \"can\"     \"citizen\"  \"case\"    \"govern\"  \"tribe\"    \n [4,] \"forc\"     \"govern\"  \"govern\"   \"person\"  \"negoti\"  \"new\"      \n [5,] \"offic\"    \"must\"    \"foreign\"  \"upon\"    \"unite\"   \"now\"      \n [6,] \"command\"  \"upon\"    \"american\" \"act\"     \"minist\"  \"made\"     \n [7,] \"conflict\" \"liberti\" \"right\"    \"subject\" \"two\"     \"line\"     \n [8,] \"men\"      \"polit\"   \"time\"     \"provis\"  \"relat\"   \"part\"     \n [9,] \"order\"    \"will\"    \"properti\" \"natur\"   \"convent\" \"boundari\" \n[10,] \"island\"   \"great\"   \"mexico\"   \"may\"     \"britain\" \"mexico\"   \n      labor       money      finance      wealth     industry     navy       \n [1,] \"condit\"    \"bank\"     \"year\"       \"peac\"     \"produc\"     \"vessel\"   \n [2,] \"work\"      \"money\"    \"debt\"       \"prosper\"  \"industri\"   \"navi\"     \n [3,] \"labor\"     \"gold\"     \"amount\"     \"will\"     \"manufactur\" \"ship\"     \n [4,] \"servic\"    \"currenc\"  \"expenditur\" \"us\"       \"product\"    \"naval\"    \n [5,] \"offic\"     \"silver\"   \"treasuri\"   \"interest\" \"import\"     \"construct\"\n [6,] \"depart\"    \"govern\"   \"will\"       \"peopl\"    \"foreign\"    \"port\"     \n [7,] \"system\"    \"public\"   \"increas\"    \"great\"    \"trade\"      \"coast\"    \n [8,] \"improv\"    \"treasuri\" \"fiscal\"     \"everi\"    \"increas\"    \"sea\"      \n [9,] \"great\"     \"issu\"     \"last\"       \"happi\"    \"revenu\"     \"commerc\"  \n[10,] \"establish\" \"note\"     \"estim\"      \"wealth\"   \"valu\"       \"new\"      \n      consitution agriculture  office     other     \n [1,] \"state\"     \"land\"       \"duti\"     \"congress\"\n [2,] \"power\"     \"report\"     \"will\"     \"act\"     \n [3,] \"constitut\" \"congress\"   \"may\"      \"last\"    \n [4,] \"unite\"     \"attent\"     \"subject\"  \"repres\"  \n [5,] \"govern\"    \"recommend\"  \"can\"      \"session\" \n [6,] \"right\"     \"secretari\"  \"consider\" \"senat\"   \n [7,] \"union\"     \"agricultur\" \"object\"   \"presid\"  \n [8,] \"shall\"     \"public\"     \"present\"  \"hous\"    \n [9,] \"author\"    \"subject\"    \"shall\"    \"day\"     \n[10,] \"independ\"  \"depart\"     \"measur\"   \"made\"    \n\n\nNow, we extract files and create a data frame of topics and documents. This shows what topic is dominant in which file in tabular form.\n\n# generate data frame\ndata.frame(tmod_slda$data$date, tmod_slda$data$president, seededlda::topics(tmod_slda)) %&gt;%\n  dplyr::rename(\n    Date = 1,\n    President = 2,\n    Topic = 3\n  ) %&gt;%\n  dplyr::mutate(\n    Date = stringr::str_remove_all(Date, \"-.*\"),\n    Date = stringr::str_replace_all(Date, \".$\", \"0\")\n  ) %&gt;%\n  dplyr::mutate_if(is.character, factor) -&gt; topic_df\n# inspect\nhead(topic_df)\n\n      Date         President    Topic\ntext1 1790 George Washington    other\ntext2 1790 George Washington   wealth\ntext3 1790 George Washington   wealth\ntext4 1790 George Washington   wealth\ntext5 1790 George Washington military\ntext6 1790 George Washington   office\n\n\nUsing the table (or data frame) we have just created, we can visualize the use of topics over time.\n\ntopic_df %&gt;%\n  dplyr::group_by(Date, Topic) %&gt;%\n  dplyr::summarise(freq = n()) %&gt;%\n  ggplot(aes(x = Date, y = freq, fill = Topic)) +\n  geom_bar(stat = \"identity\", position = \"fill\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Decade\") +\n  scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, \"RdBu\"))(ntopics + 1))) +\n  scale_y_continuous(name = \"Percent of paragraphs\", labels = seq(0, 100, 25))\n\n`summarise()` has grouped output by 'Date'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nThe figure illustrates the relative frequency of topics over time in the State of the Union (SOTU) texts. Notably, paragraphs discussing the topic of “office,” characterized by key terms such as office, serv, and duti, have become less prominent over time. This trend suggests a decreasing emphasis on this particular theme, as evidenced by the diminishing number of paragraphs dedicated to it."
  },
  {
    "objectID": "tutorials/topic/topic.html#data-driven-topic-modelling",
    "href": "tutorials/topic/topic.html#data-driven-topic-modelling",
    "title": "Topic Modeling with R",
    "section": "Data-driven Topic Modelling",
    "text": "Data-driven Topic Modelling\nIn this part of the tutorial, we show an alternative approaches for performing data-driven topic modelling using LDA.\n\nLoading and preparing data\nWhen readying the data for analysis, we follow consistent pre-processing steps, employing the tm package (Feinerer and Hornik 2024; Feinerer, Hornik, and Meyer 2008) for efficient data preparation and cleaning. First, we load the data and convert it into a corpus object. Next, we convert the text to lowercase, eliminating superfluous white spaces, and removing stop words. Subsequently, we proceed to strip the data of punctuation, symbols, and numerical characters. Finally, we apply stemming to standardize words to their base form, ensuring uniformity throughout the data set.\n\n# load data\ntextdata &lt;- base::readRDS(\"tutorials/topic/data/sotu_paragraphs.rda\", \"rb\")\n# create corpus object\ntm::Corpus(DataframeSource(textdata)) %&gt;%\n  # convert to lower case\n  tm::tm_map(content_transformer(tolower)) %&gt;%\n  # remove superfluous white spaces\n  tm::tm_map(stripWhitespace) %&gt;%\n  # remove stop words\n  tm::tm_map(removeWords, quanteda::stopwords()) %&gt;%\n  # remove punctuation\n  tm::tm_map(removePunctuation, preserve_intra_word_dashes = TRUE) %&gt;%\n  # remove numbers\n  tm::tm_map(removeNumbers) %&gt;%\n  # stemming\n  tm::tm_map(stemDocument, language = \"en\") -&gt; textcorpus\n# inspect data\nstr(textcorpus)\n\nClasses 'SimpleCorpus', 'Corpus'  hidden list of 3\n $ content: Named chr [1:8833] \"fellow-citizen senat hous repres\" \"embrac great satisfact opportun now present congratul present favor prospect public affair recent access import\"| __truncated__ \"resum consult general good can deriv encourag reflect measur last session satisfactori constitu novelti difficu\"| __truncated__ \"among mani interest object engag attent provid common defens merit particular regard prepar war one effectu mean preserv peac\" ...\n  ..- attr(*, \"names\")= chr [1:8833] \"1\" \"2\" \"3\" \"4\" ...\n $ meta   :List of 1\n  ..$ language: chr \"en\"\n  ..- attr(*, \"class\")= chr \"CorpusMeta\"\n $ dmeta  :'data.frame':    8833 obs. of  4 variables:\n  ..$ speech_doc_id: int [1:8833] 1 1 1 1 1 1 1 1 1 1 ...\n  ..$ speech_type  : Factor w/ 1 level \"State of the Union Address\": 1 1 1 1 1 1 1 1 1 1 ...\n  ..$ president    : Factor w/ 23 levels \"Abraham Lincoln\",..: 7 7 7 7 7 7 7 7 7 7 ...\n  ..$ date         : chr [1:8833] \"1790-01-08\" \"1790-01-08\" \"1790-01-08\" \"1790-01-08\" ...\n\n\n\n\nModel calculation\nHere’s the improved and expanded version of the paragraph:\nAfter preprocessing, we have a clean corpus object called textcorpus, which we use to calculate the unsupervised Latent Dirichlet Allocation (LDA) topic model (Blei, Ng, and Jordan). To perform this calculation, we first create a Document-Term Matrix (DTM) from the textcorpus. In this step, we ensure that only terms with a certain minimum frequency in the corpus are included (we set the minimum frequency to 5). This selection process not only speeds up the model calculation but also helps improve the model’s accuracy by focusing on more relevant and frequently occurring terms. By filtering out less common terms, we reduce noise and enhance the coherence of the topics identified by the LDA model.\n\n# compute document term matrix with terms &gt;= minimumFrequency\nminimumFrequency &lt;- 5\nDTM &lt;- tm::DocumentTermMatrix(textcorpus,\n  control = list(bounds = list(global = c(minimumFrequency, Inf)))\n)\n# inspect the number of documents and terms in the DTM\ndim(DTM)\n\n[1] 8833 4472\n\n\nDue to vocabulary pruning, some rows in our Document-Term Matrix (DTM) may end up being empty. Latent Dirichlet Allocation (LDA) cannot handle empty rows, so we must remove these documents from both the DTM and the corresponding metadata. This step ensures that the topic modeling process runs smoothly without encountering errors caused by empty documents. Additionally, removing these empty rows helps maintain the integrity of our analysis by focusing only on documents that contain meaningful content.\n\nsel_idx &lt;- slam::row_sums(DTM) &gt; 0\nDTM &lt;- DTM[sel_idx, ]\ntextdata &lt;- textdata[sel_idx, ]\n# inspect the number of documents and terms in the DTM\ndim(DTM)\n\n[1] 8811 4472\n\n\nThe output shows that we have removed 22 documents (8833 - 8811) from the DTM.\nAs an unsupervised machine learning method, topic models are well-suited for exploring data. The primary goal of calculating topic models is to determine the proportionate composition of a fixed number of topics within the documents of a collection. Experimenting with different parameters is essential to identify the most suitable settings for your analysis needs.\nFor parameterized models such as Latent Dirichlet Allocation (LDA), the number of topics K is the most critical parameter to define in advance. Selecting the optimal K depends on various factors. If K is too small, the collection is divided into a few very general semantic contexts. Conversely, if K is too large, the collection is divided into too many topics, leading to overlaps and some topics being barely interpretable. Finding the right balance is key to achieving meaningful and coherent topics in your analysis.\nAn alternative to deciding on a set number of topics is to extract parameters form a models using a rage of number of topics. This approach can be useful when the number of topics is not theoretically motivated or based on closer, qualitative inspection of the data. In the example below, the determination of the optimal number of topics follows Murzintcev (n.d.), but we only use two metrics (CaoJuan2009 and Deveaud2014) - it is highly recommendable to inspect the results of the four metrics available for the FindTopicsNumber function which are Griffiths2004 (see Griffiths et al. 2004), CaoJuan2009 (see Cao et al.), Arun2010 (see Arun et al. 2010), and Deveaud2014 (see Deveaud, SanJuan, and Bellot 2014).\n\n# create models with different number of topics\nresult &lt;- ldatuning::FindTopicsNumber(\n  DTM,\n  topics = seq(from = 2, to = 20, by = 1),\n  metrics = c(\"CaoJuan2009\", \"Deveaud2014\"),\n  method = \"Gibbs\",\n  control = list(seed = 77),\n  verbose = TRUE\n)\n\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Deveaud2014... done.\n\n\nWe can now plot the results. In this case, we have only use two methods CaoJuan2009 and Griffith2004. The best number of topics shows low values for CaoJuan2009 and high values for Griffith2004 (optimally, several methods should converge and show peaks and dips respectively for a certain number of topics).\n\nFindTopicsNumber_plot(result)\n\n\n\n\n\n\n\n\nFor our first analysis, however, we choose a thematic “resolution” of K = 20 topics. In contrast to a resolution of 100 or more, this number of topics can be evaluated qualitatively very easy.\n\n# number of topics\nK &lt;- 20\n# set random number generator seed\nset.seed(9161)\n# compute the LDA model, inference via 1000 iterations of Gibbs sampling\ntopicModel &lt;- topicmodels::LDA(DTM, K, method = \"Gibbs\", control = list(iter = 500, verbose = 25))\n\nK = 20; V = 4472; M = 8811\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\n# save results\ntmResult &lt;- posterior(topicModel)\n# save theta values\ntheta &lt;- tmResult$topics\n# save beta values\nbeta &lt;- tmResult$terms\n# reset topic names\ntopicNames &lt;- apply(terms(topicModel, 5), 2, paste, collapse = \" \")\n\nDepending on the size of the vocabulary, the collection size and the number K, the inference of topic models can take a very long time. This calculation may take several minutes. If it takes too long, reduce the vocabulary in the DTM by increasing the minimum frequency in the previous step.\nLet’s take a look at the 10 most likely terms within the term probabilities beta of the inferred topics.\n\n# create a data frame from the topic model data\ntidytext::tidy(topicModel, matrix = \"beta\") %&gt;%\n  # ensure topics are factors with specific levels\n  dplyr::mutate(\n    topic = paste0(\"topic\", as.character(topic)),\n    topic = factor(topic, levels = paste0(\"topic\", 1:20))\n  ) %&gt;%\n  # group the data by topic\n  dplyr::group_by(topic) %&gt;%\n  # arrange terms within each topic by beta value (ascending)\n  dplyr::arrange(topic, -beta) %&gt;%\n  # select the top 10 terms with the highest beta values for each topic\n  dplyr::top_n(10) %&gt;%\n  # add beta to term\n  dplyr::mutate(term = paste0(term, \" (\", round(beta, 3), \")\")) %&gt;%\n  # remove the beta column as it is now part of the term string\n  dplyr::select(-beta) %&gt;%\n  # ungroup the data frame\n  dplyr::ungroup() %&gt;%\n  # create an id column for each term's position within the topic\n  dplyr::mutate(id = rep(1:10, 20)) %&gt;%\n  # pivot the data to a wider format with topics as columns\n  tidyr::pivot_wider(names_from = topic, values_from = term) -&gt; topterms\n\nSelecting by beta\n\n# inspect\ntopterms\n\n# A tibble: 10 × 21\n      id topic1  topic2 topic3 topic4 topic5 topic6 topic7 topic8 topic9 topic10\n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  \n 1     1 war (0… natio… gover… state… act (… may (… congr… new (… state… year (…\n 2     2 forc (… count… power… const… congr… upon … subje… great… terri… amount…\n 3     3 milita… peopl… natio… power… last … duti … recom… line … india… expend…\n 4     4 navi (… everi… indep… repre… made … shall… consi… const… mexic… increa…\n 5     5 armi (… prosp… princ… union… sessi… law (… atten… pacif… unit … treasu…\n 6     6 men (0… great… right… gover… autho… time … legis… estab… part … end (0…\n 7     7 offic … insti… polic… peopl… provi… requi… impor… compl… tribe… estim …\n 8     8 comman… happi… war (… hous … day (… prope… upon … coast… withi… revenu…\n 9     9 naval … honor… maint… exerc… first… neces… sugge… commu… exten… fiscal…\n10    10 servic… gener… inter… gener… effec… execu… prese… impor… texa … sum (0…\n# ℹ 10 more variables: topic11 &lt;chr&gt;, topic12 &lt;chr&gt;, topic13 &lt;chr&gt;,\n#   topic14 &lt;chr&gt;, topic15 &lt;chr&gt;, topic16 &lt;chr&gt;, topic17 &lt;chr&gt;, topic18 &lt;chr&gt;,\n#   topic19 &lt;chr&gt;, topic20 &lt;chr&gt;\n\n\nFor the next steps, we want to give the topics more descriptive names than just numbers. Therefore, we simply concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic.\n\ntopicNames &lt;- apply(terms(topicModel, 5), 2, paste, collapse = \" \")\n# inspect first 3 topic names\ntopicNames[1:3]\n\n                                Topic 1                                 Topic 2 \n          \"war forc militari navi armi\"    \"nation countri peopl everi prosper\" \n                                Topic 3 \n\"govern power nation independ principl\" \n\n\n\n\nVisualization of Words and Topics\nAlthough wordclouds may not be optimal for scientific purposes they can provide a quick visual overview of a set of terms. Let’s look at some topics as wordcloud.\nIn the following code, you can change the variable topicToViz with values between 1 and 20 to display other topics.\n\n# visualize topics as word cloud\n# choose topic of interest by a term contained in its name\ntopicToViz &lt;- grep(\"mexico\", topicNames)[1]\n# select to 50 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order\ntop50terms &lt;- sort(tmResult$terms[topicToViz, ], decreasing = TRUE)[1:50]\nwords &lt;- names(top50terms)\n# extract the probabilities of each of the 50 terms\nprobabilities &lt;- sort(tmResult$terms[topicToViz, ], decreasing = TRUE)[1:50]\n# visualize the terms as wordcloud\nmycolors &lt;- brewer.pal(8, \"Dark2\")\nwordcloud(words, probabilities, random.order = FALSE, color = mycolors)\n\n\n\n\n\n\n\n\nLet us now look more closely at the distribution of topics within individual documents. To this end, we visualize the distribution in 3 sample documents.\nLet us first take a look at the contents of three sample documents:\n\nexampleIds &lt;- c(2, 100, 200)\n# first 400 characters of file 2\nstringr::str_sub(txts$text[2], 1, 400)\n\n[1] \"I embrace with great satisfaction the opportunity which now presents itself\\nof congratulating you on the present favorable prospects of our public\\naffairs. The recent accession of the important state of North Carolina to\\nthe Constitution of the United States (of which official information has\\nbeen received), the rising credit and respectability of our country, the\\ngeneral and increasing good will \"\n\n# first 400 characters of file 100\nstringr::str_sub(txts$text[100], 1, 400)\n\n[1] \"Provision is likewise requisite for the reimbursement of the loan which has\\nbeen made of the Bank of the United States, pursuant to the eleventh\\nsection of the act by which it is incorporated. In fulfilling the public\\nstipulations in this particular it is expected a valuable saving will be\\nmade.\"\n\n# first 400 characters of file 200\nstringr::str_sub(txts$text[200], 1, 400)\n\n[1] \"After many delays and disappointments arising out of the European war, the\\nfinal arrangements for fulfilling the engagements made to the Dey and\\nRegency of Algiers will in all present appearance be crowned with success,\\nbut under great, though inevitable, disadvantages in the pecuniary\\ntransactions occasioned by that war, which will render further provision\\nnecessary. The actual liberation of all \"\n\n\nAfter looking into the documents, we visualize the topic distributions within the documents.\n\nN &lt;- length(exampleIds) # Number of example documents\n\n# Get topic proportions from example documents\ntopicProportionExamples &lt;- theta[exampleIds, ]\ncolnames(topicProportionExamples) &lt;- topicNames\n\n# Reshape data for visualization\nreshape2::melt(\n  cbind(data.frame(topicProportionExamples),\n    document = factor(1:N)\n  ),\n  variable.name = \"topic\",\n  id.vars = \"document\"\n) %&gt;%\n  # create bar plot using ggplot2\n  ggplot(aes(topic, value, fill = document), ylab = \"Proportion\") +\n  # plot bars\n  geom_bar(stat = \"identity\") +\n  # rotate x-axis labels\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  # flip coordinates to create horizontal bar plot\n  coord_flip() +\n  # facet by document\n  facet_wrap(~document, ncol = N)\n\n\n\n\n\n\n\n\n\n\nTopic distributions\nThe figure above illustrates how topics are distributed within a document according to the model. In the current model, all three documents exhibit at least a small percentage of each topic.\nThe topic distribution within a document can be controlled using the alpha parameter of the model. Higher alpha priors result in an even distribution of topics within a document, while lower alpha priors ensure that the inference process concentrates the probability mass on a few topics for each document.\nIn the previous model calculation, the alpha prior was automatically estimated to fit the data, achieving the highest overall probability for the model. However, this automatic estimate may not align with the results that an analyst desires. Depending on our analysis goals, we might prefer a more concentrated (peaky) or more evenly distributed set of topics in the model.\nNext, let us change the alpha prior to a lower value to observe how this adjustment affects the topic distributions in the model. To do this, we first extarct the alpha value of teh previous model.\n\n# see alpha from previous model\nattr(topicModel, \"alpha\")\n\n[1] 2.5\n\n\nThe alpha value of the previous model was attr(topicModel, \"alpha\"). So now, we set a much lower value (0.2) when we generate a new model.\n\n# generate new LDA model with low alpha\ntopicModel2 &lt;- LDA(DTM, K,\n  method = \"Gibbs\",\n  control = list(iter = 500, verbose = 25, alpha = 0.2)\n)\n\nK = 20; V = 4472; M = 8811\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\n# save results\ntmResult &lt;- posterior(topicModel2)\n# save theta values\ntheta &lt;- tmResult$topics\n# save beta values\nbeta &lt;- tmResult$terms\n# reset topic names\ntopicNames &lt;- apply(terms(topicModel, 5), 2, paste, collapse = \" \")\n\n\ntopicNames &lt;- apply(terms(topicModel, 5), 2, paste, collapse = \" \")\ntopicNames\n\n                                     Topic 1 \n               \"war forc militari navi armi\" \n                                     Topic 2 \n        \"nation countri peopl everi prosper\" \n                                     Topic 3 \n     \"govern power nation independ principl\" \n                                     Topic 4 \n        \"state constitut power repres union\" \n                                     Topic 5 \n            \"act congress last made session\" \n                                     Topic 6 \n                   \"may upon duti shall law\" \n                                     Topic 7 \n\"congress subject recommend consider attent\" \n                                     Topic 8 \n            \"new great line construct pacif\" \n                                     Topic 9 \n        \"state territori indian mexico unit\" \n                                    Topic 10 \n   \"year amount expenditur increas treasuri\" \n                                    Topic 11 \n       \"import duti countri increas product\" \n                                    Topic 12 \n                 \"land public work made use\" \n                                    Topic 13 \n      \"depart report offic servic secretari\" \n                                    Topic 14 \n       \"relat govern continu countri friend\" \n                                    Topic 15 \n              \"object can great may without\" \n                                    Topic 16 \n            \"citizen law case govern person\" \n                                    Topic 17 \n               \"can must peopl everi condit\" \n                                    Topic 18 \n             \"bank govern public money issu\" \n                                    Topic 19 \n            \"govern treati unit state claim\" \n                                    Topic 20 \n        \"state unit american commerc vessel\" \n\n\nNow visualize the topic distributions in the three documents again. What are the differences in the distribution structure?\n\n# get topic proportions form example documents\ntopicProportionExamples &lt;- theta[exampleIds, ]\ncolnames(topicProportionExamples) &lt;- topicNames\nvizDataFrame &lt;- reshape2::melt(\n  cbind(data.frame(topicProportionExamples),\n    document = factor(1:N)\n  ),\n  variable.name = \"topic\",\n  id.vars = \"document\"\n)\n# plot alpha distribution\nggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = \"proportion\") +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  coord_flip() +\n  facet_wrap(~document, ncol = N)\n\n\n\n\n\n\n\n\nThe figure above now shows that the documents are more clearly assigned to specific topics. The difference in the probability of a document belonging to a particular topic is much more distinct, indicating a stronger association between documents and their respective dominant topics.\nBy adjusting the alpha parameter to a lower value, we have concentrated the probability mass on fewer topics for each document. This change makes the topic distribution within documents less even and more peaked, resulting in documents being more distinctly associated with specific topics.\nThis adjustment can be particularly useful when analyzing data sets where we expect documents to focus on a few key themes rather than covering a broad range of topics. It allows for a clearer interpretation of the primary topics discussed in each document, enhancing the overall clarity and interpretability of the topic model.\n\n\nTopic ranking\nDetermining the defining topics within a collection is a crucial step in topic modeling, as it helps to organize and interpret the underlying themes effectively. There are several approaches to uncover these topics and arrange them in a meaningful order. Here, we present two different methods: Ordering Topics by Probability and Counting Primary Topic Appearances. These two approaches complement each other and, when used together, can provide a comprehensive understanding of the defining topics within a collection. By combining the probabilistic ranking with the frequency count of primary topics, we can achieve a more nuanced and accurate interpretation of the underlying themes in the data.\n\nApproach 1: Ordering Topics by Probability\nThis approach involves ranking topics based on their overall probability within the given collection. By examining the distribution of words across topics and documents, we can identify which topics are more dominant and relevant. This method helps to highlight the most significant themes within the data.\n\n# mean probabilities over all paragraphs\ntopicProportions &lt;- colSums(theta) / nDocs(DTM)\n# assign the topic names we created before\nnames(topicProportions) &lt;- topicNames\n# show summed proportions in decreased order\nsoP &lt;- sort(topicProportions, decreasing = TRUE)\n# inspect ordering\npaste(round(soP, 5), \":\", names(soP))\n\n [1] \"0.06721 : congress subject recommend consider attent\"\n [2] \"0.06448 : can must peopl everi condit\"               \n [3] \"0.06313 : year amount expenditur increas treasuri\"   \n [4] \"0.06098 : land public work made use\"                 \n [5] \"0.06076 : may upon duti shall law\"                   \n [6] \"0.06072 : bank govern public money issu\"             \n [7] \"0.05322 : relat govern continu countri friend\"       \n [8] \"0.05276 : import duti countri increas product\"       \n [9] \"0.05151 : citizen law case govern person\"            \n[10] \"0.05042 : state constitut power repres union\"        \n[11] \"0.04691 : state unit american commerc vessel\"        \n[12] \"0.04573 : state territori indian mexico unit\"        \n[13] \"0.04453 : war forc militari navi armi\"               \n[14] \"0.04368 : new great line construct pacif\"            \n[15] \"0.04282 : act congress last made session\"            \n[16] \"0.04263 : object can great may without\"              \n[17] \"0.04099 : govern treati unit state claim\"            \n[18] \"0.03731 : nation countri peopl everi prosper\"        \n[19] \"0.03646 : depart report offic servic secretari\"      \n[20] \"0.03374 : govern power nation independ principl\"     \n\n\nWe recognize some topics that are way more likely to occur in the corpus than others. These describe rather general thematic coherence. Other topics correspond more to specific contents.\n\n\nApproach 2: Counting Primary Topic Appearances\nAnother method is to count how often a topic appears as the primary topic within individual paragraphs or documents. This approach focuses on the frequency with which each topic takes precedence in the text, providing insight into which topics are most commonly addressed and therefore, potentially more important.\n\ncountsOfPrimaryTopics &lt;- rep(0, K)\nnames(countsOfPrimaryTopics) &lt;- topicNames\nfor (i in 1:nDocs(DTM)) {\n  topicsPerDoc &lt;- theta[i, ] # select topic distribution for document i\n  # get first element position from ordered list\n  primaryTopic &lt;- order(topicsPerDoc, decreasing = TRUE)[1]\n  countsOfPrimaryTopics[primaryTopic] &lt;- countsOfPrimaryTopics[primaryTopic] + 1\n}\n# sort by primary topic\nso &lt;- sort(countsOfPrimaryTopics, decreasing = TRUE)\n# show ordering\npaste(so, \":\", names(so))\n\n [1] \"708 : year amount expenditur increas treasuri\"   \n [2] \"684 : congress subject recommend consider attent\"\n [3] \"630 : may upon duti shall law\"                   \n [4] \"529 : bank govern public money issu\"             \n [5] \"521 : can must peopl everi condit\"               \n [6] \"491 : import duti countri increas product\"       \n [7] \"482 : relat govern continu countri friend\"       \n [8] \"472 : land public work made use\"                 \n [9] \"425 : new great line construct pacif\"            \n[10] \"424 : citizen law case govern person\"            \n[11] \"389 : state unit american commerc vessel\"        \n[12] \"382 : state territori indian mexico unit\"        \n[13] \"378 : war forc militari navi armi\"               \n[14] \"377 : state constitut power repres union\"        \n[15] \"368 : act congress last made session\"            \n[16] \"365 : object can great may without\"              \n[17] \"342 : nation countri peopl everi prosper\"        \n[18] \"315 : govern treati unit state claim\"            \n[19] \"273 : depart report offic servic secretari\"      \n[20] \"256 : govern power nation independ principl\"     \n\n\nSorting topics by the Rank-1 method highlights topics with specific thematic coherences, placing them at the upper ranks of the list. This sorting approach is valuable for several subsequent analysis steps:\n\nSemantic Interpretation of Topics: By examining topics ranked higher in the list, researchers can gain insights into the most salient and distinctive themes present in the collection. Understanding these topics facilitates their semantic interpretation and allows for deeper exploration of the underlying content.\nAnalysis of Time Series: Examining the temporal evolution of the most important topics over time can reveal trends, patterns, and shifts in discourse. Researchers can track how the prominence of certain topics fluctuates over different time periods, providing valuable context for understanding changes in the subject matter.\nFiltering Based on Sub-Topics: The sorted list of topics can serve as a basis for filtering the original collection to focus on specific sub-topics of interest. Researchers can selectively extract documents or passages related to particular themes, enabling targeted analysis and investigation of niche areas within the broader context.\n\nBy leveraging the Rank-1 method to sort topics, researchers can enhance their understanding of the thematic landscape within the collection and facilitate subsequent analytical tasks aimed at extracting meaningful insights and knowledge.\n\n\n\nFiltering documents\nThe inclusion of topic probabilities for each document or paragraph in a topic model enables its application for thematic filtering of a collection. This filtering process involves selecting only those documents that surpass a predetermined threshold of probability for specific topics. For instance, we may choose to retain documents containing a particular topic, such as topic ‘X’, with a probability exceeding 20 percent.\nIn the subsequent steps, we will implement this filtering approach to select documents based on their topical content and visualize the resulting document distribution over time. This analysis will provide insights into the prevalence and distribution of specific themes within the collection, allowing for a more targeted exploration of relevant topics across different temporal intervals.\n\n# selected by a term in the topic name (e.g. 'militari')\ntopicToFilter &lt;- grep(\"militari\", topicNames)[1]\ntopicThreshold &lt;- 0.2\nselectedDocumentIndexes &lt;- which(theta[, topicToFilter] &gt;= topicThreshold)\nfilteredCorpus &lt;- txts$text[selectedDocumentIndexes]\n# show length of filtered corpus\nlength(filteredCorpus)\n\n[1] 578\n\n# show first 5 paragraphs\nhead(filteredCorpus, 5)\n\n[1] \"The interests of the United States require that our intercourse with other\\nnations should be facilitated by such provisions as will enable me to\\nfulfill my duty in that respect in the manner which circumstances may\\nrender most conducive to the public good, and to this end that the\\ncompensation to be made to the persons who may be employed should,\\naccording to the nature of their appointments, be defined by law, and a\\ncompetent fund designated for defraying the expenses incident to the\\nconduct of foreign affairs.\"                                                                \n[2] \"Your attention seems to be not less due to that particular branch of our\\ntrade which belongs to the Mediterranean. So many circumstances unite in\\nrendering the present state of it distressful to us that you will not think\\nany deliberations misemployed which may lead to its relief and protection.\"                                                                                                                                                                                                                                                                                                 \n[3] \"The laws you have already passed for the establishment of a judiciary\\nsystem have opened the doors of justice to all descriptions of persons. You\\nwill consider in your wisdom whether improvements in that system may yet be\\nmade, and particularly whether an uniform process of execution on sentences\\nissuing from the Federal courts be not desirable through all the States.\"                                                                                                                                                                                                                      \n[4] \"The patronage of our commerce, of our merchants and sea men, has called for\\nthe appointment of consuls in foreign countries. It seems expedient to\\nregulate by law the exercise of that jurisdiction and those functions which\\nare permitted them, either by express convention or by a friendly\\nindulgence, in the places of their residence. The consular convention, too,\\nwith His Most Christian Majesty has stipulated in certain cases the aid of\\nthe national authority to his consuls established here. Some legislative\\nprovision is requisite to carry these stipulations into full effect.\"\n[5] \"\\\"In vain may we expect peace with the Indians on our frontiers so long as a\\nlawless set of unprincipled wretches can violate the rights of hospitality,\\nor infringe the most solemn treaties, without receiving the punishment they\\nso justly merit.\\\"\"                                                                                                                                                                                                                                                                                                                                                  \n\n\nOur filtered corpus contains 578 documents related to the topic 1 to at least 20 %.\n\n\nTopic proportions over time\nIn the final step, we offer a comprehensive overview of the topics present in the data across different time periods. To achieve this, we aggregate the mean topic proportions per decade for all State of the Union (SOTU) speeches. These aggregated topic proportions provide a distilled representation of the prevalent themes over time and can be effectively visualized, such as through a bar plot. This visualization offers valuable insights into the evolving discourse captured within the SOTU speeches, highlighting overarching trends and shifts in thematic emphasis across decades.\n\n# append decade information for aggregation\ntextdata$decade &lt;- paste0(substr(textdata$date, 0, 3), \"0\")\n# get mean topic proportions per decade\ntopic_proportion_per_decade &lt;- aggregate(theta, by = list(decade = textdata$decade), mean)\n# set topic names to aggregated columns\ncolnames(topic_proportion_per_decade)[2:(K + 1)] &lt;- topicNames\n# reshape data frame and generate plot\nreshape2::melt(topic_proportion_per_decade, id.vars = \"decade\") %&gt;%\n  ggplot(aes(x = decade, y = value, fill = variable)) +\n  geom_bar(stat = \"identity\") +\n  labs(y = \"Proportion\", x = \"Decade\") +\n  scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, \"RdBu\"))(20))) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThe visualization shows that topics around the relation between the federal government and the states as well as inner conflicts clearly dominate the first decades. Security issues and the economy are the most important topics of recent SOTU addresses."
  },
  {
    "objectID": "tutorials/topic/topic.html#footnotes",
    "href": "tutorials/topic/topic.html#footnotes",
    "title": "Topic Modeling with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/comp/comp.html#restart-your-computer-regularly",
    "href": "tutorials/comp/comp.html#restart-your-computer-regularly",
    "title": "Working with Computers",
    "section": "Restart Your Computer Regularly",
    "text": "Restart Your Computer Regularly\n\nWhile it can be convenient to use sleep mode repeatedly, you should shut down and restart your computer regularly. This is necessary to install already downloaded updates and allows your computer to clear out accumulated data. Rather than using sleep mode, shut down and restart your computer at least once per week so automatic software updates can be installed. In cases of high security risk, you may be forced to restart your computer immediately, as updates often close security gaps in specific programs or the operating system."
  },
  {
    "objectID": "tutorials/comp/comp.html#keep-your-computer-up-to-date",
    "href": "tutorials/comp/comp.html#keep-your-computer-up-to-date",
    "title": "Working with Computers",
    "section": "Keep Your Computer Up-to-Date",
    "text": "Keep Your Computer Up-to-Date\n\nKeeping your computer up-to-date is crucial for smooth performance and security. Outdated software lacks the latest security features and can put your computer at risk.\nAlthough updates can be annoying, they help close security gaps and improve the functionality of the software you use. Regularly check for updates to ensure your system is protected and runs efficiently. If you are unsure how to check for updates, a step-by-step tutorial is available here."
  },
  {
    "objectID": "tutorials/comp/comp.html#monitor-your-computers-performance",
    "href": "tutorials/comp/comp.html#monitor-your-computers-performance",
    "title": "Working with Computers",
    "section": "Monitor Your Computer’s Performance",
    "text": "Monitor Your Computer’s Performance\n\nRegularly monitor your computer’s performance to identify and resolve potential issues early. Task Manager (Windows) or Activity Monitor (Mac) can help you keep an eye on resource usage and terminate unnecessary processes that slow down your system."
  },
  {
    "objectID": "tutorials/comp/comp.html#manage-startup-programs",
    "href": "tutorials/comp/comp.html#manage-startup-programs",
    "title": "Working with Computers",
    "section": "Manage Startup Programs",
    "text": "Manage Startup Programs\n\nMany programs are set to start automatically when your computer boots up, which can significantly slow down startup times. Use Task Manager (Windows) or System Preferences (Mac) to manage and disable unnecessary startup programs."
  },
  {
    "objectID": "tutorials/comp/comp.html#clean-up-your-hard-drive",
    "href": "tutorials/comp/comp.html#clean-up-your-hard-drive",
    "title": "Working with Computers",
    "section": "Clean Up Your Hard Drive",
    "text": "Clean Up Your Hard Drive\n\nOver time, your computer accumulates unnecessary files that can slow down performance. Regularly clean up your hard drive by deleting temporary files, clearing cache, and removing unused programs. Tools like Disk Cleanup (Windows) or CleanMyMac (Mac) can automate this process."
  },
  {
    "objectID": "tutorials/comp/comp.html#use-anti-virus-software",
    "href": "tutorials/comp/comp.html#use-anti-virus-software",
    "title": "Working with Computers",
    "section": "Use Anti-Virus Software",
    "text": "Use Anti-Virus Software\nUsing reliable antivirus software is essential to protect your computer from malware, viruses, and other security threats. Here are some key points to consider:\n\nWhat Antivirus Software Does: Antivirus software scans your computer for malicious software (malware) and checks if any installed software behaves differently than expected if it were not infected. It also prevents, detects, and removes malicious software to protect your system.\nSymantec Endpoint Protection (SEP): This antivirus software is installed on all UQ computers. SEP protects your computer from malware and regularly checks for infections. To run a manual scan, click on the antivirus software icon in the lower right corner of your PC and follow the instructions. While SEP is not free for private use, UQ members can get a discount through a special deal with the manufacturer.\nFree Antivirus Alternatives: There are several free antivirus options available if you prefer not to pay for antivirus software. One popular choice is the free version of Avira. Avira not only protects your computer from malware but also includes features to improve your computer’s performance. Depending on the version, Avira can also implement these improvements automatically.\nMalwarebytes: Another excellent option is Malwarebytes, which also offers a free version. Malwarebytes is known for its extensive and up-to-date malware database, which allows it to detect even the most recent malware threats. It provides comprehensive protection and is a valuable addition to your cybersecurity toolkit.\nRegular Scans and Updates: Regularly running antivirus scans and keeping your software updated ensures that your computer remains protected against the latest threats. Most antivirus programs offer scheduled scans and automatic updates to keep your system secure without requiring manual intervention.\n\nBy using reliable antivirus software and keeping it updated, you can significantly reduce the risk of malware infections and maintain your computer’s performance and security."
  },
  {
    "objectID": "tutorials/comp/comp.html#optimise-power-settings",
    "href": "tutorials/comp/comp.html#optimise-power-settings",
    "title": "Working with Computers",
    "section": "Optimise Power Settings",
    "text": "Optimise Power Settings\n\nAdjust your computer’s power settings to balance performance and energy consumption. In Windows, use the Power Options settings to select a balanced or high-performance plan. On a Mac, access Energy Saver preferences to customise your power settings."
  },
  {
    "objectID": "tutorials/comp/comp.html#keep-your-computer-physically-clean",
    "href": "tutorials/comp/comp.html#keep-your-computer-physically-clean",
    "title": "Working with Computers",
    "section": "Keep Your Computer Physically Clean",
    "text": "Keep Your Computer Physically Clean\n\nDust and debris can accumulate inside your computer, causing it to overheat and slow down. Regularly clean your computer’s exterior and interior components, such as the keyboard, vents, and fans. Use compressed air to remove dust from hard-to-reach areas.\n\nBy following these tips, you can ensure your computer remains efficient, secure, and capable of handling your daily tasks. Proper maintenance and regular updates are key to extending the life and performance of your machine."
  },
  {
    "objectID": "tutorials/comp/comp.html#look-out-for-leeches-and-malware",
    "href": "tutorials/comp/comp.html#look-out-for-leeches-and-malware",
    "title": "Working with Computers",
    "section": "Look out for leeches and malware",
    "text": "Look out for leeches and malware\n\nWhen you download software, it is quite common that, in addition to the software you are looking for, other additional software will be downloaded and installed as a default. To avoid this, make sure to uncheck such options when installing the software that you want. This simply requires that you pay attention and read the options that you can check or uncheck during the installation when installing software."
  },
  {
    "objectID": "tutorials/comp/comp.html#use-anti-virus-software-1",
    "href": "tutorials/comp/comp.html#use-anti-virus-software-1",
    "title": "Working with Computers",
    "section": "Use anti-virus software",
    "text": "Use anti-virus software\n\nAntivirus software checks if any software on your computer has been reported as malware or if your software differs from what it should look like if it were not infected.\nSymantec Endpoint Protection (SEP) is an anti-virus software that is installed on all UQ computers. This software protects your computer from malware but also checks if your computer is already “infected”. Such checks are performed regularly but to run such a check manually, you can simply click on the antivirus software icon in the lower right corner of your PC and follow the instructions. While Symantec Endpoint Protection is not free and you have to pay a fee if you want to install it on a private PC, UQ has a deal with the manufacturer that gives UQ members a discount.\nThere are also free alternatives available such as the free version of Avira in case you do not want to pay for anti-virus software. Both the free and the commercial versions of Avira have the advantage that they also allow you to check if the performance of your PC can be improved (in addition to merely protecting your computer) and - depending on the version - they can also implement these improvements.\nAnother option that helps to detect software on your computer is Malwarebytes which also has a free version and which has the most up-to-date data base of malware which means that it is able to detect even very “fresh” malware."
  },
  {
    "objectID": "tutorials/comp/comp.html#no-data-on-desktop-or-c-drive",
    "href": "tutorials/comp/comp.html#no-data-on-desktop-or-c-drive",
    "title": "Working with Computers",
    "section": "No data on Desktop or C-drive",
    "text": "No data on Desktop or C-drive\n\nWhen you start your computer, different parts of the computer are started at different times with different priorities. The Desktop is always started with the highest priority which means that if you have a lot of stuff on your desktop, then the computer will load all that stuff once it kicks into action (which will cause it to work quite heavily at each start and also slow down quite dramatically).\nThis means that you should avoid storing data on any part of your system that is activated routinely. Rather, try to separate things that need to be loaded from things that only need to be loaded if they are actually used. For this reason, you should also avoid storing data on your C-drive. In fact, the C-drive should only contain programs as it is activated automatically at each start.\nYou can, for example, store all your projects on your D-Drive or, even better, on OneDrive, Google’s MyDrive, or in Dropbox where it is only started once you actively click and open a folder. If you use cloud-based storage options (OneDrive, Google’s MyDrive, or Dropbox) the files are also backed up automatically. However, you should not use either of these for sensitive data (sensitive data should be stored on your PC, an external hard drive and UQ’s RDM.)\nIf you want to have data accessible via your desktop, you can still do so by using links (also called short-cuts): place a link to your data (stored on another drive) on your desktop and you can load your data easily without it being activated at every start."
  },
  {
    "objectID": "tutorials/comp/comp.html#tidy-your-room",
    "href": "tutorials/comp/comp.html#tidy-your-room",
    "title": "Working with Computers",
    "section": "Tidy your room!",
    "text": "Tidy your room!\n\nJust like in real life, you should clean your computer. Full bins, for instance, will slow down your computer so you should empty it regularly. In addition, your computer will store and collect files when it is running. These files (temp files, cookies, etc.) also slow your computer down. As such files are redundant, they should be deleted regularly. You can remove such files manually using the msconfig prompt (you find a video tutorial on how to do this here). If you want to optimise your computer manually via the msconfig prompt, simply enter msconfig in the Window’s search box in the lower left corner of your PC (or search for it in the search box that opens when you click on the Window’s symbol). However, an easier way is to use software to help you with cleaning your computer."
  },
  {
    "objectID": "tutorials/comp/comp.html#software-to-clean-your-computer",
    "href": "tutorials/comp/comp.html#software-to-clean-your-computer",
    "title": "Working with Computers",
    "section": "Software to clean your computer",
    "text": "Software to clean your computer\n\nWhile UQ provides various software applications that keep your computer secure, it does not have any specific recommendations for software to keep your computer digitally clean.\nLuckily, there are numerous software applications that can help you with keeping your computer clean and up-to-date (you will find a list of software options for PCs here). We will only look at two options here (The two applications we will discuss are CCleaner and Avira) but a quick Google search will provide you with many different alternatives.\nThe most widely used program to clean your computer (if you have a PC rather than a Mac) is CCleaner. There are different versions of CCleaner but the free version suffices to delete any superfluous files and junk from your computer. When using this program, you should, however, be careful not to remove information that is useful. For instance, I like to keep all tabs of my current session in my browser and I therefore have to change the default options in CCleaner to avoid having to reopen all my tabs when I next open my browser. Here is a short video tutorial on how to use the CCleaner.\nIn addition, the free version of Avira also has a function that you can use to clean your computer. In fact, Avira will also inform you about any software that is out-of-date and other issues. Here is a short video tutorial on how to use the Avira for cleaning your computer and performing an anti-virus scan."
  },
  {
    "objectID": "tutorials/comp/comp.html#encryption-and-computer-security",
    "href": "tutorials/comp/comp.html#encryption-and-computer-security",
    "title": "Working with Computers",
    "section": "Encryption and Computer Security",
    "text": "Encryption and Computer Security\nEnsuring that your computer and network are secured means that you have far less a chance of a data breach or hack.\nAs some information is sensitive (especially when it comes to exams and attendance in courses), I encrypt folders and files that contain such information. To encrypt a file or folder I right-click on the file or folder and go to properties &gt; advanced, the I check encrypt contents to secure data and confirm the changes by checking OK. Then I back-up the encryption key where I check enable certificate privacy and create password and store the encrypted file in the original folder. You can find a step-by-step guide on how to encrypt files in this video.\nYou can also encrypt your entire computer. Information about how to do this can be found here and tips specific for\n\nMacs can be found here\nWindows 10 can be found here\nWindows 7 can be found here\n\nIt is also recommendable to use or create strong passwords. Here are some tips for creating secure passwords:\n\nDon’t just use one password - use a different password for every account\nUse a pass phrase - instead of a singular word, try a sequence of words for instance, DogsandCatsareawesome (Do not use this as your password)\nInclude numbers, capital letters and symbols\nThe longer the password, the better\nDon’t write passwords down\nTurn on two-factor authentication\n\nAn alternative is to use a password manager. Again, the Digital Essentials module has a lot of information about password management (password managers explained in detail in section 4).\nPassword managers provide a similar level of convenience to “Login with Facebook” but are much safer. Password managers create an encrypted database of all your usernames and passwords, that only you can access with a master password. This means you only need to remember one password to have access to all of your accounts. Most password managers will include the ability to generate secure passwords that you can use for new or existing account logins. Because you only need to remember one master password, you can generate and store complex passwords for your needs. This way, you are not relying on your memory and easy passwords to remember many different account login details.\nAlso, to find out if your email has been compromised, you can check this here\nRecently, UQ has adopted Multi-Factor Authentication which is more secure than simple authentication. You should use it when the option is available (Signing in with a password and an email to your account with a pin).\nAs a general tip, avoid unsecured wifi and, if it’s available, Eduroam is usually a better option than free wifi/cafe wifi.\nFor Beginners\n\nHave good strong passwords and encrypt your computer’s hard drive\n\nFor Intermediates\n\nGet set up on a password manager\n\nFor Advanced passwordists\n\nTry to ensure that your team/cluster is encrypted and practicing safe habits."
  },
  {
    "objectID": "tutorials/regex/regex.html#footnotes",
    "href": "tutorials/regex/regex.html#footnotes",
    "title": "Regular Expressions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#notes-on-loading-corpus-data-into-r",
    "href": "tutorials/corplingr/corplingr.html#notes-on-loading-corpus-data-into-r",
    "title": "Corpus Linguistics with R",
    "section": "Notes on loading corpus data into R",
    "text": "Notes on loading corpus data into R\nBefore we continue with the case studies, it is important to see how what we will be doing in the case studies differs from what you will most likely do if you conduct corpus-based research.\nIf a corpus is not accessed via a web application, corpora (collections of electronic language samples) typically - almost always - come in the form of text or audio files in a folder. That is, when using corpora, researchers typically download that corpus from a repository (for instance a website) or from some other storage media (for instance a CD, USB-stick, etc.). This means that non-web-based corpora are typically somewhere on a researcher’s computer where they can then be loaded into some software, e.g. AntConc.\nFor the present tutorial, however, we will simply load data that is available via the LADAL GitHub repository. Nonetheless, it is important to know how to load corpus data into R - which is why I will show this below.\nLoading corpus data into R consists of two steps:\n\ncreate a list of paths of the corpus files\nloop over these paths and load the data in the files identified by the paths.\n\nTo create a list of corpus files, you could use the code chunk below (the code chunk assumes that the corpus data is in a folder called Corpus in the data sub-folder of your Rproject folder).\n\ncorpusfiles &lt;- list.files(here::here(\"data/Corpus\"), # path to the corpus data\n  # file types you want to analyze, e.g. txt-files\n  pattern = \".*.txt\",\n  # full paths - not just the names of the files\n  full.names = T\n)\n\nYou can then use the sapply function to loop over the paths and load the data int R using e.g. the scan function as shown below. In addition to loading the file content, we also paste all the content together using the paste0 function and remove superfluous white spaces using the str_squish function from the stringr package.\n\ncorpus &lt;- sapply(corpusfiles, function(x) {\n  x &lt;- scan(x,\n    what = \"char\",\n    sep = \"\",\n    quote = \"\",\n    quiet = T,\n    skipNul = T\n  )\n  x &lt;- paste0(x, sep = \" \", collapse = \" \")\n  x &lt;- stringr::str_squish(x)\n})\n\nOnce you have loaded your data into R, you can then continue with processing and transforming the data according to your needs.\n\n\n\n\nNOTEThere are many different ways in which you can load text data into R. What I have shown above is just one way of doing this. However, I found this procedure to load text data very useful. In the case study which exemplifies how you can analyze sociolinguistic variation, we show how you can load text data in a very similar yet slightly different way (the tidyverse style of loading text data)."
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#preparation-and-session-set-up",
    "href": "tutorials/corplingr/corplingr.html#preparation-and-session-set-up",
    "title": "Corpus Linguistics with R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThe case studies shown below are based on R. Thus, you should already be familiar with R and RStudio. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. In addition, it is recommended to be familiar with regular expressions (this tutorial contains an overview of regular expressions that are used in this tutorial).\nYou should also have downloaded and installed R and RStudio. this tutorial contains links to detailed how-tos on how to download and install R and RStudio.\nFor this case study, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# installing packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"here\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"flextable\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"cfa\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# load packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(flextable)\nlibrary(quanteda)\nlibrary(cfa)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#using-childes-data",
    "href": "tutorials/corplingr/corplingr.html#using-childes-data",
    "title": "Corpus Linguistics with R",
    "section": "Using CHILDES data",
    "text": "Using CHILDES data\nThe Child Language Data Exchange System (CHILDES) (MacWhinney) is a browsable data base which provides corpora consisting of transcripts of conversations with children. CHILDES was established in 1984 by Brian MacWhinney and Catherine Snow and it represents the central repository for data of first language acquisition. Its earliest transcripts date from the 1960s, and it now has contents (transcripts, audio, and video) in 26 languages from 130 different corpora, all of which are publicly available worldwide.\n\nCHILDES is the child language part of the TalkBank system which is a system for sharing and studying conversational interactions.\n\nTo download corpora from CHILDES:\n\nGo to the CHILDES website - the landing page looks like the website shown in the image to the right. For the present tutorial, the only relevant part of that website is section labeled Database which contains the links to the different CHILDS corpora that you can download fro free.\nIn the section called Database click on Index to Corpora which will take you to a table which contains links to different kinds of corpora - all containing transcripts of children’s speech. The types of corpora available cover many different language, including monolingual and bilingual children, children with speech disorders, transcripts of frog stories, etc.\n\n\n\nTo download a corpus, click on one of the section, e.g. on Eng-NA which stands for English recorded in North America (but you can, of course, also download other CHILDES corpora), and then scroll down to the corpus you are interested in and click on it, e.g. scroll down to and click on HSLLD.\n\n\n\nClick on Download transcripts and then download and store the zip-folder somewhere on your computer.\nNext, unzip the zip-file and store the resulting unzipped corpus in the data sub-folder in your Rproject folder.\n\nOnce you have downloaded, stored the data on your computer, and unzipped it, you are good to go and you can now access and analyze data from CHILDES."
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#hslld-corpus",
    "href": "tutorials/corplingr/corplingr.html#hslld-corpus",
    "title": "Corpus Linguistics with R",
    "section": "HSLLD corpus",
    "text": "HSLLD corpus\nFor this case study, we will use data from the Home-School Study of Language and Literacy Development corpus (HSLLD) which part of the CHILDES data base. The Home-School Study of Language and Literacy Development began in 1987 under the leadership of Patton Tabors and with Catherine E. Snow and David K. Dickinson as primary investigators. The original purpose of the HSLLD was to investigate the social prerequisites to literacy success.\nThe initial number of participants was 83 American English speaking, racially diverse, preschool age children from low-income families growing up in or around Boston, Massachusetts. Seventy-four of these children were still participating at age 5. The sample consists of 38 girls and 36 boys. Forty-seven children were Caucasian, 16 were African American, six were of Hispanic origin, and five were biracial.\nChildren were visited once a year in their home from age 3 – 5 and then again when they were in 2nd and 4th grade. Each visit lasted between one and three hours. Home visits consisted of a number of different tasks depending on the year. An outline of the different tasks for each visit is presented below.\nActivities during Home Visit 1 (HV1): Book reading (BR), Elicited report (ER), Mealtime (MT), Toy Play (TP)\nActivities during Home Visit 2 (HV2): Book reading (BR), Elicited report (ER), Mealtime (MT), Toy Play (TP)\nActivities during Home Visit 3 (HV3): Book reading (BR), Elicited report (ER), Experimental task (ET), Mealtime (MT), Reading (RE), Toy play (TP)\nActivities during Home Visit 5 (HV5): Book reading (BR), Letter writing (LW), Mealtime (MT)\nActivities during Home Visit 7 (HV7): Experimental task (ET), Letter writing (LW), Mother definitions (MD), Mealtime (MT)"
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#data-processing",
    "href": "tutorials/corplingr/corplingr.html#data-processing",
    "title": "Corpus Linguistics with R",
    "section": "Data processing",
    "text": "Data processing\nWe now load the data and inspect its structure using the str function - as the HSLLD has many files, we will only check the first 3.\n\nhslld &lt;- readRDS(\"tutorials/corplingr/data/hslld.rda\", \"rb\")\n\n# If you have already downloaded the data file to data/hslld.rda you can also load it locally\n# hslld &lt;- readRDS(here::here(\"data/hslld.rda\"))\n\n# inspect\nstr(hslld[1:3])\n\n\n\nList of 3\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha   : chr [1:1931] \"@UTF8\" \"@PID:\" \"11312/c-00034768-1\" \"@Begin\" ...\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1pt2.cha: chr [1:21] \"@UTF8\" \"@PID:\" \"11312/a-00012630-1\" \"@Begin\" ...\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/admbr1.cha   : chr [1:1369] \"@UTF8\" \"@PID:\" \"11312/c-00034769-1\" \"@Begin\" ...\n\n\nWe continue and split the data up into files. The sapply function loops each element in the hslld object and performs specified actions on the (here, loading the content via the scan function, getting rid of white spaces and splitting the files when it finds the following sequences *ABC1: or %ABC:)\n\n# create version of corpus fit for concordancing\ncorpus &lt;- sapply(hslld, function(x) {\n  # clean data\n  x &lt;- stringr::str_trim(x, side = \"both\") # remove superfluous white spaces at the edges of strings\n  x &lt;- stringr::str_squish(x) # remove superfluous white spaces within strings\n  x &lt;- paste0(x, collapse = \" \") # paste all utterances in a file together\n  # split files into individual utterances\n  x &lt;- strsplit(gsub(\"([%|*][a-z|A-Z]{2,4}[0-9]{0,1}:)\", \"~~~\\\\1\", x), \"~~~\")\n})\n# inspect results\nstr(corpus[1:3])\n\nList of 3\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha   : chr [1:793] \"@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Invest\"| __truncated__ \"*MOT: okay her favorite books (.) I don't read the whole stories they're  too long . \" \"%mor: co|okay det:poss|her adj|favorite n|book-PL pro:sub|I  mod|do~neg|not v|read&ZERO det:art|the adj|whole n\"| __truncated__ \"%gra: 1|4|COM 2|4|DET 3|4|MOD 4|0|INCROOT 5|8|SUBJ 6|8|AUX 7|6|NEG 8|4|CMOD  9|11|DET 10|11|MOD 11|8|OBJ 12|13|\"| __truncated__ ...\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1pt2.cha: chr \"@UTF8 @PID: 11312/a-00012630-1 @Begin @Languages: eng @Participants: CHI Target_Child, INV Investigator @ID: en\"| __truncated__\n $ D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/admbr1.cha   : chr [1:517] \"@UTF8 @PID: 11312/c-00034769-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , UNC Uncle \"| __truncated__ \"*MOT: gonna read this one too which one do you want me to read first ? \" \"%mor: part|go-PRESP~inf|to v|read&ZERO det:dem|this pro:indef|one adv|too  pro:rel|which det:num|one v|do pro:p\"| __truncated__ \"%gra: 1|0|INCROOT 2|3|INF 3|1|COMP 4|5|DET 5|3|OBJ 6|3|JCT 7|9|LINK 8|9|QUANT  9|1|CJCT 10|11|SUBJ 11|9|COMP 12\"| __truncated__ ...\n\n\nWe have now loaded the files into R, but the format is not yet structured in a way that we can use it - remember: we want the data to be in a tabular format.\nExtract file information\nNow, we extract information about the recording, e.g., the participants, the age of the child, the date of the recording etc. For this, we extract the first element of each file (because this first element contains all the relevant information bout the recording). To do this, we again use the sapply function (which is our looping function) and then tell R that it shall only retain the first element of each element (x &lt;- x[1]).\n\n# extract file info for each file\nfileinfo &lt;- sapply(corpus, function(x) {\n  # extract first element of each corpus file because this contains the file info\n  x &lt;- x[1]\n})\n# inspect\nfileinfo[1:3]\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/acebr1.cha \n\"@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD \" \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/acebr1pt2.cha \n                                                                                                                                                                                                                                                                  \"@UTF8 @PID: 11312/a-00012630-1 @Begin @Languages: eng @Participants: CHI Target_Child, INV Investigator @ID: eng|HSLLD|CHI|||||Target_Child||| @ID: eng|HSLLD|INV|||||Investigator||| @Media: acebr1pt2, audio, notrans @Comment: This is a dummy file to permit playback from the TalkBank  browser. Please use the slider at the left to control media  playback. @End\" \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/admbr1.cha \n                                                         \"@UTF8 @PID: 11312/c-00034769-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , UNC Uncle Relative, EX1  Investigator @ID: eng|HSLLD|CHI|4;01.09|male|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|12+|| @ID: eng|HSLLD|UNC|||||Relative||| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 05-MAY-1985 @Media: admbr1, audio, unlinked @Comment: SES of Mot is lower @Date: 14-JUN-1989 @Location: Boston , MA USA @Situation: Home Visit 1 @Activities: Book Reading 1 The Very Hungry Caterpillar @Bg: bookreading @Types: long, book, TD \" \n\n\nNow, we have one element for each file that contains all the relevant information about the file, like when the recording took place, how old the target child was, how was present during the recording etc.\nExtract file content\nNow, we extract the raw content from which we will extract the speaker, the utterance, the pos-tagged utterance, and any comments.Here, we loop over the corpus object with the sapply function and we remove the first element in each list (and we retain the second to last element of each element (x &lt;- x[2:length(x)])), then we paste everything else together using the paste0 function and then, we split the whole conversation into utterances that start with a speaker id (e.g. *MOT:). The latter is done by the sequence stringr::str_split(stringr::str_replace_all(x, \"(\\\\*[A-Z])\", \"~~~\\\\1\"), \"~~~\").\n\ncontent &lt;- sapply(corpus, function(x) {\n  x &lt;- x[2:length(x)]\n  x &lt;- paste0(x, collapse = \" \")\n  x &lt;- stringr::str_split(stringr::str_replace_all(x, \"(\\\\*[A-Z])\", \"~~~\\\\1\"), \"~~~\")\n})\n# inspect data\ncontent[[1]][1:6]\n\n[1] \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                           \n[2] \"*MOT: okay her favorite books (.) I don't read the whole stories they're  too long .  %mor: co|okay det:poss|her adj|favorite n|book-PL pro:sub|I  mod|do~neg|not v|read&ZERO det:art|the adj|whole n|story-PL  pro:sub|they~cop|be&PRES adv|too adj|long .  %gra: 1|4|COM 2|4|DET 3|4|MOD 4|0|INCROOT 5|8|SUBJ 6|8|AUX 7|6|NEG 8|4|CMOD  9|11|DET 10|11|MOD 11|8|OBJ 12|13|SUBJ 13|11|CMOD 14|15|JCT 15|13|PRED  16|4|PUNCT  %tim: 37:16  \"\n[3] \"*MOT: I give my own version .  %mor: pro:sub|I v|give det:poss|my adj|own n|version .  %gra: 1|2|SUBJ 2|0|ROOT 3|5|DET 4|5|MOD 5|2|OBJ 6|2|PUNCT  \"                                                                                                                                                                                                                                                                                         \n[4] \"*EX1: okay .  %mor: co|okay .  %gra: 1|0|INCROOT 2|1|PUNCT  \"                                                                                                                                                                                                                                                                                                                                                                               \n[5] \"*EX1: that's fine .  %mor: pro:dem|that~cop|be&3S adj|fine .  %gra: 1|2|SUBJ 2|0|ROOT 3|2|PRED 4|2|PUNCT  \"                                                                                                                                                                                                                                                                                                                                 \n[6] \"*EX1: whatever you usually do .  %mor: pro:int|whatever pro:per|you adv|usual&dadj-LY v|do .  %gra: 1|4|LINK 2|4|SUBJ 3|4|JCT 4|0|ROOT 5|4|PUNCT  \"                                                                                                                                                                                                                                                                                         \n\n\nThe data now consists of utterances but also the pos-tagged utterances and any comments. However, we use this form of the data to extract the clean utterances, the pos-tagged utterances and the comments and store them in different columns.\nExtract information\nNow, we extract how many elements (or utterances) there are in each file by looping over the content object and extracting the number of elements within each element of the content object by using the lenght function.\n\nelements &lt;- sapply(content, function(x) {\n  x &lt;- length(x)\n})\n# inspect\nhead(elements)\n\n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/acebr1.cha \n                                                                261 \nD:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/acebr1pt2.cha \n                                                                  1 \n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/admbr1.cha \n                                                                178 \n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/aimbr1.cha \n                                                                346 \n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/alibr1.cha \n                                                                435 \n   D:\\\\Uni\\\\UiT\\\\Workshops\\\\RCHILDES\\\\data\\\\HSLLD/HV1/BR/allbr1.cha \n                                                                135 \n\n\nGenerate table\nWe now have the file names, the metadata for each file, and the content of each file (that is split into utterances). We use this information to generate a first table which holds the file name in one column, the file information in one column, and the raw file content in another column. To combine these three pieces of information though, we need to repeat the file names and the file information as often as there are utterances in each file. We perform this repetition using the rep function. Once we have as many file names and file information as there are utterances in each file, we can combine these three vectors into a table using the data.frame function.\n\nfiles &lt;- rep(names(elements), elements)\nfileinfo &lt;- rep(fileinfo, elements)\nrawcontent &lt;- as.vector(unlist(content))\nchitb &lt;- data.frame(\n  1:length(rawcontent),\n  files,\n  fileinfo,\n  rawcontent\n)\n\nThe table in its current form is shown below. We can see that the table has three columns: the first column holds the path to each file, the second contains the file information, and the third the utterances.\n\n\nX1.length.rawcontent.filesfileinforawcontent1D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD 2D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *MOT: okay her favorite books (.) I don't read the whole stories they're  too long .  %mor: co|okay det:poss|her adj|favorite n|book-PL pro:sub|I  mod|do~neg|not v|read&ZERO det:art|the adj|whole n|story-PL  pro:sub|they~cop|be&PRES adv|too adj|long .  %gra: 1|4|COM 2|4|DET 3|4|MOD 4|0|INCROOT 5|8|SUBJ 6|8|AUX 7|6|NEG 8|4|CMOD  9|11|DET 10|11|MOD 11|8|OBJ 12|13|SUBJ 13|11|CMOD 14|15|JCT 15|13|PRED  16|4|PUNCT  %tim: 37:16  3D:\\Uni\\UiT\\Workshops\\RCHILDES\\data\\HSLLD/HV1/BR/acebr1.cha@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *MOT: I give my own version .  %mor: pro:sub|I v|give det:poss|my adj|own n|version .  %gra: 1|2|SUBJ 2|0|ROOT 3|5|DET 4|5|MOD 5|2|OBJ 6|2|PUNCT  \n\n\nProcess table\nWe can now use the information in the two last columns to extract specific pieces of information from the data (which we will store in additional columns that we add to the table). But first, we rename the id column (which is simply an index of each utterance) using the rename function from the dplyr package. Then, we clean the file name column (called files) so that it only contains the name of the file, so we remove the rest of the path information that we do not need anymore. We do this by using the mutate function from the dplyr package (which changes columns or creates new columns). Within the mutate function, we use the gsub function which substitutes something with something else: here the full path is replaced with on that part of the path that contains the file name. The gsub function has the following form\n gsub(*look for pattern*, *replacement of the pattern*, object) \nThis means that the gsub function needs an object and in that object it looks for a pattern and then replaces instances f that pattern with something.\nIn our case, that what we look for is the file name which is located between the symbol / and the file ending (.cha). So, we extract everything that comes between a / and a .cha in the path and keep that what is between the / and a .cha in R’s memory (this is done by placing something in round brackets in a regular expression). Then, we paste that what we have extracted back (and which is stored in memory) by using the \\\\1 which grabs the first element that is in memory and puts it into the replace with part of the gsub function.\n\nhslld &lt;- chitb %&gt;%\n  # rename id column\n  dplyr::rename(id = colnames(chitb)[1]) %&gt;%\n  # clean file names\n  dplyr::mutate(files = gsub(\".*/(.*?).cha\", \"\\\\1\", files))\n\nLet’s have a look at the data.\n\n\nidfilesfileinforawcontent1acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD 2acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *MOT: okay her favorite books (.) I don't read the whole stories they're  too long .  %mor: co|okay det:poss|her adj|favorite n|book-PL pro:sub|I  mod|do~neg|not v|read&ZERO det:art|the adj|whole n|story-PL  pro:sub|they~cop|be&PRES adv|too adj|long .  %gra: 1|4|COM 2|4|DET 3|4|MOD 4|0|INCROOT 5|8|SUBJ 6|8|AUX 7|6|NEG 8|4|CMOD  9|11|DET 10|11|MOD 11|8|OBJ 12|13|SUBJ 13|11|CMOD 14|15|JCT 15|13|PRED  16|4|PUNCT  %tim: 37:16  3acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *MOT: I give my own version .  %mor: pro:sub|I v|give det:poss|my adj|own n|version .  %gra: 1|2|SUBJ 2|0|ROOT 3|5|DET 4|5|MOD 5|2|OBJ 6|2|PUNCT  4acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *EX1: okay .  %mor: co|okay .  %gra: 1|0|INCROOT 2|1|PUNCT  5acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *EX1: that's fine .  %mor: pro:dem|that~cop|be&3S adj|fine .  %gra: 1|2|SUBJ 2|0|ROOT 3|2|PRED 4|2|PUNCT  6acebr1@UTF8 @PID: 11312/c-00034768-1 @Begin @Languages: eng @Participants: CHI Target_Child , MOT Mother , EX1 Investigator @ID: eng|HSLLD|CHI|3;07.08|female|||Target_Child||| @ID: eng|HSLLD|MOT||female|||Mother|14|| @ID: eng|HSLLD|EX1|||||Investigator||| @Birth of CHI: 11-DEC-1984 @Media: acebr1, audio, unlinked @Comment: SES of Mot is lower @Date: 19-JUL-1988 @Location: Boston , MA USA @Situation: Home Visit 1; took place in kitchen; Chi and Mot sat next  to each other in separate chairs , television is running;  7pm. @Activities: Book Reading 1 (The Very Hungry Caterpillar) @Bg: bookreading @Types: long, book, TD *EX1: whatever you usually do .  %mor: pro:int|whatever pro:per|you adv|usual&dadj-LY v|do .  %gra: 1|4|LINK 2|4|SUBJ 3|4|JCT 4|0|ROOT 5|4|PUNCT  \n\n\nWe now continue in the same manner (by remove what is before what interests us and what comes after) and thereby extract pieces of information that we store in new columns.\nCreating a speaker column. We create a new column called speaker using the mutate function from the dplyr package. Then, we use the str_replace_all function from the stringr package to remove everything that comes after a :. Everything that comes after can be defined by a regular expression - in this case the sequence .*. The . is a regular expression that stands for any symbol - be it a letter, or a number, or any punctuation symbol, or a white space. The * is a numerating regular expression that tells R how many times the regular expression (the .) is repeated - in our case, the * stands for zero to an infinite number. So the sequence .* stands for any symbol, repeated zero to an infinite number of times. In combination, the sequence :.* stands for *look for a colon and anything that comes after. And because we have put this into the str_replace_all function, the colon and everything that comes after is removed.\n\nhslld &lt;- hslld %&gt;%\n  dplyr::mutate(\n    speaker = stringr::str_remove_all(rawcontent, \":.*\"),\n    speaker = stringr::str_remove_all(speaker, \"\\\\W\")\n  )\n\nIn the following, we will create many different columns, but we will always follow the same scheme: generate a new column using the mutate function from the dplyr package and then remove stuff that we do not need by using the str_remove_all function from the stringr package or just the gsub function - which is a simple replacement function. We can also use str_squish to get rid of superfluous white spaces. We will always remove sequences that are defined by a string (a sequence of characters and a regular expression consisting of the regular expression that determines what type of symbol R is supposed to look for and a numerator which tells R how many times that symbol can occur). For example, %mor:.* tells R to look for the sequence %mor: and any symbol, repeated between zero and an infinite number of times, that comes after the %mor: sequence. As this is put into the str_replace_all function and applied to the rawcontent file, it will replace everything that comes after %mor: and the sequence %mor: itself.\nCreating an utterance column.\n\nhslld &lt;- hslld %&gt;%\n  dplyr::mutate(\n    utterance = stringr::str_remove_all(rawcontent, \"%mor:.*\"),\n    utterance = stringr::str_remove_all(utterance, \"%.*\"),\n    utterance = stringr::str_remove_all(utterance, \"\\\\*\\\\w{2,6}:\"),\n    utterance = stringr::str_squish(utterance)\n  )\n\nCreating a column with the pos-tagged utterances.\n\nhslld &lt;- hslld %&gt;%\n  dplyr::mutate(\n    postag = stringr::str_remove_all(rawcontent, \".*%mor:\"),\n    postag = stringr::str_remove_all(postag, \"%.*\"),\n    postag = stringr::str_remove_all(postag, \"\\\\*\\\\w{2,6}:\"),\n    postag = stringr::str_squish(postag)\n  )\n\nCreating a column with comments. In the following chunk, we use the ? in combination with .*. In this case, the ? does not mean the literal symbol ? but it tells R to be what is called non-greedy which means that R will look for something until the first occurrence of something. So the sequence .*?% tells R to look for any symbol repeated between zero and an infinite number of times until the first occurrence(!) of the symbol %. If we did not include the ?, R would look until the last (not the first) occurrence of %.\n\nhslld &lt;- hslld %&gt;%\n  dplyr::mutate(\n    comment = stringr::str_remove_all(rawcontent, \".*%mor:\"),\n    comment = stringr::str_remove(comment, \".*?%\"),\n    comment = stringr::str_remove_all(comment, \".*|.*\"),\n    comment = stringr::str_squish(comment)\n  )\n\nCreating a column with the participants that were present during the recording.\n\nhslld &lt;- hslld %&gt;%\n  dplyr::mutate(participants = gsub(\".*@Participants:(.*?)@.*\", \"\\\\1\", fileinfo))\n\nCreating a column with the age of the target child. In the following, the sequence [0-9]{1,3} means look for any sequence containing between 1 and 3 (this is defined by the {1,3}) numbers (the numbers are defined by the [0-9] part). Also, when we put \\\\ before something, then we tell R that this refers to the actual symbol and not its meaning as a regular expression. For example, the symbol | is a regular expression that means or as in You can paint my walls blue OR orange, but if we put \\\\ before |, we tell R that we really mean the symbol |.\n\nhslld &lt;- hslld %&gt;%\n  dplyr::mutate(age_targetchild = gsub(\".*\\\\|([0-9]{1,3};[0-9]{1,3}\\\\.[0-9]{1,3})\\\\|.*\", \"\\\\1\", fileinfo))\n\nCreating a column with the age of the target child in years.\n\nhslld &lt;- hslld %&gt;%\n  dplyr::mutate(age_years_targetchild = stringr::str_remove_all(age_targetchild, \";.*\"))\n\nCreating a column with the gender of the target child.\n\nhslld &lt;- hslld %&gt;%\n  dplyr::mutate(gender_targetchild = gsub(\".*\\\\|([female]{4,6})\\\\|.*\", \"\\\\1\", fileinfo))\n\nCreating columns with the date-of-birth of the target child, more comments, and the date of the recording.\n\nhslld &lt;- hslld %&gt;%\n  # create dob_targetchild column\n  dplyr::mutate(dob_targetchild = gsub(\".*@Birth of CHI:(.*?)@.*\", \"\\\\1\", fileinfo)) %&gt;%\n  # create comment_file column\n  dplyr::mutate(comment_file = gsub(\".*@Comment: (.*?)@.*\", \"\\\\1\", fileinfo)) %&gt;%\n  # create date column\n  dplyr::mutate(date = gsub(\".*@Date: (.*?)@.*\", \"\\\\1\", fileinfo))\n\nCreating columns with the location where the recording took place and the situation type of the recording.\n\nhslld &lt;- hslld %&gt;%\n  # create location column,\n  dplyr::mutate(location = gsub(\".*@Location: (.*?)@.*\", \"\\\\1\", fileinfo)) %&gt;%\n  # create situation column\n  dplyr::mutate(situation = gsub(\".*@Situation: (.*?)@.*\", \"\\\\1\", fileinfo))\n\nCreating columns with the activity during the recording and the home-visit number.\n\nhslld &lt;- hslld %&gt;%\n  # create homevisit_activity column\n  dplyr::mutate(homevisit_activity = stringr::str_remove_all(situation, \";.*\")) %&gt;%\n  # create activity column\n  dplyr::mutate(activity = gsub(\".*@Activities: (.*?)@.*\", \"\\\\1\", fileinfo)) %&gt;%\n  # create homevisit column\n  dplyr::mutate(homevisit = stringr::str_sub(files, 4, 6))\n\nCreating a column with the number of words in each utterance.\n\nhslld &lt;- hslld %&gt;%\n  # create words column\n  dplyr::mutate(\n    words = stringr::str_replace_all(utterance, \"\\\\W\", \" \"),\n    words = stringr::str_squish(words),\n    words = stringr::str_count(words, \"\\\\w+\")\n  )\n\nCleaning the data: removing rows without speakers, rows where the age of the target child was incorrect, and removing superfluous columns.\n\nhslld &lt;- hslld %&gt;%\n  # remove rows without speakers (contain only metadata)\n  dplyr::filter(speaker != \"\") %&gt;%\n  # remove rows with incorrect age of child\n  dplyr::filter(nchar(age_years_targetchild) &lt; 5) %&gt;%\n  # remove superfluous columns\n  dplyr::select(-fileinfo, -rawcontent, -situation) %&gt;%\n  # create words column\n  dplyr::mutate(\n    collection = \"EngNA\",\n    corpus = \"HSLLD\"\n  ) %&gt;%\n  dplyr::rename(transcript_id = files) %&gt;%\n  # code activity\n  dplyr::mutate(visit = substr(transcript_id, 6, 6)) %&gt;%\n  dplyr::mutate(\n    situation = substr(transcript_id, 4, 5),\n    situation = str_replace_all(situation, \"br\", \"Book reading\"),\n    situation = str_replace_all(situation, \"er\", \"Elicited report\"),\n    situation = str_replace_all(situation, \"et\", \"Experimental task\"),\n    situation = str_replace_all(situation, \"lw\", \"Letter writing\"),\n    situation = str_replace_all(situation, \"md\", \"Mother defined situation\"),\n    situation = str_replace_all(situation, \"mt\", \"Meal time\"),\n    situation = str_replace_all(situation, \"re\", \"Reading\"),\n    situation = str_replace_all(situation, \"tp\", \"Toy play\")\n  )\n\n\n\nidtranscript_idspeakerutterancepostagcommentparticipantsage_targetchildage_years_targetchildgender_targetchilddob_targetchildcomment_filedatelocationhomevisit_activityactivityhomevisitwordscollectioncorpusvisitsituation2acebr1MOTokay her favorite books (.) I don't read the whole stories they're too long .co|okay det:poss|her adj|favorite n|book-PL pro:sub|I mod|do~neg|not v|read&ZERO det:art|the adj|whole n|story-PL pro:sub|they~cop|be&PRES adv|too adj|long . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br115EngNAHSLLD1Book Readingading3acebr1MOTI give my own version .pro:sub|I v|give det:poss|my adj|own n|version . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br15EngNAHSLLD1Book Readingading4acebr1EX1okay .co|okay . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br11EngNAHSLLD1Book Readingading5acebr1EX1that's fine .pro:dem|that~cop|be&3S adj|fine . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br13EngNAHSLLD1Book Readingading6acebr1EX1whatever you usually do .pro:int|whatever pro:per|you adv|usual&dadj-LY v|do . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br14EngNAHSLLD1Book Readingading7acebr1EX1just read to her as if we weren't even here .adv|just v|read&ZERO prep|to pro:obj|her prep|as conj|if pro:sub|we cop|be&PAST~neg|not adv|even adv|here . CHI Target_Child , MOT Mother , EX1 Investigator 3;07.083female 11-DEC-1984 SES of Mot is lower 19-JUL-1988 Boston , MA USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br111EngNAHSLLD1Book Readingading\n\n\nNow that we have the data in a format that we can use, we can use this table to continue with our case studies."
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#case-study-1-use-of-no",
    "href": "tutorials/corplingr/corplingr.html#case-study-1-use-of-no",
    "title": "Corpus Linguistics with R",
    "section": "Case study 1: Use of NO",
    "text": "Case study 1: Use of NO\nTo extract all instances of a single word, in this example the word no, that are uttered by a specific interlocutor we filter by speaker and define that we only want rows where the speaker is equal to CHI (target child).\n\nno &lt;- hslld %&gt;%\n  dplyr::filter(speaker == \"CHI\") %&gt;%\n  dplyr::filter(stringr::str_detect(utterance, \"\\\\b[Nn][Oo]\\\\b\"))\n\n\n\nidtranscript_idspeakerutterancepostagcommentparticipantsage_targetchildage_years_targetchildgender_targetchilddob_targetchildcomment_filedatelocationhomevisit_activityactivityhomevisitwordscollectioncorpusvisitsituation267admbr1CHIno you can't read it .co|no pro:per|you mod|can~neg|not v|read&ZERO pro:per|it . CHI Target_Child , MOT Mother , UNC Uncle Relative, EX1  Investigator 4;01.094female 05-MAY-1985 SES of Mot is lower 14-JUN-1989 Boston , MA USA Home Visit 1 Book Reading 1 The Very Hungry Caterpillar br16EngNAHSLLD1Book Readingading409admbr1CHIno .co|no . CHI Target_Child , MOT Mother , UNC Uncle Relative, EX1  Investigator 4;01.094female 05-MAY-1985 SES of Mot is lower 14-JUN-1989 Boston , MA USA Home Visit 1 Book Reading 1 The Very Hungry Caterpillar br11EngNAHSLLD1Book Readingading411admbr1CHIno no I'm loco@s:spa &=laughingly .co|no co|no pro:sub|I~cop|be&1S L2|loco . CHI Target_Child , MOT Mother , UNC Uncle Relative, EX1  Investigator 4;01.094female 05-MAY-1985 SES of Mot is lower 14-JUN-1989 Boston , MA USA Home Visit 1 Book Reading 1 The Very Hungry Caterpillar br18EngNAHSLLD1Book Readingading552aimbr1CHIno !co|no ! CHI Target_Child , MOT Mother , BRO Robbie Brother , BAB  Baby Brother , INV Investigator 3;10.163female 22-JUL-1984 SES of Mot is lower 07-JUN-1988 Boston , MA , USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br11EngNAHSLLD1Book Readingading554aimbr1CHIno [&lt;] !co|no ! CHI Target_Child , MOT Mother , BRO Robbie Brother , BAB  Baby Brother , INV Investigator 3;10.163female 22-JUL-1984 SES of Mot is lower 07-JUN-1988 Boston , MA , USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br11EngNAHSLLD1Book Readingading750aimbr1CHIno .co|no . CHI Target_Child , MOT Mother , BRO Robbie Brother , BAB  Baby Brother , INV Investigator 3;10.163female 22-JUL-1984 SES of Mot is lower 07-JUN-1988 Boston , MA , USA Home Visit 1Book Reading 1 (The Very Hungry Caterpillar) br11EngNAHSLLD1Book Readingading\n\n\nWe summarize the results in a table.\n\nno_no &lt;- no %&gt;%\n  dplyr::group_by(transcript_id, gender_targetchild, age_years_targetchild) %&gt;%\n  dplyr::summarise(nos = nrow(.))\nhead(no_no)\n\n# A tibble: 6 × 4\n# Groups:   transcript_id, gender_targetchild [6]\n  transcript_id gender_targetchild age_years_targetchild   nos\n  &lt;chr&gt;         &lt;chr&gt;              &lt;chr&gt;                 &lt;int&gt;\n1 aceet7        male               9                      4420\n2 acemt7        male               10                     4420\n3 acetp2        female             4                      4420\n4 admbr1        female             4                      4420\n5 admbr2        female             4                      4420\n6 admbr3        male               5                      4420\n\n\nWe can also extract the number of words uttered by children to check if the use of no shows a relative increase or decrease over time.\n\nno_words &lt;- hslld %&gt;%\n  dplyr::filter(speaker == \"CHI\") %&gt;%\n  dplyr::group_by(transcript_id, gender_targetchild, age_years_targetchild) %&gt;%\n  dplyr::mutate(nos = stringr::str_detect(utterance, \"\\\\b[Nn][Oo]\\\\b\")) %&gt;%\n  dplyr::summarise(\n    nos = sum(nos),\n    words = sum(words)\n  ) %&gt;%\n  # add relative frequency\n  dplyr::mutate(freq = round(nos / words * 1000, 3))\n# inspect data\nhead(no_words)\n\n# A tibble: 6 × 6\n# Groups:   transcript_id, gender_targetchild [6]\n  transcript_id gender_targetchild age_years_targetchild   nos words  freq\n  &lt;chr&gt;         &lt;chr&gt;              &lt;chr&gt;                 &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 acebr1        female             3                         0   149   0  \n2 acebr2        female             4                         0   322   0  \n3 acebr5        female             7                         0   270   0  \n4 aceer1        female             3                         0     4   0  \n5 aceer2        female             4                         0    29   0  \n6 aceet7        male               9                         7   458  15.3\n\n\nWe can also visualize the trends using the ggplot function . To learn how to visualize data in R see this tutorial.\n\nno_words %&gt;%\n  dplyr::mutate(age_years_targetchild = as.numeric(age_years_targetchild)) %&gt;%\n  ggplot(aes(x = age_years_targetchild, y = freq)) +\n  geom_smooth() +\n  theme_bw() +\n  labs(x = \"Age of target child\", y = \"Relative frequency of NOs \\n (per 1,000 words)\")"
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#case-study-2-extracting-questions",
    "href": "tutorials/corplingr/corplingr.html#case-study-2-extracting-questions",
    "title": "Corpus Linguistics with R",
    "section": "Case study 2: extracting questions",
    "text": "Case study 2: extracting questions\nHere, we want to extract all questions uttered by mothers. We operationalize questions as utterances containing a question mark.\n\nquestions &lt;- hslld %&gt;%\n  dplyr::filter(speaker == \"MOT\") %&gt;%\n  dplyr::filter(stringr::str_detect(utterance, \"\\\\?\"))\n# inspect data\nhead(questions)\n\n  id transcript_id speaker                                        utterance\n1  9        acebr1     MOT                                            Chi ?\n2 10        acebr1     MOT                   you wan(t) (t)a hear a story ?\n3 15        acebr1     MOT                      will you show me the moon ?\n4 17        acebr1     MOT           Chi you don't know where the moon is ?\n5 19        acebr1     MOT                               is that the moon ?\n6 21        acebr1     MOT okay where's the egg that's laying on the leaf ?\n                                                                                                                postag\n1                                                                                                         n:prop|Chi ?\n2                                                                 pro:per|you v|want inf|to v|hear det:art|a n|story ?\n3                                                          mod|will pro:per|you v|show pro:obj|me det:art|the n|moon ?\n4                            n:prop|Chi pro:per|you mod|do~neg|not v|know pro:int|where det:art|the n|moon cop|be&3S ?\n5                                                                             cop|be&3S comp|that det:art|the n|moon ?\n6 co|okay pro:int|where~cop|be&3S det:art|the n|egg pro:rel|that~aux|be&3S part|lay-PRESP prep|on det:art|the n|leaf ?\n  comment                                       participants age_targetchild\n1          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n2          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n3          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n4          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n5          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n6          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n  age_years_targetchild gender_targetchild dob_targetchild         comment_file\n1                     3             female    11-DEC-1984  SES of Mot is lower \n2                     3             female    11-DEC-1984  SES of Mot is lower \n3                     3             female    11-DEC-1984  SES of Mot is lower \n4                     3             female    11-DEC-1984  SES of Mot is lower \n5                     3             female    11-DEC-1984  SES of Mot is lower \n6                     3             female    11-DEC-1984  SES of Mot is lower \n          date         location homevisit_activity\n1 19-JUL-1988  Boston , MA USA        Home Visit 1\n2 19-JUL-1988  Boston , MA USA        Home Visit 1\n3 19-JUL-1988  Boston , MA USA        Home Visit 1\n4 19-JUL-1988  Boston , MA USA        Home Visit 1\n5 19-JUL-1988  Boston , MA USA        Home Visit 1\n6 19-JUL-1988  Boston , MA USA        Home Visit 1\n                                       activity homevisit words collection\n1 Book Reading 1 (The Very Hungry Caterpillar)        br1     1      EngNA\n2 Book Reading 1 (The Very Hungry Caterpillar)        br1     8      EngNA\n3 Book Reading 1 (The Very Hungry Caterpillar)        br1     6      EngNA\n4 Book Reading 1 (The Very Hungry Caterpillar)        br1     9      EngNA\n5 Book Reading 1 (The Very Hungry Caterpillar)        br1     4      EngNA\n6 Book Reading 1 (The Very Hungry Caterpillar)        br1    11      EngNA\n  corpus visit         situation\n1  HSLLD     1 Book Readingading\n2  HSLLD     1 Book Readingading\n3  HSLLD     1 Book Readingading\n4  HSLLD     1 Book Readingading\n5  HSLLD     1 Book Readingading\n6  HSLLD     1 Book Readingading\n\n\nWe could now check if the rate of questions changes over time.\n\nqmot &lt;- hslld %&gt;%\n  dplyr::filter(speaker == \"MOT\") %&gt;%\n  dplyr::mutate(\n    questions = ifelse(stringr::str_detect(utterance, \"\\\\?\") == T, 1, 0),\n    utterances = 1\n  ) %&gt;%\n  dplyr::group_by(age_years_targetchild) %&gt;%\n  dplyr::summarise(\n    utterances = sum(utterances),\n    questions = sum(questions),\n    percent = round(questions / utterances * 100, 2)\n  )\n# inspect data\nhead(qmot)\n\n# A tibble: 6 × 4\n  age_years_targetchild utterances questions percent\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 10                          4249       925    21.8\n2 11                           343       141    41.1\n3 12                            56        22    39.3\n4 3                          27209      9089    33.4\n5 4                          45068     14487    32.1\n6 5                          37634     10844    28.8\n\n\n\nqmot %&gt;%\n  dplyr::mutate(age_years_targetchild = as.numeric(age_years_targetchild)) %&gt;%\n  ggplot(aes(x = age_years_targetchild, y = percent)) +\n  geom_smooth() +\n  theme_bw() +\n  labs(x = \"Age of target child\", y = \"Percent \\n (questions)\")"
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#case-study-3-extracting-aux-parts",
    "href": "tutorials/corplingr/corplingr.html#case-study-3-extracting-aux-parts",
    "title": "Corpus Linguistics with R",
    "section": "Case study 3: extracting aux + parts",
    "text": "Case study 3: extracting aux + parts\nHere we want to extract all occurrences of an auxiliary plus a participle (e.g. is swimming) produced by mothers.\n\nauxv &lt;- hslld %&gt;%\n  dplyr::filter(speaker == \"MOT\") %&gt;%\n  dplyr::filter(stringr::str_detect(postag, \"aux\\\\|\\\\S{1,} part\\\\|\"))\n# inspect data\nhead(auxv)\n\n   id transcript_id speaker                                        utterance\n1  21        acebr1     MOT okay where's the egg that's laying on the leaf ?\n2  56        acebr1     MOT                      and here he is coming out !\n3  68        acebr1     MOT                   looks like he's eating a lot .\n4 202        acebr1     MOT        see the dog's getting closer to the cat .\n5 204        acebr1     MOT              (be)cause he's getting closer [!] .\n6 205        acebr1     MOT                       he's gonna catch the cat .\n                                                                                                                postag\n1 co|okay pro:int|where~cop|be&3S det:art|the n|egg pro:rel|that~aux|be&3S part|lay-PRESP prep|on det:art|the n|leaf ?\n2                                                    coord|and adv|here pro:sub|he aux|be&3S part|come-PRESP adv|out !\n3                                            v|look-3S conj|like pro:sub|he~aux|be&3S part|eat-PRESP det:art|a n|lot .\n4                            v|see det:art|the n|dog~aux|be&3S part|get-PRESP adj|close-CP prep|to det:art|the n|cat .\n5                                                      conj|because pro:sub|he~aux|be&3S part|get-PRESP adj|close-CP .\n6                                                pro:sub|he~aux|be&3S part|go-PRESP~inf|to v|catch det:art|the n|cat .\n  comment                                       participants age_targetchild\n1          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n2          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n3          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n4          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n5          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n6          CHI Target_Child , MOT Mother , EX1 Investigator          3;07.08\n  age_years_targetchild gender_targetchild dob_targetchild         comment_file\n1                     3             female    11-DEC-1984  SES of Mot is lower \n2                     3             female    11-DEC-1984  SES of Mot is lower \n3                     3             female    11-DEC-1984  SES of Mot is lower \n4                     3             female    11-DEC-1984  SES of Mot is lower \n5                     3             female    11-DEC-1984  SES of Mot is lower \n6                     3             female    11-DEC-1984  SES of Mot is lower \n          date         location homevisit_activity\n1 19-JUL-1988  Boston , MA USA        Home Visit 1\n2 19-JUL-1988  Boston , MA USA        Home Visit 1\n3 19-JUL-1988  Boston , MA USA        Home Visit 1\n4 19-JUL-1988  Boston , MA USA        Home Visit 1\n5 19-JUL-1988  Boston , MA USA        Home Visit 1\n6 19-JUL-1988  Boston , MA USA        Home Visit 1\n                                       activity homevisit words collection\n1 Book Reading 1 (The Very Hungry Caterpillar)        br1    11      EngNA\n2 Book Reading 1 (The Very Hungry Caterpillar)        br1     6      EngNA\n3 Book Reading 1 (The Very Hungry Caterpillar)        br1     7      EngNA\n4 Book Reading 1 (The Very Hungry Caterpillar)        br1     9      EngNA\n5 Book Reading 1 (The Very Hungry Caterpillar)        br1     6      EngNA\n6 Book Reading 1 (The Very Hungry Caterpillar)        br1     6      EngNA\n  corpus visit         situation\n1  HSLLD     1 Book Readingading\n2  HSLLD     1 Book Readingading\n3  HSLLD     1 Book Readingading\n4  HSLLD     1 Book Readingading\n5  HSLLD     1 Book Readingading\n6  HSLLD     1 Book Readingading\n\n\nWe can now extract all the particle forms from the pos-tagged utterance\n\nauxv_verbs &lt;- auxv %&gt;%\n  dplyr::mutate(participle = gsub(\".*part\\\\|(\\\\w{1,})-.*\", \"\\\\1\", postag)) %&gt;%\n  dplyr::pull(participle)\nhead(auxv_verbs)\n\n[1] \"lay\"  \"come\" \"eat\"  \"get\"  \"get\"  \"go\"  \n\n\n\nauxv_verbs_df &lt;- auxv_verbs %&gt;%\n  as.data.frame(.) %&gt;%\n  dplyr::rename(\"verb\" = colnames(.)[1]) %&gt;%\n  dplyr::group_by(verb) %&gt;%\n  dplyr::summarise(freq = n()) %&gt;%\n  dplyr::arrange(-freq) %&gt;%\n  head(20)\n# inspect\nhead(auxv_verbs_df)\n\n# A tibble: 6 × 2\n  verb     freq\n  &lt;chr&gt;   &lt;int&gt;\n1 go       1927\n2 call      308\n3 do        243\n4 eat       205\n5 get       184\n6 suppose   146\n\n\nWe can again visualize the results. In this case, we create a bar plot (see the geom_bar).\n\nauxv_verbs_df %&gt;%\n  ggplot(aes(x = reorder(verb, -freq), y = freq)) +\n  geom_bar(stat = \"identity\") +\n  theme_bw() +\n  labs(x = \"Verb\", y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90))"
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#case-study-4-ratio-of-verbs-to-words",
    "href": "tutorials/corplingr/corplingr.html#case-study-4-ratio-of-verbs-to-words",
    "title": "Corpus Linguistics with R",
    "section": "Case study 4: ratio of verbs to words",
    "text": "Case study 4: ratio of verbs to words\nHere we extract all lexical verbs and words uttered by children by year and then see if the rate of verbs changes over time.\n\nnverbs &lt;- hslld %&gt;%\n  dplyr::filter(speaker == \"CHI\") %&gt;%\n  dplyr::mutate(\n    nverbs = stringr::str_count(postag, \"^v\\\\|| v\\\\|\"),\n    age_years_targetchild = as.numeric(age_years_targetchild)\n  ) %&gt;%\n  dplyr::group_by(age_years_targetchild) %&gt;%\n  dplyr::summarise(\n    words = sum(words),\n    verbs = sum(nverbs)\n  ) %&gt;%\n  dplyr::mutate(verb.word.ratio = round(verbs / words, 3))\n# inspect data\nnverbs\n\n# A tibble: 10 × 4\n   age_years_targetchild  words verbs verb.word.ratio\n                   &lt;dbl&gt;  &lt;int&gt; &lt;int&gt;           &lt;dbl&gt;\n 1                     3  56864  5424           0.095\n 2                     4 101992 10355           0.102\n 3                     5 112173 11935           0.106\n 4                     6   8796   934           0.106\n 5                     7  59755  5405           0.09 \n 6                     8   5523   588           0.106\n 7                     9  46321  4739           0.102\n 8                    10  20310  2169           0.107\n 9                    11   1441   160           0.111\n10                    12    173    13           0.075\n\n\nWe can also visualize the results to show any changes over time.\n\nnverbs %&gt;%\n  ggplot(aes(x = age_years_targetchild, y = verb.word.ratio)) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 0.2)) +\n  theme_bw() +\n  labs(x = \"Age of target child\", y = \"Verb-Word Ratio\")"
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#case-study-5-type-token-ratio-over-time",
    "href": "tutorials/corplingr/corplingr.html#case-study-5-type-token-ratio-over-time",
    "title": "Corpus Linguistics with R",
    "section": "Case study 5: type-token ratio over time",
    "text": "Case study 5: type-token ratio over time\nHere we extract all tokens (words with repetition) and types (words without repetition) uttered by children by year and then see if the type-token ratio changes over time.\nIn a first step, we create a table with the age of the children in years, we then collapse all utterances of the children into one long utterance and then clean this long utterance by removing digits and superfluous white spaces.\n\n\n\n\nTIPA more accurate way of doing this would be to create one utterance for each child per home visit as this would give us a distribution of type-token ratios rather than a single value.\n\n\n\n\n\n\n\nutterance_tb &lt;- hslld %&gt;%\n  dplyr::filter(speaker == \"CHI\") %&gt;%\n  dplyr::group_by(age_years_targetchild) %&gt;%\n  dplyr::summarise(allutts = paste0(utterance, collapse = \" \")) %&gt;%\n  dplyr::ungroup() %&gt;%\n  dplyr::mutate(\n    age_years_targetchild = as.numeric(age_years_targetchild),\n    # clean utterance\n    allutts = stringr::str_replace_all(allutts, \"\\\\W\", \" \"),\n    allutts = stringr::str_replace_all(allutts, \"\\\\d\", \" \"),\n    allutts = stringr::str_remove_all(allutts, \"xxx\"),\n    allutts = stringr::str_remove_all(allutts, \"zzz\"),\n    allutts = tolower(allutts)\n  ) %&gt;%\n  # remove superfluous white spaces\n  dplyr::mutate(allutts = gsub(\" {2,}\", \" \", allutts)) %&gt;%\n  dplyr::mutate(allutts = stringr::str_squish(allutts))\n# inspect data\nhead(utterance_tb)\n\n# A tibble: 6 × 2\n  age_years_targetchild allutts                                                 \n                  &lt;dbl&gt; &lt;chr&gt;                                                   \n1                    10 laughs mutters www i get one of th em right here sure j…\n2                    11 i m adding adding two long necks yeah yeah two giraffes…\n3                    12 yes mommy it s very good it s good yes it was great i l…\n4                     3 hm where he s yellow and green and pink and green and y…\n5                     4 this one no you can t read it yeah i ll reach for the s…\n6                     5 mommy a turtle bee uhuh a caterpillar i don t know how …\n\n\nExtract the number of tokens, the number of types and calculating the type-token ratio.\n\ntokens &lt;- stringr::str_count(utterance_tb$allutts, \" \") + 1\ntypes &lt;- stringr::str_split(utterance_tb$allutts, \" \")\ntypes &lt;- sapply(types, function(x) {\n  x &lt;- length(names(table(x)))\n})\nttr &lt;- utterance_tb %&gt;%\n  dplyr::mutate(\n    tokens = tokens,\n    types = types\n  ) %&gt;%\n  dplyr::select(-allutts) %&gt;%\n  dplyr::mutate(TypeTokenRatio = round(types / tokens, 3))\n# inspect\nttr\n\n# A tibble: 10 × 4\n   age_years_targetchild tokens types TypeTokenRatio\n                   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;          &lt;dbl&gt;\n 1                    10  19748  2001          0.101\n 2                    11   1421   456          0.321\n 3                    12    173    92          0.532\n 4                     3  53017  2770          0.052\n 5                     4  96499  4081          0.042\n 6                     5 107182  4079          0.038\n 7                     6   8474  1158          0.137\n 8                     7  58121  3275          0.056\n 9                     8   5330   922          0.173\n10                     9  44964  2906          0.065\n\n\nPlot the type-token ratio against age of the target child.\n\nttr %&gt;%\n  ggplot(aes(x = age_years_targetchild, y = TypeTokenRatio)) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 0.75)) +\n  theme_bw() +\n  labs(x = \"Age of target child\", y = \"Type-Token Ratio\")"
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#data-processing-1",
    "href": "tutorials/corplingr/corplingr.html#data-processing-1",
    "title": "Corpus Linguistics with R",
    "section": "Data processing",
    "text": "Data processing\nIn a first step, we load the load the data into R. The way that the corpus data is loaded in this example is somewhat awkward because the data is in a server directory rather than on a hard drive on a simple PC. If the corpus data is not stored in a directory of a server, then you should not use the code shown immediately below but code in the window following the code immediately below.\n\n# define path to corpus\ncorpuspath &lt;- \"tutorials/corplingr/data/ICEIrelandSample/\"\n# define corpusfiles\nfiles &lt;- paste(corpuspath, \"S1A-00\", 1:20, \".txt\", sep = \"\")\nfiles &lt;- gsub(\"[0-9]([0-9][0-9][0-9])\", \"\\\\1\", files)\n# load corpus files\ncorpus &lt;- sapply(files, function(x) {\n  x &lt;- readLines(x)\n  x &lt;- paste(x, collapse = \" \")\n  x &lt;- tolower(x)\n})\n# inspect corpus\nstr(corpus)\n\n Named chr [1:20] \"&lt;s1a-001 riding&gt;  &lt;i&gt; &lt;s1a-001$a&gt; &lt;#&gt; well how did the riding go tonight &lt;s1a-001$b&gt; &lt;#&gt; it was good so it was \"| __truncated__ ...\n - attr(*, \"names\")= chr [1:20] \"tutorials/corplingr/data/ICEIrelandSample/S1A-001.txt\" \"tutorials/corplingr/data/ICEIrelandSample/S1A-002.txt\" \"tutorials/corplingr/data/ICEIrelandSample/S1A-003.txt\" \"tutorials/corplingr/data/ICEIrelandSample/S1A-004.txt\" ...\n\n\nIf the corpus data is stored on your own computer (on not on a serves as is the case in the present example), you need to adapt the path though as the code below only works on my computer. Just exchange the corpuspath with the path to the data on your computer (e.g. with \"D:\\\\Uni\\\\UQ\\\\LADAL\\\\SLCLADAL.github.io\\\\data\\\\ICEIrelandSample\").\nData processing and extraction\nNow that the corpus data is loaded, we can prepare the searches by defining the search patterns. We will use regular expressions to retrieve all variants of the swear words. The sequence \\\\b denotes word boundaries while the sequence [a-z]{0,3} means that the sequences ass can be followed by a string consisting of any character symbol that is maximally three characters long (so that the search would also retrieve asses). We separate the search patterns by | as this means or.\n\nsearchpatterns &lt;- c(\"\\\\bass[ingedholes]{0,6}\\\\b|\\\\bbitch[a-z]{0,3}\\\\b|\\\\b[a-z]{0,}fuck[a-z]{0,3}\\\\b|\\\\bshit[a-z]{0,3}\\\\b|\\\\bcock[a-z]{0,3}\\\\b|\\\\bwanker[a-z]{0,3}\\\\b|\\\\bboll[io]{1,1}[a-z]{0,3}\\\\b|\\\\bcrap[a-z]{0,3}\\\\b|\\\\bbugger[a-z]{0,3}\\\\b|\\\\bcunt[a-z]{0,3}\\\\b\")\n\nAfter defining the search pattern(s), we extract the kwics (keyword(s) in context) of the swear words.\n\n# extract kwic\nkwicswears &lt;- quanteda::kwic(quanteda::tokens(corpus), searchpatterns, window = 10, valuetype = \"regex\")\n\n\n\ndocnamefromtoprekeywordpostpatterntutorials/corplingr/data/ICEIrelandSample/S1A-003.txt1,3481,348suppose the worrying thing was then you realised it didbugger-allyou know &lt; & &gt; laughter &lt; / & &gt;\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt525525was uh they just want my money and all thisshite&lt; # &gt; fuck them &lt; # &gt; i '\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt529529want my money and all this shite &lt; # &gt;fuckthem &lt; # &gt; i ' m never joining them\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt664664flick through them bits &lt; # &gt; it ' sshite&lt; s1a-005 $ a &gt; &lt; # &gt; all the\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt1,0121,0125 sylls &lt; / unclear &gt; i ' ve tofuckingdeal with that guy because he ' s a mason\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt1,0261,026guy because he ' s a mason &lt; # &gt;fuckthat &lt; s1a-005 $ c &gt; &lt; # &gt; &lt;\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt1,6001,600all &lt; # &gt; i ' m like dad youfuckingjoined this &lt; & &gt; laughter &lt; / & &gt;\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt1,7831,783try again &lt; # &gt; it ' s all justbollocks&lt; s1a-005 $ b &gt; &lt; # &gt; it '\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt2,9212,921visiting &lt; / [ &gt; and she was like ohfucking&lt; s1a-005 $ b &gt; &lt; # &gt; &lt; [\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\btutorials/corplingr/data/ICEIrelandSample/S1A-005.txt3,5993,599marching down the corridor going you ' ve been outfuckingand whoring haven't you you bastard &lt; # &gt; and\\bass[ingedholes]{0,6}\\b|\\bbitch[a-z]{0,3}\\b|\\b[a-z]{0,}fuck[a-z]{0,3}\\b|\\bshit[a-z]{0,3}\\b|\\bcock[a-z]{0,3}\\b|\\bwanker[a-z]{0,3}\\b|\\bboll[io]{1,1}[a-z]{0,3}\\b|\\bcrap[a-z]{0,3}\\b|\\bbugger[a-z]{0,3}\\b|\\bcunt[a-z]{0,3}\\b\n\n\nWe now clean the kwic so that it is easier to see the relevant information.\n\nkwicswearsclean &lt;- kwicswears %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::rename(\n    \"File\" = colnames(.)[1],\n    \"StartPosition\" = colnames(.)[2],\n    \"EndPosition\" = colnames(.)[3],\n    \"PreviousContext\" = colnames(.)[4],\n    \"Token\" = colnames(.)[5],\n    \"FollowingContext\" = colnames(.)[6],\n    \"SearchPattern\" = colnames(.)[7]\n  ) %&gt;%\n  dplyr::select(-StartPosition, -EndPosition, -SearchPattern) %&gt;%\n  dplyr::mutate(\n    File = str_remove_all(File, \".*/\"),\n    File = stringr::str_remove_all(File, \".txt\")\n  )\n\n\n\nFilePreviousContextTokenFollowingContextS1A-003suppose the worrying thing was then you realised it didbugger-allyou know &lt; & &gt; laughter &lt; / & &gt;S1A-005was uh they just want my money and all thisshite&lt; # &gt; fuck them &lt; # &gt; i 'S1A-005want my money and all this shite &lt; # &gt;fuckthem &lt; # &gt; i ' m never joining themS1A-005flick through them bits &lt; # &gt; it ' sshite&lt; s1a-005 $ a &gt; &lt; # &gt; all theS1A-0055 sylls &lt; / unclear &gt; i ' ve tofuckingdeal with that guy because he ' s a masonS1A-005guy because he ' s a mason &lt; # &gt;fuckthat &lt; s1a-005 $ c &gt; &lt; # &gt; &lt;S1A-005all &lt; # &gt; i ' m like dad youfuckingjoined this &lt; & &gt; laughter &lt; / & &gt;S1A-005try again &lt; # &gt; it ' s all justbollocks&lt; s1a-005 $ b &gt; &lt; # &gt; it 'S1A-005visiting &lt; / [ &gt; and she was like ohfucking&lt; s1a-005 $ b &gt; &lt; # &gt; &lt; [S1A-005marching down the corridor going you ' ve been outfuckingand whoring haven't you you bastard &lt; # &gt; and\n\n\nWe now create another kwic but with much more context because we want to extract the speaker that has uttered the swear word. To this end, we remove everything that proceeds the $ symbol as the speakers are identified by characters that follow the $ symbol, remove everything that follows the &gt; symbol which end the speaker identification sequence, remove remaining white spaces, and convert the remaining character to upper case.\n\n# extract kwic\nkwiclong &lt;- kwic(quanteda::tokens(corpus), searchpatterns, window = 1000, valuetype = \"regex\")\nkwiclong &lt;- as.data.frame(kwiclong)\ncolnames(kwiclong) &lt;- c(\"File\", \"StartPosition\", \"EndPosition\", \"PreviousContext\", \"Token\", \"FollowingContext\", \"SearchPattern\")\nkwiclong &lt;- kwiclong %&gt;%\n  dplyr::select(-StartPosition, -EndPosition, -SearchPattern) %&gt;%\n  dplyr::mutate(\n    File = str_remove_all(File, \".*/\"),\n    File = str_remove_all(File, \".txt\"),\n    Speaker = str_remove_all(PreviousContext, \".*\\\\$\"),\n    Speaker = str_remove_all(Speaker, \"&gt;.*\"),\n    Speaker = str_squish(Speaker),\n    Speaker = toupper(Speaker)\n  ) %&gt;%\n  dplyr::select(Speaker)\n# inspect results\nhead(kwiclong)\n\n  Speaker\n1       A\n2       B\n3       B\n4       B\n5       B\n6       B\n\n\nWe now add the Speaker to our initial kwic. This way, we combine the swear word kwic with the speaker and as we already have the file, we can use the file plus speaker identification to check if the speaker was a man or a woman.\n\nswire &lt;- cbind(kwicswearsclean, kwiclong)\n\n\n\nFilePreviousContextTokenFollowingContextSpeakerS1A-003suppose the worrying thing was then you realised it didbugger-allyou know &lt; & &gt; laughter &lt; / & &gt;AS1A-005was uh they just want my money and all thisshite&lt; # &gt; fuck them &lt; # &gt; i 'BS1A-005want my money and all this shite &lt; # &gt;fuckthem &lt; # &gt; i ' m never joining themBS1A-005flick through them bits &lt; # &gt; it ' sshite&lt; s1a-005 $ a &gt; &lt; # &gt; all theBS1A-0055 sylls &lt; / unclear &gt; i ' ve tofuckingdeal with that guy because he ' s a masonBS1A-005guy because he ' s a mason &lt; # &gt;fuckthat &lt; s1a-005 $ c &gt; &lt; # &gt; &lt;BS1A-005all &lt; # &gt; i ' m like dad youfuckingjoined this &lt; & &gt; laughter &lt; / & &gt;BS1A-005try again &lt; # &gt; it ' s all justbollocks&lt; s1a-005 $ b &gt; &lt; # &gt; it 'AS1A-005visiting &lt; / [ &gt; and she was like ohfucking&lt; s1a-005 $ b &gt; &lt; # &gt; &lt; [CS1A-005marching down the corridor going you ' ve been outfuckingand whoring haven't you you bastard &lt; # &gt; andB\n\n\nNow, we inspect the extracted swear word tokens to check if our search strings have indeed captured swear words.\n\n# convert tokens to lower case\nswire$Token &lt;- tolower(swire$Token)\n# inspect tokens\ntable(swire$Token)\n\n\n       ass      assed      bitch    bitches     bitchy    bollock   bollocks \n         2          1          1          1          2          1          3 \n    bugger bugger-all       crap       fuck   fuck-all     fucked    fucking \n         2          2          9          8          1          1         16 \n     fucks       shit      shite     wanker \n         1          1          3          2 \n\n\nFUCK and its variants is by far the most common swear word in our corpus. However, we do not need the type of swear word to answer our research question and we thus summarize the table to show which speaker in which files has used how many swear words.\n\nswire &lt;- swire %&gt;%\n  dplyr::group_by(File, Speaker) %&gt;%\n  dplyr::summarise(Swearwords = n())\n\n\n\nFileSpeakerSwearwordsS1A-003A1S1A-005A1S1A-005B10S1A-005C1S1A-010A2S1A-011A2S1A-011B3S1A-014B2S1A-014C3S1A-014D2\n\n\nNow that we extract how many swear words the speakers in the corpus have used, we can load the biodata of the speakers.\n\n# load bio data\nbio &lt;- base::readRDS(\"tutorials/corplingr/data/d01.rda\", \"rb\")\n\n\n\nidtext.idsubfilespk.refzonedatesexageresidereligword.count1S1A-0011Anorthern ireland1990-1994male34-41belfastprotestant7652S1A-0011Bnorthern ireland1990-1994female34-41belfastprotestant1,2984S1A-0021Anorthern ireland2002-2005female26-33belfastcatholic3915S1A-0021Bnorthern ireland2002-2005female19-25belfastcatholic476S1A-0021Cnorthern ireland2002-2005male50+belfastcatholic2007S1A-0021Dnorthern ireland2002-2005female50+belfastcatholic4648S1A-0021Emixed between ni and roi2002-2005male34-41englandcatholic6399S1A-0021Fnorthern ireland2002-2005female26-33belfastcatholic30810S1A-0021Gnorthern ireland2002-2005female50+belfastcatholic7811S1A-0021Hmixed between ni and roi2002-2005male19-25englandcatholic98\n\n\n\nbio &lt;- bio %&gt;%\n  dplyr::rename(\n    File = text.id,\n    Speaker = spk.ref,\n    Gender = sex,\n    Age = age,\n    Words = word.count\n  ) %&gt;%\n  dplyr::select(File, Speaker, Gender, Age, Words)\n\n\n\nFileSpeakerGenderAgeWordsS1A-001Amale34-41765S1A-001Bfemale34-411,298S1A-002Afemale26-33391S1A-002Bfemale19-2547S1A-002Cmale50+200S1A-002Dfemale50+464S1A-002Emale34-41639S1A-002Ffemale26-33308S1A-002Gfemale50+78S1A-002Hmale19-2598\n\n\nIn a next step, we combine the table with the speaker information with the table showing the swear word use.\n\n# combine frequencies and biodata\nswire &lt;- dplyr::left_join(bio, swire, by = c(\"File\", \"Speaker\")) %&gt;%\n  # replace NA with 0\n  dplyr::mutate(\n    Swearwords = ifelse(is.na(Swearwords), 0, Swearwords),\n    File = factor(File),\n    Speaker = factor(Speaker),\n    Gender = factor(Gender),\n    Age = factor(Age)\n  )\n# inspect data\nhead(swire)\n\n     File Speaker Gender   Age Words Swearwords\n1 S1A-001       A   male 34-41   765          0\n2 S1A-001       B female 34-41  1298          0\n3 S1A-002       A female 26-33   391          0\n4 S1A-002       B female 19-25    47          0\n5 S1A-002       C   male   50+   200          0\n6 S1A-002       D female   50+   464          0\n\n\n\n\nFileSpeakerGenderAgeWordsSwearwordsS1A-001Amale34-417650S1A-001Bfemale34-411,2980S1A-002Afemale26-333910S1A-002Bfemale19-25470S1A-002Cmale50+2000S1A-002Dfemale50+4640S1A-002Emale34-416390S1A-002Ffemale26-333080S1A-002Gfemale50+780S1A-002Hmale19-25980\n\n\nWe now clean the table by removing speakers for which we do not have any information on their age and gender. Also, we summarize the table to extract the mean frequencies of swear words (per 1,000 words) by age and gender.\n\n# clean data\nswire &lt;- swire %&gt;%\n  dplyr::filter(\n    is.na(Gender) == F,\n    is.na(Age) == F\n  ) %&gt;%\n  dplyr::group_by(Age, Gender) %&gt;%\n  dplyr::mutate(\n    SumWords = sum(Words),\n    SumSwearwords = sum(Swearwords),\n    FrequencySwearwords = round(SumSwearwords / SumWords * 1000, 3)\n  )\n\n\n\nFileSpeakerGenderAgeWordsSwearwordsSumWordsSumSwearwordsFrequencySwearwordsS1A-001Amale34-41765022,21320.09S1A-001Bfemale34-411,298015,01730.20S1A-002Afemale26-33391035,137130.37S1A-002Bfemale19-2547062,53500.00S1A-002Cmale50+200064,04400.00S1A-002Dfemale50+464038,68300.00S1A-002Emale34-41639022,21320.09S1A-002Ffemale26-33308035,137130.37S1A-002Gfemale50+78038,68300.00S1A-002Hmale19-259808,82600.00\n\n\nTabulating and visualizing the data\nWe now summarize and visualize the data and exclude speakers between the ages of 0 and 18 as there are too few speakers within that age range to be representative.\n\nswire %&gt;%\n  dplyr::filter(Age != \"0-18\") %&gt;%\n  dplyr::group_by(Age, Gender) %&gt;%\n  dplyr::summarise(Swears_ptw = SumSwearwords / SumWords * 1000) %&gt;%\n  unique() %&gt;%\n  tidyr::spread(Gender, Swears_ptw)\n\n# A tibble: 5 × 3\n# Groups:   Age [5]\n  Age   female   male\n  &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 19-25  0     0     \n2 26-33  0.370 0.484 \n3 34-41  0.200 0.0900\n4 42-49  0     0     \n5 50+    0     0     \n\n\nNow that we have prepared our data, we can plot swear word use by gender.\n\nswire %&gt;%\n  dplyr::filter(Age != \"0-18\") %&gt;%\n  dplyr::group_by(Age, Gender) %&gt;%\n  dplyr::summarise(Swears_ptw = SumSwearwords / SumWords * 1000) %&gt;%\n  unique() %&gt;%\n  ggplot(aes(x = Age, y = Swears_ptw, group = Gender, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  theme_bw() +\n  scale_fill_manual(values = c(\"orange\", \"darkgrey\")) +\n  labs(y = \"Relative frequency \\n swear words per 1,000 words\")\n\n\n\n\n\n\n\n\nThe graph suggests that the genders do not differ in their use of swear words except for the age bracket from 26 to 41: men swear more among speakers aged between 26 and 33 while women swear more between 34 and 41 years of age."
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#statistical-analysis",
    "href": "tutorials/corplingr/corplingr.html#statistical-analysis",
    "title": "Corpus Linguistics with R",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nWe now perform a statistical test, e.g. a Configural Frequency Analysis (CFA) to check if specifically which groups in the data significantly over and under-use swearwords.\n\ncfa_swear &lt;- swire %&gt;%\n  dplyr::group_by(Gender, Age) %&gt;%\n  dplyr::summarise(\n    Words = sum(Words),\n    Swearwords = sum(Swearwords)\n  ) %&gt;%\n  dplyr::mutate(Words = Words - Swearwords) %&gt;%\n  tidyr::gather(Type, Frequency, Words:Swearwords) %&gt;%\n  dplyr::filter(Age != \"0-18\")\n\nAfter transforming the data, it has the following format.\n\n\nGenderAgeTypeFrequencyfemale19-25Words62,535female26-33Words35,124female34-41Words15,014female42-49Words10,785female50+Words38,683male19-25Words8,826male26-33Words20,654male34-41Words22,211male42-49Words40,923male50+Words64,044female19-25Swearwords0female26-33Swearwords13female34-41Swearwords3female42-49Swearwords0female50+Swearwords0male19-25Swearwords0male26-33Swearwords10male34-41Swearwords2male42-49Swearwords0male50+Swearwords0\n\n\n\n# define configurations\nconfigs &lt;- cfa_swear %&gt;%\n  dplyr::select(Age, Gender, Type)\n# define counts\ncounts &lt;- cfa_swear$Frequency\n\nNow that configurations and counts are separated, we can perform the configural frequency analysis.\n\n# perform cfa\ncfa(configs, counts)$table %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::filter(\n    p.chisq &lt; .1,\n    stringr::str_detect(label, \"Swear\")\n  ) %&gt;%\n  dplyr::select(-z, -p.z, -sig.z, -sig.chisq, -Q)\n\n                    label  n expected chisq        p.chisq\n1 26-33 female Swearwords 13    2.492 44.30 0.000000005565\n2 26-33 male   Swearwords 10    2.408 23.93 0.000082325635\n\n\nAfter filtering out significant over use of non-swear words from the results of the CFA, we find that men and women in the age bracket 26 to 33 use significantly more swear words and other groups in the data.\nIt has to be borne in mind, though, that this is merely a case study and that a more fine-grained analysis on a substantially larger data set were necessary to get a more reliable impression."
  },
  {
    "objectID": "tutorials/corplingr/corplingr.html#footnotes",
    "href": "tutorials/corplingr/corplingr.html#footnotes",
    "title": "Corpus Linguistics with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/litsty/litsty.html#what-is-computational-literary-stylistics",
    "href": "tutorials/litsty/litsty.html#what-is-computational-literary-stylistics",
    "title": "Introduction",
    "section": "What is computational literary stylistics?",
    "text": "What is computational literary stylistics?\nComputational literary stylistics refers to analyses of the language of literary texts by computational means using linguistic concepts and categories, with the goal of finding patters among the literary texts and explaining how literary meaning/s is/are created by specific language choices. Computational literary stylistics has been linked with distant reading which aims to find patterns in large amounts of literary data that would not be detectable by traditional close reading techniques.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"tidytext\")\ninstall.packages(\"janeaustenr\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"forcats\")\ninstall.packages(\"gutenbergr\")\ninstall.packages(\"flextable\")\ninstall.packages(\"quanteda.textstats\")\ninstall.packages(\"quanteda.textplots\")\ninstall.packages(\"lexRankr\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nWe now activate these packages as shown below.\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\nlibrary(tidyverse)\nlibrary(janeaustenr)\nlibrary(tidytext)\nlibrary(forcats)\nlibrary(quanteda)\nlibrary(gutenbergr)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/litsty/litsty.html#word-frequency-distributions",
    "href": "tutorials/litsty/litsty.html#word-frequency-distributions",
    "title": "Introduction",
    "section": "Word-Frequency Distributions",
    "text": "Word-Frequency Distributions\nWord-frequency distributions can be used to determine if a text represents natural language (or a simple replacement cipher, for example) or if the text does not represent natural language (or a more complex cipher). In the following, we will check if the language used in texts we have downloaded from Project Gutenberg aligns with distributions that we would expect when dealing with natural language. In a first step, we determine both the term-frequency and the idf.\n\nbook_words &lt;- books %&gt;%\n    tidytext::unnest_tokens(word, text) %&gt;%\n    dplyr::count(book, word, sort = TRUE) %&gt;%\n    dplyr::group_by(book) %&gt;%\n    dplyr::mutate(total = sum(n))\n\n\n\nbookwordntotalDarwinthe10,301157,002Darwinof7,864157,002Austenthe4,656127,996Darwinand4,443157,002Austento4,323127,996Darwinin4,017157,002Austenof3,838127,996Austenand3,763127,996Darwinto3,613157,002Darwina2,471157,002Austenher2,260127,996Austeni2,095127,996Darwinthat2,086157,002Austena2,036127,996Austenin1,991127,996\n\n\nFrom the above table it is evident that the usual suspects the, and, to and so-forth are leading in terms of their usage frequencies in the novels. Now let us look at the distribution of n/total for each term in each of the novels (which represents the normalized term frequency).\n\nggplot(book_words, aes(n / total, fill = book)) +\n    geom_histogram(show.legend = FALSE) +\n    xlim(NA, 0.005) +\n    facet_wrap(~book, ncol = 2, scales = \"free_y\")\n\n\n\n\nTerm frequency distributions\n\n\n\n\nFrom the plots it is clear that we are dealing with a negative exponential distribution and that many words occur only rarely and that only few words occur frequently. In other words, only few words occur frequently while most words occur rarely. This relationship represents a distribution that is captured by Zipf’s law."
  },
  {
    "objectID": "tutorials/litsty/litsty.html#zipfs-law",
    "href": "tutorials/litsty/litsty.html#zipfs-law",
    "title": "Introduction",
    "section": "Zipf’s Law",
    "text": "Zipf’s Law\nZipf’s Law represents an empirical power law or power function that was established in the 1930s. Zipf’s law is one of the most fundamental laws in linguistics (see Zipf) and it states that the frequency of a word is inversely proportional to its rank in a text or collection of texts.\nLet\n\nN be the number of elements in a text (or collection of texts);\nk be their rank;\ns be the value of the exponent characterizing the distribution.\n\nZipf’s law then predicts that out of a population of N elements, the normalized frequency of the element of rank k, f(k;s,N), is:\n\\[\\begin{equation}\nf(k;s,N)={\\frac {1/k^{s}}{\\sum \\limits _{n=1}^{N}(1/n^{s})}}\n\\end{equation}\\]\nIn the code chunk below, we check if Zipf’s Law applies to the words that occur in texts that we have downloaded from Project Gutenberg.\n\nfreq_by_rank &lt;- book_words %&gt;%\n    dplyr::group_by(book) %&gt;%\n    dplyr::mutate(\n        rank = row_number(),\n        `term frequency` = n / total\n    ) %&gt;%\n    dplyr::ungroup()\n\n\n\nbookwordntotalrankterm frequencyDarwinthe10,301157,00210.0656106291640Darwinof7,864157,00220.0500885339040Austenthe4,656127,99610.0363761367543Darwinand4,443157,00230.0282990025605Austento4,323127,99620.0337744929529Darwinin4,017157,00240.0255856613292Austenof3,838127,99630.0299853120410Austenand3,763127,99640.0293993562299Darwinto3,613157,00250.0230124457013Darwina2,471157,00260.0157386530108Austenher2,260127,99650.0176568017751Austeni2,095127,99660.0163676989906Darwinthat2,086157,00270.0132864549496Austena2,036127,99670.0159067470858Austenin1,991127,99680.0155551735992\n\n\nTo get a better understanding of Zipf’s law, let us visualize the distribution by plotting on the logged rank of elements on the x-axis and logged frequency of the terms on the y-axis. If Zipf’s law holds, then we should see more or less straight lines that go from top left to bottom right.\n\nfreq_by_rank %&gt;%\n    ggplot(aes(rank, `term frequency`, color = book)) +\n    geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +\n    scale_x_log10() +\n    scale_y_log10()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nZipf’s law for a sample of literary texts\n\n\n\n\nWe can see that the plot has a negative slope which corroborates the inverse relationship of rank with respect to term frequency which shows that the words in the texts from Project Gutenberg follow Zipf’s law. This would ascertain that we are dealing with natural language and not a made up nonsense language or a complex cipher."
  },
  {
    "objectID": "tutorials/litsty/litsty.html#lexical-diversity",
    "href": "tutorials/litsty/litsty.html#lexical-diversity",
    "title": "Introduction",
    "section": "Lexical Diversity",
    "text": "Lexical Diversity\nLexical diversity is a complexity measure that provides information about the lexicon size of a text, i.e. how many different words occur in a text given the size of the text. The Type-Token-Ratio (TTR) provides information about the number of word tokens (individual instances of a word) divided by the number of different word types (word forms). Let’s briefly elaborate on that and look a bit more closely at the terms types and tokens. The sentence The dog chased the cat contains five tokens but only 4 types because the occurs twice. Now, a text that is 100 words long and consist of 50 distinct words would have a TTR of .5 (50/100) while a text that is 100 words long but consist of 80 distinct words would have a TTR of .8 (80/100). Thus, typically, higher values indicate higher lexical diversity and more complex texts or more advanced learners of a language commonly have higher TTRs compared to simpler texts or less advanced language learners.\nAs such, we can use lexical diversity measures to analyze the complexity of the language in which a text is written which can be used to inspect the advances a language learner makes when acquiring a language: initially, the learner will have high TTR as they do not have a large vocabulary. The TTRs will increase as lexicon of the learner grows.\nIn the following example, we calculate the TTRs for the literary texts we have downloaded from Project Gutenberg. Ina first step, we tokenize the texts, i.e. we split the texts into individual words (tokens).\n\nbooks_texts &lt;- books %&gt;%\n    dplyr::group_by(book) %&gt;%\n    dplyr::summarise(text = paste(text, collapse = \" \"))\ntexts &lt;- books_texts %&gt;%\n    dplyr::pull(text)\nnames(texts) &lt;- books_texts %&gt;%\n    dplyr::pull(book)\ntokens_texts &lt;- texts %&gt;%\n    quanteda::corpus() %&gt;%\n    quanteda::tokens()\n# inspect data\nhead(tokens_texts)\n\nTokens consisting of 4 documents.\nAusten :\n [1] \"[\"            \"Illustration\" \":\"            \"GEORGE\"       \"ALLEN\"       \n [6] \"PUBLISHER\"    \"156\"          \"CHARING\"      \"CROSS\"        \"ROAD\"        \n[11] \"LONDON\"       \"RUSKIN\"      \n[ ... and 152,253 more ]\n\nDarwin :\n [1] \"Click\"       \"on\"          \"any\"         \"of\"          \"the\"        \n [6] \"filenumbers\" \"below\"       \"to\"          \"quickly\"     \"view\"       \n[11] \"each\"        \"ebook\"      \n[ ... and 176,904 more ]\n\nPoe :\n [1] \"The\"      \"Raven\"    \"by\"       \"Edgar\"    \"Allan\"    \"Poe\"     \n [7] \"Once\"     \"upon\"     \"a\"        \"midnight\" \"dreary\"   \",\"       \n[ ... and 1,327 more ]\n\nShakespeare :\n [1] \"THE\"         \"TRAGEDY\"     \"OF\"          \"ROMEO\"       \"AND\"        \n [6] \"JULIET\"      \"by\"          \"William\"     \"Shakespeare\" \"Contents\"   \n[11] \"THE\"         \"PROLOGUE\"   \n[ ... and 32,877 more ]\n\n\nNext, we calculate the TTR using the textstat_lexdiv function from the quanteda package and visualize the resulting TTRs for the literary texts that we have downloaded from Project Gutenberg.\n\ndfm(tokens_texts) %&gt;%\n    quanteda.textstats::textstat_lexdiv(measure = \"TTR\") %&gt;%\n    ggplot(aes(x = TTR, y = reorder(document, TTR))) +\n    geom_point() +\n    xlab(\"Type-Token-Ratio (TTR)\") +\n    ylab(\"\")\n\n\n\n\n\n\n\n\nWe can see that Darwin’s On the Origin of Species has the lowest lexical diversity while Edgar Allan Poe’s The Raven has the highest. This would suggest that the language in The Raven is more complex than the language of On the Origin of Species. However, this is too simplistic and shows that simple Type-Token Ratios are severely affected by text length (as well as orthographic errors) and should only be used to compare texts\n\nthat are comparatively long (at least 200 words)\nthat are approximately of the same length\nthat were error corrected so that orthographic errors do not confound the ratios\n\n\nAverage Sentence Length\nThe average sentence length (ASL) is another measure of textual complexity with more sophisticated language use being associated with longer and more complex sentences. As such, we can use the ASL as an alternative measure of the linguistic complexity of a text or texts.\n\nlibrary(lexRankr)\nbooks_sentences &lt;- books %&gt;%\n    dplyr::group_by(book) %&gt;%\n    dplyr::summarise(text = paste(text, collapse = \" \")) %&gt;%\n    lexRankr::unnest_sentences(sentence, text)\n\n\n\nbooksent_idsentenceAusten1                            [Illustration:                              GEORGE ALLEN                                PUBLISHER                         156 CHARING CROSS ROAD                                 LONDON                              RUSKIN HOUSE                                    ]                             [Illustration:                _Reading Jane’s Letters._      _Chap 34._                                    ]                                 PRIDE.Austen2                                  and                                PREJUDICE                                   by                              Jane Austen,                            with a Preface by                            George Saintsbury                                   and                            Illustrations by                              Hugh Thomson                          [Illustration: 1894]                        Ruskin       156.Austen3Charing                        House.Austen4       Cross Road.Austen5                                London                              George Allen.Austen6             CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.Austen7                  TOOKS COURT, CHANCERY LANE, LONDON.Austen8                            [Illustration:                           _To J.Austen9Comyns Carr                       in acknowledgment of all I                        owe to his friendship and                     advice, these illustrations are                          gratefully inscribed_                             _Hugh Thomson_                                    ] PREFACE.Austen10[Illustration] _Walt Whitman has somewhere a fine and just distinction between “loving by allowance” and “loving with personal love.” This distinction applies to books as well as to men and women; and in the case of the not very numerous authors who are the objects of the personal affection, it brings a curious consequence with it.Austen11There is much more difference as to their best work than in the case of those others who are loved “by allowance” by convention, and because it is felt to be the right and proper thing to love them.Austen12And in the sect--fairly large and yet unusually choice--of Austenians or Janites, there would probably be found partisans of the claim to primacy of almost every one of the novels.Austen13To some the delightful freshness and humour of_ Northanger Abbey, _its completeness, finish, and_ entrain, _obscure the undoubted critical facts that its scale is small, and its scheme, after all, that of burlesque or parody, a kind in which the first rank is reached with difficulty._ Persuasion, _relatively faint in tone, and not enthralling in interest, has devotees who exalt above all the others its exquisite delicacy and keeping.Austen14The catastrophe of_ Mansfield Park _is admittedly theatrical, the hero and heroine are insipid, and the author has almost wickedly destroyed all romantic interest by expressly admitting that Edmund only took Fanny because Mary shocked him, and that Fanny might very likely have taken Crawford if he had been a little more assiduous; yet the matchless rehearsal-scenes and the characters of Mrs.Austen15Norris and others have secured, I believe, a considerable party for it._ Sense and Sensibility _has perhaps the fewest out-and-out admirers; but it does not want them._ _I suppose, however, that the majority of at least competent votes would, all things considered, be divided between_ Emma _and the present book; and perhaps the vulgar verdict (if indeed a fondness for Miss Austen be not of itself a patent of exemption from any possible charge of vulgarity) would go for_ Emma.\n\n\nLet’s now visualize the results for potential differences or trends.\n\nbooks_sentences %&gt;%\n    dplyr::mutate(sentlength = stringr::str_count(sentence, \"\\\\w+\")) %&gt;%\n    ggplot(aes(x = sentlength, y = reorder(book, sentlength, mean), group = book)) +\n    stat_summary(fun = mean, geom = \"point\") +\n    stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n    xlab(\"Average Sentence Length (ASL)\") +\n    ylab(\"\")\n\n\n\n\n\n\n\n\nWe can see that that The Raven (which does not contain any punctuation) is (unsurprisingly) the text with the longest ASL while Shakespeare’s play Romeo and Juliet (which contains a lost of dialogues) is deemed the work with the shortest ASL. With the exception of Poe’s The Raven, the ALS results reflect text complexity with Darwin’s On the Origin of Species being more complex or written like than the other texts with Romeo and Juliet being the most similar to spoken dialogue."
  },
  {
    "objectID": "tutorials/litsty/litsty.html#similarity-among-literary-texts",
    "href": "tutorials/litsty/litsty.html#similarity-among-literary-texts",
    "title": "Introduction",
    "section": "Similarity among literary texts",
    "text": "Similarity among literary texts\nWe will now explore how similar the language of literary works is. This approach can, of course, be extended to syntactic features or, for instance, to determine if certain texts belong to a certain literary genre or were written by a certain author. When extending this approach to syntactic features, one would naturally use features like the ALS, TTRs, or the frequency of adjectives as the basis for determining similarity. Regarding authorship, things like bigrams, spacing or punctuation methods would be relevant features.\nTo assess the similarity of literary works (based on the words that occur in the texts), we first create a feature list, i.e. a matrix with word frequencies. In the present case, we remove stop words as well as symbols (it can be useful to retain these but this is depends on the task at hand).\n\nfeature_mat &lt;- books %&gt;%\n    dplyr::group_by(book) %&gt;%\n    dplyr::sample_n(100) %&gt;%\n    dplyr::summarise(text = paste0(text, collapse = \" \")) %&gt;%\n    dplyr::ungroup() %&gt;%\n    quanteda::corpus(text_field = \"text\", docid_field = \"book\") %&gt;%\n    quanteda::tokens() %&gt;%\n    quanteda::dfm(remove_punct = TRUE, remove_symbols = TRUE) %&gt;%\n    quanteda::dfm_remove(pattern = stopwords(\"en\"))\n# inspect data\nfeature_mat\n\nDocument-feature matrix of: 4 documents, 1,224 features (71.18% sparse) and 0 docvars.\n             features\ndocs          ladyship’s desire  , looked felt life furnish parted  .  “\n  Austen               1      1 88      2    1    2       1      1 63 18\n  Darwin               0      0 91      0    0    3       0      0 29  0\n  Poe                  0      0 95      0    1    0       0      0 22 29\n  Shakespeare          0      0 61      0    0    1       0      0 62  0\n[ reached max_nfeat ... 1,214 more features ]\n\n\nWe see that the texts are represented as the row names and the terms the column names. The content of the matrix consists of the term frequencies.\nWe can now perform agglomerative hierarchical clustering and visualize the results in a dendrogram to assess the similarity of texts.\n\nbooks_dist &lt;- as.dist(quanteda.textstats::textstat_dist(feature_mat))\nbooks_clust &lt;- hclust(books_dist)\nplot(books_clust)\n\n\n\n\n\n\n\n\nAccording to the dendrogram, Conan Doyle’s The Adventures of Sherlock Holmes and Shakespeare’s Romeo and Juliet are the most similar texts. Edgar Allen Poe’s The Raven is the most idiosyncratic texts as it is on a branch by its own and is amalgamated with the other texts only as a very last step at the root of the tree."
  },
  {
    "objectID": "tutorials/litsty/litsty.html#networks-of-personas",
    "href": "tutorials/litsty/litsty.html#networks-of-personas",
    "title": "Introduction",
    "section": "Networks of Personas",
    "text": "Networks of Personas\nA final procedure we will perform is a network analysis of the personas in Shakespeare’s Romeo and Juliet. We directly load a co-occurrence matrix which provides information about how often character’s in that play have been in the same scene (as the extraction of this information is a bit cumbersome, I have done that for you and you can simply load the matrix into R).\n\n# load data\nromeo &lt;- read.delim(\"tutorials/litsty/data/romeo.txt\", sep = \"\\t\")\n# convert into feature co-occurrence matrix\nromeo_fcm &lt;- as.fcm(as.matrix(romeo))\n# inspect data\nromeo_fcm\n\nFeature co-occurrence matrix of: 23 by 23 features.\n               features\nfeatures        Abraham Benvolio LordCapulet Gregory LadyCapulet LadyMontague\n  Abraham             1        1           1       1           1            1\n  Benvolio            1        7           3       1           2            1\n  LordCapulet         1        3           9       1           7            1\n  Gregory             1        1           1       1           1            1\n  LadyCapulet         1        2           7       1          10            1\n  LadyMontague        1        1           1       1           1            1\n  LordMontague        1        2           2       1           3            1\n  PrinceEscalus       1        2           2       1           3            1\n  Romeo               1        7           5       1           4            1\n  Sampson             1        1           1       1           1            1\n               features\nfeatures        LordMontague PrinceEscalus Romeo Sampson\n  Abraham                  1             1     1       1\n  Benvolio                 2             2     7       1\n  LordCapulet              2             2     5       1\n  Gregory                  1             1     1       1\n  LadyCapulet              3             3     4       1\n  LadyMontague             1             1     1       1\n  LordMontague             3             3     3       1\n  PrinceEscalus            3             3     3       1\n  Romeo                    3             3    14       1\n  Sampson                  1             1     1       1\n[ reached max_feat ... 13 more features, reached max_nfeat ... 13 more features ]\n\n\nAs the quanteda package has a very neat and easy to use function (textplot_network) for generating network graphs, we make use this function and can directly generate the network.\n\nquanteda.textplots::textplot_network(romeo_fcm, min_freq = 0.1, edge_alpha = 0.1, edge_size = 5)\n\n\n\n\n\n\n\n\nThe thickness of the lines indicates how often characters have co-occurred. We could now generate different network graphs for the personas in different plays to see how these plays and personas differ or we could apply the network analysis to other types of information such as co-occurrences of words.\nWe have reached the end of this tutorial. Please feel free to explore more of our content at https://slcladal.github.io/index.html - for computational literary stylistics, especially the tutorials on part-of-speech tagging and syntactic parsing as well as on lexicography with R provide relevant additional information."
  },
  {
    "objectID": "tutorials/litsty/litsty.html#footnotes",
    "href": "tutorials/litsty/litsty.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [bibliography file](/assets/bibliography.bib and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/introquant/introquant.html#errare-humanum-est",
    "href": "tutorials/introquant/introquant.html#errare-humanum-est",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Errare Humanum Est",
    "text": "Errare Humanum Est\nAll of the above indicates that we need empirical evidence to determine the rules and laws governing empirical reality, but it does not tell us why we need science. As we have established above, only empirical evidence can lead the way to determining what the characteristics of the world we live in are. Science comes in as we, as human beings, are not very good at seeing and understanding the world as it is (in fact, we are not able to do so!). To elaborate, when it comes to things we are scared of, we rely on emotional narratives rather than facts about what can really do us harm. For instance, we are more scared of what is sometimes referred to as stranger danger, i.e. that someone unknown will harm us or people dear to us, than of people we know, even though most murders and sexual exploits are committed by people known to the victim, such as family and friends. Another example is represented by the movie Jaws - a thriller about cows or mosquitoes would probably not be so popular, even though both kill more people than sharks (mosquitoes are in fact among the most dangerous animals for humans due to the diseases they transmit).\nBesides being more strongly influenced by emotional narratives than facts, there are other in-built biases such as our drive to stick to our existing opinions rather than correcting them or switching sides once these are proven incorrect. In other words, we seek confirmation for our beliefs rather than challenging them. For example,"
  },
  {
    "objectID": "tutorials/introquant/introquant.html#the-monty-hall-problem",
    "href": "tutorials/introquant/introquant.html#the-monty-hall-problem",
    "title": "Introduction to Quantitative Reasoning",
    "section": "The Monty Hall Problem",
    "text": "The Monty Hall Problem\n\n\n\n\n\nAnother reason why we need science, i.e. a methodological approach to evaluating evidence, is that we are simply bad with numbers. Take the famous Monty Hall example:\nMonty Hall was the host of the TV game show Let’s make a deal, in which every participant was given the choice between three doors - behind two of them there was a goat while the other door hid a prize.\n\n\n\n\n\n\n\n\n\nThe game in the show went like this:\n\n\n\n\n\n1. Every participant would choose one of three doors.\n\n\n\n2. After the participant had chosen a door, Monty Hall would show the participant a goat behind one of the doors not chosen by the participant.\n\n\n\n3. Monty would then offer the participant the option to switch doors (to the other door not initially selected).\n\n\n\n\nNow, what do you think - should you switch doors at that point or does it not make a difference?\nWell, in fact, you should switch doors and it does make a difference but it is really difficult for us humans to wrap our heads around it. The chances of winning the prize actually increase when you switch (you can check it out yourselves when you go to this website which simulates many scenarios and confirms that after switching your chance of winning increases to 2 in 3).\n\n\n\n\n\n\n\nBut let’s go over why you should switch together!\n\n\n\n\n\nInitially, all three doors had a chance of 1 in 3 of hiding the prize. When you chose your door initially, you thus had a 1 in 3 chance of winning the prize while the other doors together had a 2 in 3 chance of winning.\n\n\n\nWhen Monty Hall opens one door (he will always open a door behind which there is a goat), the 2 in 3 chance concentrate on the door that is left unopened.\n\n\n\n\n\n\n\n\n\n\n\nTo make it clearer: imagine Monty Hall gave you 20 doors to choose from.\n\n\n\n\n\nYou pick one (with a 1 in 20 chance of winning) while the other 19 doors combined have a 19 out of 20 chance to win.\n\n\n\nNext, Monty Hall opens 18 out of the 19 doors you have not initially selected and asks you whether you want to switch to the one unopened door.\n\n\n\n\nIn this case, you would definitely switch, as it is more intuitive for us to see that the chances of winning concentrate on the door that Monty leaves unopened and which we did not initially select."
  },
  {
    "objectID": "tutorials/introquant/introquant.html#the-birthday-conundrum",
    "href": "tutorials/introquant/introquant.html#the-birthday-conundrum",
    "title": "Introduction to Quantitative Reasoning",
    "section": "The Birthday Conundrum",
    "text": "The Birthday Conundrum\nAnother interesting way to show that we are not good with numbers relates to the so-called Birthday Conundrum. The birthday conundrum deals with exponential growth and it can best be exemplified by the following scenario: imagine you are in a class consisting of 23 students.\n\n\n\n\nThink Break!\n\n\n\n\n\n\n`\n\nHow likely is it that 2 students have the same birthday in a class of 23 students?\n\n\n\nIn a room of just 23 people there’s a 50-50 chance of two people having the same birthday. The vast majority of people will intuitively come up with much higher numbers of students that are required to find a pair of students that have the same birthday. But why is that?\nWell, we are quite good with our numeric intuition when it comes to simple addition, subtraction, and multiplication but we are really bad when it comes to exponential growth.\nLet us go back to our problem: What are the chances that two students in a class of 23 have their birthday on the same day?\nWe could list all possible couples and count all the ways they could fit but that would be really labour-intensive. In fact, it would be the same as asking What’s the chance to get one or more heads in 23-coin flips?.\nIs there an easier way to solve the coin-flip and the birthday problem?\nYes, there is. We can turn the problem on its head: instead of determining the likelihood of each and every path to get heads, we simply calculate the chance to get only tails (or all separate birthdays).\nIf there’s a 1% chance of getting only tails (it is, in fact, \\(.5^23\\) to be precise but let’s stick with 1 percent for the sake of simplicity here), there’s a 99% chance of having at least one head. We do not know if it’s only 1 head or 2 or 15 or 23: there is at least once head, and that is what matters here. If we subtract the probability of the negative of our problem from 1, we get the probability of the scenario that we are interested in.\nThe same principle applies to birthdays. Instead of finding all possibilities, we find the chance that everyone has a different birthday (the negative scenario). We then determine the probability of the positive and have the chance of at least two people having the same birthday. It can be 1 pair, or 2 or 20, but at least two people have the same birthday.\nSo let’s go over how to calculate the actual probability.\nn = 23 days_in_year = 365\nThe chance of 2 people having different birthdays is:\n\\[\\begin{equation}\n1 - \\frac{365}{365} * \\frac{364}{365} * \\frac{363}{365} * ... * \\frac{365-23}{365} =  1 - 0.4927028 = 0.5072972\n\\end{equation}\\]\nThe chance of getting a single miss, i.e. that two specific people have the same birthday, is very high (99.7260%), but when that chance is multiplied given all the possible pairs, the odds of two people having the same birthday decrease very fast. The probability of two students in a class of 23 have the same birthday is 0.5072972 (or approximately 50 percent).\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nWhat is the probability of two people having the same birthday in a class of 73 students?\n\n\n\nAnswer\n\n\n# Number of students\nn &lt;- 73\n\n# Total number of days in a year\ndays_in_year &lt;- 365\n\n# Calculate the probability that all students have different birthdays\nprob_all_different &lt;- prod((days_in_year - 0:(n - 1)) / days_in_year)\n\n# Calculate the probability that at least two students share the same   birthday\nprob_at_least_one_shared &lt;- 1 - prob_all_different\n\n# Print the result\nprob_at_least_one_shared\n\n[1] 0.9995608\n\n\nThe probability of 2 students having the same birthday in a class of 73 students is 0.9996."
  },
  {
    "objectID": "tutorials/introquant/introquant.html#fast-and-slow-thinking",
    "href": "tutorials/introquant/introquant.html#fast-and-slow-thinking",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Fast and Slow Thinking",
    "text": "Fast and Slow Thinking\nA similar point can be made by another example: assume that a ball and a bat together cost $1.10. The bat costs $1 more than the ball. What does the ball cost?\nMost students initially think that the ball costs 10c but this is not the correct answer. Do you know why? Well, if the ball costs 10c and the bat costs $1 more, then the bat would cost $1.10 and both together would cost $1.20 and not $1.10. The correct answer is of course 5c ($0.05 + $1.05 = $1.10).\nAccording to the psychologist Daniel Kahnemann, one possible explanation for this bias is that humans have two different ways of thinking: fast thinking which is very intuitive and quick and slow thinking which takes longer and is more deliberate. While fast thinking is typically a very economical way to decide, slow and deliberate is more precise but more expensive as it takes more time and effort. Science is essentially a method to approach problems that we would normally use fast thinking to resolve with slow and deliberate thought instead."
  },
  {
    "objectID": "tutorials/introquant/introquant.html#randomness",
    "href": "tutorials/introquant/introquant.html#randomness",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Randomness",
    "text": "Randomness\nA very powerful cognitive bias that underlies superstitious beliefs or behaviours is that we are very bad at dealing with randomness. In other words, we are prone to see causes or patterns although there may be none. In an experiment involving pigeons performed by the psychologist B. F. Skinner, Skinner provided pigeons at random intervals with food pallets. After a while, Skinner noticed that the pigeons exhibited unusual behaviours such as twirling around in circles or picking in a corner. According to Skinner, the behaviour that pigeons performed while receiving food was positively enforced. In other words, a pigeon thought that when it performed a certain behaviour, this would cause food to fall from the feeder. Something similar can be observed among athletes who stick to certain behaviours such as not shaving etc. because they didn’t shave when they last won. The underlying mechanism is that we assign a causal relationship between some behaviour and a certain outcome although the behaviour and the outcome may be completely unrelated.\n\n\n\n\n\nPart 1 of a video about Skinner’s experiments with Pigeons.\n\n\n\n\n\n\n\n\n\nPart 2 of a video about Skinner’s experiments with Pigeons.\n\n\n\n\nA similar cognitive bias underlies many ghost sightings. We are prone to seeing faces or human figures in random patterns. For example, the famous picture of a face on Mars or faces on toast. Similarly, if you hear leaves turning over at night, it is likely that you think that someone is following you, as you assign noises rather to agents, such as people, rather than natural forces.\nBruce Hood, a psychologist at Cardiff University, offers a very interesting evolutionary explanation for this phenomenon. To elaborate, imagine a professor offered you $10 to wear a jumper he brought along for only a minute or so. Most people would take the offer and earn the $10. However, the professor adds that the jumper belonged to a brutal psychopathic serial killer and asks again whether you would wear the jumper. While some students would still wear the jumper, some students would not wear it given this information and even students who would may state that they would feel less comfortable. The underlying mechanism is that we assume that the jumper is not merely a piece of cloth but that it has changed somehow and acquired something by having been worn by a brutal psychopathic serial killer. While this is completely natural, it is irrational as the jumper is, in fact, merely a piece of cloth. Hood explains that people often assign value to material things, which goes to say that people treat objects not only as material consisting of atoms but as things that have something like an essence or a soul. The underlying mechanism, he hypothesises, adds an evolutionary advantage as people who had this belief were less likely to get close to items or people suffering from diseases and were thus less likely to get infected. This goes to show that irrational thinking or responses can be grounded in rational behaviour.\nIn addition to being generally bad with numbers and assuming agents rather than natural causes, there is another bias, called confirmation bias, which is an inbuilt hindrance to accurate knowledge. Let’s turn to another example to illustrate this. This example is called the Wason Selection Task after Peter Wason who came up with this test. Imagine you are presented with cards that have letters on one side and numbers on the other. Four of these cards are placed on a table before you. Card 1 is an A, card 2 is a K, card 3 is a 2, and card 4 is a 7. You are told that whenever a vowel is on one side of the card, the other side is an even number. Which of these four cards do you have to turn over to determine whether this rule holds true?\nCard 1Card 2Card 3Card 4AK27\nWhat have you guessed? The most common answer is cards 1 and 2 while the correct answer is actually cards 1 and 4 because the rule does not say whether there are even or odd numbers behind consonants; thus, turning over cards 2 and 3 does not help you in determining whether the rule holds true or not - in fact, cards 2 and 3 are irrelevant for the problem.\nLet us now turn to another example to further illustrate cognitive bias: Imagine that I have a rule in my mind and write down 3 numbers which are generated in accordance with my rule. Your task is to find out what the rule that I have in mind is. To help you find out, you are allowed to propose a fourth number and I have to answer whether the proposed number is aligned with my rule or not. After proposing a number, you may then propose what the rule is and I have to tell you whether the proposed rule is the rule I had in mind or not. The numbers I write down are 1, 2 and 4. What do you think the rule I have in mind is and which number would you propose?\nNumber 1Number 2Number 3Number 4124?\nTypically, students first propose 8 (which is in accordance with my rule) and the rule that is typically proposed first is Double the previous number. Unfortunately, this is not the rule according to which I generated the numbers. The next guess is typically 16 (which is also in accordance with my rule) and students propose the rule Square the previous number (which is again incorrect). It is only when students propose numbers that contradict their hypothesised rule that they get closer to finding the actual rule I have in mind.\nActually, the rule I have in mind is very simple as it is The current number must always be bigger than the previous number. This is to show that we intuitively test whether our hypothesised rule is correct, rather than testing whether it is false. A well-reasoned proposal would thus be numbers like 3 or 7, which conflict with the hypothesised rule, rather than numbers that comply with it.\nThis goes to show that we, as humans, aim to support ideas we already have rather than testing our beliefs. Science, however, does exactly the opposite: in the scientific process, ideas, hypotheses and theories are challenged. Support for ideas or theories comes from failed attempts to disprove them rather than from findings which support them.\nWhy have we had a look at these examples and quizzes? Basically, the intention here was to convince you that we as humans do not necessarily come to rational conclusions but that there are in-built mechanisms that systematically lead us astray and cause us to misjudge phenomena. It is important to understand that these biases often have a rational cause but that they are (a) part of human nature and (b) that they are constantly at work and thus constantly lead us astray. And it is here where the scientific method comes in as the scientific method is simply a procedure which prevents us from coming to erroneous conclusions."
  },
  {
    "objectID": "tutorials/introquant/introquant.html#the-anthropocentric-bias",
    "href": "tutorials/introquant/introquant.html#the-anthropocentric-bias",
    "title": "Introduction to Quantitative Reasoning",
    "section": "The Anthropocentric Bias",
    "text": "The Anthropocentric Bias\nAnother bias that we as humans are rarely aware of is the anthropological bias. The anthropological bias refers to conceptualising the world around us in a way that assumes that the world is just as we as humans see the world. Another name for this bias is Experiential Realism. To exemplify what this means, think about what the world would look like for a bee of we were the size of microbes. Bees see ultraviolet - a wavelength of light that we, as humans, cannot directly perceive. This means that the world looks very different for bees. Many flowers have evolved to be seen by bees as they rely on bees to spread their pollen. For a bee, a summer meadow looks something like the dark night’s sky does for us: a dark blue background with islands of light that are the flowers that reflect ultraviolet radiation (see below).\n\n\n\n\n\n\n\n\n\nVyvyan Evans and Melanie Green describe the anthropological bias and its origin as follows:\n\nHowever, the parts of this external reality to which we have access are largely constrained by the ecological niche we have adapted to and the nature of our embodiment. In other words, language does not directly reflect the world. Rather, it reflects our unique human construal of the world: our ‘world view’ as it appears to us through the lens of our embodiment.\n\n(Evans and Green, 46)\n\n\nThere are other examples which show that our perception of the world depends on our senses and how they function. Have a look at the experiment below.\n\n\n\n\nExperiment Time!\n\n\n\n\n\n\n`\n\nLook at the dot between the red and green squares for about 30 seconds straight (try to really focus on that white dot!). Once the 30 seconds are over, immediately look at the white dot between the mirrored images of the sand dunes. What happens?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left sand dunes appear greenish while the right sand dunes look red. This is because the neuronal networks that are responsible for making us see red and green have run out of neurotransmitters. As the neurons cannot fire anymore, we see the sand dunes without red on one side (which makes the dune appear green) and without green on the other side (which makes the dune appear red).\nThis example should just draw your attention to the fact that our perception of the world rests upon our senses and our brain.\nAnother factor which influences the way we perceive the world has to do with something that is called Gestalt and has been studied in subfield of psychology (Gestalt-psychology or Gestalt theory). Gestalt theory has to do with how we perceive shapes by grouping or lumping (unrelated) elements together. Consider the Figure below. Do you see the triangle?\n\n\n\n\n\n\n\n\n\nWe cannot help but see a triangle, although there is no triangle. We simply fill in the blanks and groups the triangles together so that they form a Gestalt - a shape. This becomes obvious if we re-arrange the triangles because, now, you see Pacman!"
  },
  {
    "objectID": "tutorials/introquant/introquant.html#patterns-and-matters-of-habit",
    "href": "tutorials/introquant/introquant.html#patterns-and-matters-of-habit",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Patterns and Matters of Habit",
    "text": "Patterns and Matters of Habit\nRelated to but different from the phenomena with Gestalt theory is another interesting way in which our mind manipulates the input to our senses. Have a look at the upside-down faces of Margaret Thatcher below. Does one face strike you as odd?\n\n\n\n\n\n\n\n\n\nProbably the picture of Margaret Thatcher on the right strikes you as somewhat odd but have a look at what happens if we turn the pictures around.\n\n\n\n\n\n\n\n\n\nMost people are somewhat shocked how distorted the picture on the right really is. This is because our brain compensates the skewness of the Margaret Thatcher’s face in the right picture while it does not normalise the picture when shown in the way that we normally see people. The interesting point is that our brain automatically adjusts the picture of a face based on our previous experience with faces. One could say that our perception - especially of human faces - autocorrects aspects of our visual input based on patterns and expectations build from previous input.\n\nThe autocorrection effect also plays on another bias which has to do with face recognition. Because reading and interpreting faces is and has been so important and informative for us, we are prone not only to see or find patterns in randomness but more specifically we are prone to seeing or constructing faces from ambiguous input. To see what I mean, have a look at the picture below.\nAlthough the picture does not show a face, we interpret the various elements in the picture to form - in combination - a face. Of course, the face is not emerging from a merely chaotic assemblies of elements but the elements in the picture are arranged to create that effect. And although the arrangement is deliberate in the picture above, it still goes to show that humans are particularly available for suggestive, ambiguous input to be interpreted as human faces. One reason for this is that the context, here understood as the combinatorial effect of various unrelated elements, influences our perception and interpretation of visual input.\nLet me clarify this with another example. What symbol do you see in the red circle?\n\n\n\n\n\n\n\n\n\nGiven that context, with the letters A and C to the left and right of the symbol, the most natural interpretation of the symbol is, of course, a B. However, if the context changes (as you can see below), the interpretation changes with it although the symbol and all its features remain the same.\n\n\n\n\n\n\n\n\n\nThis goes to show that our perception and interpretation of the world around us is not solely based on the input or the element itself but rather that categorisation is context-dependent."
  },
  {
    "objectID": "tutorials/introquant/introquant.html#logical-fallacies-and-biases",
    "href": "tutorials/introquant/introquant.html#logical-fallacies-and-biases",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Logical Fallacies and Biases",
    "text": "Logical Fallacies and Biases\nIn this section, we will have a look at things we do or ways we argue that will prevent us from finding out what is really going on. These logical fallacies are very common and no one is exempt from them - we should however, be aware of them and aim to avoid them if we do not want to arrive at wrong conclusions or make erroneous judgments.\nLogical fallacies can be defined as flawed, deceptive, or false arguments that can be proven wrong with reasoning and we will briefly inspect some of the the most common logical fallacies below.\nConfirmation Bias and Cherry Picking\nAs creatures of habit, we focus on certain things while we ignore others. This is quite helpful to orient ourselves in a highly complex world but it is unfortunately quite detrimental to the scientific endeavour. This is particularly so as we tend to ignore things or ideas that do not match our expectations. This is called confirmation bias or cherry picking and it is very common - even in science. In order to avoid this logical fallacy, rather than finding evidence that supports our assumptions and views, we need to actively look for evidence that shows that our assumptions are wrong. But while this is the better methodological approach towards finding real answers, it goes against how we intuitively operate. Unfortunately, seeking to confirm one’s view is not only human but leads to deeply misguided conclusions.\nAd Hominem\nAn ad hominem is when, instead of providing a (factual) counterargument or pointing out problems or inconsistencies in an argument, the person bringing forward the argument is attacked - often using slurs such as racist, lefty, Nazi, or the like. As such, the ad hominem fallacy represents the tendency to use personal attacks rather than logic to counter an argument. Ad hominem are also sometimes referred to as mudslinging in public discourse and they are manipulative in that they guide the attention away from the arguments to the personal and emotional level without addressing core issues.\nAppeal to Authority\nAn appeal to authority is a fallacy when someone refers to an authoritative figure as a justification or in support of an argument rather than explaining what this person has argued, found out, or stated. However, this fallacy is tricky because an authority’s stance can represent evidence, but it becomes a fallacy if the person rather than what they showed is used as a justification. An authority should only be referred to as a stand-in for research or a study which that person represents or has conducted - the person itself is typically irrelevant to the topic or the argument.\nAn example of this fallacy would be if someone said X has said that Y is the case and assumes that Y is true because the authority, X, has said so. Instead, the study or research conducted by X is the cause for Y being likely true, rather than X merely saying so.\nStraw Man\nThe straw man fallacy occurs when a position is misrepresented so that it is easier to argue against it and show it to be a flawed stance. The purpose of this fallacy is to make one’s own position appear superior or stronger than it actually is.\nThe straw man fallacy derives its name from harmless, lifeless straw men - scarecrows. Instead of arguing against the position an opponent actually holds, an easily defeated puppet version of the opponent’s position is attacked, even though they were never arguing for it in the first place.\nArgument from Ignorance\nThe argument from ignorance occurs when it is argued that a proposition must be true because it has not been proven to be false or if there is no evidence against it. An example of the argument from ignorance would be if someone posed No one has ever been able to prove that extraterrestrials exist, so they must not be real or No one knows how the world came into being, therefore God must be the cause. An appeal to ignorance doesn’t prove anything and only shifts the burden of proof away from the person making a claim.\nFalse Dichotomy\nA false dilemma or false dichotomy presents limited options — typically by focusing on two extremes — when in fact more possibilities exist. The phrase “America: Love it or leave it” is an example of a false dilemma.\nThe false dilemma fallacy is a manipulative tool designed to polarise the audience, promoting one side and demonising another. It’s common in political discourse as a way of strong-arming the public into supporting controversial legislation or policies.\nSlippery Slope\nA slippery slope argument assumes that a certain course of action will necessarily lead to a chain of future events. The slippery slope fallacy takes a benign premise or starting point and suggests that it will lead to unlikely or ridiculous outcomes with no supporting evidence.\nYou may have used this fallacy on your parents as a teenager: “But you have to let me go to the party! If I don’t go to the party, I’ll be a loser with no friends. Next thing you know, I’ll end up alone and jobless, living in your basement when I’m 30!”\nCircular Argument\nCircular arguments occur when a person’s argument repeats what they already assumed before without arriving at a new conclusion. For example, if someone says, “According to my brain, my brain is reliable,” that’s a circular argument.\nCircular arguments often use a claim as both a premise and a conclusion. This fallacy only appears to be an argument when in fact it’s just restating one’s assumptions.\nRed Herring\nWhen it comes to fallacies, a red herring is an argument that uses confusion or distraction to shift attention away from a topic and toward a false conclusion. Red herrings usually contain an unimportant fact, idea, or event that has little relevance to the real issue. Red herrings are very common and they are used when someone wants to shift the focus away from a topic or an argument to something that is easier or safer to address. As such, red herrings are related to shifting from rational to emotional - similar to ad hominem attacks.\nSunk Cost\nThe so-called sunk cost fallacy occurs when someone continues doing something because of the effort they have already put in - regardless of whether the additional costs outweigh the potential benefits. Sunk cost is a term borrowed from economy where it refers to expenses that can no longer be recovered. An example would be if you continue watching a TV show only because you have already watched some episodes although you are actually not enjoying watching the show - you just to go through with it so the initial costs of watching the TV shows was not “in vain”.\nFalling prey to cognitive biases or logical fallacies means that we are constantly deceiving ourselves and therefore need to protect ourselves from our own wrong judgments. To protect ourselves from these natural (systematic) delusions that have been discussed so far, we have developed sceptical or critical thinking skills. Among these skills, the hypothetic-deductive method of science is known as the scientific method . However, these skills are not just there - they have to be learned and trained! Such sceptical skills form the basis of science because science, in its essence, is the search for answers about how the world really works. And in order to avoid being led astray, we have to safeguard us from our own biases by following a methodological and careful approach. The next section will focus on the relationship between science and why it must be methodological.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nGiven what you have learned in this chapter, can you come up with explanations for a belief in ghosts?\n\n\n\nAnswer\n\nExperiences of ghosts (or what people experience as ghosts) can be caused by many factors, for example, pareidolia which can cause people to interpret random images, or patterns of light and shadow, as faces.\n\n\nSomeone tells you that his grandfather smoked a pack of cigarettes and drank a bottle of whiskey every day and thus claims that smoking and drinking does not harm your health. What is problematic about the conclusion and why is anecdotal evidence not appropriate?\n\n\n\nAnswer\n\nWhen aiming to answer how one factor (e.g., smoking) affects another factor (e.g., developing cancer), we are typically interested in general patterns or trends rather than unusual or unrepresentative cases. As such, individual anecdotes are not useful to answer questions asking for the relationship between factors, because we do not know how common or prototypical the anecdotal case is. Even many anecdotes do not qualify as data as anecdotes are likely biased and not representative. To determine the relationship between factors, we would need systematic and unbiased observation ."
  },
  {
    "objectID": "tutorials/introquant/introquant.html#clever-hans",
    "href": "tutorials/introquant/introquant.html#clever-hans",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Clever Hans",
    "text": "Clever Hans\nSo, what is science? Well, let’s start with the example of Clever Hans to illustrate how science is applied to phenomena.\nClever Hans was a horse who responded to questions requiring mathematical calculations by tapping his hoof. If asked by his master, William Von Osten, what is the sum of 3 plus 2, the horse would tap his hoof five times. It appeared the animal was responding to human language and was capable of grasping mathematical concepts. It was 1891 when Hans became public but only in 1904 it was discovered by Oskar Pfungst that the horse was responding to subtle physical cues. Yet, more than a dozen scientists had observed Hans and were convinced there was no signalling or trickery. But the scientists were wrong.\nPfungst noted that when the correct answer was not known to anyone present, Clever Hans didn’t know it either. And when the horse couldn’t see the person who did know the answer, the horse didn’t respond correctly. This led Pfungst to conclude that the horse was getting visual cues, albeit subtle ones. It turned out that Von Osten and others were cuing Hans unconsciously by tensing their muscles until Hans produced the correct answer. The horse truly was clever, not because he understood human language but because he could perceive very subtle muscle movements.\nThe fact that it took a methodological and very careful approach to find out why the horse appeared to be able to do math is very telling. So, we use science because it effectively protects us from being deceived by others (which would be bad) and, more importantly, by ourselves (which is much worse). The consistent application of the scientific method not only brings insight into how the world really works but it allows us to find ways in which the world around us can then be used to our advantage (fire, wheel, agriculture, magnetism, steam engine, telephone, microwave, nuclear fusion, etc.),\nSo, what is science?\n\nScience is an unbiased, fundamentally methodological enterprise that aims at building and organising knowledge about the empirical world in the form of falsifiable explanations and predictions by means of observation and experimentation. (MS)\n\n\nScience is the effort to understand how the universe works through the scientific method, with observable evidence as the basis of that understanding.\n\n\n\n\n\n\n\n\n\n\nWhich varieties of science are there?\n\nEmpirical Science(s) examine phenomena of reality through the scientific method (cf.), with the aim of explaining and / or predicting them (for example, functional linguistics, biology, astronomy, sociology, …).\nFormal Science(s) examine systems of abstract constructs using axiomatically set or derived rules (for example, mathematics, formal logic, theoretical computer science, system theory, chaos theory, formal linguistics, …)."
  },
  {
    "objectID": "tutorials/introquant/introquant.html#popper-falsification-and-scientific-progress",
    "href": "tutorials/introquant/introquant.html#popper-falsification-and-scientific-progress",
    "title": "Introduction to Quantitative Reasoning",
    "section": "Popper, falsification, and scientific progress",
    "text": "Popper, falsification, and scientific progress\n\nSir Karl Raimund Popper was an Austrian-British academic and public figure who vigorously defended liberal democracy and the principles of social criticism that he believed made a flourishing open society possible. In the context of scientific thinking, Popper is important because he is arguably the most important philosopher of science - probably best known for his realisation that nothing can be proven in the empirical sciences, but that hypotheses can and need to be falsified. This means that in contrast to the formal sciences, where statements can be proven once certain axioms are accepted, any knowledge that is produced by the empirical sciences is always, and has to be, preliminary which implies that the empirical sciences represent an ongoing process.\n\n\n\n\nWhat does this mean and why can empirical sciences not prove?\n\n\n\n\n\n\nWell, logically speaking, no number of observations of something can confirm a scientific theory. However, a single counterexample can be enough to show that a theory is false. Imagine someone states that all swans are white. Any number of white swans cannot prove that that statement is correct, but a single black swan immediately falsifies the statement.\nIt is important to note here that to say that something is falsifiable does not mean that it is false or wrong or fake: it simply means that it can, in principle, be shown to be false by observation or by experiment.\nAccording to Popper, falsifiability is the defining criterion of what is and what is not science: a theory can be considered scientific if, and only if, it is falsifiable. This led him to refute claims that both psychoanalysis and Marxism are scientific because these theories are not falsifiable.\nThere is an apparent progress of scientific knowledge, meaning that our understanding of the universe appears to be improving over time. In Popper’s view, this advancement of scientific knowledge is an evolutionary process: competing theories are systematically subjected to attempts at falsification which leads to error elimination - so that falsification performs a similar function for science that natural selection performs for biological evolution. Theories that better survive the process of refutation are not more true, but rather, more fit. Consequently, just as a species’ biological fitness does not ensure continued survival, neither does rigorous testing protect a scientific theory from refutation in the future. Yet, as it appears that the engine of biological evolution has, over many generations, produced adaptive traits equipped to deal with more and more complex problems of survival, likewise, the evolution of theories through the scientific method may progress. For Popper, it is in the interplay between the tentative theories (conjectures) and error elimination (refutation) that scientific knowledge advances - in a process very much akin to the interplay between genetic variation and natural selection."
  },
  {
    "objectID": "tutorials/introquant/introquant.html#what-is-empirical-linguistics",
    "href": "tutorials/introquant/introquant.html#what-is-empirical-linguistics",
    "title": "Introduction to Quantitative Reasoning",
    "section": "What is (empirical) Linguistics?",
    "text": "What is (empirical) Linguistics?\nLinguistics is commonly defined as the the scientific study of language or individual languages and linguists try to uncover the systems behind language, to describe these systems, and to theoretically explain and model them (Evans and Green). Linguists who work empirically conduct research on linguistic phenomena based on observations of reality. As such, (empirical) linguistics is descriptive rather than prescriptive in nature.\n\n\n\nHow does the scientific method work in linguistics?\n\n\n\n\n\n\nEmpirical research typically follows the scheme described below. This scheme is also referred to as the scientific circle and we will use the example of having lost your keys to exemplify how it works.\nSo, let’s imagine you lost your key: in a first step we make an observation (keys are not here). In a second step, we ask ourselves where the keys may be (research question). In a third step, we come up with an idea where the keys may be (hypothesis). Then, we think about where we have lost the keys before (literature review). Next, we look for the keys where we expect them to be (empirical testing). Then, we evaluate the result of the test (was hypothesis correct), and finally, we either have found the keys (hypothesis was correct) or not (keys are still missing), which causes us to come up with another idea and we need to go through the same steps again.\nA slightly more elaborate depiction of this scenario with the equivalent steps in the scientific circle is listed below.\n\nMake an observation (e.g., My keys are gone!)\nFormulate a research question (e.g., Where are my keys?)\nDeduce a test hypothesis (H1) based on observation (e.g. My keys are on the table next to the TV!)\nFormulate null hypothesis (H0) (e.g., My keys are not on the table next to the TV!)\nDetermining the level of significance at which the H0 is rejected\nFormulate potential results: what results are possible and what do they mean for the H0 and H1? (My keys are not on the table next to the TV!: H0 cannot be rejected, formulate new H1)\nDesign experiment/study/research (e.g., I will go over to the TV and see if my keys on the table next to the TV.)\nConduct experiment/study/research (e.g., Actually go over to the TV and see if my keys on the table next to the TV.)\nStatistical analysis\nInterpretation of the results (e.g., My keys are not on the table next to the TV so I must have lost them elsewhere!)\nIn case H0 could not be rejected: Formulate new H1. (e.g., My keys are on the kitchen table!)\n\n\n\n\n\nThink Break!\n\n\n\n\n\n\n`\n\nApply the scientific circle to a study of the existence of the Loch Ness monster.\nYou want to investigate whether the speech of young or old people is more fluent: how could you go about testing this?\n\n\n\nWe will stop here with our introduction to quantitative reasoning. If you are interested in learning more, we highly recommend that you continue with our tutorial on basic concepts in quantitative research."
  },
  {
    "objectID": "tutorials/svm/svm.html#footnotes",
    "href": "tutorials/svm/svm.html#footnotes",
    "title": "Semantic Vector Space Models in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am indebted to Paul Warren who kindly pointed out some errors in a previous version of this tutorial.↩︎"
  },
  {
    "objectID": "tutorials/intror/intror.html#goals-of-this-tutorial",
    "href": "tutorials/intror/intror.html#goals-of-this-tutorial",
    "title": "Getting started with R",
    "section": "Goals of this tutorial",
    "text": "Goals of this tutorial\nThe goals of this tutorial are:\n\nHow to get started with R\nHow to orient yourself to R and RStudio\nHow to create and work in R projects\nHow to know where to look for help and to learn more about R\nUnderstand the basics of working with data: load data, save data, working with tables, create a simple plot\nLearn some best practices for using R scripts, using data, and projects\nUnderstand the basics of objects, functions, and indexing"
  },
  {
    "objectID": "tutorials/intror/intror.html#audience",
    "href": "tutorials/intror/intror.html#audience",
    "title": "Getting started with R",
    "section": "Audience",
    "text": "Audience\nThe intended audience for this tutorial is beginner-level, with no previous experience using R. Thus, no prior knowledge of R is required.\nIf you want to know more, would like to get some more practice, or would like to have another approach to R, please check out the workshops and resources on R provided by the UQ library. In addition, there are various online resources available to learn R (you can check out a very recommendable introduction here)."
  },
  {
    "objectID": "tutorials/intror/intror.html#installing-r-and-rstudio",
    "href": "tutorials/intror/intror.html#installing-r-and-rstudio",
    "title": "Getting started with R",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\n\nYou have NOT yet installed R on your computer?\n\nYou have a Windows computer? Then click here for downloading and installing R\nYou have a Mac? Then click here for downloading and installing R\n\nYou have NOT yet installed RStudio on your computer?\n\nClick here for downloading and installing RStudio.\n\n\nYou can find a more elaborate explanation of how to download and install R and RStudio here that was created by the UQ library."
  },
  {
    "objectID": "tutorials/intror/intror.html#folder-structure-and-r-projects",
    "href": "tutorials/intror/intror.html#folder-structure-and-r-projects",
    "title": "Getting started with R",
    "section": "Folder Structure and R projects",
    "text": "Folder Structure and R projects\nBefore actually starting with writing code, you should prepare the session by going through the following steps:\n\n1. Create a folder for your project\nIn that folder, create the following sub-folders (you can, of course, adapt this folder template to match your needs)\n\ndata (you do not create this folder for the present workshop as you can simply use the data folder that you downloaded for this workshop instead)\nimages\ntables\ndocs\n\nThe folder for your project could look like the the one shown below.\n\nOnce you have created your project folder, you can go ahead with RStudio.\n\n\n2. Open RStudio\nThis is what RStudio looks like when you first open it:\n\nIn RStudio, click on File\n\nYou can use the drop-down menu to create a R project\n\n\n3. R Projects\nIn RStudio, click on New Project\n\nNext, confirm by clicking OK and select Existing Directory.\nThen, navigate to where you have just created the project folder for this workshop.\n\nOnce you click on Open, you have created a new R project\n\n\n4. R Notebooks\nIn this project, click on File\n\nClick on New File and then on R Notebook as shown below.\n\nThis R Notebook will be the file in which you do all your work.\n\n\n5. Updating R\nIn case you encounter issues when opening the R Notebook (e.g., if you receive an error message saying that you need to update packages which then do not install properly), you may have to update your R version.\nTo update your current R version to the recent release please copy the code chunk shown below into the console pane (the bottom left pane) and click on Enter to run the code. The code will automatically update your version of R to the most recent release. During the update, you may be asked to specify some options - in that case, you can simply click on Accept and Next and accept the default settings.\n\n# install installr package\ninstall.packages(\"installr\")\n# load installr package\nlibrary(installr)\n# update r\nupdateR()\n\n\n\n6. Optimizing R project options\nWhen you work with projects, it is recommendable to control the so-called environment. This means that you make your R Project self-contained by storing all packages that are used in project in a library in the R Project (instead of in the general R library on your computer). Having a library in your R Project means that you can share your project folder wit other people and they will automatically have the same package versions that you have sued which makes your code more robust and reproducible.\nSo, how to create such an environment? You simply click on Tools (at the very top right of RStudio), then click onProject Options then click on Environments and then check Use renv with this project. Now, when you install packages, they will be installed in the package library (rather than the general R library on your computer).\n\n\n7. Getting started with R Notebooks\nYou can now start writing in this R Notebook. For instance, you could start by changing the title of the R Notebook and describe what you are doing (what this Notebook contains).\nBelow is a picture of what this document looked like when I started writing it.\n\nWhen you write in the R Notebook, you use what is called R Markdown which is explained below."
  },
  {
    "objectID": "tutorials/intror/intror.html#r-markdown",
    "href": "tutorials/intror/intror.html#r-markdown",
    "title": "Getting started with R",
    "section": "R Markdown",
    "text": "R Markdown\nThe Notebook is an R Markdown document: a Rmd (R Markdown) file is more than a flat text document: it’s a program that you can run in R and which allows you to combine prose and code, so readers can see the technical aspects of your work while reading about their interpretive significance.\nYou can get a nice and short overview of the formatting options in R Markdown (Rmd) files here.\nR Markdown allows you to make your research fully transparent and reproducible! If a couple of years down the line another researcher or a journal editor asked you how you have done your analysis, you can simply send them the Notebook or even the entire R-project folder.\nAs such, Rmd files are a type of document that allows to\n\ninclude snippets of code (and any outputs such as tables or graphs) in plain text while\nencoding the structure of your document by using simple typographical symbols to encode formatting (rather than HTML tags or format types such as Main header or Header level 1 in Word).\n\nMarkdown is really quite simple to learn and these resources may help:\n\nThe Markdown Wikipedia page includes a very handy chart of the syntax.\nJohn Gruber developed Markdown and his introduction to the syntax is worth browsing.\nThis interactive Markdown tutorial will teach you the syntax in a few minutes."
  },
  {
    "objectID": "tutorials/intror/intror.html#rstudio-panes",
    "href": "tutorials/intror/intror.html#rstudio-panes",
    "title": "Getting started with R",
    "section": "RStudio: Panes",
    "text": "RStudio: Panes\nThe GUI - Graphical User Interface - that RStudio provides divides the screen into four areas that are called panes:\n\nFile editor\nEnvironment variables\nR console\nManagement panes (File browser, plots, help display and R packages).\n\nThe two most important are the R console (bottom left) and the File editor (or Script in the top left). The Environment variables and Management panes are on the right of the screen and they contain:\n\nEnvironment (top): Lists all currently defined objects and data sets\nHistory (top): Lists all commands recently used or associated with a project\nPlots (bottom): Graphical output goes here\nHelp (bottom): Find help for R packages and functions. Don’t forget you can type ? before a function name in the console to get info in the Help section.\nFiles (bottom): Shows the files available to you in your working directory\n\nThese RStudio panes are shown below.\n\n\n\n\n\n\n\n\n\n\nR Console (bottom left pane)\nThe console pane allows you to quickly and immediately execute R code. You can experiment with functions here, or quickly print data for viewing.\nType next to the &gt; and press Enter to execute.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nYou can use R like a calculator. Try typing 2+8 into the R console.\n\n\n\nAnswer\n\n\n2 + 8\n\n[1] 10\n\n\n\n\n\nHere, the plus sign is the operator. Operators are symbols that represent some sort of action. However, R is, of course, much more than a simple calculator. To use R more fully, we need to understand objects, functions, and indexing - which we will learn about as we go.\nFor now, think of objects as nouns and functions as verbs."
  },
  {
    "objectID": "tutorials/intror/intror.html#running-commands-from-a-script",
    "href": "tutorials/intror/intror.html#running-commands-from-a-script",
    "title": "Getting started with R",
    "section": "Running commands from a script",
    "text": "Running commands from a script\nTo run code from a script, insert your cursor on a line with a command, and press CTRL/CMD+Enter.\nOr highlight some code to only run certain sections of the command, then press CTRL/CMD+Enter to run.\nAlternatively, use the Run button at the top of the pane to execute the current line or selection (see below).\n\n\nScript Editor (top left pane)\nIn contrast to the R console, which quickly runs code, the Script Editor (in the top left) does not automatically execute code. The Script Editor allows you to save the code essential to your analysis. You can re-use that code in the moment, refer back to it later, or publish it for replication.\nNow, that we have explored RStudio, we are ready to get started with R!"
  },
  {
    "objectID": "tutorials/intror/intror.html#setting-up-an-r-session",
    "href": "tutorials/intror/intror.html#setting-up-an-r-session",
    "title": "Getting started with R",
    "section": "Setting up an R session",
    "text": "Setting up an R session\nAt the beginning of a session, it is common practice to define some basic parameters. This is not required or even necessary, but it may just help further down the line. This session preparation may include specifying options. In the present case, we\n\nwant R to show numbers as numbers up to 100 decimal points (and not show them in mathematical notation (in mathematical notation, 0.007 would be represented as 0.7e-3))\nwant R to show maximally 100 results (otherwise, it can happen that R prints out pages-after-pages of some numbers).\n\nAgain, the session preparation is not required or necessary but it can help avoid errors.\n\n# set options\noptions(stringsAsFactors = F)\noptions(scipen = 100)\noptions(max.print = 100)\n\nIn script editor pane of RStudio, this would look like this:"
  },
  {
    "objectID": "tutorials/intror/intror.html#packages",
    "href": "tutorials/intror/intror.html#packages",
    "title": "Getting started with R",
    "section": "Packages",
    "text": "Packages\nWhen using R, most of the functions are not loaded or even installing automatically. Instead, most functions are in contained in what are called packages.\nR comes with about 30 packages (“base R”). There are over 10,000 user-contributed packages; you can discover these packages online. A prevalent collection of packages is the Tidyverse, which includes ggplot2, a package for making graphics.\nBefore being able to use a package, we need to install the package (using the install.packages function) and load the package (using the library function). However, a package only needs to be installed once(!) and can then simply be loaded. When you install a package, this will likely install several other packages it depends on. You should have already installed tidyverse before the workshop.\nYou must load the package in any new R session where you want to use that package. Below I show what you need to type when you want to install the tidyverse, the tidytext, the quanteda, the readxl, and the tm packages (which are the packages that we will need in this workshop).\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"readxl\")\ninstall.packages(\"tm\")\ninstall.packages(\"tokenizers\")\ninstall.packages(\"here\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nTo load these packages, use the library function which takes the package name as its main argument.\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(quanteda)\nlibrary(readxl)\nlibrary(tm)\nlibrary(tokenizers)\nlibrary(here)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nThe session preparation section of your Rmd file will thus also state which packages a script relies on.\nIn script editor pane of RStudio, the code blocks that install and activate packages would look like this:"
  },
  {
    "objectID": "tutorials/intror/intror.html#getting-help",
    "href": "tutorials/intror/intror.html#getting-help",
    "title": "Getting started with R",
    "section": "Getting help",
    "text": "Getting help\nWhen working with R, you will encounter issues and face challenges. A very good thing about R is that it provides various ways to get help or find information about the issues you face.\n\nFinding help within R\nTo get help regrading what functions a package contains, which arguments a function takes or to get information about how to use a function, you can use the help function or the apropos. function or you can simply type a ? before the package or two ?? if this does not give you any answers.\n\nhelp(tidyverse)\napropos(\"tidyverse\")\n?require\n\nThere are also other “official” help resources from R/RStudio.\n\nRead official package documentation, see vignettes, e.g., Tidyverse https://cran.r-project.org/package=tidyverse\nUse the RStudio Cheat Sheets at https://www.rstudio.com/resources/cheatsheets/\nUse the RStudio Help viewer by typing ? before a function or package\nCheck out the keyboard shortcuts Help under Tools in RStudio for some good tips\n\n\n\nFinding help online\nOne great thing about R is that you can very often find an answer to your question online.\n\nGoogle your error! See http://r4ds.had.co.nz/introduction.html#getting-help-and-learning-more for excellent suggestions on how to find help for a specific question online."
  },
  {
    "objectID": "tutorials/intror/intror.html#loading-data-from-the-web",
    "href": "tutorials/intror/intror.html#loading-data-from-the-web",
    "title": "Getting started with R",
    "section": "Loading data from the web",
    "text": "Loading data from the web\nTo show, how data can be downloaded from the web, we will download a tab-separated txt-file. Translated to prose, the code below means Create an object called icebio and in that object, store the result of the read.delim function.\nread.delim stands for read delimited file and it takes the URL from which to load the data (or the path to the data on your computer) as its first argument. The sep stand for separator and the \\t stands for tab-separated and represents the second argument that the read.delim function takes. The third argument, header, can take either T(RUE) or F(ALSE) and it tells R if the data has column names (headers) or not."
  },
  {
    "objectID": "tutorials/intror/intror.html#functions-and-objects",
    "href": "tutorials/intror/intror.html#functions-and-objects",
    "title": "Getting started with R",
    "section": "Functions and Objects",
    "text": "Functions and Objects\nIn R, functions always have the following form: function(argument1, argument2, ..., argumentN). Typically a function does something to an object (e.g. a table), so that the first argument typically specifies the data to which the function is applied. Other arguments then allow to add some information. Just as a side note, functions are also objects that do not contain data but instructions.\nTo assign content to an object, we use &lt;- or = so that the we provide a name for an object, and then assign some content to it. For example, MyObject &lt;- 1:3 means Create an object called MyObject. this object should contain the numbers 1 to 3.\n\n# load data\nicebio &lt;- read.delim(\"tutorials/intror/data/BiodataIceIreland.txt\",\n  sep = \"\\t\", header = T\n)"
  },
  {
    "objectID": "tutorials/intror/intror.html#inspecting-data",
    "href": "tutorials/intror/intror.html#inspecting-data",
    "title": "Getting started with R",
    "section": "Inspecting data",
    "text": "Inspecting data\nThere are many ways to inspect data. We will briefly go over the most common ways to inspect data.\nThe head function takes the data-object as its first argument and automatically shows the first 6 elements of an object (or rows if the data-object has a table format).\n\nhead(icebio)\n\n  id file.speaker.id text.id spk.ref             zone      date    sex   age\n1  1     &lt;S1A-001$A&gt; S1A-001       A northern ireland 1990-1994   male 34-41\n2  2     &lt;S1A-001$B&gt; S1A-001       B northern ireland 1990-1994 female 34-41\n3  3     &lt;S1A-002$?&gt; S1A-002       ?             &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;\n4  4     &lt;S1A-002$A&gt; S1A-002       A northern ireland 2002-2005 female 26-33\n5  5     &lt;S1A-002$B&gt; S1A-002       B northern ireland 2002-2005 female 19-25\n6  6     &lt;S1A-002$C&gt; S1A-002       C northern ireland 2002-2005   male   50+\n  word.count\n1        765\n2       1298\n3         23\n4        391\n5         47\n6        200\n\n\nWe can also use the head function to inspect more or less elements and we can specify the number of elements (or rows) that we want to inspect as a second argument. In the example below, the 4 tells R that we only want to see the first 4 rows of the data.\n\nhead(icebio, 4)\n\n  id file.speaker.id text.id spk.ref             zone      date    sex   age\n1  1     &lt;S1A-001$A&gt; S1A-001       A northern ireland 1990-1994   male 34-41\n2  2     &lt;S1A-001$B&gt; S1A-001       B northern ireland 1990-1994 female 34-41\n3  3     &lt;S1A-002$?&gt; S1A-002       ?             &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;\n4  4     &lt;S1A-002$A&gt; S1A-002       A northern ireland 2002-2005 female 26-33\n  word.count\n1        765\n2       1298\n3         23\n4        391\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nDownload and inspect the first 7 rows of the data set that you can find under this URL: tutorials/intror/data/lmmdata.txt. Can you guess what the data is about?\n\n\n\nAnswer\n\n\nex1data &lt;- read.delim(\"tutorials/intror/data/lmmdata.txt\", sep = \"\\t\")\nhead(ex1data, 7)\n\n  Date         Genre    Text Prepositions Region\n1 1736       Science   albin       166.01  North\n2 1711     Education    anon       139.86  North\n3 1808 PrivateLetter  austen       130.78  North\n4 1878     Education    bain       151.29  North\n5 1743     Education barclay       145.72  North\n6 1908     Education  benson       120.77  North\n7 1906         Diary  benson       119.17  North\n\n\nThe data is about texts and the different columns provide information about the texts such as when the texts were written (Date), the genre the texts represent (Genre), the name of the texts (Text), the relative frequencies of prepositions the texts contain (Prepositions), and the region where the author was from (Region)."
  },
  {
    "objectID": "tutorials/intror/intror.html#accessing-individual-cells-in-a-table",
    "href": "tutorials/intror/intror.html#accessing-individual-cells-in-a-table",
    "title": "Getting started with R",
    "section": "Accessing individual cells in a table",
    "text": "Accessing individual cells in a table\nIf you want to access specific cells in a table, you can do so by typing the name of the object and then specify the rows and columns in square brackets (i.e. data[row, column]). For example, icebio[2, 4] would show the value of the cell in the second row and fourth column of the object icebio. We can also use the colon to define a range (as shown below, where 1:5 means from 1 to 5 and 1:3 means from 1 to 3) The command icebio[1:5, 1:3] thus means:\nShow me the first 5 rows and the first 3 columns of the data-object that is called icebio.\n\nicebio[1:5, 1:3]\n\n  id file.speaker.id text.id\n1  1     &lt;S1A-001$A&gt; S1A-001\n2  2     &lt;S1A-001$B&gt; S1A-001\n3  3     &lt;S1A-002$?&gt; S1A-002\n4  4     &lt;S1A-002$A&gt; S1A-002\n5  5     &lt;S1A-002$B&gt; S1A-002\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nHow would you inspect the content of the cells in 4th column, rows 3 to 5 of the icebio data set?\n\n\n\nAnswer\n\n\nicebio[3:5, 4]\n\n[1] \"?\" \"A\" \"B\"\n\n\n\n\n\nInspecting the structure of data\nYou can use the str function to inspect the structure of a data set. This means that this function will show the number of observations (rows) and variables (columns) and tell you what type of variables the data consists of\n\nint = integer\nchr = character string\nnum = numeric\nfct = factor\n\n\nstr(icebio)\n\n'data.frame':   1332 obs. of  9 variables:\n $ id             : int  1 2 3 4 5 6 7 8 9 10 ...\n $ file.speaker.id: chr  \"&lt;S1A-001$A&gt;\" \"&lt;S1A-001$B&gt;\" \"&lt;S1A-002$?&gt;\" \"&lt;S1A-002$A&gt;\" ...\n $ text.id        : chr  \"S1A-001\" \"S1A-001\" \"S1A-002\" \"S1A-002\" ...\n $ spk.ref        : chr  \"A\" \"B\" \"?\" \"A\" ...\n $ zone           : chr  \"northern ireland\" \"northern ireland\" NA \"northern ireland\" ...\n $ date           : chr  \"1990-1994\" \"1990-1994\" NA \"2002-2005\" ...\n $ sex            : chr  \"male\" \"female\" NA \"female\" ...\n $ age            : chr  \"34-41\" \"34-41\" NA \"26-33\" ...\n $ word.count     : int  765 1298 23 391 47 200 464 639 308 78 ...\n\n\nThe summary function summarizes the data.\n\nsummary(icebio)\n\n       id         file.speaker.id      text.id            spk.ref         \n Min.   :   1.0   Length:1332        Length:1332        Length:1332       \n 1st Qu.: 333.8   Class :character   Class :character   Class :character  \n Median : 666.5   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 666.5                                                           \n 3rd Qu.: 999.2                                                           \n Max.   :1332.0                                                           \n     zone               date               sex                age           \n Length:1332        Length:1332        Length:1332        Length:1332       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   word.count    \n Min.   :   0.0  \n 1st Qu.:  66.0  \n Median : 240.5  \n Mean   : 449.9  \n 3rd Qu.: 638.2  \n Max.   :2565.0"
  },
  {
    "objectID": "tutorials/intror/intror.html#tabulating-data",
    "href": "tutorials/intror/intror.html#tabulating-data",
    "title": "Getting started with R",
    "section": "Tabulating data",
    "text": "Tabulating data\nWe can use the table function to create basic tables that extract raw frequency information. The following command tells us how many instances there are of each level of the variable date in the icebio.\n\n\n\n\nTIP\n\n\n\n\n\n\n`\nIn order to access specific columns of a data frame, you can first type the name of the data set followed by a $ symbol and then the name of the column (or variable).\n\n\n\ntable(icebio$date)\n\n\n1990-1994 1995-2001 2002-2005 \n      905        67       270 \n\n\nAlternatively, you could, of course, index the column by using its position in the data set like this: icebio[, 6] - the result of table(icebio[, 6]) and table(icebio$date) are the same! Also note that here we leave out indexes for rows to tell R that we want all rows.\nWhen you want to cross-tabulate columns, it is often better to use the ftable function (ftable stands for frequency table).\n\nftable(icebio$age, icebio$sex)\n\n       female male\n                  \n0-18        5    7\n19-25     163   65\n26-33      83   36\n34-41      35   58\n42-49      35   97\n50+        63  138\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nUsing the table function, how many women are in the data collected between 2002 and 2005?\n\n\n\nAnswer\n\n\ntable(icebio$date, icebio$sex)\n\n           \n            female male\n  1990-1994    338  562\n  1995-2001      4   58\n  2002-2005    186   84\n\n\n\n\nUsing the ftable function, how many men are are from northern Ireland in the data collected between 1990 and 1994?\n\n\n\nAnswer\n\n\nftable(icebio$date, icebio$zone, icebio$sex)\n\n                                    female male\n                                               \n1990-1994 mixed between ni and roi      18   13\n          non-corpus speaker             7   22\n          northern ireland             104  289\n          republic of ireland          209  238\n1995-2001 mixed between ni and roi       0    0\n          non-corpus speaker             1    1\n          northern ireland               2   36\n          republic of ireland            1   21\n2002-2005 mixed between ni and roi      19    7\n          non-corpus speaker             7    9\n          northern ireland             122   41\n          republic of ireland           38   27"
  },
  {
    "objectID": "tutorials/intror/intror.html#saving-data-to-your-computer",
    "href": "tutorials/intror/intror.html#saving-data-to-your-computer",
    "title": "Getting started with R",
    "section": "Saving data to your computer",
    "text": "Saving data to your computer\nTo save tabular data on your computer, you can use the write.table function. This function requires the data that you want to save as its first argument, the location where you want to save the data as the second argument and the type of delimiter as the third argument.\n\nwrite.table(icebio, here::here(\"tutorials/intror/data\", \"icebio.txt\"), sep = \"\\t\")\n\nA word about paths\nIn the code chunk above, the sequence here::here(\"tutorials/intror/data\", \"icebio.txt\") is a handy way to define a path. A path is simply the location where a file is stored on your computer or on the internet (which typically is a server - which is just a fancy term for a computer - somewhere on the globe). The here function from thehere package allows to simply state in which folder a certain file is and what file you are talking about.\nIn this case, we want to access the file icebio (which is a txt file and thus has the appendix .txt) in the data folder. R will always start looking in the folder in which your project is stored. If you want to access a file that is stored somewhere else on your computer, you can also define the full path to the folder in which the file is. In my case, this would be D:/Uni/UQ/SLC/LADAL/SLCLADAL.github.io/data. However, as the data folder in in the folder where my Rproj file is, I only need to specify that the file is in the data folder within the folder in which my Rproj file is located.\nA word about package naming\nAnother thing that is notable in the sequence here::here(\"tutorials/intror/data\", \"icebio.txt\") is that I specified that the here function is part of the here package. This is what I meant by writing here::here which simply means use the here function from here package (package::function). This may appear to be somewhat redundant but it happens quite frequently, that different packages have functions that have the same names. In such cases, R will simply choose the function from the package that was loaded last. To prevent R from using the wrong function, it makes sense to specify the package AND the function (as I did in the sequence here::here). I only use functions without specify the package if the function is part of base R."
  },
  {
    "objectID": "tutorials/intror/intror.html#loading-data-from-your-computer",
    "href": "tutorials/intror/intror.html#loading-data-from-your-computer",
    "title": "Getting started with R",
    "section": "Loading data from your computer",
    "text": "Loading data from your computer\nTo load tabular data from within your project folder (if it is in a tab-separated txt-file) you can also use the read.delim function. The only difference to loading from the web is that you use a path instead of a URL. If the txt-file is in the folder called data in your project folder, you would load the data as shown below.\n\nicebio &lt;- read.delim(here::here(\"tutorials/intror/data\", \"icebio.txt\"), sep = \"\\t\", header = T)\n\nHowever, you can always just use the full path (and you must do this is the data is not in your project folder).\n\n\n\n\nNOTEYou may have to change the path to the data!\n\n\n\n\n\n\n\nicebio &lt;- read.delim(here::here(\"tutorials/intror/data\", \"icebio.txt\"),\n  sep = \"\\t\", header = T\n)\n\nTo if this has worked, we will use the head function to see first 6 rows of the data\n\nhead(icebio)\n\n  id file.speaker.id text.id spk.ref             zone      date    sex   age\n1  1     &lt;S1A-001$A&gt; S1A-001       A northern ireland 1990-1994   male 34-41\n2  2     &lt;S1A-001$B&gt; S1A-001       B northern ireland 1990-1994 female 34-41\n3  3     &lt;S1A-002$?&gt; S1A-002       ?             &lt;NA&gt;      &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;\n4  4     &lt;S1A-002$A&gt; S1A-002       A northern ireland 2002-2005 female 26-33\n5  5     &lt;S1A-002$B&gt; S1A-002       B northern ireland 2002-2005 female 19-25\n6  6     &lt;S1A-002$C&gt; S1A-002       C northern ireland 2002-2005   male   50+\n  word.count\n1        765\n2       1298\n3         23\n4        391\n5         47\n6        200"
  },
  {
    "objectID": "tutorials/intror/intror.html#loading-excel-data",
    "href": "tutorials/intror/intror.html#loading-excel-data",
    "title": "Getting started with R",
    "section": "Loading Excel data",
    "text": "Loading Excel data\nTo load Excel spreadsheets, you can use the read_excel function from the readxl package as shown below. However, it may be necessary to install and activate the readxl package first.\n\nicebio &lt;- readxl::read_excel(here::here(\"tutorials/intror/data\", \"ICEdata.xlsx\"))\n\nWe now briefly check column names to see if the loading of the data has worked.\n\ncolnames(icebio)\n\n[1] \"id\"              \"file.speaker.id\" \"text.id\"         \"spk.ref\"        \n[5] \"zone\"            \"date\"            \"sex\"             \"age\"            \n[9] \"word.count\""
  },
  {
    "objectID": "tutorials/intror/intror.html#loading-text-data",
    "href": "tutorials/intror/intror.html#loading-text-data",
    "title": "Getting started with R",
    "section": "Loading text data",
    "text": "Loading text data\nThere are many functions that we can use to load text data into R. For example, we can use the readLines function as shown below.\n\ntext &lt;- readLines(here::here(\"tutorials/intror/data\", \"text2.txt\"))\n# inspect first text element\ntext[1]\n\n[1] \"The book is presented as a manuscript written by its protagonist, a middle-aged man named Harry Haller, who leaves it to a chance acquaintance, the nephew of his landlady. The acquaintance adds a short preface of his own and then has the manuscript published. The title of this \\\"real\\\" book-in-the-book is Harry Haller's Records (For Madmen Only).\"\n\n\nTo load many texts, we can use a loop to read all texts in a folder as shown below. In a first step, we define the paths of the texts and then, we use the sapply function to loop over the paths and read them into R.\n\n# define paths\npaths &lt;- list.files(here::here(\"tutorials/intror/data/testcorpus\"), full.names = T)\n# load texts\ntexts &lt;- sapply(paths, function(x) {\n  readLines(x)\n})\n# inspect first text element\ntexts[1]\n\n$&lt;NA&gt;\nNULL\n\n\nA method achieving the same result which uses piping (more on what that is below) and tidyverse R code is shown below.\n\n# define paths\ntexts &lt;- list.files(here::here(\"tutorials/intror/data/testcorpus\"), full.names = T, pattern = \".*txt\") %&gt;%\n  purrr::map_chr(~ readr::read_file(.))\n# inspect first text element\ntexts[1]\n\n[1] NA"
  },
  {
    "objectID": "tutorials/intror/intror.html#renaming-piping-and-filtering",
    "href": "tutorials/intror/intror.html#renaming-piping-and-filtering",
    "title": "Getting started with R",
    "section": "Renaming, Piping, and Filtering",
    "text": "Renaming, Piping, and Filtering\nTo rename existing columns in a table, you can use the rename command which takes the table as the first argument, the new name as the second argument, the an equal sign (=), and finally, the old name es the third argument. For example, renaming a column OldName as NewName in a table called MyTable would look like this: rename(MyTable, NewName = OldName).\nPiping is done using the %&gt;% sequence and it can be translated as and then. In the example below, we create a new object (icebio_edit) from the existing object (icebio) and then we rename the columns in the new object. When we use piping, we do not need to name the data we are using as this is provided by the previous step.\n\nicebio_edit &lt;- icebio %&gt;%\n  dplyr::rename(\n    Id = id,\n    FileSpeakerId = file.speaker.id,\n    File = colnames(icebio)[3],\n    Speaker = colnames(icebio)[4]\n  )\n# inspect data\nicebio_edit[1:5, 1:6]\n\n# A tibble: 5 × 6\n     Id FileSpeakerId File    Speaker zone             date     \n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;            &lt;chr&gt;    \n1     1 &lt;S1A-001$A&gt;   S1A-001 A       northern ireland 1990-1994\n2     2 &lt;S1A-001$B&gt;   S1A-001 B       northern ireland 1990-1994\n3     3 &lt;S1A-002$?&gt;   S1A-002 ?       NA               NA       \n4     4 &lt;S1A-002$A&gt;   S1A-002 A       northern ireland 2002-2005\n5     5 &lt;S1A-002$B&gt;   S1A-002 B       northern ireland 2002-2005\n\n\nA very handy way to rename many columns simultaneously, you can use the str_to_title function which capitalizes first letter of a word. In the example below, we capitalize all first letters of the column names of our current data.\n\ncolnames(icebio_edit) &lt;- stringr::str_to_title(colnames(icebio_edit))\n# inspect data\nicebio_edit[1:5, 1:6]\n\n# A tibble: 5 × 6\n     Id Filespeakerid File    Speaker Zone             Date     \n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;            &lt;chr&gt;    \n1     1 &lt;S1A-001$A&gt;   S1A-001 A       northern ireland 1990-1994\n2     2 &lt;S1A-001$B&gt;   S1A-001 B       northern ireland 1990-1994\n3     3 &lt;S1A-002$?&gt;   S1A-002 ?       NA               NA       \n4     4 &lt;S1A-002$A&gt;   S1A-002 A       northern ireland 2002-2005\n5     5 &lt;S1A-002$B&gt;   S1A-002 B       northern ireland 2002-2005\n\n\nTo remove rows based on values in columns you can use the filter function.\n\nicebio_edit2 &lt;- icebio_edit %&gt;%\n  dplyr::filter(\n    Speaker != \"?\",\n    Zone != is.na(Zone),\n    Date == \"2002-2005\",\n    Word.count &gt; 5\n  )\n# inspect data\nhead(icebio_edit2)\n\n# A tibble: 6 × 9\n     Id Filespeakerid File    Speaker Zone          Date  Sex   Age   Word.count\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n1     4 &lt;S1A-002$A&gt;   S1A-002 A       northern ire… 2002… fema… 26-33        391\n2     5 &lt;S1A-002$B&gt;   S1A-002 B       northern ire… 2002… fema… 19-25         47\n3     6 &lt;S1A-002$C&gt;   S1A-002 C       northern ire… 2002… male  50+          200\n4     7 &lt;S1A-002$D&gt;   S1A-002 D       northern ire… 2002… fema… 50+          464\n5     8 &lt;S1A-002$E&gt;   S1A-002 E       mixed betwee… 2002… male  34-41        639\n6     9 &lt;S1A-002$F&gt;   S1A-002 F       northern ire… 2002… fema… 26-33        308\n\n\nTo select specific columns you can use the select function.\n\nicebio_selection &lt;- icebio_edit2 %&gt;%\n  dplyr::select(File, Speaker, Word.count)\n# inspect data\nhead(icebio_selection)\n\n# A tibble: 6 × 3\n  File    Speaker Word.count\n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;\n1 S1A-002 A              391\n2 S1A-002 B               47\n3 S1A-002 C              200\n4 S1A-002 D              464\n5 S1A-002 E              639\n6 S1A-002 F              308\n\n\nYou can also use the select function to remove specific columns.\n\nicebio_selection2 &lt;- icebio_edit2 %&gt;%\n  dplyr::select(-Id, -File, -Speaker, -Date, -Zone, -Age)\n# inspect data\nhead(icebio_selection2)\n\n# A tibble: 6 × 3\n  Filespeakerid Sex    Word.count\n  &lt;chr&gt;         &lt;chr&gt;       &lt;dbl&gt;\n1 &lt;S1A-002$A&gt;   female        391\n2 &lt;S1A-002$B&gt;   female         47\n3 &lt;S1A-002$C&gt;   male          200\n4 &lt;S1A-002$D&gt;   female        464\n5 &lt;S1A-002$E&gt;   male          639\n6 &lt;S1A-002$F&gt;   female        308"
  },
  {
    "objectID": "tutorials/intror/intror.html#ordering-data",
    "href": "tutorials/intror/intror.html#ordering-data",
    "title": "Getting started with R",
    "section": "Ordering data",
    "text": "Ordering data\nTo order data, for instance, in ascending order according to a specific column you can use the arrange function.\n\nicebio_ordered_asc &lt;- icebio_selection2 %&gt;%\n  dplyr::arrange(Word.count)\n# inspect data\nhead(icebio_ordered_asc)\n\n# A tibble: 6 × 3\n  Filespeakerid Sex    Word.count\n  &lt;chr&gt;         &lt;chr&gt;       &lt;dbl&gt;\n1 &lt;S1B-009$D&gt;   female          6\n2 &lt;S1B-005$C&gt;   female          7\n3 &lt;S1B-009$C&gt;   male            7\n4 &lt;S1B-020$F&gt;   male            7\n5 &lt;S1B-006$G&gt;   female          9\n6 &lt;S2A-050$B&gt;   male            9\n\n\nTo order data in descending order you can also use the arrange function and simply add a - before the column according to which you want to order the data.\n\nicebio_ordered_desc &lt;- icebio_selection2 %&gt;%\n  dplyr::arrange(-Word.count)\n# inspect data\nhead(icebio_ordered_desc)\n\n# A tibble: 6 × 3\n  Filespeakerid Sex    Word.count\n  &lt;chr&gt;         &lt;chr&gt;       &lt;dbl&gt;\n1 &lt;S2A-055$A&gt;   female       2355\n2 &lt;S2A-047$A&gt;   male         2340\n3 &lt;S2A-035$A&gt;   female       2244\n4 &lt;S2A-048$A&gt;   male         2200\n5 &lt;S2A-015$A&gt;   male         2172\n6 &lt;S2A-054$A&gt;   female       2113\n\n\nThe output shows that the female speaker in file S2A-005 with the speaker identity A has the highest word count with 2,355 words.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nUsing the data called icebio, create a new data set called ICE_Ire_ordered and arrange the data in descending order by the number of words that each speaker has uttered. Who is the speaker with the highest word count?\n\n\n\nAnswer\n\n\nICE_Ire_ordered &lt;- icebio %&gt;%\n  dplyr::arrange(-word.count)\n# inspect data\nhead(ICE_Ire_ordered)\n\n# A tibble: 6 × 9\n     id file.speaker.id text.id spk.ref zone        date  sex   age   word.count\n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n1   956 &lt;S2A-037$A&gt;     S2A-037 A       republic o… 1990… male  NA          2565\n2   919 &lt;S2A-016$A&gt;     S2A-016 A       republic o… 1995… fema… 34-41       2482\n3   933 &lt;S2A-023$A&gt;     S2A-023 A       northern i… 1990… male  50+         2367\n4   992 &lt;S2A-055$A&gt;     S2A-055 A       northern i… 2002… fema… 42-49       2355\n5   979 &lt;S2A-047$A&gt;     S2A-047 A       republic o… 2002… male  50+         2340\n6   997 &lt;S2A-059$A&gt;     S2A-059 A       republic o… 1990… fema… NA          2305"
  },
  {
    "objectID": "tutorials/intror/intror.html#creating-and-changing-variables",
    "href": "tutorials/intror/intror.html#creating-and-changing-variables",
    "title": "Getting started with R",
    "section": "Creating and changing variables",
    "text": "Creating and changing variables\nNew columns are created, and existing columns can be changed, by using the mutate function. The mutate function takes two arguments (if the data does not have to be specified): the first argument is the (new) name of column that you want to create and the second is what you want to store in that column. The = tells R that the new column will contain the result of the second argument.\nIn the example below, we create a new column called Texttype.\nThis new column should contain\n\nthe value PrivateDialoge if Filespeakerid contains the sequence S1A,\nthe value PublicDialogue if Filespeakerid contains the sequence S1B,\nthe value UnscriptedMonologue if Filespeakerid contains the sequence S2A,\nthe value ScriptedMonologue if Filespeakerid contains the sequence S2B,\nthe value of Filespeakerid if Filespeakerid neither contains S1A, S1B, S2A, nor S2B.\n\n\nicebio_texttype &lt;- icebio_selection2 %&gt;%\n  dplyr::mutate(\n    Texttype =\n      dplyr::case_when(\n        stringr::str_detect(Filespeakerid, \"S1A\") ~ \"PrivateDialoge\",\n        stringr::str_detect(Filespeakerid, \"S1B\") ~ \"PublicDialogue\",\n        stringr::str_detect(Filespeakerid, \"S2A\") ~ \"UnscriptedMonologue\",\n        stringr::str_detect(Filespeakerid, \"S2B\") ~ \"ScriptedMonologue\",\n        TRUE ~ Filespeakerid\n      )\n  )\n# inspect data\nhead(icebio_texttype)\n\n# A tibble: 6 × 4\n  Filespeakerid Sex    Word.count Texttype      \n  &lt;chr&gt;         &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         \n1 &lt;S1A-002$A&gt;   female        391 PrivateDialoge\n2 &lt;S1A-002$B&gt;   female         47 PrivateDialoge\n3 &lt;S1A-002$C&gt;   male          200 PrivateDialoge\n4 &lt;S1A-002$D&gt;   female        464 PrivateDialoge\n5 &lt;S1A-002$E&gt;   male          639 PrivateDialoge\n6 &lt;S1A-002$F&gt;   female        308 PrivateDialoge"
  },
  {
    "objectID": "tutorials/intror/intror.html#if-statements",
    "href": "tutorials/intror/intror.html#if-statements",
    "title": "Getting started with R",
    "section": "If-statements",
    "text": "If-statements\nWe should briefly talk about if-statements (or case_when in the present case). The case_when function is both very powerful and extremely helpful as it allows you to assign values based on a test. As such, case_when-statements can be read as:\nWhen/If X is the case, then do A and if X is not the case do B! (When/If -&gt; Then -&gt; Else)\nThe nice thing about ifelse or case_when-statements is that they can be used in succession as we have done above. This can then be read as:\nIf X is the case, then do A, if Y is the case, then do B, else do Z\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n1.Using the data called icebio, create a new data set called ICE_Ire_AgeGroup in which you create a column called AgeGroup where all speakers who are younger than 42 have the value young and all speakers aged 42 and over old.\nTip: use if-statements to assign the old and young values.\n\n\nAnswer\n\n\nICE_Ire_AgeGroup &lt;- icebio %&gt;%\n  dplyr::mutate(AgeGroup = dplyr::case_when(\n    age == \"42-49\" ~ \"old\",\n    age == \"50+\" ~ \"old\",\n    age == \"0-18\" ~ \"young\",\n    age == \"19-25\" ~ \"young\",\n    age == \"26-33\" ~ \"young\",\n    age == \"34-41\" ~ \"young\",\n    TRUE ~ age\n  ))\n# inspect data\nhead(ICE_Ire_AgeGroup)\n\n# A tibble: 6 × 10\n     id file.speaker.id text.id spk.ref zone        date  sex   age   word.count\n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n1     1 &lt;S1A-001$A&gt;     S1A-001 A       northern i… 1990… male  34-41        765\n2     2 &lt;S1A-001$B&gt;     S1A-001 B       northern i… 1990… fema… 34-41       1298\n3     3 &lt;S1A-002$?&gt;     S1A-002 ?       NA          NA    NA    NA            23\n4     4 &lt;S1A-002$A&gt;     S1A-002 A       northern i… 2002… fema… 26-33        391\n5     5 &lt;S1A-002$B&gt;     S1A-002 B       northern i… 2002… fema… 19-25         47\n6     6 &lt;S1A-002$C&gt;     S1A-002 C       northern i… 2002… male  50+          200\n# ℹ 1 more variable: AgeGroup &lt;chr&gt;\n\ntable(ICE_Ire_AgeGroup$AgeGroup)\n\n\n   NA   old young \n  547   333   452"
  },
  {
    "objectID": "tutorials/intror/intror.html#summarizing-data",
    "href": "tutorials/intror/intror.html#summarizing-data",
    "title": "Getting started with R",
    "section": "Summarizing data",
    "text": "Summarizing data\nSummarizing is really helpful and can be done using the summarise function.\n\nicebio_summary1 &lt;- icebio_texttype %&gt;%\n  dplyr::summarise(Words = sum(Word.count))\n# inspect data\nhead(icebio_summary1)\n\n# A tibble: 1 × 1\n   Words\n   &lt;dbl&gt;\n1 141876\n\n\nTo get summaries of sub-groups or by variable level, we can use the group_by function and then use the summarise function.\n\nicebio_summary2 &lt;- icebio_texttype %&gt;%\n  dplyr::group_by(Texttype, Sex) %&gt;%\n  dplyr::summarise(\n    Speakers = n(),\n    Words = sum(Word.count)\n  )\n# inspect data\nhead(icebio_summary2)\n\n# A tibble: 6 × 4\n# Groups:   Texttype [3]\n  Texttype            Sex    Speakers Words\n  &lt;chr&gt;               &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 PrivateDialoge      female      105 60024\n2 PrivateDialoge      male         18  9628\n3 PublicDialogue      female       63 24647\n4 PublicDialogue      male         41 16783\n5 UnscriptedMonologue female        3  6712\n6 UnscriptedMonologue male         16 24082\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nUse the icebio and determine the number of words uttered by female speakers from Northern Ireland above an age of 50.\n\n\n\nAnswer\n\n\nwords_fni50 &lt;- icebio %&gt;%\n  dplyr::select(zone, sex, age, word.count) %&gt;%\n  dplyr::group_by(zone, sex, age) %&gt;%\n  dplyr::summarize(Words = sum(word.count)) %&gt;%\n  dplyr::filter(\n    sex == \"female\",\n    age == \"50+\",\n    zone == \"northern ireland\"\n  )\n\n`summarise()` has grouped output by 'zone', 'sex'. You can override using the\n`.groups` argument.\n\n# inspect data\nwords_fni50\n\n# A tibble: 1 × 4\n# Groups:   zone, sex [1]\n  zone             sex    age   Words\n  &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 northern ireland female 50+   23210\n\n\n\n\nLoad the file exercisedata.txt and determine the mean scores of groups A and B.\n\nTip: to extract the mean, combine the summary function with the mean function.\n\n\nAnswer\n\n\nexercisedata &lt;- read.delim(here::here(\"tutorials/intror/data\", \"exercisedata.txt\"), sep = \"\\t\", header = T) %&gt;%\n  dplyr::group_by(Group) %&gt;%\n  dplyr::summarize(Mean = mean(Score))\n# inspect data\nexercisedata\n\n# A tibble: 2 × 2\n  Group  Mean\n  &lt;chr&gt; &lt;dbl&gt;\n1 A      14.9\n2 B      11.8"
  },
  {
    "objectID": "tutorials/intror/intror.html#gathering-and-spreading-data",
    "href": "tutorials/intror/intror.html#gathering-and-spreading-data",
    "title": "Getting started with R",
    "section": "Gathering and spreading data",
    "text": "Gathering and spreading data\nThe tidyr package has two very useful functions for gathering and spreading data that can be sued to transform data to long and wide formats (you will see what this means below). The functions are called gather and spread.\nWe will use the data set called icebio_summary2, which we created above, to demonstrate how this works.\nWe will first check out the spread-function to create different columns for women and men that show how many of them are represented in the different text types.\n\nicebio_summary_wide &lt;- icebio_summary2 %&gt;%\n  dplyr::select(-Words) %&gt;%\n  tidyr::spread(Sex, Speakers)\n# inspect\nicebio_summary_wide\n\n# A tibble: 3 × 3\n# Groups:   Texttype [3]\n  Texttype            female  male\n  &lt;chr&gt;                &lt;int&gt; &lt;int&gt;\n1 PrivateDialoge         105    18\n2 PublicDialogue          63    41\n3 UnscriptedMonologue      3    16\n\n\nThe data is now in what is called a wide-format as values are distributed across columns.\nTo reformat this back to a long-format where each column represents exactly one variable, we use the gather-function:\n\nicebio_summary_long &lt;- icebio_summary_wide %&gt;%\n  tidyr::gather(Sex, Speakers, female:male)\n# inspect\nicebio_summary_long\n\n# A tibble: 6 × 3\n# Groups:   Texttype [3]\n  Texttype            Sex    Speakers\n  &lt;chr&gt;               &lt;chr&gt;     &lt;int&gt;\n1 PrivateDialoge      female      105\n2 PublicDialogue      female       63\n3 UnscriptedMonologue female        3\n4 PrivateDialoge      male         18\n5 PublicDialogue      male         41\n6 UnscriptedMonologue male         16"
  },
  {
    "objectID": "tutorials/intror/intror.html#loading-text-data-1",
    "href": "tutorials/intror/intror.html#loading-text-data-1",
    "title": "Getting started with R",
    "section": "Loading text data",
    "text": "Loading text data\nTo load text data from the web, we can use the read_file function which takes the URL of the text as its first argument. In this case will will load the 2016 rally speeches Donald Trump.\n\nTrump &lt;- base::readRDS(\"tutorials/intror/data/Trump.rda\", \"rb\")\n# inspect data\nstr(Trump)\n\n'data.frame':   2694 obs. of  1 variable:\n $ SPEECH: chr  \"...Thank you so much.  That's so nice.  Isn't he a great guy.  He doesn't get a fair press; he doesn't get it. \"| __truncated__ \"With that said, our country is really headed in the wrong direction with a president who is doing an absolutely\"| __truncated__ \"And I'm a conservative, actually very conservative, and I'm a Republican.  And I'm very disappointed by our Rep\"| __truncated__ \"You look at Obamacare.  A total catastrophe and by the way it really kicks in in '16 and it is going to be a di\"| __truncated__ ...\n\n\nIt is very easy to extract frequency information and to create frequency lists. We can do this by first using the unnest_tokens function which splits texts into individual words, an then use the count function to get the raw frequencies of all word types in a text.\n\nTrump %&gt;%\n  tibble(text = SPEECH) %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  dplyr::count(word, sort = T)\n\n# A tibble: 6,102 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the    5924\n 2 to     5460\n 3 and    5438\n 4 i      4873\n 5 a      3592\n 6 you    3055\n 7 of     2953\n 8 we     2565\n 9 it     2421\n10 that   2317\n# ℹ 6,092 more rows\n\n\nExtracting N-grams is also very easy as the unnest_tokens function can an argument called token in which we can specify that we want to extract n-grams, If we do this, then we need to specify the n as a separate argument. Below we specify that we want the frequencies of all 4-grams.\n\nTrump %&gt;%\n  tibble(text = SPEECH) %&gt;%\n  unnest_tokens(word, text, token = \"ngrams\", n = 4) %&gt;%\n  dplyr::count(word, sort = T) %&gt;%\n  head(10)\n\n# A tibble: 10 × 2\n   word                    n\n   &lt;chr&gt;               &lt;int&gt;\n 1 and we’re going to     93\n 2 we are going to        75\n 3 &lt;NA&gt;                   68\n 4 it’s going to be       65\n 5 we’re going to do      61\n 6 we’re going to have    61\n 7 not going to happen    60\n 8 and by the way         53\n 9 thank you very much    52\n10 we’re going to win     50"
  },
  {
    "objectID": "tutorials/intror/intror.html#splitting-up-texts",
    "href": "tutorials/intror/intror.html#splitting-up-texts",
    "title": "Getting started with R",
    "section": "Splitting-up texts",
    "text": "Splitting-up texts\nWe can use the str_split function to split texts. However, there are two issues when using this (very useful) function:\n\nthe pattern that we want to split on disappears\nthe output is a list (a special type of data format)\n\nTo remedy these issues, we\n\ncombine the str_split function with the unlist function\nadd something right at the beginning of the pattern that we use to split the text. To add something to the beginning of the pattern that we want to split the text by, we use the str_replace_all function. The str_replace_all function takes three arguments, 1. the text, 2. the pattern that should be replaced, 3. the replacement. In the example below, we add ~~~ to the sequence SPEECH and then split on the ~~~ rather than on the sequence “SPEECH” (in other words, we replace SPEECH with ~~~SPEECH and then split on ~~~).\n\n\nTrump_split &lt;- unlist(str_split(\n  stringr::str_replace_all(Trump, \"SPEECH\", \"~~~SPEECH\"),\n  pattern = \"~~~\"\n))\n# inspect data\nnchar(Trump_split) # ; str(Trump_split)\n\n [1]  21311  26963   2701   6004   1342  32880  28497 425099   1404 311770\n[11]  43776"
  },
  {
    "objectID": "tutorials/intror/intror.html#cleaning-texts",
    "href": "tutorials/intror/intror.html#cleaning-texts",
    "title": "Getting started with R",
    "section": "Cleaning texts",
    "text": "Cleaning texts\nWhen working with texts, we usually need to clean the data. Below, we do some very basic cleaning using a pipeline.\n\nTrump_split_clean &lt;- Trump_split %&gt;%\n  # replace elements\n  stringr::str_replace_all(fixed(\"\\n\"), \" \") %&gt;%\n  # remove strange symbols\n  stringr::str_replace_all(\"[^[:alnum:][:punct:]]+\", \" \") %&gt;%\n  # combine contractions\n  stringr::str_replace_all(\" re \", \"'re \") %&gt;%\n  stringr::str_replace_all(\" ll \", \"'ll \") %&gt;%\n  stringr::str_replace_all(\" d \", \"'d \") %&gt;%\n  stringr::str_replace_all(\" m \", \"'m \") %&gt;%\n  stringr::str_replace_all(\" s \", \"'s \") %&gt;%\n  stringr::str_replace_all(\"n t \", \"n't \") %&gt;%\n  # remove \\\"\n  stringr::str_remove_all(\"\\\"\") %&gt;%\n  # remove superfluous white spaces\n  stringr::str_squish()\n# remove very short elements\nTrump_split_clean &lt;- Trump_split_clean[nchar(Trump_split_clean) &gt; 5]\n# inspect data\nnchar(Trump_split_clean)\n\n [1]  20878  26754   2687   5960   1321  32539  28303 422165   1404 309275\n[11]  43477\n\n\nInspect text\n\nTrump_split_clean[5]"
  },
  {
    "objectID": "tutorials/intror/intror.html#concordancing-and-kwics",
    "href": "tutorials/intror/intror.html#concordancing-and-kwics",
    "title": "Getting started with R",
    "section": "Concordancing and KWICs",
    "text": "Concordancing and KWICs\nCreating concordances or key-word-in-context displays is one of the most common practices when dealing with text data. Fortunately, there exist ready-made functions that make this a very easy task in R. We will use the kwic function from the quanteda package to create kwics here.\n\nkwic_multiple &lt;- quanteda::kwic(quanteda::tokens(\"Trump_split_clean\"),\n  pattern = phrase(\"great again\"),\n  window = 3,\n  valuetype = \"regex\"\n) %&gt;%\n  as.data.frame()\n# inspect data\nhead(kwic_multiple)\n\n[1] docname from    to      pre     keyword post    pattern\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nWe can now also select concordances based on specific features. For example, we only want those instances of “great again” if the preceding word was “america”.\n\nkwic_multiple_select &lt;- kwic_multiple %&gt;%\n  # last element before search term is \"america\"\n  dplyr::filter(str_detect(pre, \"america$\"))\n# inspect data\nhead(kwic_multiple_select)\n\n[1] docname from    to      pre     keyword post    pattern\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nAgain, we can use the write.table function to save our kwics to disc.\n\nwrite.table(kwic_multiple_select, here::here(\"tutorials/intror/data\", \"kwic_multiple_select.txt\"), sep = \"\\t\")\n\nAs most of the data that we use is on out computers (rather than being somewhere on the web), we now load files with text from your computer. It is important to note that you need to use \\\\ when you want to load data from a Windows PC (rather than single \\).\nTo load many files, we first create a list of all files in a the directory that we want to load data from and then use the sapply function (which works just like a loop). The sapply function takes a a vector of elements and then performs a sequence of steps on each of these elements. In the example below, we feed the file locations to the sapply function and then we scan each text (i.e. we read it into R), then we paste all the content of one file together.\n\n\n\n\nNOTEYou may have to change the path to the data!\n\n\n\n\n\n\n\nfiles &lt;- list.files(here::here(\"tutorials/intror/data\", \"ICEIrelandSample\"),\n  pattern = \".txt\", full.names = T\n)\nICE_Ire_sample &lt;- sapply(files, function(x) {\n  x &lt;- scan(x, what = \"char\")\n  x &lt;- paste(x, sep = \" \", collapse = \" \")\n})\n# inspect data\nstr(ICE_Ire_sample)\n\n Named chr [1:100] \"&lt;S1A-001 Riding&gt; &lt;I&gt; &lt;S1A-001$A&gt; &lt;#&gt; Well how did the riding go tonight &lt;S1A-001$B&gt; &lt;#&gt; It was good so it was &lt;#&gt; Just I I coul ...\n - attr(*, \"names\")= chr [1:100] \"/Users/laurenceanthony/Documents/projects/LADALQ/tutorials/intror/data/ICEIrelandSample/S1A-001.txt\" \"/Users/laurenceanthony/Documents/projects/LADALQ/tutorials/intror/data/ICEIrelandSample/S1A-002.txt\" \"/Users/laurenceanthony/Documents/projects/LADALQ/tutorials/intror/data/ICEIrelandSample/S1A-003.txt\" \"/Users/laurenceanthony/Documents/projects/LADALQ/tutorials/intror/data/ICEIrelandSample/S1A-004.txt\" ...\n\n\nAs the texts do not have column names (but simply names), we can clean these by removing everything before a / and by removing the .txt.\n\nnames(ICE_Ire_sample) &lt;- names(ICE_Ire_sample) %&gt;%\n  stringr::str_remove_all(\".*/\") %&gt;%\n  stringr::str_remove_all(\".txt\")\n# inspect\nnames(ICE_Ire_sample)\n\n  [1] \"S1A-001\"                     \"S1A-002\"                    \n  [3] \"S1A-003\"                     \"S1A-004\"                    \n  [5] \"S1A-005\"                     \"S1A-006\"                    \n  [7] \"S1A-007\"                     \"S1A-008\"                    \n  [9] \"S1A-009\"                     \"S1A-010\"                    \n [11] \"S1A-011\"                     \"S1A-012\"                    \n [13] \"S1A-013\"                     \"S1A-014\"                    \n [15] \"S1A-015\"                     \"S1A-016\"                    \n [17] \"S1A-017\"                     \"S1A-018\"                    \n [19] \"S1A-019\"                     \"S1A-020\"                    \n [21] \"S1A-021\"                     \"S1A-022\"                    \n [23] \"S1A-023\"                     \"S1A-024\"                    \n [25] \"S1A-025\"                     \"S1A-026\"                    \n [27] \"S1A-027\"                     \"S1A-028\"                    \n [29] \"S1A-029\"                     \"S1A-030\"                    \n [31] \"S1A-031\"                     \"S1A-032\"                    \n [33] \"S1A-033\"                     \"S1A-034\"                    \n [35] \"S1A-035\"                     \"S1A-036\"                    \n [37] \"S1A-037\"                     \"S1A-038\"                    \n [39] \"S1A-039\"                     \"S1A-040\"                    \n [41] \"S1A-041\"                     \"S1A-042\"                    \n [43] \"S1A-043\"                     \"S1A-044\"                    \n [45] \"S1A-045\"                     \"S1A-046\"                    \n [47] \"S1A-047\"                     \"S1A-048\"                    \n [49] \"S1A-049\"                     \"S1A-050\"                    \n [51] \"S1A-051\"                     \"S1A-052 Buttermilk\"         \n [53] \"S1A-053 Student grants 1\"    \"S1A-054 Student grants 2\"   \n [55] \"S1A-055 Hospitals\"           \"S1A-056 Holistic medicine 1\"\n [57] \"S1A-057 Studying 2\"          \"S1A-058 Holistic medicine 2\"\n [59] \"S1A-059 Glasses 1\"           \"S1A-060 Glasses 2\"          \n [61] \"S1A-061 Modern man\"          \"S1A-062 America trip 1\"     \n [63] \"S1A-063 Shoes\"               \"S1A-064 O'Connell Street\"   \n [65] \"S1A-065 America trip 2\"      \"S1A-066 Radio music\"        \n [67] \"S1A-067 Apprenticeship\"      \"S1A-068 Rock bands\"         \n [69] \"S1A-069 Christmas trees\"     \"S1A-070 Friends\"            \n [71] \"S1A-071 Elocution\"           \"S1A-072 Driver's licence\"   \n [73] \"S1A-073 Politics\"            \"S1A-074 Local shops\"        \n [75] \"S1A-075 Present\"             \"S1A-076 Boyfriends 2\"       \n [77] \"S1A-077 Books\"               \"S1A-078 Medical project\"    \n [79] \"S1A-079 Driving\"             \"S1A-080 Motorbikes\"         \n [81] \"S1A-081 Croke Park\"          \"S1A-082 Kissogram\"          \n [83] \"S1A-083 Donkey story\"        \"S1A-084 General election\"   \n [85] \"S1A-085 Birthday cake\"       \"S1A-086 Baby\"               \n [87] \"S1A-087 Line dancing\"        \"S1A-088 Therapy inaugural\"  \n [89] \"S1A-089 American men\"        \"S1A-090 Designer clothes\"   \n [91] \"S1A-091 Haircut - Mortgage\"  \"S1A-092 Househunting\"       \n [93] \"S1A-093 Motorbikes\"          \"S1A-094 Health\"             \n [95] \"S1A-095 Strep infection\"     \"S1A-096 Sisters 1\"          \n [97] \"S1A-097 Sisters 2\"           \"S1A-098 Bad weather\"        \n [99] \"S1A-099 Dresses\"             \"S1A-100 College plans\""
  },
  {
    "objectID": "tutorials/intror/intror.html#further-splitting-of-texts",
    "href": "tutorials/intror/intror.html#further-splitting-of-texts",
    "title": "Getting started with R",
    "section": "Further splitting of texts",
    "text": "Further splitting of texts\nTo split the texts into speech units where each speech unit begins with the speaker that has uttered it, we again use the sapply function.\n\nICE_Ire_split &lt;- as.vector(unlist(sapply(ICE_Ire_sample, function(x) {\n  x &lt;- as.vector(str_split(str_replace_all(x, \"(&lt;S1A-)\", \"~~~\\\\1\"), \"~~~\"))\n})))\n# inspect\nhead(ICE_Ire_split)\n\n[1] \"\"                                                                                                                                                                                    \n[2] \"&lt;S1A-001 Riding&gt; &lt;I&gt; \"                                                                                                                                                               \n[3] \"&lt;S1A-001$A&gt; &lt;#&gt; Well how did the riding go tonight \"                                                                                                                                 \n[4] \"&lt;S1A-001$B&gt; &lt;#&gt; It was good so it was &lt;#&gt; Just I I couldn't believe that she was going to let me jump &lt;,&gt; that was only the fourth time you know &lt;#&gt; It was great &lt;&&gt; laughter &lt;/&&gt; \"\n[5] \"&lt;S1A-001$A&gt; &lt;#&gt; What did you call your horse \"                                                                                                                                       \n[6] \"&lt;S1A-001$B&gt; &lt;#&gt; I can't remember &lt;#&gt; Oh Mary s Town &lt;,&gt; oh\\n\""
  },
  {
    "objectID": "tutorials/intror/intror.html#basics-of-regular-expressions",
    "href": "tutorials/intror/intror.html#basics-of-regular-expressions",
    "title": "Getting started with R",
    "section": "Basics of regular expressions",
    "text": "Basics of regular expressions\nNext, we extract the File and the Speaker and combine Text, File, and Speaker in a table.\nWe use this to show the power of regular expressions (to learn more about regular expression, have a look at this very recommendable tutorial). Regular expressions are symbols or sequences of symbols that stand for\n\nsymbols or patterns (e.g. [a-z] stands for any lowercase character)\nthe frequency of symbols or patterns (e.g. {1,3} stands for between 1 and 3)\nclasses of symbols (e.g. [:punct:] stands for any punctuation symbol)\nstructural properties (e.g. [^[:blank:]] stands for any non-space character, \\t stands for tab-stop and \\n stands for a line break)\n\nWe can not go into any detail here and only touch upon the power of regular expressions.\nThe symbol . is one of the most powerful and most universal regular expressions as it represents (literally) any symbol or character and it thus stands for a pattern. The * is a regular expression that refers to the frequency of a pattern and it stands for 0 to an infinite number of instances. Thus, .* stands for 0 to an infinite number of any character. You can find an overview of the regular expressions that you can use in R here.\nAlso, if you put patterns in round brackets, R will remember the sequence within brackets and you can paste it back into a string from memory when you replace something.\nWhen referring to symbols that are used a regular expressions such as \\ or \\$, you need to inform R that you actually mean the real symbol and not the regular expression and you do that by typing two \\\\ before the sequence in question. Have a look at the example below and try to see what the regular expressions (.*(S1A-[0-9]{3,3}).*, \\n, and .*\\\\$([A-Z]{1,2}\\\\?{0,1})&gt;.*) stand for.\n\nICE_Ire_split_tb &lt;- ICE_Ire_split %&gt;%\n  as.data.frame()\n# add column names\ncolnames(ICE_Ire_split_tb)[1] &lt;- \"Text\"\n# add file and speaker\nICE_Ire_split_tb &lt;- ICE_Ire_split_tb %&gt;%\n  dplyr::filter(\n    !str_detect(Text, \"&lt;I&gt;\"),\n    Text != \"\"\n  ) %&gt;%\n  dplyr::mutate(\n    File = str_replace_all(Text, \".*(S1A-[0-9]{3,3}).*\", \"\\\\1\"),\n    File = str_remove_all(File, \"\\\\\\n\"),\n    Speaker = str_replace_all(Text, \".*\\\\$([A-Z]{1,2}\\\\?{0,1})&gt;.*\", \"\\\\1\"),\n    Speaker = str_remove_all(Speaker, \"\\\\\\n\")\n  )\n\n\n\nTextFileSpeaker&lt;S1A-001$A&gt; &lt;#&gt; Well how did the riding go tonight S1A-001A&lt;S1A-001$B&gt; &lt;#&gt; It was good so it was &lt;#&gt; Just I I couldn't believe that she was going to let me jump &lt;,&gt; that was only the fourth time you know &lt;#&gt; It was great &lt;&&gt; laughter &lt;/&&gt; S1A-001B&lt;S1A-001$A&gt; &lt;#&gt; What did you call your horse S1A-001A&lt;S1A-001$B&gt; &lt;#&gt; I can't remember &lt;#&gt; Oh Mary s Town &lt;,&gt; ohS1A-001B&lt;S1A-001$A&gt; &lt;#&gt; And how did Mabel doS1A-001A&lt;S1A-001$B&gt; &lt;#&gt; Did you not see her whenever she was going over the jumps &lt;#&gt; There was one time her horse refused and it refused three times &lt;#&gt; And then &lt;,&gt; she got it round and she just lined it up straight and she just kicked it and she hit it with the whip &lt;,&gt; and over it went the last time you know &lt;#&gt; And Stephanie told her she was very determined and very well-ridden &lt;&&gt; laughter &lt;/&&gt; because it had refused the other times you know &lt;#&gt; But Stephanie wouldn t let her give up on it &lt;#&gt; She made her keep coming back and keep coming back &lt;,&gt; until &lt;,&gt; it jumped it you know &lt;#&gt; It was good S1A-001B&lt;S1A-001$A&gt; &lt;#&gt; Yeah I m not so sure her jumping  s improving that much &lt;#&gt; She uh &lt;,&gt; seemed to be holding the reins very tight S1A-001A&lt;S1A-001$B&gt; &lt;#&gt; Yeah she was &lt;#&gt; That s what Stephanie said &lt;#&gt; &lt;{&gt; &lt;[&gt; She &lt;/[&gt; needed to &lt;,&gt; give the horse its headS1A-001B&lt;S1A-001$A&gt; &lt;#&gt; &lt;[&gt; Mm &lt;/[&gt; &lt;/{&gt;S1A-001A&lt;S1A-001$A&gt; &lt;#&gt; She wasn t really getting into the jumping position the way she used to S1A-001A"
  },
  {
    "objectID": "tutorials/intror/intror.html#combining-tables",
    "href": "tutorials/intror/intror.html#combining-tables",
    "title": "Getting started with R",
    "section": "Combining tables",
    "text": "Combining tables\nWe often want to combine different tables. This is very easy in R and we will show how it can be done by combining our bio data about speakers that are represented in the ICE Ireland corpus with the texts themselves so that we get a table which holds both the text as well as the speaker information.\nThus, we now join the text data with the bio data by using the left_join function. We join the text with the bio data based on the contents of the File and the Speaker columns. In contract to right_join, and full_join, left_join will drop all rows from the right table that are not present in left table (and vice verse for right_join. In contrast, full_join will retain all rows from both the left and the right table.\n\nICE_Ire &lt;- dplyr::left_join(ICE_Ire_split_tb, icebio_edit, by = c(\"File\", \"Speaker\"))\n\n\n\nTextFileSpeakerIdFilespeakeridZoneDateSexAgeWord.count&lt;S1A-001$A&gt; &lt;#&gt; Well how did the riding go tonight S1A-001A1&lt;S1A-001$A&gt;northern ireland1990-1994male34-41765&lt;S1A-001$B&gt; &lt;#&gt; It was good so it was &lt;#&gt; Just I I couldn't believe that she was going to let me jump &lt;,&gt; that was only the fourth time you know &lt;#&gt; It was great &lt;&&gt; laughter &lt;/&&gt; S1A-001B2&lt;S1A-001$B&gt;northern ireland1990-1994female34-411,298&lt;S1A-001$A&gt; &lt;#&gt; What did you call your horse S1A-001A1&lt;S1A-001$A&gt;northern ireland1990-1994male34-41765&lt;S1A-001$B&gt; &lt;#&gt; I can't remember &lt;#&gt; Oh Mary s Town &lt;,&gt; ohS1A-001B2&lt;S1A-001$B&gt;northern ireland1990-1994female34-411,298&lt;S1A-001$A&gt; &lt;#&gt; And how did Mabel doS1A-001A1&lt;S1A-001$A&gt;northern ireland1990-1994male34-41765&lt;S1A-001$B&gt; &lt;#&gt; Did you not see her whenever she was going over the jumps &lt;#&gt; There was one time her horse refused and it refused three times &lt;#&gt; And then &lt;,&gt; she got it round and she just lined it up straight and she just kicked it and she hit it with the whip &lt;,&gt; and over it went the last time you know &lt;#&gt; And Stephanie told her she was very determined and very well-ridden &lt;&&gt; laughter &lt;/&&gt; because it had refused the other times you know &lt;#&gt; But Stephanie wouldn t let her give up on it &lt;#&gt; She made her keep coming back and keep coming back &lt;,&gt; until &lt;,&gt; it jumped it you know &lt;#&gt; It was good S1A-001B2&lt;S1A-001$B&gt;northern ireland1990-1994female34-411,298&lt;S1A-001$A&gt; &lt;#&gt; Yeah I m not so sure her jumping  s improving that much &lt;#&gt; She uh &lt;,&gt; seemed to be holding the reins very tight S1A-001A1&lt;S1A-001$A&gt;northern ireland1990-1994male34-41765&lt;S1A-001$B&gt; &lt;#&gt; Yeah she was &lt;#&gt; That s what Stephanie said &lt;#&gt; &lt;{&gt; &lt;[&gt; She &lt;/[&gt; needed to &lt;,&gt; give the horse its headS1A-001B2&lt;S1A-001$B&gt;northern ireland1990-1994female34-411,298&lt;S1A-001$A&gt; &lt;#&gt; &lt;[&gt; Mm &lt;/[&gt; &lt;/{&gt;S1A-001A1&lt;S1A-001$A&gt;northern ireland1990-1994male34-41765&lt;S1A-001$A&gt; &lt;#&gt; She wasn t really getting into the jumping position the way she used to S1A-001A1&lt;S1A-001$A&gt;northern ireland1990-1994male34-41765\n\n\nYou can then perform concordancing on the Text column in the table.\n\nkwic_iceire &lt;- quanteda::kwic(quanteda::tokens(ICE_Ire$Text),\n  pattern = phrase(\"Irish\"),\n  window = 5,\n  valuetype = \"regex\"\n) %&gt;%\n  as.data.frame()\n\n\n\ndocnamefromtoprekeywordpostpatterntext14303737Ireland you know it wasIrishbacon and it was lovelyIrishtext17606262/ & &gt; being goodIrishCatholics we always had toIrishtext17841313&gt; We should do theIrish&lt; . &gt; ver &lt;Irishtext17842323&lt; / . &gt; theIrishversion of the Matrix &lt;Irishtext60632222tickets for it in theIrishNews &lt; # &gt; IIrishtext85831111&lt; # &gt; &lt; &Irish&gt; &lt; [ &gt; NiIrishtext85834545ni / &lt; / &Irish&gt;Irishtext90301818[ &gt; Yeah and BaileysIrishCream cake &lt; / [Irishtext103874141&lt; [ &gt; &lt; &Irish&gt; Scoil San Treasa &lt;Irishtext103874949San Treasa &lt; / &Irish&gt; &lt; / [ &gt;Irish"
  },
  {
    "objectID": "tutorials/intror/intror.html#tokenization-and-counting-words",
    "href": "tutorials/intror/intror.html#tokenization-and-counting-words",
    "title": "Getting started with R",
    "section": "Tokenization and counting words",
    "text": "Tokenization and counting words\nWe will now use the tokenize_words function from the tokenizer package to find out how many words are in each file. Before we count the words, however, we will clean the data by removing everything between pointy brackets (e.g. &lt;#&gt;) as well as all punctuation.\n\nwords &lt;- as.vector(sapply(Trump_split_clean, function(x) {\n  x &lt;- tm::removeNumbers(x)\n  x &lt;- tm::removePunctuation(x)\n  x &lt;- unlist(tokenize_words(x))\n  x &lt;- length(x)\n}))\nwords\n\n [1]  3846  4641   521  1119   241  5880  5301 76772   266 56940  7848\n\n\nThe nice thing about the tokenizer package is that it also allows to split texts into sentences. To show this, we return to the rally speeches by Donald Trump and split the first of his rally speeches into sentences.\n\nSentences &lt;- unlist(tokenize_sentences(Trump_split_clean[6]))\n# inspect\nhead(Sentences)\n\n[1] \"SPEECH 6, Thank you.\"                                                                           \n[2] \"It’s true, and these are the best and the finest.\"                                              \n[3] \"When Mexico sends its people, they’re not sending their best.\"                                  \n[4] \"They’re not sending you.\"                                                                       \n[5] \"They’re not sending you.\"                                                                       \n[6] \"They’re sending people that have lots of problems, and they’re bringing those problems with us.\"\n\n\nWe now want to find associations between words. To do this, we convert all characters to lower case, remove (some) non lexical words (also called stop words), remove punctuation, and superfluous white spaces and then create a document-term-matrix (DTM) which shows how often any word occurs in any of the sentences (in this case, the sentences are treated as documents).\nOnce we have a DTM, we can then use the findAssocs function to see which words associate most strongly with target words that we want to investigate. We can use the argument “corlimit” to show the terms that are most strongly associated with our target words.\n\n# clean sentences\nSentences &lt;- Sentences %&gt;%\n  # convert to lowercase\n  tolower() %&gt;%\n  # remove stop words\n  tm::removeWords(stopwords(\"english\")) %&gt;%\n  # remove punctuation\n  tm::removePunctuation() %&gt;%\n  # remove numbers\n  tm::removeNumbers() %&gt;%\n  # remove superfluous white spaces\n  stringr::str_squish()\n# create DTM\nDTM &lt;- DocumentTermMatrix(VCorpus(VectorSource(Sentences)))\nfindAssocs(DTM, c(\"problems\", \"america\"), corlimit = c(.5, .5))\n\n$problems\nbrilliantly     devalue  obligation      russia         buy  everything \n       0.67        0.67        0.67        0.67        0.50        0.50 \n\n$america\namericas   avenue     bank \n    0.71     0.57     0.57 \n\n\nWe now turn to data visualization basics."
  },
  {
    "objectID": "tutorials/intror/intror.html#basics-of-ggplot2-syntax",
    "href": "tutorials/intror/intror.html#basics-of-ggplot2-syntax",
    "title": "Getting started with R",
    "section": "Basics of ggplot2 syntax",
    "text": "Basics of ggplot2 syntax\nSpecify data, aesthetics and geometric shapes\nggplot(data, aes(x=, y=, color=, shape=, size=)) +\ngeom_point(), or geom_histogram(), or geom_boxplot(), etc.\n\nThis combination is very effective for exploratory graphs.\nThe data must be a data frame.\nThe aes() function maps columns of the data frame to aesthetic properties of geometric shapes to be plotted.\nggplot() defines the plot; the geoms show the data; each component is added with +\nSome examples should make this clear"
  },
  {
    "objectID": "tutorials/intror/intror.html#practical-examples",
    "href": "tutorials/intror/intror.html#practical-examples",
    "title": "Getting started with R",
    "section": "Practical examples",
    "text": "Practical examples\nWe will now create some basic visualizations or plots.\nBefore we start plotting, we will create data that we want to plot. In this case, we will extract the mean word counts by gender and age.\n\nplotdata &lt;- ICE_Ire %&gt;%\n  # only private dialogue\n  dplyr::filter(\n    stringr::str_detect(File, \"S1A\"),\n    # without speaker younger than 19\n    Age != \"0-18\",\n    Age != \"NA\"\n  ) %&gt;%\n  dplyr::group_by(Sex, Age) %&gt;%\n  dplyr::summarise(Words = mean(Word.count))\n# inspect\nhead(plotdata)\n\n# A tibble: 6 × 3\n# Groups:   Sex [2]\n  Sex    Age   Words\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 female 19-25  700.\n2 female 26-33  680.\n3 female 34-41  691.\n4 female 42-49  572.\n5 female 50+    721.\n6 male   19-25  677.\n\n\nIn the example below, we specify that we want to visualize the plotdata and that the x-axis should represent Age and the y-axis Words(the mean frequency of words). We also tell R that we want to group the data by Sex (i.e. that we want to distinguish between men and women). Then, we add geom_line which tells R that we want a line graph. The result of this is shown below.\n\nggplot(plotdata, aes(x = Age, y = Words, color = Sex, group = Sex)) +\n  geom_line()\n\n\n\n\n\n\n\n\nOnce you have a basic plot like the one above, you can prettify the plot. For example, you can\n\nchange the width of the lines (size = 1.25)\nchange the y-axis limits (coord_cartesian(ylim = c(0, 1000)))\nuse a different theme (theme_bw() means black and white theme)\nmove the legend to the top\nchange the default colors to colors you like (*scale_color_manual …`)\nchange the linetype (scale_linetype_manual ...)\n\n\nggplot(plotdata, aes(\n  x = Age, y = Words,\n  color = Sex,\n  group = Sex,\n  linetype = Sex\n)) +\n  geom_line(size = 1.25) +\n  coord_cartesian(ylim = c(0, 1500)) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_color_manual(\n    breaks = c(\"female\", \"male\"),\n    values = c(\"gray20\", \"gray50\")\n  ) +\n  scale_linetype_manual(\n    breaks = c(\"female\", \"male\"),\n    values = c(\"solid\", \"dotted\")\n  )\n\n\n\n\n\n\n\n\nAn additional and very handy feature of this way of producing graphs is that you\n\ncan integrate them into pipes\ncan easily combine plots.\n\n\nICE_Ire %&gt;%\n  dplyr::filter(\n    Sex != \"NA\",\n    Age != \"NA\",\n    is.na(Sex) == F,\n    is.na(Age) == F\n  ) %&gt;%\n  dplyr::mutate(\n    Age = factor(Age),\n    Sex = factor(Sex)\n  ) %&gt;%\n  ggplot(aes(\n    x = Age,\n    y = Word.count,\n    color = Sex,\n    linetype = Sex\n  )) +\n  geom_point() +\n  stat_summary(fun = mean, geom = \"line\", aes(group = Sex)) +\n  coord_cartesian(ylim = c(0, 2000)) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_color_manual(\n    breaks = c(\"female\", \"male\"),\n    values = c(\"indianred\", \"darkblue\")\n  ) +\n  scale_linetype_manual(\n    breaks = c(\"female\", \"male\"),\n    values = c(\"solid\", \"dotted\")\n  )\n\n\n\n\n\n\n\n\nYou can also create different types of graphs very easily and split them into different facets.\n\nICE_Ire %&gt;%\n  drop_na() %&gt;%\n  dplyr::filter(Age != \"NA\") %&gt;%\n  dplyr::mutate(Date = factor(Date)) %&gt;%\n  ggplot(aes(\n    x = Age,\n    y = Word.count,\n    fill = Sex\n  )) +\n  facet_grid(vars(Date)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 2000)) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(\n    breaks = c(\"female\", \"male\"),\n    values = c(\"#E69F00\", \"#56B4E9\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCreate a box plot showing the Date on the x-axis and the words uttered by speakers on the y-axis and group by Sex.\n\n\n\nAnswer\n\n\nICE_Ire %&gt;%\n  drop_na() %&gt;%\n  dplyr::filter(Sex != \"NA\") %&gt;%\n  dplyr::mutate(Date = factor(Date)) %&gt;%\n  ggplot(aes(\n    x = Date,\n    y = Word.count,\n    fill = Sex\n  )) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 2000)) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(\n    breaks = c(\"female\", \"male\"),\n    values = c(\"#E69F00\", \"#56B4E9\")\n  )\n\n\n\n\n\n\n\n\n\n\nCreate a scatter plot showing the Date on the x-axis and the words uttered by speakers on the y-axis and create different facets for Sex.\n\n\n\nAnswer\n\n\nICE_Ire %&gt;%\n  drop_na() %&gt;%\n  dplyr::filter(\n    Sex != \"NA\",\n    Date != \"NA\"\n  ) %&gt;%\n  dplyr::mutate(\n    Date = factor(Date),\n    Sex = factor(Sex)\n  ) %&gt;%\n  ggplot(aes(Date, Word.count,\n    color = Date\n  )) +\n  facet_wrap(vars(Sex), ncol = 2) +\n  geom_point() +\n  coord_cartesian(ylim = c(0, 2000)) +\n  theme_bw() +\n  scale_color_manual(\n    breaks = c(\"1990-1994\", \"2002-2005\"),\n    values = c(\"#E69F00\", \"#56B4E9\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nAdvanced\nCreate a bar plot showing the number of men and women by Date.\n\n\n Solution"
  },
  {
    "objectID": "tutorials/intror/intror.html#extracting-session-information",
    "href": "tutorials/intror/intror.html#extracting-session-information",
    "title": "Getting started with R",
    "section": "Extracting session information",
    "text": "Extracting session information\nYou can extract the session information by running the sessionInfo function (without any arguments)\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] flextable_0.9.7  here_1.0.1       tokenizers_0.3.0 tm_0.7-14       \n [5] NLP_0.3-0        readxl_1.4.3     quanteda_4.1.0   tidytext_0.4.2  \n [9] lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4     \n[13] purrr_1.0.2      readr_2.1.5      tidyr_1.3.1      tibble_3.2.1    \n[17] ggplot2_3.5.1    tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] fastmatch_1.1-4         gtable_0.3.6            xfun_0.49              \n [4] htmlwidgets_1.6.4       lattice_0.22-6          tzdb_0.4.0             \n [7] vctrs_0.6.5             tools_4.4.1             generics_0.1.3         \n[10] parallel_4.4.1          klippy_0.0.0.9500       fansi_1.0.6            \n[13] janeaustenr_1.0.0       pkgconfig_2.0.3         Matrix_1.7-1           \n[16] data.table_1.16.2       assertthat_0.2.1        uuid_1.2-1             \n[19] lifecycle_1.0.4         farver_2.1.2            compiler_4.4.1         \n[22] textshaping_0.4.0       munsell_0.5.1           codetools_0.2-20       \n[25] fontLiberation_0.1.0    fontquiver_0.2.1        SnowballC_0.7.1        \n[28] htmltools_0.5.8.1       yaml_2.3.10             pillar_1.9.0           \n[31] openssl_2.2.2           fontBitstreamVera_0.1.1 stopwords_2.3          \n[34] zip_2.3.1               tidyselect_1.2.1        digest_0.6.37          \n[37] stringi_1.8.4           slam_0.1-54             labeling_0.4.3         \n[40] rprojroot_2.0.4         fastmap_1.2.0           grid_4.4.1             \n[43] colorspace_2.1-1        cli_3.6.3               magrittr_2.0.3         \n[46] utf8_1.2.4              withr_3.0.2             gdtools_0.4.0          \n[49] scales_1.3.0            timechange_0.3.0        officer_0.6.7          \n[52] rmarkdown_2.28          cellranger_1.1.0        ragg_1.3.3             \n[55] askpass_1.2.1           hms_1.1.3               evaluate_1.0.1         \n[58] knitr_1.48              rlang_1.1.4             Rcpp_1.0.13            \n[61] glue_1.8.0              xml2_1.3.6              jsonlite_1.8.9         \n[64] R6_2.5.1                systemfonts_1.1.0"
  },
  {
    "objectID": "tutorials/coll/coll.html#identifying-collocations-in-sentences",
    "href": "tutorials/coll/coll.html#identifying-collocations-in-sentences",
    "title": "Analyzing Collocations and N-grams in R",
    "section": "Identifying collocations in sentences",
    "text": "Identifying collocations in sentences\nData preparation\nIn a first step, we split our example text into sentences and clean the data (removing punctuation, converting to lower case, etc.).\n\ntext %&gt;%\n    # concatenate the elements in the 'text' object\n    paste0(collapse = \" \") %&gt;%\n    # separate possessives and contractions\n    stringr::str_replace_all(fixed(\"'\"), fixed(\" '\")) %&gt;%\n    stringr::str_replace_all(fixed(\"’\"), fixed(\" '\")) %&gt;%\n    # split text into sentences\n    tokenizers::tokenize_sentences() %&gt;%\n    # unlist sentences\n    unlist() %&gt;%\n    # remove non-word characters\n    stringr::str_replace_all(\"\\\\W\", \" \") %&gt;%\n    stringr::str_replace_all(\"[^[:alnum:] ]\", \" \") %&gt;%\n    # remove superfluous white spaces\n    stringr::str_squish() %&gt;%\n    # convert to lower case and save in 'sentences' object\n    tolower() -&gt; sentences\n\n\n\n.the origin of species by charles darwin an historical sketch of the progress of opinion on the origin of species introduction when on board h m sbeagle as naturalist i was much struck with certain facts in the distribution of the organic beings inhabiting south america and in the geological relations of the present to the past inhabitants of that continentthese facts as will be seen in the latter chapters of this volume seemed to throw some light on the origin of species that mystery of mysteries as it has been called by one of our greatest philosopherson my return home it occurred to me in 1837 that something might perhaps be made out on this question by patiently accumulating and reflecting on all sorts of facts which could possibly have any bearing on itafter five years work i allowed myself to speculate on the subject and drew up some short notes these i enlarged in 1844 into a sketch of the conclusions which then seemed to me probable from that period to the present day i have steadily pursued the same objecti hope that i may be excused for entering on these personal details as i give them to show that i have not been hasty in coming to a decisionmy work is now 1859 nearly finished but as it will take me many more years to complete it and as my health is far from strong i have been urged to publish this abstracti have more especially been induced to do this as mrwallace who is now studying the natural history of the malay archipelago has arrived at almost exactly the same general conclusions that i have on the origin of speciesin 1858 he sent me a memoir on this subject with a request that i would forward it to sir charles lyell who sent it to the linnean society and it is published in the third volume of the journal of that society\n\n\nNext, we tabulate the data and reformat it so that we have the relevant information to calculate the association statistics (word 1 and word 2 as well as O11, O12, O21, and O22).\n\n# tokenize the 'sentences' data using quanteda package\nsentences %&gt;%\n    quanteda::tokens() %&gt;%\n    # create a document-feature matrix (dfm) using quanteda\n    quanteda::dfm() %&gt;%\n    # create a feature co-occurrence matrix (fcm) without considering trigrams\n    quanteda::fcm(tri = FALSE) %&gt;%\n    # tidy the data using tidytext package\n    tidytext::tidy() %&gt;%\n    # rearrange columns for better readability\n    dplyr::relocate(term, document, count) %&gt;%\n    # rename columns for better interpretation\n    dplyr::rename(\n        w1 = 1,\n        w2 = 2,\n        O11 = 3\n    ) -&gt; coll_basic\n\n\n\nw1w2O11thethe24,287theorigin170theof37,291thespecies6,222theby5,415thecharles28thedarwin11thean2,049thehistorical7thesketch8\n\n\nWe now enhance our table by calculating all observed frequencies (O11, O12, O21, O22) as well as row totals (R1, R2), column totals (C1, C2), and the overall total (N).\n\n# calculate the total number of observations (N)\ncoll_basic %&gt;%\n    dplyr::mutate(N = sum(O11)) %&gt;%\n    # calculate R1, O12, and R2\n    dplyr::group_by(w1) %&gt;%\n    dplyr::mutate(\n        R1 = sum(O11),\n        O12 = R1 - O11,\n        R2 = N - R1\n    ) %&gt;%\n    dplyr::ungroup(w1) %&gt;%\n    # calculate C1, O21, C2, and O22\n    dplyr::group_by(w2) %&gt;%\n    dplyr::mutate(\n        C1 = sum(O11),\n        O21 = C1 - O11,\n        C2 = N - C1,\n        O22 = R2 - O21\n    ) -&gt; colldf\n\n\n\nw1w2O11NR1O12R2C1O21C2O22thethe24,2879,405,996643,895619,6088,762,101643,895619,6088,762,1018,142,493theorigin1709,405,996643,895643,7258,762,1012,8842,7149,403,1128,759,387theof37,2919,405,996643,895606,6048,762,101450,460413,1698,955,5368,348,932thespecies6,2229,405,996643,895637,6738,762,10189,99483,7729,316,0028,678,329theby5,4159,405,996643,895638,4808,762,10180,78575,3709,325,2118,686,731thecharles289,405,996643,895643,8678,762,1014514239,405,5458,761,678thedarwin119,405,996643,895643,8848,762,1011791689,405,8178,761,933thean2,0499,405,996643,895641,8468,762,10133,80931,7609,372,1878,730,341thehistorical79,405,996643,895643,8888,762,1011851789,405,8118,761,923thesketch89,405,996643,895643,8878,762,1011521449,405,8448,761,957\n\n\nTo determine which terms collocate significantly and with what association strength, we use the following information (that is provided by the table above):\n\nO11 = Number of times word1 occurs with word2\nO12 = Number of times word1 occurs without word2\nO21 = Number of times CoocTerm occurs without Term\nO22 = Number of terms that are not coocTerm or Term\n\nExample:\n\n\n\n\nw2 present\nw2 absent\n\n\n\n\n\nw1 present\nO11\nO12\n= R1\n\n\nw1 absent\nO21\nO22\n= R2\n\n\n\n= C1\n= C2\n= N\n\n\n\nWe could calculate all collocations in the corpus (based on co-occurrence within the same sentence) or we can find collocations of a specific term - here, we will find collocations fo the term selection.\nNow that we have all the relevant information, we will reduce the data and add additional information to the data so that the computing of the association measures runs smoothly.\n\n# reduce and complement data\ncolldf %&gt;%\n    # determine Term\n    dplyr::filter(\n        w1 == \"selection\",\n        # set minimum number of occurrences of w2\n        (O11 + O21) &gt; 10,\n        # set minimum number of co-occurrences of w1 and w2\n        O11 &gt; 5\n    ) %&gt;%\n    dplyr::rowwise() %&gt;%\n    dplyr::mutate(\n        E11 = R1 * C1 / N,\n        E12 = R1 * C2 / N,\n        E21 = R2 * C1 / N,\n        E22 = R2 * C2 / N\n    ) -&gt; colldf_redux\n\n\n\nw1w2O11NR1O12R2C1O21C2O22E11E12E21E22selectionthe1,7839,405,99626,79325,0109,379,203643,895642,1128,762,1018,737,0911,834.1363024,958.86642,060.8648,737,142selectionorigin199,405,99626,79326,7749,379,2032,8842,8659,403,1129,376,3388.2150826,784.782,875.7859,376,327selectionof1,5569,405,99626,79325,2379,379,203450,460448,9048,955,5368,930,2991,283.1362925,509.86449,176.8648,930,026selectionspecies1759,405,99626,79326,6189,379,20389,99489,8199,316,0029,289,384256.3481026,536.6589,737.6529,289,465selectionby3349,405,99626,79326,4599,379,20380,78580,4519,325,2119,298,752230.1162526,562.8880,554.8849,298,648selectionan909,405,99626,79326,7039,379,20333,80933,7199,372,1879,345,48496.3050126,696.6933,712.6959,345,490selectionon2009,405,99626,79326,5939,379,20371,20971,0099,334,7879,308,194202.8389926,590.1671,006.1619,308,197selectionwhen639,405,99626,79326,7309,379,20326,60726,5449,379,3899,352,65975.7901026,717.2126,531.2109,352,672selections389,405,99626,79326,7559,379,2035,9755,9379,400,0219,373,26617.0198026,775.985,957.9809,373,245selectionas2969,405,99626,79326,4979,379,203103,198102,9029,302,7989,276,301293.9597326,499.04102,904.0409,276,299\n\n\nNow we can calculate the collocation statistics (the association strength).\n\ncolldf_redux %&gt;%\n    # determine number of rows\n    dplyr::mutate(Rws = nrow(.)) %&gt;%\n    # work row-wise\n    dplyr::rowwise() %&gt;%\n    # calculate fishers' exact test\n    dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22),\n        ncol = 2, byrow = T\n    ))[1]))) %&gt;%\n    # extract AM\n    # 1. bias towards top left\n    dplyr::mutate(\n        btl_O12 = ifelse(C1 &gt; R1, 0, R1 - C1),\n        btl_O11 = ifelse(C1 &gt; R1, R1, R1 - btl_O12),\n        btl_O21 = ifelse(C1 &gt; R1, C1 - R1, C1 - btl_O11),\n        btl_O22 = ifelse(C1 &gt; R1, C2, C2 - btl_O12),\n\n        # 2. bias towards top right\n        btr_O11 = 0,\n        btr_O21 = R1,\n        btr_O12 = C1,\n        btr_O22 = C2 - R1\n    ) %&gt;%\n    # 3. calculate AM\n    dplyr::mutate(\n        upp = btl_O11 / R1,\n        low = btr_O11 / R1,\n        op = O11 / R1\n    ) %&gt;%\n    dplyr::mutate(AM = op / upp) %&gt;%\n    # remove superfluous columns\n    dplyr::select(-any_of(c(\n        \"btr_O21\", \"btr_O12\", \"btr_O22\", \"btl_O12\",\n        \"btl_O11\", \"btl_O21\", \"btl_O22\", \"btr_O11\"\n    ))) %&gt;%\n    # extract x2 statistics\n    dplyr::mutate(X2 = (O11 - E11)^2 / E11 + (O12 - E12)^2 / E12 + (O21 - E21)^2 / E21 + (O22 - E22)^2 / E22) %&gt;%\n    # extract association measures\n    dplyr::mutate(\n        phi = sqrt((X2 / N)),\n        Dice = (2 * O11) / (R1 + C1),\n        LogDice = log((2 * O11) / (R1 + C1)),\n        MI = log2(O11 / E11),\n        MS = min((O11 / C1), (O11 / R1)),\n        t.score = (O11 - E11) / sqrt(O11),\n        z.score = (O11 - E11) / sqrt(E11),\n        PMI = log2((O11 / N) / (C1 / N * R1 / N)),\n        DeltaP12 = (O11 / (O11 + O12)) - (O21 / (O21 + O22)),\n        DeltaP21 = (O11 / (O11 + O21)) - (O21 / (O12 + O22)),\n        DP = (O11 / R1) - (O21 / R2),\n        LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ((O12 + 0.5) * (O21 + 0.5))),\n        # calculate LL aka G2\n        G2 = 2 * (O11 * log(O11 / E11) + O12 * log(O12 / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22))\n    ) %&gt;%\n    # determine Bonferroni corrected significance\n    dplyr::mutate(Sig_corrected = dplyr::case_when(\n        p / Rws &gt; .05 ~ \"n.s.\",\n        p / Rws &gt; .01 ~ \"p &lt; .05*\",\n        p / Rws &gt; .001 ~ \"p &lt; .01**\",\n        p / Rws &lt;= .001 ~ \"p &lt; .001***\",\n        T ~ \"N.A.\"\n    )) %&gt;%\n    # round p-value\n    dplyr::mutate(p = round(p, 5)) %&gt;%\n    # filter out non significant results\n    dplyr::filter(\n        Sig_corrected != \"n.s.\",\n        # filter out instances where the w1 and w2 repel each other\n        E11 &lt; O11\n    ) %&gt;%\n    # arrange by DeltaP12 (association measure)\n    dplyr::arrange(-DeltaP12) %&gt;%\n    # remove superfluous columns\n    dplyr::select(-any_of(c(\n        \"TermCoocFreq\", \"AllFreq\", \"NRows\", \"O12\", \"O21\",\n        \"O22\", \"R1\", \"R2\", \"C1\", \"C2\", \"E11\", \"E12\", \"E21\",\n        \"E22\", \"upp\", \"low\", \"op\", \"Rws\"\n    ))) -&gt; assoc_tb\n\n\n\nw1w2O11NpAMX2phiDiceLogDiceMIMSt.scorez.scorePMIDeltaP12DeltaP21DPLogOddsRatioG2Sig_correctedselectionnatural5159,405,9960.000000.0202588412,720.224280.0170059130.019726510-3.9257922.83027620.01922143819.50276752.0110072.83027620.0165659890.0176037800.0165659891.99709321,150.64107p &lt; .001***selectionof1,5569,405,9960.000000.05807487061.118240.0025490770.006520650-5.0327810.27816760.0034542476.9173707.6174460.27816760.010213234-0.0466716200.0102132340.204503857.40403p &lt; .001***selectionto7769,405,9960.000000.02896278938.305240.0020180260.006317289-5.0644650.31569980.0035452895.4749396.1079690.31569980.005708574-0.0201951370.0057085740.226009835.59006p &lt; .001***selectionthrough1499,405,9960.000000.010922953313.562970.0057737770.007370035-4.9103331.93908750.0055611549.02331417.6696451.93908750.0041226520.0094864650.0041226521.3596350181.59628p &lt; .001***selectionby3349,405,9960.000000.01246594347.438720.0022457640.006209448-5.0816830.53748530.0041344315.6842666.8481610.53748530.003888348-0.0044928270.0038883480.379247441.64215p &lt; .001***selectiontheory829,405,9960.000000.011239035180.976930.0043864100.004810936-5.3368641.98024280.0030605016.76032313.4283821.98024280.0022913520.0104714820.0022913521.3893366103.32874p &lt; .001***selectionbeen2339,405,9960.000010.00869630121.983810.0015287940.005349312-5.2307870.43939420.0038626684.0077404.6669700.43939420.002289787-0.0025668300.0022897870.310006219.77955p &lt; .001***selectionvariations779,405,9960.000000.009361702122.949550.0036154390.004397738-5.4266651.71656740.0028738856.10499011.0676161.71656740.0020051540.0084946880.0020051541.204788576.55053p &lt; .001***selectionwill1499,405,9960.000000.00556115428.382080.0017370800.004904300-5.3176430.62277570.0043862234.2793715.3102750.62277570.0019551970.0007775050.0019551970.438499324.34962p &lt; .001***selectionpower639,405,9960.000000.013188193179.892600.0043732500.003991131-5.5236812.21097150.0023513606.22289613.3898872.21097150.0018487590.0126867690.0018487591.552583994.91857p &lt; .001***"
  },
  {
    "objectID": "tutorials/coll/coll.html#identifying-collocations-using-kwics",
    "href": "tutorials/coll/coll.html#identifying-collocations-using-kwics",
    "title": "Analyzing Collocations and N-grams in R",
    "section": "Identifying collocations using kwics",
    "text": "Identifying collocations using kwics\nIn this section, we will extract collocations and calculate association measures based on concordances and the corpus the concordances were extracted from.\nWe start by cleaning our corpus and splitting it into chapters.\n\n# clean corpus\ntext %&gt;%\n    # concatenate the elements in the 'text' object\n    paste0(collapse = \" \") %&gt;%\n    # separate possessives and contractions\n    stringr::str_replace_all(fixed(\"'\"), fixed(\" '\")) %&gt;%\n    stringr::str_replace_all(fixed(\"’\"), fixed(\" '\")) %&gt;%\n    # split text into different chapters\n    stringr::str_split(\"CHAPTER [IVX]{1,4}\") %&gt;%\n    # unlist sentences\n    unlist() %&gt;%\n    # remove non-word characters\n    stringr::str_replace_all(\"\\\\W\", \" \") %&gt;%\n    stringr::str_replace_all(\"[^[:alpha:] ]\", \" \") %&gt;%\n    # remove superfluous white spaces\n    stringr::str_squish() %&gt;%\n    # convert to lower case and save in 'sentences' object\n    tolower() -&gt; texts\n\n\n\n.the origin of species by charles darwin an historical sketch of the progress of opinion on the origivariation under domestication causes of variability effects of habit and the use or disuse of partscvariation under nature variability individual differences doubtful species wide ranging much diffusestruggle for existence its bearing on natural selection the term used in a wide sense geometrical ranatural selection or the survival of the fittest natural selection its power compared with man s self under changing conditions of life organic beings present individual differences in almost every palaws of variation effects of changed conditions use and disuse combined with natural selection organdifficulties of the theory difficulties of the theory of descent with modification absence or raritymiscellaneous objections to the theory of natural selection longevity modifications not necessarily instinct instincts comparable with habits but different in their origin instincts graduated aphides \n\n\n\n\n\nWe split the corpus into chapter to mirror the fact that most text data will come in the form of corpora which consist of different files containing texts.\n\n\n\n\n\n\nNext, we generate a frequency list of words that occur around a keyword (we use the keyword selection in this example but you can also choose a different word).\nfor this we use the tokens_select function (from the quanteda package) which has the following arguments:\n\nx: a text or collection of texts. The text needs to be tokenised, i.e. split it into individual words, which is why we use the text in the tokens() function.\npattern: a keyword defined by a search pattern\n\nwindow: the size of the context window (how many word before and after)\n\nvaluetype: the type of pattern matching\n\n“glob” for “glob”-style wildcard expressions;\n\n“regex” for regular expressions; or\n\n“fixed” for exact matching\n\n\nselection: a character to define if the key word should be retained in the resulting frequency list or if it should be removed. The argument offers two options\n\n“keep”\n\n“remove”\n\n\ncase_insensitive: logical; if TRUE, ignore case when matching a pattern or dictionary values\n\n\nkwic_words &lt;- quanteda::tokens_select(tokens(texts),\n    pattern = \"selection\",\n    window = 5,\n    selection = \"keep\"\n) %&gt;%\n    unlist() %&gt;%\n    # tabulate results\n    table() %&gt;%\n    # convert into data frame\n    as.data.frame() %&gt;%\n    # rename columns\n    dplyr::rename(\n        token = 1,\n        n = 2\n    ) %&gt;%\n    # add a column with type\n    dplyr::mutate(type = \"kwic\")\n\n\n\ntokenntypea54kwicable2kwicabounding1kwicabove2kwicabsolute1kwicabsurd1kwicaccordance2kwicaccording2kwicaccount4kwicaccumulate2kwic\n\n\nNext, we create a frequency table of the entire clean corpus.\n\ncorpus_words &lt;- texts %&gt;%\n    # tokenize the corpus files\n    quanteda::tokens() %&gt;%\n    # unlist the tokens to create a data frame\n    unlist() %&gt;%\n    as.data.frame() %&gt;%\n    # rename the column to 'token'\n    dplyr::rename(token = 1) %&gt;%\n    # group by 'token' and count the occurrences\n    dplyr::group_by(token) %&gt;%\n    dplyr::summarise(n = n()) %&gt;%\n    # add column stating where the frequency list is 'from'\n    dplyr::mutate(type = \"corpus\")\n\n\n\ntokenntypea3,163corpusabdomen3corpusaberrant7corpusaberration2corpusabhorrent1corpusabilities1corpusability3corpusabjectly1corpusable54corpusably3corpus\n\n\nNext, we combine the two frequency lists.\n\nfreq_df &lt;- dplyr::left_join(corpus_words, kwic_words, by = c(\"token\")) %&gt;%\n    # rename columns and select relevant columns\n    dplyr::rename(\n        corpus = n.x,\n        kwic = n.y\n    ) %&gt;%\n    dplyr::select(-type.x, -type.y) %&gt;%\n    # replace NA values with 0 in 'corpus' and 'kwic' columns\n    tidyr::replace_na(list(corpus = 0, kwic = 0))\n\n\n\ntokencorpuskwica3,16354abdomen30aberrant70aberration20abhorrent10abilities10ability30abjectly10able542ably30\n\n\nWe now calculate the frequencies of the observed and expected frequencies as well as the row and column totals.\n\nfreq_df %&gt;%\n    dplyr::filter(corpus &gt; 0) %&gt;%\n    dplyr::mutate(\n        corpus = as.numeric(corpus),\n        kwic = as.numeric(kwic)\n    ) %&gt;%\n    dplyr::mutate(\n        corpus = corpus - kwic,\n        C1 = sum(kwic),\n        C2 = sum(corpus),\n        N = C1 + C2\n    ) %&gt;%\n    dplyr::rowwise() %&gt;%\n    dplyr::mutate(\n        R1 = corpus + kwic,\n        R2 = N - R1,\n        O11 = kwic,\n        O12 = R1 - O11,\n        O21 = C1 - O11,\n        O22 = C2 - O12\n    ) %&gt;%\n    dplyr::mutate(\n        E11 = (R1 * C1) / N,\n        E12 = (R1 * C2) / N,\n        E21 = (R2 * C1) / N,\n        E22 = (R2 * C2) / N\n    ) %&gt;%\n    dplyr::select(-corpus, -kwic) -&gt; stats_tb\n\n\n\ntokenC1C2NR1R2O11O12O21O22E11E12E21E22a5,830188,275194,1053,163190,942543,1095,776185,16695.001622833,067.99837725,734.998185,207.0abdomen5,830188,275194,1053194,102035,830188,2720.090105872.90989415,829.910188,272.1aberrant5,830188,275194,1057194,098075,830188,2680.210247036.78975305,829.790188,268.2aberration5,830188,275194,1052194,103025,830188,2730.060070581.93992945,829.940188,273.1abhorrent5,830188,275194,1051194,104015,830188,2740.030035290.96996475,829.970188,274.0abilities5,830188,275194,1051194,104015,830188,2740.030035290.96996475,829.970188,274.0ability5,830188,275194,1053194,102035,830188,2720.090105872.90989415,829.910188,272.1abjectly5,830188,275194,1051194,104015,830188,2740.030035290.96996475,829.970188,274.0able5,830188,275194,10554194,0512525,828188,2231.6219056752.37809435,828.378188,222.6ably5,830188,275194,1053194,102035,830188,2720.090105872.90989415,829.910188,272.1\n\n\nTo determine which terms collocate significantly and with what association strength, we use the following information (that is provided by the table above):\n\nO11 = Number of times wordx occurs in kwic\nO12 = Number of times wordx occurs in corpus (without kwic)\nO21 = Number of times other words occur in kwic\nO22 = Number of times other words occur in corpus\n\nExample:\n\n\n\n\nkwic\ncorpus\n\n\n\n\n\ntoken\nO11\nO12\n= R1\n\n\nother tokens\nO21\nO22\n= R2\n\n\n\n= C1\n= C2\n= N\n\n\n\n\nstats_tb %&gt;%\n    # determine number of rows\n    dplyr::mutate(Rws = nrow(.)) %&gt;%\n    # work row-wise\n    dplyr::rowwise() %&gt;%\n    # calculate fishers' exact test\n    dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22),\n        ncol = 2, byrow = T\n    ))[1]))) %&gt;%\n    # extract AM\n    # 1. bias towards top left\n    dplyr::mutate(\n        btl_O12 = ifelse(C1 &gt; R1, 0, R1 - C1),\n        btl_O11 = ifelse(C1 &gt; R1, R1, R1 - btl_O12),\n        btl_O21 = ifelse(C1 &gt; R1, C1 - R1, C1 - btl_O11),\n        btl_O22 = ifelse(C1 &gt; R1, C2, C2 - btl_O12),\n\n        # 2. bias towards top right\n        btr_O11 = 0,\n        btr_O21 = R1,\n        btr_O12 = C1,\n        btr_O22 = C2 - R1\n    ) %&gt;%\n    # 3. calculate AM\n    dplyr::mutate(\n        upp = btl_O11 / R1,\n        low = btr_O11 / R1,\n        op = O11 / R1\n    ) %&gt;%\n    dplyr::mutate(AM = op / upp) %&gt;%\n    # remove superfluous columns\n    dplyr::select(-any_of(c(\n        \"btr_O21\", \"btr_O12\", \"btr_O22\", \"btl_O12\",\n        \"btl_O11\", \"btl_O21\", \"btl_O22\", \"btr_O11\"\n    ))) %&gt;%\n    # extract x2 statistics\n    dplyr::mutate(X2 = (O11 - E11)^2 / E11 + (O12 - E12)^2 / E12 + (O21 - E21)^2 / E21 + (O22 - E22)^2 / E22) %&gt;%\n    # extract expected frequency\n    dplyr::mutate(Exp = E11) %&gt;%\n    # extract association measures\n    dplyr::mutate(\n        phi = sqrt((X2 / N)),\n        MS = min((O11 / C1), (O11 / R1)),\n        Dice = (2 * O11) / (R1 + C1),\n        LogDice = log((2 * O11) / (R1 + C1)),\n        MI = log2(O11 / E11),\n        t.score = (O11 - E11) / sqrt(O11),\n        z.score = (O11 - E11) / sqrt(E11),\n        PMI = log2((O11 / N) / ((O11 + O12) / N) *\n            ((O11 + O21) / N)),\n        DeltaP12 = (O11 / (O11 + O12)) - (O21 / (O21 + O22)),\n        DeltaP21 = (O11 / (O11 + O21)) - (O21 / (O12 + O22)),\n        DP = (O11 / R1) - (O21 / R2),\n        LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ((O12 + 0.5) * (O21 + 0.5))),\n        # calculate LL aka G2\n        G2 = 2 * (O11 * log(O11 / E11) + O12 * log(O12 / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22))\n    ) %&gt;%\n    # determine Bonferroni corrected significance\n    dplyr::mutate(Sig_corrected = dplyr::case_when(\n        p / Rws &gt; .05 ~ \"n.s.\",\n        p / Rws &gt; .01 ~ \"p &lt; .05*\",\n        p / Rws &gt; .001 ~ \"p &lt; .01**\",\n        p / Rws &lt;= .001 ~ \"p &lt; .001***\",\n        T ~ \"N.A.\"\n    )) %&gt;%\n    # round p-value\n    dplyr::mutate(p = round(p, 5)) %&gt;%\n    # filter out non significant results\n    dplyr::filter(\n        Sig_corrected != \"n.s.\",\n        # filter out instances where the w1 and w2 repel each other\n        E11 &lt; O11\n    ) %&gt;%\n    # arrange by phi (association measure)\n    dplyr::arrange(-DeltaP12) %&gt;%\n    # remove superfluous columns\n    dplyr::select(-any_of(c(\n        \"TermCoocFreq\", \"AllFreq\", \"NRows\", \"O12\", \"O21\",\n        \"O22\", \"R1\", \"R2\", \"C1\", \"C2\", \"E11\", \"E12\", \"E21\",\n        \"E22\", \"upp\", \"low\", \"op\", \"Rws\"\n    ))) -&gt; assoc_tb2\n\n\n\ntokenNO11pAMX2ExpphiMSDiceLogDiceMIt.scorez.scorePMIDeltaP12DeltaP21DPLogOddsRatioG2Sig_correctedselection194,1055400.00000117,487.5009916.219056700.300154950.09262435680.1695447410-1.7746385.05719822.5399430130.057948-5.0571980.97267070.064527160.972670710.557635p &lt; .001***methodical194,105100.000001322.958320.300352900.040790110.00171526590.0034246575-5.6767545.0571983.067297717.698645-5.0571980.9700147-0.029196960.97001476.521043p &lt; .001***accumulative194,10530.00003196.884000.090105870.022341260.00051457980.0010286302-6.8795275.0571981.68002829.693947-5.0571980.9699797-0.030434830.96997975.421228p &lt; .001***rigorous194,10530.00003196.884000.090105870.022341260.00051457980.0010286302-6.8795275.0571981.68002829.693947-5.0571980.9699797-0.030434830.96997975.421228p &lt; .001***cotton194,10520.00090164.589000.060070580.018241520.00034305320.0006858711-7.2848215.0571981.37173727.915075-5.0571980.9699747-0.030611670.96997475.084585p &lt; .001***incompetent194,10520.00090164.589000.060070580.018241520.00034305320.0006858711-7.2848215.0571981.37173727.915075-5.0571980.9699747-0.030611670.96997475.084585p &lt; .001***rigid194,10520.00090164.589000.060070580.018241520.00034305320.0006858711-7.2848215.0571981.37173727.915075-5.0571980.9699747-0.030611670.96997475.084585p &lt; .001***agreeable194,10510.03004132.294330.030035290.012898670.00017152660.0003429943-7.9777975.0571980.96996475.596803-5.0571980.9699697-0.030788510.96996974.573587p &lt; .001***amoimt194,10510.03004132.294330.030035290.012898670.00017152660.0003429943-7.9777975.0571980.96996475.596803-5.0571980.9699697-0.030788510.96996974.573587p &lt; .001***architecture194,10510.03004132.294330.030035290.012898670.00017152660.0003429943-7.9777975.0571980.96996475.596803-5.0571980.9699697-0.030788510.96996974.573587p &lt; .001***"
  },
  {
    "objectID": "tutorials/coll/coll.html#visualising-collocations",
    "href": "tutorials/coll/coll.html#visualising-collocations",
    "title": "Analyzing Collocations and N-grams in R",
    "section": "Visualising collocations",
    "text": "Visualising collocations\n\nDotplots\nWe can now visualize the association strengths in a dotplot as shown in the code chunk below.\n\n# sort the assoc_tb2 data frame in descending order based on the 'phi' column\nassoc_tb2 %&gt;%\n    dplyr::arrange(-phi) %&gt;%\n    # select the top 20 rows after sorting\n    head(20) %&gt;%\n    # create a ggplot with 'token' on the x-axis (reordered by 'phi') and 'phi' on the y-axis\n    ggplot(aes(x = reorder(token, phi, mean), y = phi)) +\n    # add a scatter plot with points representing the 'phi' values\n    geom_point() +\n    # flip the coordinates to have horizontal points\n    coord_flip() +\n    # set the theme to a basic white and black theme\n    theme_bw() +\n    # set the x-axis label to \"Token\" and y-axis label to \"Association strength (phi)\"\n    labs(x = \"Token\", y = \"Association strength (phi)\")\n\n\n\n\n\n\n\n\n\n\nBarplots\nAnother option sis to visualize the association strengths in a barplot as shown in the code chunk below.\n\n# sort the assoc_tb2 data frame in descending order based on the 'phi' column\nassoc_tb2 %&gt;%\n    dplyr::arrange(-phi) %&gt;%\n    # select the top 20 rows after sorting\n    head(20) %&gt;%\n    # create a ggplot with 'token' on the x-axis (reordered by 'phi') and 'phi' on the y-axis\n    ggplot(aes(x = reorder(token, phi, mean), y = phi, label = phi)) +\n    # add a bar plot using the 'phi' values\n    geom_bar(stat = \"identity\") +\n    # add text labels above the bars with rounded 'phi' values\n    geom_text(aes(y = phi - 0.005, label = round(phi, 3)), color = \"white\", size = 3) +\n    # flip the coordinates to have horizontal bars\n    coord_flip() +\n    # set the theme to a basic white and black theme\n    theme_bw() +\n    # set the x-axis label to \"Token\" and y-axis label to \"Association strength (phi)\"\n    labs(x = \"Token\", y = \"Association strength (phi)\")\n\n\n\n\n\n\n\n\n\n\nDendrograms\nAnother method for visualizing collocations are dendrograms (tree-diagrams) which show how similarity to indicate groupings based on numeric values (e.g., association strength).\nWe start by extracting the tokens that we want to show (the top 20 collocates of selection).\n\n# sort the assoc_tb2 data frame in descending order based on the 'phi' column\ntop20colls &lt;- assoc_tb2 %&gt;%\n    dplyr::arrange(-phi) %&gt;%\n    # select the top 20 rows after sorting\n    head(20) %&gt;%\n    # extract the 'token' column\n    dplyr::pull(token)\n# inspect the top 20 tokens with the highest 'phi' values\ntop20colls\n\n [1] \"selection\"    \"natural\"      \"through\"      \"theory\"       \"unconscious\" \n [6] \"methodical\"   \"acts\"         \"sexual\"       \"accumulated\"  \"by\"          \n[11] \"power\"        \"man\"          \"action\"       \"principle\"    \"accumulative\"\n[16] \"rigorous\"     \"survival\"     \"effects\"      \"process\"      \"aided\"       \n\n\nWe then need to generate a feature co-occurrence matrix from a document-feature matrix based on the cleaned, lower case sentences of our text.\n\n# tokenize the 'sentences' data using quanteda package\nkeyword_fcm &lt;- sentences %&gt;%\n    quanteda::tokens() %&gt;%\n    # create a document-feature matrix (dfm) from the tokens\n    quanteda::dfm() %&gt;%\n    # select features based on 'top20colls' and the term \"selection\" pattern\n    quanteda::dfm_select(pattern = c(top20colls, \"selection\")) %&gt;%\n    # Create a symmetric feature co-occurrence matrix (fcm)\n    quanteda::fcm(tri = FALSE)\n# inspect the first 6 rows and 6 columns of the resulting fcm\nkeyword_fcm[1:6, 1:6]\n\nFeature co-occurrence matrix of: 6 by 6 features.\n         features\nfeatures   by natural aided effects power man\n  by      461     282    25      37    54  81\n  natural 282      49     9      28    38  18\n  aided    25       9     0       1     1   0\n  effects  37      28     1       3     1   5\n  power    54      38     1       1     9  15\n  man      81      18     0       5    15  10\n\n\nThen we generate the dendrogram based on a distance matrix generated from the feature co-occurrence matrix.\n\n# create a hierarchical clustering object using the distance matrix of the fcm as data\nhclust(dist(keyword_fcm),\n    # use ward.D as linkage method\n    method = \"ward.D2\"\n) %&gt;%\n    # generate visualization (dendrogram)\n    ggdendrogram() +\n    # add title\n    ggtitle(\"20 most strongly collocating terms of 'selection'\")\n\n\n\n\n\n\n\n\n\n\nNetwork Graphs\nNetwork graphs, or networks for short, are a powerful and versatile visual representation used to depict relationships or connections among various elements. Network graphs typically consist of nodes, representing individual entities, and edges, indicating the connections or interactions between these entities. Nodes can represent diverse entities such as words (collocates), interlocutors, objects, or concepts, while edges convey the relationships or associations between them.\nHere we generate a basic network graph of the collocates of our keyword based on the fcm.\n\n# create a network plot using the fcm\nquanteda.textplots::textplot_network(keyword_fcm,\n    # set the transparency of edges to 0.8 for visibility\n    edge_alpha = 0.8,\n    # set the color of edges to gray\n    edge_color = \"gray\",\n    # set the size of edges to 2 for better visibility\n    edge_size = 2,\n    # adjust the size of vertex labels\n    # based on the logarithm of row sums of the fcm\n    vertex_labelsize = log(rowSums(keyword_fcm))\n)\n\n\n\n\n\n\n\n\n\n\nBiplots\nAn alternative way to display co-occurrence patterns are bi-plots which are used to display the results of a Correspondence Analysis. Bi-plots are useful, in particular, when one is not interested in one particular keyterm and its collocations but in the overall similarity of many terms. Semantic similarity in this case refers to a shared semantic and this distributional profile. As such, words can be deemed semantically similar if they have a similar co-occurrence profile - i.e. they co-occur with the same elements. Biplots can be used to visualize collocations because collocates co-occur and thus share semantic properties which renders then more similar to each other compared with other terms.\n\n# perform correspondence analysis\nres.ca &lt;- CA(as.matrix(keyword_fcm), graph = FALSE)\n# plot results\nfviz_ca_row(res.ca, repel = TRUE, col.row = \"gray20\")"
  },
  {
    "objectID": "tutorials/coll/coll.html#identifying-n-grams-using-quanteda",
    "href": "tutorials/coll/coll.html#identifying-n-grams-using-quanteda",
    "title": "Analyzing Collocations and N-grams in R",
    "section": "Identifying n-grams using quanteda",
    "text": "Identifying n-grams using quanteda\nThe quanteda package (see Benoit et al.) offers excellent and very fast functions for extracting N-grams. It’s a fun way to discover meaningful word pairs in your text! Below, we use the textstat_collocations function for extracting N-grams. This function uses the following main arguments\n\nx: a character, corpus, or tokens object.\n\nmethod: association measure for detecting collocations. Currently this is limited to “lambda”.\n\nsize: integer; the length of the ngram. The default is 2 - if you want to extract tri-grams set size = 3 and if you want to extract four-grams set size = 4 and so on.\n\nmin_count: numeric; minimum frequency of collocations that will be scored.\n\nsmoothing: numeric; a smoothing parameter added to the observed counts (default is 0.5).\n\ntolower: logical; if TRUE, tokens are transformed to lower-case.\n\n\n# concatenate the elements in the 'text' object\ntext %&gt;%\n    paste0(collapse = \" \") %&gt;%\n    # convert to lower case\n    tolower() %&gt;%\n    # convert the concatenated text into tokens\n    quanteda::tokens() %&gt;%\n    # identify and extract bigrams that occur at leats 10 times\n    quanteda.textstats::textstat_collocations(size = 2, min_count = 10) %&gt;%\n    # convert into a data frame and save results in an object called 'ngrams'\n    as.data.frame() %&gt;%\n    # order by lambda\n    dplyr::arrange(-lambda) -&gt; ngrams\n\n\n\ncollocationcountcount_nestedlengthlambdazla plata100214.1726418.972104asa gray100213.58485411.365881de candolle200213.2321359.069094malay archipelago110211.7953498.099435fritz miiller120211.78199814.798749close interbreeding110211.0605147.626630informs me140210.5472547.319025new zealand270210.5307317.372304reproductive systems120210.07853215.838139laws governing140210.07630914.349877i am600210.0634157.084662systematic affinity12029.84481817.664599consecutive formations13029.29354513.332193reciprocal crosses15029.28470617.907863united states29029.05808826.394583"
  },
  {
    "objectID": "tutorials/coll/coll.html#identifying-n-grams-using-quanteda-1",
    "href": "tutorials/coll/coll.html#identifying-n-grams-using-quanteda-1",
    "title": "Analyzing Collocations and N-grams in R",
    "section": "Identifying n-grams using quanteda",
    "text": "Identifying n-grams using quanteda\nCreating N-gram lists manually, especially bi-grams, is surprisingly easy. In our example text, we’ll craft a bi-gram list by doing something quite straightforward: taking each word and introducing it to the next word in line. The difference to the previous method is that we retain the original order of the bi-grams here.\nIn a first step, we split the text into words and remove any non-word characters.\n\n# process the text\ntext %&gt;%\n    # convert all text to lowercase\n    tolower() %&gt;%\n    # remove non-word characters, keeping spaces\n    str_replace_all(\"[^[:alpha:][:space:]]*\", \"\") %&gt;%\n    # remove punctuation\n    tm::removePunctuation() %&gt;%\n    # squish consecutive spaces into a single space\n    stringr::str_squish() %&gt;%\n    # split the text into individual words, separated by spaces\n    stringr::str_split(\" \") %&gt;%\n    # unlist the result into a single vector of words  and save result in \"text_words\"\n    unlist() -&gt; text_words\n\nNow, we generate a table with the N-grams(in our case bi-grams).\n\n# create data frame\ntext_bigrams &lt;- data.frame(\n    text_words[1:length(text_words) - 1],\n    text_words[2:length(text_words)]\n) %&gt;%\n    dplyr::rename(\n        Word1 = 1,\n        Word2 = 2\n    ) %&gt;%\n    dplyr::mutate(Bigram = paste0(Word1, \" \", Word2)) %&gt;%\n    dplyr::group_by(Bigram) %&gt;%\n    dplyr::summarise(Frequency = n()) %&gt;%\n    dplyr::arrange(-Frequency)\n\n\n\nBigramFrequencyof the2,673in the1,440the same959to the791on the743have been624that the574it is500natural selection405and the351from the346in a339of a337with the336to be324\n\n\nIt is very useful to perform an N-gram analysis before a collocation analysis to fuse compound words (e.g. New York would become NewYork or New South Wales would become NewSouthWales) to avoid treating new or south as independent elements."
  },
  {
    "objectID": "tutorials/coll/coll.html#footnotes",
    "href": "tutorials/coll/coll.html#footnotes",
    "title": "Analyzing Collocations and N-grams in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m extremely grateful to Joseph Flanagan who provided very helpful feedback and pointed out errors in previous versions of this tutorial. All remaining errors are, of course, my own.↩︎\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/txtsum/txtsum.html",
    "href": "tutorials/txtsum/txtsum.html",
    "title": "Automated Text Summarization with R",
    "section": "",
    "text": "Introduction\nThis tutorial shows how to summarize texts automatically using R by extracting the most prototypical sentences.\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to summarize textual data using R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify selected useful methods associated with summarizing texts.\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file. \n\n\n\n\n\n\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# install packages\ninstall.packages(\"xml2\")\ninstall.packages(\"rvest\")\ninstall.packages(\"lexRankr\")\ninstall.packages(\"textmineR\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"igraph\")\ninstall.packages(\"here\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNext we activate the packages.\n\n# activate packages\nlibrary(xml2)\nlibrary(rvest)\nlibrary(lexRankr)\nlibrary(textmineR)\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(igraph)\nlibrary(here)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and have also initiated the session by executing the code shown above, you are good to go.\n\n\nBasic Text summarization\nThis section shows an easy to use text summarizing method which extracts the most prototypical sentences from a text. As such, this text summarizer does not generate sentences based on prototypical words but evaluates how prototypical or central sentences are and then orders the sentences in a text according to their prototypicality (or centrality).\nFor this example, we will download text from a Guardian article about a meeting between Angela Merkel and Donald Trump at the G20 summit in 2017. In a first step, we define the url of the webpage hosting the article.\n\n# url to scrape\nurl &lt;- \"https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit\"\n\nNext, we extract the text of the article using thexml2  and thervest` packages.\n\n# read page html\npage &lt;- xml2::read_html(url)\n# extract text from page html using selector\npage %&gt;%\n  # extract paragraphs\n  rvest::html_nodes(\"p\") %&gt;%\n  # extract text\n  rvest::html_text() %&gt;%\n  # remove empty elements\n  .[. != \"\"] -&gt; text\n# inspect data\nhead(text)\n\n[1] \"German chancellor plans to make climate change, free trade and mass migration key themes in Hamburg, putting her on collision course with US\"                                                                                                       \n[2] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"  \n[3] \"The G20 summit brings together the world’s biggest economies, representing 85% of global gross domestic product (GDP), and Merkel’s chosen agenda looks likely to maximise American isolation while attempting to minimise disunity amongst others.\"\n[4] \"The meeting, which is set to be the scene of large-scale street protests, will also mark the first meeting between Trump and the Russian president, Vladimir Putin, as world leaders.\"                                                              \n[5] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\"          \n[6] \"Last week, the new UN secretary-general, António Guterres, warned the Trump team if the US disengages from too many issues confronting the international community it will be replaced as world leader.\"                                            \n\n\nNow that we have the text, we apply the lexRank function from the lexRankr package to determine the prototypicality (or centrality) and extract the three most central sentences.\n\n# perform lexrank for top 3 sentences\ntop3sentences &lt;- lexRankr::lexRank(text,\n  # only 1 article; repeat same docid for all of input vector\n  docId = rep(1, length(text)),\n  # return 3 sentences\n  n = 3,\n  continuous = TRUE\n)\n\nParsing text into sentences and tokens...DONE\nCalculating pairwise sentence similarities...DONE\nApplying LexRank...DONE\nFormatting Output...DONE\n\n# inspect\ntop3sentences\n\n  docId sentenceId\n1     1        1_2\n2     1        1_5\n3     1       1_16\n                                                                                                                                                                                                                                                       sentence\n1             A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\n2                     Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\n3 But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\n       value\n1 0.06017053\n2 0.05656337\n3 0.04974733\n\n\nNext, we extract and display the sentences from the table.\n\ntop3sentences$sentence\n\n[1] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"            \n[2] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\"                    \n[3] \"But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\"\n\n\nThe output show the three most prototypical (or central) sentences of the article. The articles are already in chronological order - if the sentences were not in chronological order, we could also have ordered them by sentenceId before displaying them using dplyr and stringr package functions as shown below (in our case the order does not change as the prototypicality and the chronological order are identical).\n\ntop3sentences %&gt;%\n  dplyr::mutate(sentenceId = as.numeric(stringr::str_remove_all(sentenceId, \".*_\"))) %&gt;%\n  dplyr::arrange(sentenceId) %&gt;%\n  dplyr::pull(sentence)\n\n[1] \"A clash between Angela Merkel and Donald Trump appears unavoidable after Germany signalled that it will make climate change, free trade and the management of forced mass global migration the key themes of the G20 summit in Hamburg next week.\"            \n[2] \"Trump has already rowed with Europe once over climate change and refugees at the G7 summit in Italy, and now looks set to repeat the experience in Hamburg but on a bigger stage, as India and China join in the criticism of Washington.\"                    \n[3] \"But the G7, and Trump’s subsequent decision to shun the Paris climate change treaty, clearly left a permanent mark on her, leading to her famous declaration of independence four days later at a Christian Social Union (CSU) rally in a Bavarian beer tent.\"\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nExtract the top 10 sentences from every chapter of Charles Darwin’s On the Origin of Species. You can download the text using this command: darwin &lt;- base::readRDS(\"tutorials/txtsum/data/origindarwin.rda\", \"rb\"). You will then have to paste the whole text together, split it into chapters, create a list of sentences in each chapter, and then apply text summarization to each element in the list. \n\n\n\nAnswer\n\n\ndarwin &lt;- base::readRDS(\"tutorials/txtsum/data/origindarwin.rda\", \"rb\") %&gt;%\n  # collapse into single document\n  paste0(collapse = \" \") %&gt;%\n  # split into chapters\n  stringr::str_split(\"CHAPTER\")\n\n# split chapters into sentences\nchapters &lt;- sapply(darwin, function(x) {\n  x &lt;- stringi::stri_split_boundaries(x, type = \"sentence\")\n})\n\nchapters_clean &lt;- lapply(chapters, function(x) {\n  # remove chapter headings\n  x &lt;- stringr::str_remove_all(x, \"[A-Z]{2,} {0,1}[0-9]{0,}\")\n})\n\n# extract top 3 sentences from each chapter\ntop3s &lt;- lapply(chapters_clean, function(x) {\n  x &lt;- lexRankr::lexRank(x,\n    # only 1 article; repeat same docid for all of input vector\n    # docId = rep(1, length(text)),\n    # return 3 sentences\n    n = 3,\n    continuous = TRUE\n  ) %&gt;%\n    dplyr::pull(sentence) %&gt;%\n    # remove special characters\n    stringr::str_remove_all(\"[^[:alnum:] ]\") %&gt;%\n    # remove superfluous white spaces\n    stringr::str_squish()\n})\n\n# inspect top 3 sentences of first 5 chapters\ntop3s[1:5]\n\n\n\n\nYou can go ahead and play with the text summarization and see if it is useful for you or if you can trust the results based on your data.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2025. Automated text summarization with R. Brisbane: The University of Queensland. url: https://ladal.edu.au/tutorials/txtsum.html (Version 2025.01.13).\n@manual{schweinberger2025txtsum,\n  author = {Schweinberger, Martin},\n  title = {Automated Text Summarization with R},\n  note = {tutorials/txtsum/txtsum.html},\n  year = {2025},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2025.01.13}\n}\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] here_1.0.1      igraph_2.1.1    quanteda_4.1.0  lubridate_1.9.3\n [5] forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4     purrr_1.0.2    \n [9] readr_2.1.5     tidyr_1.3.1     tibble_3.2.1    ggplot2_3.5.1  \n[13] tidyverse_2.0.0 textmineR_3.0.5 Matrix_1.7-1    lexRankr_0.5.2 \n[17] rvest_1.0.4     xml2_1.3.6     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     stringi_1.8.4      lattice_0.22-6    \n [5] hms_1.1.3          digest_0.6.37      magrittr_2.0.3     evaluate_1.0.1    \n [9] grid_4.4.1         timechange_0.3.0   fastmap_1.2.0      rprojroot_2.0.4   \n[13] jsonlite_1.8.9     RcppProgress_0.4.2 httr_1.4.7         stopwords_2.3     \n[17] selectr_0.4-2      fansi_1.0.6        scales_1.3.0       codetools_0.2-20  \n[21] klippy_0.0.0.9500  cli_3.6.3          rlang_1.1.4        munsell_0.5.1     \n[25] withr_3.0.2        yaml_2.3.10        tools_4.4.1        tzdb_0.4.0        \n[29] colorspace_2.1-1   fastmatch_1.1-4    curl_5.2.3         assertthat_0.2.1  \n[33] vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4    htmlwidgets_1.6.4 \n[37] pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.6       glue_1.8.0        \n[41] Rcpp_1.0.13        xfun_0.49          tidyselect_1.2.1   knitr_1.48        \n[45] SnowballC_0.7.1    htmltools_0.5.8.1  rmarkdown_2.28     compiler_4.4.1    \n\n\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "tutorials/load/load.html#preparation-and-session-set-up",
    "href": "tutorials/load/load.html#preparation-and-session-set-up",
    "title": "Loading, saving, and generating data in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"xlsx\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"flextable\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"here\")\ninstall.packages(\"faux\")\ninstall.packages(\"data.tree\")\n# install klippy for copy-to-clipboard button in code chunks\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we can activate them as shown below.\n\n# load packages\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(flextable)\nlibrary(xlsx)\nlibrary(openxlsx)\nlibrary(here)\nlibrary(data.tree)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and initiated the session by executing the code shown above, you are good to go.\n\n\n\n\nNOTE This tutorial assumes that you will be loading data from your own computer, as is often the case.\n This tutorial assumes that you have a designated subfolder named data within the directory where your R project (the Rproj file) is located. It is assumed that your data sets are stored in this data subfolder. Additionally, we provide guidance on how to load multiple text files into R, a common scenario when working with corpora. These multiple texts are expected to be situated within a folder named textcorpus, which is located within the data subfolder.\n If you have a different setup, you will need to adjust the path to the data in order for the tutorial to function correctly on your own computer. It is worth mentioning that the here function is utilized to create paths that originate from the directory where the Rproj is located.\n\n\n\n\n\n\nIn other words, your directory should have the structure as shown below.\n\n\n                       levelName\n1  myproject                    \n2   ¦--Rproj                    \n3   ¦--load.Rmd                 \n4   °--data                     \n5       ¦--testdat.csv          \n6       ¦--testdat2.csv         \n7       ¦--testdat.xlsx         \n8       ¦--testdat.txt          \n9       ¦--testdat.rda          \n10      ¦--english.rda          \n11      °--testcorpus           \n12          ¦--linguistics01.txt\n13          ¦--linguistics02.txt\n14          ¦--linguistics03.txt\n15          ¦--linguistics04.txt\n16          ¦--linguistics05.txt\n17          ¦--linguistics06.txt\n18          °--linguistics07.txt\n\n\nThe data used in this tutorial can be downloaded using the links below:\n\ntestdat.csv\ntestdat2.csv\ntestdat.xlsx\ntestdat.txt\ntestdat.rda\nenglish.txt\n\nlinguistics01.txt\nlinguistics02.txt\nlinguistics03.txt\nlinguistics04.txt\nlinguistics05.txt\nlinguistics06.txt\nlinguistics07.txt"
  },
  {
    "objectID": "tutorials/load/load.html#csv",
    "href": "tutorials/load/load.html#csv",
    "title": "Loading, saving, and generating data in R",
    "section": "CSV",
    "text": "CSV\nA common data type when working with tabulated data are comma separated files (csv). To load such files, we can use the read.csv function as shown below.\n\n# load csv file\ndatcsv &lt;- read.csv(here::here(\"tutorials/load/data\", \"testdat.csv\"),\n    # indicate the data has column names\n    header = TRUE\n)\n# inspect first 6 rows using the head() function\nhead(datcsv)\n\n  Variable1 Variable2\n1         6        67\n2        65        16\n3        12        56\n4        56        34\n5        45        54\n6        84        42\n\n\nThe data is not spectacular and consist of a table with 2 columns (Variable1, and Variable2).\nSometimes, csv files are actually not comma-separated but use a semi-colon as a separator. In such cases, we can use the read.delim function to load the csv and specify that the separator (sep) is “;”.\n\n# load csv with ; as the separator\ndatcsv2 &lt;- read.delim(here::here(\"tutorials/load/data\", \"testdat2.csv\"),\n    # define separator\n    sep = \";\",\n    # indicate the the data has column names\n    header = TRUE\n)\n# inspect data\nhead(datcsv2)\n\n  Variable1 Variable2\n1         6        67\n2        65        16\n3        12        56\n4        56        34\n5        45        54\n6        84        42\n\n\nTo save a data set as a csv on your computer (here it is saved within the data folder within the folder where the Rproj is located).\n\n# save data as a csv without row names\nwrite.csv(datcsv, here::here(\"tutorials/load/data\", \"testdat.csv\"), row.names = F)"
  },
  {
    "objectID": "tutorials/load/load.html#xlsx",
    "href": "tutorials/load/load.html#xlsx",
    "title": "Loading, saving, and generating data in R",
    "section": "XLSX",
    "text": "XLSX\nTo load excel data, you can use the read.xlsx function from the openxlsx package. We have activated the openxlsx package in the session preparation so we do not need to activate it again here. If you get an error message telling you that R did not find the read.xlsx function, you need to activate the openxlsx package by running the library(openxlsx).\n\n# load data\ndatxlsx &lt;- openxlsx::read.xlsx(\n    # define path where data is stored\n    here::here(\"tutorials/load/data\", \"testdat.xlsx\"),\n    # define spreadsheet to load\n    sheet = 1\n)\n# inspect first 6 rows using the head() function\nhead(datxlsx)\n\n  Variable1 Variable2\n1         6        67\n2        65        16\n3        12        56\n4        56        34\n5        45        54\n6        84        42\n\n\nTo save xlsx files, we can use the write.xlsx from the openxlsx package as shown below.\n\nwrite.xlsx(\n    # define object to be stored\n    datxlsx,\n    # define path where data should be stored\n    here::here(\"tutorials/load/data\", \"testdat.xlsx\")\n)"
  },
  {
    "objectID": "tutorials/load/load.html#txt-tabulated",
    "href": "tutorials/load/load.html#txt-tabulated",
    "title": "Loading, saving, and generating data in R",
    "section": "TXT (tabulated)",
    "text": "TXT (tabulated)\nIf the data is tabular and stored as a txt-file, there are various functions to read in the data. The most common functions are read.delim and read.table. The read.delim function is very flexible and allows you to specify the separator and inform R that the first row contains column headers rather than data points (if the data does not contain column headers, then you do not need to specify header = T because header = F is the default).\n\n# load tab txt 1\ndattxt &lt;- read.delim(here::here(\"tutorials/load/data\", \"testdat.txt\"),\n    sep = \"\\t\", header = TRUE\n)\n# inspect data\nhead(dattxt)\n\nThe read.table function is very similar and can also be used to load various types of tabulated data. Again, we let R know that the first row contains column headers rather than data points.\n\n# load tab txt\ndattxt2 &lt;- read.table(here::here(\"tutorials/load/data\", \"testdat.txt\"), header = TRUE)\n# inspect\nhead(dattxt2)\n\nTo save tabulated txt files, we use the write.table function. In the write.table function we define the separator (in this case we write a tab-separated file) and inform R to not add row names (i.e, that R should not number rows and store this information in a separate column).\n\n# save txt\nwrite.table(dattxt, here::here(\"tutorials/load/data\", \"testdat.txt\"), sep = \"\\t\", row.names = F)"
  },
  {
    "objectID": "tutorials/load/load.html#txt",
    "href": "tutorials/load/load.html#txt",
    "title": "Loading, saving, and generating data in R",
    "section": "TXT",
    "text": "TXT\nUnstructured data (most commonly data representing raw text) is also very common - particularly when working with corpus data.\nTo load text data into R (here in the form of a txt file), we can use the scan function. Reading in texts using the scan function will result in loading vectors of stings where each string represents a separate word.\n\ntesttxt &lt;- scan(here::here(\"tutorials/load/data\", \"english.txt\"), what = \"char\")\n# inspect\ntesttxt\n\n [1] \"Linguistics\" \"is\"          \"the\"         \"scientific\"  \"study\"      \n [6] \"of\"          \"language\"    \"and\"         \"it\"          \"involves\"   \n[11] \"the\"         \"analysis\"    \"of\"          \"language\"    \"form,\"      \n[16] \"language\"    \"meaning,\"    \"and\"         \"language\"    \"in\"         \n[21] \"context.\"   \n\n\nIn contract, the readLines function will read in complete lines and result in a vector of strings representing lines (if the entire text is in 1 line, the the entire text will be loaded as a single string).\n\ntesttxt2 &lt;- readLines(here::here(\"tutorials/load/data\", \"english.txt\"))\n# inspect\ntesttxt2\n\n[1] \"Linguistics is the scientific study of language and it involves the analysis of language form, language meaning, and language in context. \"\n\n\nTo save text data, we can use the writeLines function as shown below.\n\nwriteLines(text2, here::here(\"tutorials/load/data\", \"english.txt\"))"
  },
  {
    "objectID": "tutorials/load/load.html#multiple-txts",
    "href": "tutorials/load/load.html#multiple-txts",
    "title": "Loading, saving, and generating data in R",
    "section": "Multiple TXTs",
    "text": "Multiple TXTs\nWhen dealing with text data, it is quite common to encounter scenarios where we need to load multiple files containing texts. In such cases, we typically begin by storing the file locations in an object (referred to as fls in this context) and then proceed to load the files using the sapply function, which allows for looping. Within the sapply function, we have the option to utilize either scan or writeLines for reading the text. In the example below, we employ scan and subsequently merge the individual elements into a single text using the paste function. The output demonstrates the successful loading of 7 txt files from the testcorpus located within the data folder.\n\n# extract file paths\nfls &lt;- list.files(here::here(\"tutorials/load/data\", \"testcorpus\"), pattern = \"txt\", full.names = T)\n# load files\ntxts &lt;- sapply(fls, function(x) {\n    x &lt;- scan(x, what = \"char\") %&gt;%\n        paste0(collapse = \" \")\n})\n# inspect\nstr(txts)\n\n Named chr [1:7] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"/Users/laurenceanthony/Documents/projects/LADALQ/tutorials/load/data/testcorpus/linguistics01.txt\" \"/Users/laurenceanthony/Documents/projects/LADALQ/tutorials/load/data/testcorpus/linguistics02.txt\" \"/Users/laurenceanthony/Documents/projects/LADALQ/tutorials/load/data/testcorpus/linguistics03.txt\" \"/Users/laurenceanthony/Documents/projects/LADALQ/tutorials/load/data/testcorpus/linguistics04.txt\" ...\n\n\nTo save multiple txt files, we follow a similar procedure and first determine the paths that define where R will store the files and then loop over the files and store them in the testcorpus folder.\n\n# define where to save each file\nouts &lt;- file.path(paste(here::here(), \"/\", \"data/testcorpus\", \"/\", \"text\", 1:7, \".txt\", sep = \"\"))\n# save the files\nlapply(seq_along(txts), function(i) {\n    writeLines(txts[[i]],\n        con = outs[i]\n    )\n})"
  },
  {
    "objectID": "tutorials/load/load.html#creating-tabular-data",
    "href": "tutorials/load/load.html#creating-tabular-data",
    "title": "Loading, saving, and generating data in R",
    "section": "Creating tabular data",
    "text": "Creating tabular data\nTo create a simple data frame, we can simply generate the columns and then bind them together using the data.frame function as shown below.\n\n# create a data frame from scratch\nage &lt;- c(25, 30, 56)\ngender &lt;- c(\"male\", \"female\", \"male\")\nl1 &lt;- c(\"english\", \"german\", \"english\")\nmydat &lt;- data.frame(age, gender, l1)\n# inspect\nhead(mydat)\n\n  age gender      l1\n1  25   male english\n2  30 female  german\n3  56   male english\n\n\nYou can also generate more complex data sets where columns or variables correlate with each other. Below, we will generate a data set with 4 correlated variables: Proficiency (the proficiency of a speaker), Abroad (whether or not subjects have been abroad), University (if they went to a standard or excellent university), and PluralError (if they produced a number marking error in a test sentence).\nWe start by setting seed so the generated data will be the same each time we generate the data.\n\nset.seed(678)\n\nNext, we create a correlation matrix, Here, we will create 4 variables and for each of these variables we have to determine how strongly each variable should be correlated with each other variable. The diagonal values are 1 as each variable correlates perfectly with itself.\n\ncmat &lt;- c(\n    1.00, 0.05, 0.05, -0.5,\n    0.05, 1.00, 0.05, -0.3,\n    0.05, 0.05, 1.00, -0.1,\n    -0.50, -0.30, -0.10, 1.0\n)\n\nNext, we generate the data using the rnorm_multi function. In this function, we need to specify:\n\nhow many data points the data set should consist of (n)\n\nthe number of variables (vars)\n\nthe means (mu)\n\nthe standard deviation (sd)\n\nthe correlations (here we specify the correlation matrix we defined above)\n\nthe names of the variables (varnames).\n\nIf all variables should have the same mean, then we only need to provide a singe value but we need to provide 4 values, if we want the variables to have different means).\n\ndat &lt;- faux::rnorm_multi(\n    n = 400, vars = 4, mu = 1, sd = 1, cmat,\n    varnames = c(\"Proficiency\", \"Abroad\", \"University\", \"PluralError\")\n)\n# inspect\nhead(dat)\n\n  Proficiency    Abroad University PluralError\n1   1.9374602 1.5593635  1.2234356   0.7427049\n2  -0.6465982 1.3792828  1.6469481   1.7141511\n3   0.4280436 0.7467852  0.7790628   1.1734548\n4   1.3360640 3.5875551  0.3079962   0.5445548\n5   2.8670503 2.1240673  0.4753890  -0.9205674\n6   0.4730098 1.0867764  0.7660182  -0.8112105\n\n\nIf you want to generate numeric data, then this would be all you need to do. If you want to generate categorical variables, however, we need to convert these numeric values into factors. In the example below, we convert all values higher than 1 (the mean) into one level, and all other values into a second level.\n\n# modify data\ndat &lt;- dat %&gt;%\n    dplyr::mutate(\n        Proficiency = ifelse(Proficiency &gt; 1, \"Advanced\", \"Intermediate\"),\n        Abroad = ifelse(Abroad &gt; 1, \"Abroad\", \"Home\"),\n        University = ifelse(University &gt; 1, \"Excellent\", \"Standard\"),\n        PluralError = ifelse(PluralError &gt; 1, \"Error\", \"Correct\")\n    )\n# inspect\nhead(dat)\n\n   Proficiency Abroad University PluralError\n1     Advanced Abroad  Excellent     Correct\n2 Intermediate Abroad  Excellent       Error\n3 Intermediate   Home   Standard       Error\n4     Advanced Abroad   Standard     Correct\n5     Advanced Abroad   Standard     Correct\n6 Intermediate Abroad   Standard     Correct\n\n\nAnd again, we could then save this data on our computer as shown in the sections above. For instance, we could save it as an MS Excel file as shown below.\n\nwrite.xlsx(dat, here::here(\"tutorials/load/data\", \"dat.xlsx\"))"
  },
  {
    "objectID": "tutorials/load/load.html#creating-text-data",
    "href": "tutorials/load/load.html#creating-text-data",
    "title": "Loading, saving, and generating data in R",
    "section": "Creating text data",
    "text": "Creating text data\nYou may also want to create textual data (e.g., to create sample sentences or short test texts). Thus, we will briefly focus on how to create textual data in R.\nThe easiest way to generate text data is to simply create strings and combine them as shown below.\n\ntext &lt;- c(\"This is an example sentence.\", \"This is a second example sentence\")\n# inspect\ntext\n\n[1] \"This is an example sentence.\"      \"This is a second example sentence\"\n\n\nIf you need to generate many sentences that have a standard format, you can make use of the paste function.\n\nnum &lt;- 1:4\nstart &lt;- \"This is sentence number \"\nend &lt;- \".\"\ntexts &lt;- paste(start, num, end, sep = \"\")\n# inspect\ntexts\n\n[1] \"This is sentence number 1.\" \"This is sentence number 2.\"\n[3] \"This is sentence number 3.\" \"This is sentence number 4.\"\n\n\nOr, you can combine these text snippets into a single string.\n\nonetext &lt;- paste(start, num, end, sep = \"\", collapse = \" \")\n# inspect\nonetext\n\n[1] \"This is sentence number 1. This is sentence number 2. This is sentence number 3. This is sentence number 4.\"\n\n\nThe text can then be saved using the writeLines function as shown below.\n\nwriteLines(onetext, here::here(\"tutorials/load/data\", \"onetext.txt\"))\n\nThis is all for this tutorial. We hope it is useful and that you have a better idea about how to load, save and generate data now."
  },
  {
    "objectID": "tutorials/load/load.html#footnotes",
    "href": "tutorials/load/load.html#footnotes",
    "title": "Loading, saving, and generating data in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/pdf2txt/pdf2txt.html#extract-text-from-one-pdf",
    "href": "tutorials/pdf2txt/pdf2txt.html#extract-text-from-one-pdf",
    "title": "Converting PDFs to txt files with R",
    "section": "Extract text from one pdf",
    "text": "Extract text from one pdf\nThe pdf we will convert is a pdf of the Wikipedia article about corpus linguistics. The first part of that pdf is shown below.\n\nGiven that the pdf contains tables, urls, reference, etc., the text that we will extract from the pdf will be rather messy - cleaning the content of the text would be another matter (it would be data processing rather than extraction) and we will thus only focus on the conversion process here and not focus on the data cleaning and processing aspect.\nWe begin the extraction by defining a path to the pdf. Once we have defined a path, i.e. where R is supposed to look for that file, we continue by extracting the text from the pdf.\n\n# you can use an url or a path that leads to a pdf document\npdf_path &lt;- \"tutorials/pdf2txt/data/PDFs/pdf0.pdf\"\n# extract text\ntxt_output &lt;- pdftools::pdf_text(pdf_path) %&gt;%\n    paste0(collapse = \" \") %&gt;%\n    paste0(collapse = \" \") %&gt;%\n    stringr::str_squish()\n\n\n\n.Corpus linguistics - Wikipedia https://en.wikipedia.org/wiki/Corpus_linguistics Corpus linguistics Corpus linguistics is the study of language as expressed in corpora (samples) of \"real world\" text. Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field in its natural context (\"realia\"), and with minimal experimental-interference. The field of corpus linguistics features divergent views about the value of corpus annotation. These views range from John McHardy Sinclair, who advocates minimal annotation so texts speak for themselves,[1] to the Survey of English Usage team (University College, London), who advocate annotation as allowing greater linguistic understanding through rigorous recording.[2] The text-corpus method is a digestive approach that derives a set of abstract rules that govern a natural language from texts in that language, and explores how that language relates to other languages. Originally derived manually, cor"
  },
  {
    "objectID": "tutorials/pdf2txt/pdf2txt.html#extracting-text-from-many-pdfs",
    "href": "tutorials/pdf2txt/pdf2txt.html#extracting-text-from-many-pdfs",
    "title": "Converting PDFs to txt files with R",
    "section": "Extracting text from many pdfs",
    "text": "Extracting text from many pdfs\nTo convert many pdf-files, we write a function that preforms the conversion for many documents.\n\nconvertpdf2txt &lt;- function(dirpath) {\n    files &lt;- list.files(dirpath, full.names = T)\n    x &lt;- sapply(files, function(x) {\n        x &lt;- pdftools::pdf_text(x) %&gt;%\n            paste0(collapse = \" \") %&gt;%\n            stringr::str_squish()\n        return(x)\n    })\n}\n\nWe can now apply the function to the folder in which we have stored the pdf-files we want to convert. In the present case, I have stored 4 pdf-files of Wikipedia articles in a folder called PDFs which is located in my data folder as described in the section above which detailed how to set up the Rproject folder on your computer). The output is a vector with the texts of the pdf-files.\n\n# apply function\ntxts &lt;- convertpdf2txt(here::here(\"data\", \"PDFs/\"))\n\n\n\n.\n\n\nThe table above shows the first 1000 characters of the texts extracted from 4 pdf-files of Wikipedia articles associated with language technology (corpus linguistics, linguistics, natural language processing, and computational linguistics)."
  },
  {
    "objectID": "tutorials/pdf2txt/pdf2txt.html#saving-the-texts",
    "href": "tutorials/pdf2txt/pdf2txt.html#saving-the-texts",
    "title": "Converting PDFs to txt files with R",
    "section": "Saving the texts",
    "text": "Saving the texts\nTo save the texts in txt-files on your disc, you can simply replace the predefined location (the data folder of your Rproject located by the string here::here(\"data\") with the folder where you want to store the txt-files and then execute the code below. Also, we will name the texts (or the txt-files if you like) as pdftext plus their index number.\n\n# add names to txt files\nnames(txts) &lt;- paste0(here::here(\"data\", \"pdftext\"), 1:length(txts), sep = \"\")\n# save result to disc\nlapply(seq_along(txts), function(i) {\n    writeLines(\n        text = unlist(txts[i]),\n        con = paste(names(txts)[i], \".txt\", sep = \"\")\n    )\n})\n\nIf you check the data folder in your Rproject folder, you should find 4 files called pdftext1, pdftext2, pdftext3, pdftext4."
  },
  {
    "objectID": "tutorials/pdf2txt/pdf2txt.html#spell-correction",
    "href": "tutorials/pdf2txt/pdf2txt.html#spell-correction",
    "title": "Converting PDFs to txt files with R",
    "section": "Spell correction",
    "text": "Spell correction\nIn a first step, we write a function that loops over each text and checks which words occur in a English language dictionary (which we do not specify as it is the default). This spell checking makes use of the the hunspell package (see here for more information). Hunspell is based on MySpell and is backward-compatible with MySpell and aspell dictionaries. This means that we can import and/or make use of many different language dictionaries and it is quite likely that the dictionaries for other languages may already available on your system!\n\n# create token list\ntokens_ocr &lt;- sapply(ocrs, function(x) {\n    x &lt;- hunspell::hunspell_parse(x)\n})\n\n\n\n.\n\n\nIn a next step, we can correct errors resulting from the OCR process, correct the errors and paste th texts back together (which is all done by the code chunk below).\n\n# clean\nclean_ocrtext &lt;- sapply(tokens_ocr, function(x) {\n    correct &lt;- hunspell::hunspell_check(x)\n    x &lt;- ifelse(correct == F,\n        x[hunspell::hunspell_check(x)],\n        x\n    )\n    x &lt;- paste0(x, collapse = \" \")\n})\n\n\n\n.\n\n\nWe have reached the end of this tutorial and we hope that the tutoral helps you in performing OCR on your own pdfs."
  },
  {
    "objectID": "tutorials/vc/vc.html",
    "href": "tutorials/vc/vc.html",
    "title": "Creating Vowel Charts in R",
    "section": "",
    "text": "Introduction\nThis tutorial exemplifies how to create a vowel chart with Praat and R.\n\nThis tutorial is aimed at beginners and intermediate users of R with the aim of showcasing how to create personalized vowel chart using Praat and R. The aim is not to provide a fully-fledged analysis but rather to show and exemplify easily generate these vowel charts without much prior knowledge.\n\n\n\nThe entire R Notebook for the tutorial can be downloaded here. If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file. \n\n\n\n\n\n\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"vowels\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nWe now load the packages.\n\n# load packages\nlibrary(tidyverse)\nlibrary(vowels)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and once you have also initiated the session by executing the code shown above, you are good to go.\n\n\nVowel sounds\nWhen learning or studying a language - the case in point here being English - it is likely that you are confronted with different classes of sounds, e.g. consonants and vowels (Rogers). Consonants differ from vowels in that they are formed with an obstruction of the air stream coming from the lungs and they cannot form the nucleus of a syllable (Zsiga). In fact, consonants are classified according to the manner and place of the obstruction of the air stream. As vowels are produced without obstruction of the air stream, other criteria for differentiating between vowel sounds are needed. The criteria for differentiating between different vowel sounds are\n\nthe number of tongue positions during vowel production (to differentiate between mono-, diph-, and triphthongs),\nheight of the tongue,\nposition of the tongue,\nroundedness of the lips.\n\nThe latter two features are used in the production of vowel charts which show where in the mouth the tongue is located during the production of monophthongal vowel phones. A vowel chart for the monophthongal vowel phones in Received Pronunciation (RP) is shown below.\n\n\n\n\n\nFigure 1: Vowel Chart of monophthongal vowel sounds in Received Pronunciation (RP) (left); Tongue positions for the vowels /i, e, E, a/ (right)\n\n\n\n\n\n\n\nFigure 1: Vowel Chart of monophthongal vowel sounds in Received Pronunciation (RP) (left); Tongue positions for the vowels /i, e, E, a/ (right)\n\n\n\n\nInterestingly, a very similar figure can be created by plotting the Hertz frequency of the first formant of monophthongal vowel sound against the Hertz frequency of second formant minus the Hertz frequency of the first formant of a monophthongal vowel sound. Formants are frequencies of air waves that, if collapsed, form a complex vowel sound (Johnson; Ladefoged). In other words, vowels are periodic, i.e. rhythmic, compressions and decompressions of air and to create a vowel sound, i.e. a complex periodic wave, one needs to produce several simple periodic waves simultaneously. During acoustic analysis, the complex wave is deconstructed into its component parts, i.e. the simple periodic waves that make up that sound. This means that we do not necessarily have to plot the position of the tongue of a speaker when he or she produces vowels to create a vowel chart but that analyses of audio recordings of words in which vowels occur, can be utilized to plot a personalized vowel chart of a speaker. Such vowel charts can then be used in language learning as corrective feedback (see Paganus et al.).\nTo produce a personalized vowel chart, the following steps are necessary:\n\nInstall Praat\nRecord words in which all monophthongal vowel sounds of a given variety occur;\nMeasure and extract the first and second formant of each vowel;\nVisualize the vowel sounds.\n\nThe subsequent sections elaborate the above steps. However, before continuing a word of warning is in order. The example focuses on extracting and plotting vowel formants in an easy but also very uncontrolled way. In case vowel formant extraction is part of a proper research project, some additional steps are warranted. For instance, in a serious research project, it were necessary to control and reduce environmental noise and to optimize the recording situation, one would have to randomize the test items (words with the required phonetic environment and the respective vowel sounds) and use filler items (words that are not relevant for the analysis proper) in order to avoid participants guessing which items are relevant for the analysis, one would also use text grids in Praat to guarantee replicability instead of the simple measurements we use in the example here, etc. However, in case you are only interested in an approximation of your own vowel production and how native-like it is, the example fulfills its purpose and provides the reader with a step-by-step guide on how to plot your personalized vowel chart.\nDownloading and installing PRAAT The first step is thus to download Praat form www.praat.org and to install it on your machine by following the instructions provided on the website and by the Praat installation script. Praat is an open{source software for acoustic analysis that was developed by Paul Boersma at the University of Amsterdam.\nAfter having installed Praat we need to record the words in which the monophthongal vowel phones occur. In this example, we will simply record the words shown below.\n\n\nwordipa_symbolsphonemic_contexthadæ h_dhardɑ h_dheadeh_dheedih_dherdɜ h_dhidɪ h_dhoardɔ h_dhodɒ h_dhoodʊ h_dhudʌ h_dwho'duh_d\n\n\nThe following section describes how to record data in Praat (see Styler for a more elaborate description of how this can be done).\n\n\nRecording words in PRAAT\nTo record these words, start Praat with a double click on the Praat symbol which - after installation - appears on your Desktop. Two windows will appear: the main object window to the left and the picture window to the right (cf. Figure 2). Close the picture window on the right and choose New from the menu at the top of the main object window and select Record mono sound from the menu which pops up. For the recording it is, of course, necessary that a microphone is hook up to your machine { the better the microphone, the better the recording and thus the more accurate the graphical display we are going to produce.\n\n\n\n\n\nFigure 2: Praat’s main object window (left) and Praat’s picture window (right)\n\n\n\n\nSelecting Record mono sound opens Praat’s SoundRecorder window (cf. Figure 4). Select Record, label the recording by entering a title, e.g. vowels, in the Name field and read the words form the list shown in Table .\n\n\n\n\n\nFigure 3: Praat’s main object window\n\n\n\n\n\n\n\n\n\nFigure 4: Praat’s recording window\n\n\n\n\nEach word should be repeated at least three times with a short break between the individual items so that what you record is had, had, had … pause … hard, hard, hard, etc. Try to sound natural, i.e. avoid speaking too fast or too slow, and try not to sound artificial or too careful.\nWhile recording, there should be some green bouncing up and down in the vertical white ” stripe (no bouncing indicates that your machine is not recording properly from the microphone).Once you are finished with your recording, select Stop and next select Save to list & close (cf. Figure 8).\n\n\n\n\n\nFigure 5: Praat’s recording window during recording\n\n\n\n\n\n\n\n\n\nFigure 6: Praat’s recording window after recording\n\n\n\n\nSaving has created an object in Praat’s main object window - in case you have named your recording vowels, the new object will be called 1. Sound vowels (cf. Figure 7). Before editing the data, it is advisable to save them on your machine. To save the data select the Save option from the upper menu, then select Save as WAV file... and navigate to the directory in which you want to save the recorded data.\n\n\n\n\n\nFigure 7: Praat’s main object window with saved object\n\n\n\n\n\n\n\n\n\nFigure 8: Save the recording as a .wav file\n\n\n\n\nNext, select View & Edit in Praat’s main menu in the main object window. This will open Praat’s edit window (cf. Figure 9) - the object represents a recording of the word heed repeated three times for sake of simplicity.\n\n\n\n\n\nFigure 9: Praat’s edit window with the word heed repeated three times\n\n\n\n\nAfter recording and saving the data necessary for the task at hand, we continue by extracting the vowel formants.\n\n\nMeasure and extract vowel formants\nBefore extracting of the vowel formants, some parameters need adjusting. In a first step, go to Formant from the menu at the top of the edit window and select Formant settings.... Next, select the option Show formant and then, depending on whether the recording represents a male, a female or a child, adjust the Maximum formant (Hz) to 5000 Hz (male), 5500 Hz (female) or up to 8000 Hz (for a child) (cf. Praat User’s Guide. It may also be necessary to adjust the number of formants that Praat aims to find: the default is 5, but it may be set to any number between 3 and 7 depending on the data. To elaborate, if the formants do not exhibit a regular horizontal pattern but they are somewhat unsteady or the dots are all over the place, try to find the number of formants that provide the best results (i.e. steady horizontal lines).\n\n\n\n\n\nFigure 10: Praat’s edit window with the word heed repeated three times and formants shown\n\n\n\n\nAfter having set the parameters, listen to the recording and highlight the section which represents the vowel sound you want to extract the formants from. Highlightling is done by selecting the start and end point of the vowel sound - the beginning and end of the steady line during which the vowel is produced - within the edit window as done for the first of the three instances of heed in Figure 11.\n\n\n\n\n\nFigure 11: Praat’s edit window with the word heed repeated three times and formants shown and steady state selected\n\n\n\n\nThe vowel formants can be extracted by going to Formant in the edit window and selecting Get first formant. Having done so, a window with the mean Hertz frequency of the first formant during the steady state is shown (cf. Figure 12). Please note that you should additionally extract the start and end time of the highlighted section from the display in the edit window.\n\n\n\n\n\nFigure 12: The mean Hertz frequency of first formant of the word heed during the steady state\n\n\n\n\nTo extract the second (and in case you want to use your data in other analysis also the third formant) simply choose Get third formant (and Get second formant), note down the Hertz frequencies in a table, and also note down the start and end time of the steady state. The final table should look like Table below (some columns are removed for sake of simplicity).\n\n\nfilesubjecttrialitemF1F2vowelsms1had717.33611,868.1754vowelsms1had743.48351,903.7152vowelsms1had720.97401,938.6928vowelsms1hard734.52751,493.3289vowelsms1hard832.92281,407.8247vowelsms1hard797.28421,498.2064vowelsms1head610.89432,062.8820vowelsms1head722.25192,130.6322vowelsms1head625.11172,009.6507vowelsms1heed263.38302,833.0017vowelsms1heed301.41762,745.8471vowelsms2heed286.96562,822.5988vowelsms2herd532.79251,704.9954vowelsms2herd537.79621,819.8916vowelsms2herd524.71371,704.2321vowelsms2hid451.87662,390.7996vowelsms2hid417.03302,483.3900vowelsms2hid410.68172,360.0382vowelsms2hoard540.3306951.1443vowelsms2hoard549.9205927.0956vowelsms2hoard648.04821,093.3466vowelsms2hod698.40691,144.4669vowelsms3hod615.16211,086.4479vowelsms3hod751.01901,452.4663vowelsms3hood431.29931,478.1930vowelsms3hood404.18841,453.1036vowelsms3hood470.14691,216.3027vowelsms3hud646.05141,700.0030vowelsms3hud622.53021,510.4514vowelsms3hud749.35401,581.7578vowelsms3whod346.88121,013.0007vowelsms3whod353.82651,285.8341vowelsms3whod366.81371,016.9800\n\n\nThe next section describes how to plot the data and compare the vowels to equivalent vowels produced by native-RP speakers.\n\n\nVisualizing the vowel sounds\nWe will now process the data so that we can plot the F1 against the F2 values by speaker and word. In a first step, we load the data from the learner (nns) and the native speakers (ns).\n\n# load data\nns &lt;- read.table(\"tutorials/vc/data/rpvowels.txt\", header = T, sep = \"\\t\")\nnns &lt;- read.table(\"tutorials/vc/data/vowels.txt\", header = T, sep = \"\\t\") %&gt;%\n    dplyr::select(-file)\n\nThe data of the native speakers, i.e the reference data, is shown below.\n\n\nsubjectitemcontextF1F2F1sdF2sdrpspkhadwordlist916.351,473.15124.29815119.43696rpspkhardwordlist604.151,040.1570.9197340.06478rpspkheadwordlist599.951,925.70102.22858143.60476rpspkheedwordlist276.152,337.6025.48328223.42440rpspkherdwordlist493.551,372.4047.4091795.94648rpspkhidwordlist392.852,174.3540.83893166.85868rpspkhoardwordlist391.65629.6039.7071881.19074rpspkhodwordlist483.10864.9035.4800248.49948rpspkhoodwordlist412.851,286.6532.98209193.69870rpspkhudwordlist658.201,208.05116.1494572.51677rpspkwhodwordlist288.701,616.3030.18905225.73858\n\n\nThe reference data is taken from from Hawkins and Midgley (see here) and represents the first and second formant for the words heed, hid, head, had, hard, hod, hoard, hood, who’d, hud, and herd produced by 5 20 to 25 year old L1-speakers of Received Pronunciation.\nWe now combine the two data sets, rename the subject and item columns to Speaker and Word, add a column which holds the ipa symbols of the vowel sounds that the word represent, and we calculate the means of the F1 (F1_mean) and F2 (F2_mean) by Word and Speaker.\n\n\nSpeakerWordF1F2ipaF1_meanF2_meanLearnerhad717.33611,868.175æ727.26451,903.528Learnerhad743.48351,903.715æ727.26451,903.528Learnerhad720.97401,938.693æ727.26451,903.528Learnerhard734.52751,493.329ɑ788.24481,466.453Learnerhard832.92281,407.825ɑ788.24481,466.453Learnerhard797.28421,498.206ɑ788.24481,466.453Learnerhead610.89432,062.882e652.75262,067.722Learnerhead722.25192,130.632e652.75262,067.722Learnerhead625.11172,009.651e652.75262,067.722Learnerheed263.38302,833.002i283.92202,800.483Learnerheed301.41762,745.847i283.92202,800.483Learnerheed286.96562,822.599i283.92202,800.483Learnerherd532.79251,704.995ɜ531.76751,743.040Learnerherd537.79621,819.892ɜ531.76751,743.040Learnerherd524.71371,704.232ɜ531.76751,743.040\n\n\nWe can now generate the vowel chart by plotting the F1 values against the F2 values. In addition, we will differentiate between different vowel sounds as well as between the learner (Learner) and native speakers (NS).\n\nns &lt;- voweldata %&gt;% dplyr::filter(Speaker == \"NS\")\nnns &lt;- voweldata %&gt;% dplyr::filter(Speaker == \"Learner\")\nggplot(voweldata, aes(F2, F1, color = Speaker, group = Word, fill = Speaker)) +\n    geom_point(alpha = .1) +\n    geom_text(data = voweldata, aes(x = F2_mean, y = F1_mean, label = ipa), fontface = \"bold\") +\n    stat_ellipse(data = ns, level = 0.50, geom = \"polygon\", alpha = 0.05, aes(fill = Speaker)) +\n    stat_ellipse(data = nns, level = 0.95, geom = \"polygon\", alpha = 0.05, aes(fill = Speaker)) +\n    scale_x_reverse(breaks = seq(500, 3000, 500), labels = seq(500, 3000, 500)) +\n    scale_y_reverse() +\n    scale_color_manual(breaks = c(\"Learner\", \"NS\"), values = c(\"orange\", \"gray40\")) +\n    theme_bw() +\n    theme(\n        legend.position = \"top\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()\n    )\n\n\n\n\n\n\n\n\nThe vowel chart shows that the i-sounds by the L1-German speaker are more fronted and that the o-sounds are substantially higher by the non-native speaker compared to the RP reference vowel spaces. The short u-sound, however, is very similar, indicating that this L1-German speaker produces the short u-sound in English very native-like while the long u-sound is higher and more fronted in the speech of the L1-German speaker. Interestingly, the vowel space of the ash differs quite dramatically between the native speakers and the L1 German speaker which could be caused by the fact that German does not have an ash vowel. I hope this short tutorial helps you in creating your own personalized vowel charts with Praat and R.\n\n\nCitation & Session Info\nSchweinberger, Martin. 2025. Creating Vowel Charts in R. Brisbane: The University of Queensland. url: https://ladal.edu.au/tutorials/vc.html (Version 2025.01.13).\n@manual{schweinberger2025vc,\n  author = {Schweinberger, Martin},\n  title = {Creating Vowel Charts in R},\n  note = {tutorials/vc/vc.html},\n  year = {2025},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2025.01.13}\n}\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] flextable_0.9.7 vowels_1.2-2    lubridate_1.9.3 forcats_1.0.0  \n [5] stringr_1.5.1   dplyr_1.1.4     purrr_1.0.2     readr_2.1.5    \n [9] tidyr_1.3.1     tibble_3.2.1    ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4              generics_0.1.3          fontLiberation_0.1.0   \n [4] xml2_1.3.6              stringi_1.8.4           hms_1.1.3              \n [7] digest_0.6.37           magrittr_2.0.3          evaluate_1.0.1         \n[10] grid_4.4.1              timechange_0.3.0        fastmap_1.2.0          \n[13] jsonlite_1.8.9          zip_2.3.1               fansi_1.0.6            \n[16] scales_1.3.0            fontBitstreamVera_0.1.1 codetools_0.2-20       \n[19] klippy_0.0.0.9500       textshaping_0.4.0       cli_3.6.3              \n[22] rlang_1.1.4             fontquiver_0.2.1        munsell_0.5.1          \n[25] withr_3.0.2             yaml_2.3.10             gdtools_0.4.0          \n[28] tools_4.4.1             officer_0.6.7           uuid_1.2-1             \n[31] tzdb_0.4.0              colorspace_2.1-1        assertthat_0.2.1       \n[34] vctrs_0.6.5             R6_2.5.1                lifecycle_1.0.4        \n[37] htmlwidgets_1.6.4       MASS_7.3-61             ragg_1.3.3             \n[40] pkgconfig_2.0.3         pillar_1.9.0            gtable_0.3.6           \n[43] glue_1.8.0              data.table_1.16.2       Rcpp_1.0.13            \n[46] systemfonts_1.1.0       xfun_0.49               tidyselect_1.2.1       \n[49] knitr_1.48              farver_2.1.2            htmltools_0.5.8.1      \n[52] labeling_0.4.3          rmarkdown_2.28          compiler_4.4.1         \n[55] askpass_1.2.1           openssl_2.2.2          \n\n\n\nBack to top\nBack to HOME\n\n\n\n\n\n\n\n\n\nReferences\n\nHawkins, Sarah, and Jonathan Midgley. “Formant Frequencies of RP Monophthongs in Four Age Groups of Speakers.” Journal of the International Phonetic Association, 183–99.\n\n\nJohnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell.\n\n\nLadefoged, Peter. Elements of Acoustic Phonetics. Chigago: University of Chicago Press.\n\n\nPaganus, Annu, Vesa-Petteri Mikkonen, Tomi Mäntylä, Sami Nuuttila, Jouni Isoaho, Olli Aaltonen, and Tapio Salakoski. “The Vowel Game: Continuous Real-Time Visualization for Pronunciation Learning with Vowel Charts.” In International Conference on Natural Language Processing (in Finland), 696–703. Springer.\n\n\nRogers, Henry. The Sounds of Language: An Introduction to Phonetics. Routledge.\n\n\nStyler, Will. “Using Praat for Linguistic Research.” University of Colorado at Boulder Phonetics Lab.\n\n\nZsiga, Elizabeth C. The Sounds of Language: An Introduction to Phonetics and Phonology. John Wiley & Sons."
  },
  {
    "objectID": "tutorials/dviz/dviz.html#extensions-of-dot-plots",
    "href": "tutorials/dviz/dviz.html#extensions-of-dot-plots",
    "title": "Data Visualization with R",
    "section": "Extensions of dot plots",
    "text": "Extensions of dot plots\nIn addition, we can add regression lines with error bars by Species and, if we want to show separate windows for the plots, we can use the “facet_grid” or “facet_wrap” function and define by which variable we want to create different panels.\n\n# create scatter plot colored by genre in different panels\nggplot(pdat, aes(Date, Prepositions, color = Genre)) +\n  facet_wrap(vars(Genre), ncol = 4) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  theme_bw() +\n  theme(\n    legend.title = element_blank(),\n    axis.text.x = element_text(size = 8, angle = 90)\n  )\n\n\n\n\n\n\n\n\nIf we only want to show the lines, we simply drop the “geom_point” function.\n\n# create scatter plot colored by genre in different panels\nggplot(pdat, aes(x = Date, y = Prepositions, color = Genre)) +\n  facet_wrap(vars(Genre), ncol = 4) +\n  geom_smooth(method = \"lm\", se = F) +\n  theme_bw() +\n  theme(\n    legend.title = element_blank(),\n    axis.text.x = element_text(size = 8, angle = 90)\n  )\n\n\n\n\n\n\n\n\nAnother option is to plot density layers instead of plotting the data points.\n\n# create scatter density plot\nggplot(pdat, aes(x = Date, y = Prepositions, color = GenreRedux)) +\n  facet_wrap(vars(GenreRedux), ncol = 5) +\n  theme_bw() +\n  geom_density_2d() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    axis.text.x = element_text(size = 8, angle = 90)\n  )\n\n\n\n\n\n\n\n\nAlthough these are not scatterplots, plots with dot-symbols are very flexible and can be extended to show properties of the distribution of values. One way to create such a plot is to plot means as dot-symbols and add error bars to provide information about the underlying distribution. The plot below illustrates such a plot and additionally shows how plots can be further customized.\n\n# scatter plot with error bars\nggplot(pdat, aes(x = reorder(Genre, Prepositions, mean), y = Prepositions, group = Genre)) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = Genre)) +\n  stat_summary(\n    fun.data = mean_cl_boot,\n    # add error bars\n    geom = \"errorbar\", width = 0.2\n  ) +\n  # def. y-axis range\n  coord_cartesian(ylim = c(100, 200)) +\n  # def. font size\n  theme_bw(base_size = 15) +\n  # def. x- and y-axis\n  theme(\n    axis.text.x = element_text(size = 10, angle = 90),\n    axis.text.y = element_text(size = 10, face = \"plain\")\n  ) +\n  # def. axes labels\n  labs(x = \"Genre\", y = \"Prepositions (Frequency)\") +\n  # def. to col.\n  scale_color_manual(guide = FALSE)\n\n\n\n\n\n\n\n\nBalloon plots are an extension of scatter plots that are typically used to display data that represents * two categorical variables * one numeric variable.\n# ballon plot\npdat %&gt;%\n  dplyr::mutate(DateRedux = factor(DateRedux)) %&gt;%\n  dplyr::group_by(DateRedux, GenreRedux) %&gt;%\n  dplyr::summarise(Prepositions = mean(Prepositions)) %&gt;%\n  ggplot(aes(DateRedux, 100,\n    size = Prepositions,\n    fill = GenreRedux\n  )) +\n  facet_grid(vars(GenreRedux)) +\n  geom_point(shape = 21) +\n  scale_size_area(max_size = 15) +\n  coord_cartesian(ylim = c(50, 150)) +\n  theme_bw() +\n  theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  scale_fill_discrete(guide = \"none\")"
  },
  {
    "objectID": "tutorials/dviz/dviz.html#smoothed-line-graphs",
    "href": "tutorials/dviz/dviz.html#smoothed-line-graphs",
    "title": "Data Visualization with R",
    "section": "Smoothed line graphs",
    "text": "Smoothed line graphs\nAnother very useful function when creating line graphs with “ggplot” is “geom_smooth” which smoothes the lines to be drawn.\n\nggplot(pdat, aes(x = Date, y = Prepositions, group = GenreRedux, color = GenreRedux)) +\n  # add geom layer with lines\n  geom_smooth()\n\n\n\n\n\n\n\n\nAs this smoothed line graph is extremely useful, we will customize it to show how to modify your graph.\n\n# define aesthetics\nggplot(pdat, aes(x = Date, y = Prepositions, color = GenreRedux, linetype = GenreRedux)) +\n  # add geom layer with lines\n  geom_smooth(se = F) +\n  # legend without background color\n  guides(color = guide_legend(override.aes = list(fill = NA))) +\n  # def. legend position\n  theme(legend.position = \"top\") +\n  # def. linetype\n  scale_linetype_manual(\n    values = c(\"twodash\", \"dashed\", \"dotdash\", \"dotted\", \"solid\"),\n    # def. legend header\n    name = c(\"Genre\"),\n    # def. linetypes\n    breaks = names(table(pdat$GenreRedux)),\n    # def. labels\n    labels = names(table(pdat$GenreRedux))\n  ) +\n  # def. col.\n  scale_colour_manual(\n    values = clrs,\n    # define legend header\n    name = c(\"Genre\"),\n    # define elements\n    breaks = names(table(pdat$GenreRedux)),\n    # define labels\n    labels = names(table(pdat$GenreRedux))\n  ) +\n  # add x-axis label\n  labs(x = \"Year\") +\n  # customize x-axis tick positions\n  scale_x_continuous(\n    breaks = seq(1100, 1900, 100),\n    # add labels to x-axis tick pos.\n    labels = seq(1100, 1900, 100)\n  ) +\n  # add y-axis label\n  scale_y_continuous(\n    name = \"Relative frequency \\n(per 1,000 words)\",\n    # customize tick y-axis\n    limits = c(100, 200)\n  ) +\n  # define theme  as black and white\n  theme_bw(base_size = 10)\n\n\n\n\n\n\n\n\nAlthough the code for the customized smoothed line graph is much longer and requires addition specifications, it is a very nice way to portrait the development over time."
  },
  {
    "objectID": "tutorials/dviz/dviz.html#ribbon-plots",
    "href": "tutorials/dviz/dviz.html#ribbon-plots",
    "title": "Data Visualization with R",
    "section": "Ribbon plots",
    "text": "Ribbon plots\nRibbon plots show an area, typically between minimum and maximum values. In addition, ribbon plots commonly also show the mean as depicted below.\n# create dot plot\npdat %&gt;%\n  dplyr::mutate(DateRedux = as.numeric(DateRedux)) %&gt;%\n  dplyr::group_by(DateRedux) %&gt;%\n  dplyr::summarise(\n    Mean = mean(Prepositions),\n    Min = min(Prepositions),\n    Max = max(Prepositions)\n  ) %&gt;%\n  ggplot(aes(x = DateRedux, y = Mean)) +\n  geom_ribbon(aes(ymin = Min, ymax = Max), fill = \"gray80\") +\n  geom_line() +\n  scale_x_continuous(labels = names(table(pdat$DateRedux)))"
  },
  {
    "objectID": "tutorials/dviz/dviz.html#line-graphs-for-likert-data",
    "href": "tutorials/dviz/dviz.html#line-graphs-for-likert-data",
    "title": "Data Visualization with R",
    "section": "Line graphs for Likert data",
    "text": "Line graphs for Likert data\nA special case of line graphs is used when dealing with Likert-scaled variables. In such cases, the line graph displays the density of cumulative frequencies of responses. The difference between the cumulative frequencies of responses displays differences in preferences. We will only focus on how to create such graphs using the “ggplot” environment here as it has an inbuilt function (“ecdf”) which is designed to handle such data.\nIn a first step, we create a data set which consists of a Likert-scaled variable. The fictitious data created here consists of rating of students from three courses about how satisfied they were with their language-learning course. The response to the Likert item is numeric so that “strongly disagree/very dissatisfied” would get the lowest and “strongly agree/very satisfied” the highest numeric value.\n\nldat &lt;- base::readRDS(\"tutorials/dviz/data/lid.rda\", \"rb\")\n\nLet’s briefly inspect the data.\n\n\nCourseSatisfactionChinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1\n\n\nNow that we have data resembling a Likert-scaled item from a questionnaire, we will display the data in a cumulative line graph.\n\n# create cumulative density plot\nggplot(ldat, aes(x = Satisfaction, color = Course)) +\n  geom_step(aes(y = ..y..), stat = \"ecdf\", size = 2) +\n  labs(y = \"Cumulative Density\") +\n  scale_x_discrete(\n    limits = 1:5, breaks = 1:5,\n    labels = c(\"very dissatisfied\", \"dissatisfied\", \"neutral\", \"satisfied\", \"very satisfied\")\n  ) +\n  scale_colour_manual(values = clrs[1:3]) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe satisfaction of the German course was the lowest as the red line shows the highest density (frequency of responses) of “very dissatisfied” and “dissatisfied” ratings. The students in our fictitious data set were most satisfied with the Chinese course as the blue line is the lowest for “very dissatisfied” and “dissatisfied” ratings while the difference between the courses shrinks for “satisfied” and “very satisfied”. The Japanese language course is in-between the German and the Chinese course."
  },
  {
    "objectID": "tutorials/dviz/dviz.html#bar-plots-for-likert-data",
    "href": "tutorials/dviz/dviz.html#bar-plots-for-likert-data",
    "title": "Data Visualization with R",
    "section": "Bar plots for Likert data",
    "text": "Bar plots for Likert data\nBar plots are particularly useful when visualizing data obtained through Likert items. As this is a very common issue that empirical researchers face. There are two basic ways to display Likert items using bar plots: grouped bar plots and more elaborate scaled bar plots.\nAlthough we have seen above how to create grouped bar plots, we will repeat it here with the language course example used above when we used cumulative density line graphs to visualise how to display Likert data.\nIn a first step, we recreate the data set which we have used above. The data set consists of a Likert-scaled variable (Satisfaction) which represents rating of students from three courses about how satisfied they were with their language-learning course. The response to the Likert item is numeric so that “strongly disagree/very dissatisfied” would get the lowest and “strongly agree/very satisfied” the highest numeric value.\n\n# create likert data\nnlik &lt;- ldat %&gt;%\n  dplyr::group_by(Course, Satisfaction) %&gt;%\n  dplyr::summarize(Frequency = n())\n# inspect data\nhead(nlik)\n\n# A tibble: 6 × 3\n# Groups:   Course [2]\n  Course  Satisfaction Frequency\n  &lt;chr&gt;          &lt;int&gt;     &lt;int&gt;\n1 Chinese            1        20\n2 Chinese            2        30\n3 Chinese            3        25\n4 Chinese            4        10\n5 Chinese            5        15\n6 German             1        40\n\n\nNow that we have data resembling a Likert-scaled item from a questionnaire, we will display the data in a cumulative line graph.\n\n# create grouped bar plot\nggplot(nlik, aes(Satisfaction, Frequency, fill = Course)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  # define colors\n  scale_fill_manual(values = clrs) +\n  # add text and define color\n  geom_text(aes(label = Frequency),\n    vjust = 1.6, color = \"white\",\n    # define text position and size\n    position = position_dodge(0.9), size = 3.5\n  ) +\n  scale_x_discrete(\n    limits = c(\"1\", \"2\", \"3\", \"4\", \"5\"), breaks = c(1, 2, 3, 4, 5),\n    labels = c(\n      \"very dissatisfied\", \"dissatisfied\", \"neutral\", \"satisfied\",\n      \"very satisfied\"\n    )\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nAnother and very interesting way to display such data is by using the Likert package. In a first step, we need to activate the package, clean the data, and extract a subset for the data visualization example.\n\nsdat &lt;- base::readRDS(\"tutorials/dviz/data/sdd.rda\", \"rb\")\n\n\n\nGroupRespondentHow.did.you.like.the.course.How.did.you.like.the.teacher.Was.the.content.intersting.Was.the.content.adequate.for.the.course.Were.there.enough.discussions.Was.the.use.of.online.materials.appropriate.Was.the.teacher.appropriately.prepared.Was.the.workload.of.the.course.appropriate.Was.the.course.content.enganging.Were.there.enough.interactive.exerceises.included.in.the.sessions.GermanG14444444444GermanG24534445313GermanG35342434431GermanG43333333333GermanG51111111111GermanG63132333333GermanG75342433445GermanG85555555555GermanG95133445531GermanG103333333333GermanG114241444511GermanG123333333333GermanG135555555555GermanG143533433424GermanG154231444322\n\n\nAs you can see, we need to clean and adapt the column names. To do this, we will\n\nadd an identifier which shows which question we are dealing with (e.g. Q 1: question text)\nremove the dots between words with spaces\nadd a question mark at the end of questions\nremove superfluous white spaces\n\n\n# clean column names\ncolnames(sdat)[3:ncol(sdat)] &lt;- paste0(\"Q \", str_pad(1:10, 2, \"left\", \"0\"), \": \", colnames(sdat)[3:ncol(sdat)]) %&gt;%\n  stringr::str_replace_all(\"\\\\.\", \" \") %&gt;%\n  stringr::str_squish() %&gt;%\n  stringr::str_replace_all(\"$\", \"?\")\n# inspect column names\ncolnames(sdat)\n\n [1] \"Group\"                                                                   \n [2] \"Respondent\"                                                              \n [3] \"Q 01: How did you like the course?\"                                      \n [4] \"Q 02: How did you like the teacher?\"                                     \n [5] \"Q 03: Was the content intersting?\"                                       \n [6] \"Q 04: Was the content adequate for the course?\"                          \n [7] \"Q 05: Were there enough discussions?\"                                    \n [8] \"Q 06: Was the use of online materials appropriate?\"                      \n [9] \"Q 07: Was the teacher appropriately prepared?\"                           \n[10] \"Q 08: Was the workload of the course appropriate?\"                       \n[11] \"Q 09: Was the course content enganging?\"                                 \n[12] \"Q 10: Were there enough interactive exerceises included in the sessions?\"\n\n\nNow, that we have nice column names, we will replace the numeric values (1 to 5) with labels ranging from disagree to agree and convert our data into a data frame.\n\nlbs &lt;- c(\"disagree\", \"somewhat disagree\", \"neither agree nor disagree\", \"somewhat agree\", \"agree\")\nsurvey &lt;- sdat %&gt;%\n  dplyr::mutate_if(is.character, factor) %&gt;%\n  dplyr::mutate_if(is.numeric, factor, levels = 1:5, labels = lbs) %&gt;%\n  drop_na() %&gt;%\n  as.data.frame()\n\n\n\nGroupRespondentQ 01: How did you like the course?Q 02: How did you like the teacher?Q 03: Was the content intersting?Q 04: Was the content adequate for the course?Q 05: Were there enough discussions?Q 06: Was the use of online materials appropriate?Q 07: Was the teacher appropriately prepared?Q 08: Was the workload of the course appropriate?Q 09: Was the course content enganging?Q 10: Were there enough interactive exerceises included in the sessions?GermanG1somewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreeGermanG2somewhat agreeagreeneither agree nor disagreesomewhat agreesomewhat agreesomewhat agreeagreeneither agree nor disagreedisagreeneither agree nor disagreeGermanG3agreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeneither agree nor disagreesomewhat agreesomewhat agreeneither agree nor disagreedisagreeGermanG4neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG5disagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreeGermanG6neither agree nor disagreedisagreeneither agree nor disagreesomewhat disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG7agreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat agreeagreeGermanG8agreeagreeagreeagreeagreeagreeagreeagreeagreeagreeGermanG9agreedisagreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat agreeagreeagreeneither agree nor disagreedisagreeGermanG10neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG11somewhat agreesomewhat disagreesomewhat agreedisagreesomewhat agreesomewhat agreesomewhat agreeagreedisagreedisagreeGermanG12neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG13agreeagreeagreeagreeagreeagreeagreeagreeagreeagreeGermanG14neither agree nor disagreeagreeneither agree nor disagreeneither agree nor disagreesomewhat agreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeGermanG15somewhat agreesomewhat disagreeneither agree nor disagreedisagreesomewhat agreesomewhat agreesomewhat agreeneither agree nor disagreesomewhat disagreesomewhat disagree\n\n\nNow, we can use the plot and the likert function to visualize the survey data.\n\n# load package\nlibrary(likert)\n# generate plot\nplot(likert(survey[, 3:12]), ordered = F, wrap = 60)\n\n\n\n\n\n\n\n\nTo save this plot, you can use the save_plot function from the cowplot package as shown below.\n\nsurvey_p1 &lt;- plot(likert(survey[, 3:12]), ordered = F, wrap = 60)\n# save plot\ncowplot::save_plot(here(\"images\", \"stu_p1.png\"), # where to save the plot\n  survey_p1, # object to plot\n  base_asp = 1.5, # ratio of space fro questions vs space for plot\n  base_height = 8\n) # size! higher for smaller font size\n\nAn additional and very helpful feature is that the likert package enables grouping the data as shown below. The display columns 3 to 8 and use column 1 for grouping.\n\n# create plot\nplot(likert(survey[, 3:8], grouping = survey[, 1]))"
  },
  {
    "objectID": "tutorials/dviz/dviz.html#comparative-bar-plots-with-negative-values",
    "href": "tutorials/dviz/dviz.html#comparative-bar-plots-with-negative-values",
    "title": "Data Visualization with R",
    "section": "Comparative bar plots with negative values",
    "text": "Comparative bar plots with negative values\nAnother frequent task is to evaluate the divergence of values from a reference, for instance when dealing with language learners where native speakers serve as a reference or target. To illustrate how such data can be visualized, we load the scales package as we want to create a bar plot in which we show the divergence of learners from native speakers regarding certain features and how that divergence changes over time. Then, we create an example data set which mirrors the format we expect for the actual data.\n\n# create a vector with values called Test1\nTest1 &lt;- c(11.2, 13.5, 200, 185, 1.3, 3.5)\n# create a vector with values called Test2\nTest2 &lt;- c(12.2, 14.7, 210, 175, 1.9, 3.0)\n# create a vector with values called Test3\nTest3 &lt;- c(13.2, 15.1, 177, 173, 2.4, 2.9)\n# combine vectors in a data frame\ntestdata &lt;- data.frame(Test1, Test2, Test3)\n# add rownames\nrownames(testdata) &lt;- c(\n  \"Feature1_Student\",\n  \"Feature1_Reference\",\n  \"Feature2_Student\",\n  \"Feature2_Reference\",\n  \"Feature3_Student\",\n  \"Feature3_Reference\"\n)\n# inspect data\ntestdata\n\n                   Test1 Test2 Test3\nFeature1_Student    11.2  12.2  13.2\nFeature1_Reference  13.5  14.7  15.1\nFeature2_Student   200.0 210.0 177.0\nFeature2_Reference 185.0 175.0 173.0\nFeature3_Student     1.3   1.9   2.4\nFeature3_Reference   3.5   3.0   2.9\n\n\nWe can now determine how the learners deviate from the native speakers.\n\n# determine divergence from reference\n# row 1 (student) minus row 2 (reference)\nFeatureA &lt;- t(testdata[1, ] - testdata[2, ])\n# row 3 (student) minus row 4 (reference)\nFeatureB &lt;- t(testdata[3, ] - testdata[4, ])\n# row 5 (student) minus row 6 (reference)\nFeatureC &lt;- t(testdata[5, ] - testdata[6, ])\n# create data frame\nplottable &lt;- data.frame(\n  rep(rownames(FeatureA), 3),\n  c(FeatureA, FeatureB, FeatureC),\n  c(\n    rep(\"FeatureA\", 3),\n    rep(\"FeatureB\", 3),\n    rep(\"FeatureC\", 3)\n  )\n)\n# def. col. names\ncolnames(plottable) &lt;- c(\"Test\", \"Value\", \"Feature\")\n# inspect data\nplottable\n\n   Test Value  Feature\n1 Test1  -2.3 FeatureA\n2 Test2  -2.5 FeatureA\n3 Test3  -1.9 FeatureA\n4 Test1  15.0 FeatureB\n5 Test2  35.0 FeatureB\n6 Test3   4.0 FeatureB\n7 Test1  -2.2 FeatureC\n8 Test2  -1.1 FeatureC\n9 Test3  -0.5 FeatureC\n\n\nFinally, we graphically display the divergence using a bar plot.\n\n# create plot\nggplot(\n  plottable,\n  aes(Test, Value)\n) + # def. x/y-axes\n  # separate plots for each feature\n  facet_grid(vars(Feature), scales = \"free_y\") +\n  # create bars\n  geom_bar(stat = \"identity\", aes(fill = Test)) +\n  # black and white theme\n  theme_bw() +\n  # suppress legend\n  guides(fill = FALSE) +\n  # def. colours\n  geom_bar(stat = \"identity\", fill = rep(clrs[1:3], 3)) +\n  # axes titles\n  labs(x = \"\", y = \"Score\")"
  },
  {
    "objectID": "tutorials/dviz/dviz.html#footnotes",
    "href": "tutorials/dviz/dviz.html#footnotes",
    "title": "Data Visualization with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/table/table.html#preparation-and-session-set-up",
    "href": "tutorials/table/table.html#preparation-and-session-set-up",
    "title": "Handling tables in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"xlsx\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"here\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we can activate them as shown below.\n\n# load packages\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(flextable)\nlibrary(xlsx)\nlibrary(openxlsx)\nlibrary(here)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/table/table.html#piping",
    "href": "tutorials/table/table.html#piping",
    "title": "Handling tables in R",
    "section": "Piping",
    "text": "Piping\nPiping, done with the sequence %&gt;%, is a very easy, intuitive, quick, and handy way to process data. Essentially piping means that we take an element that is to the left of the piping symbol and then do something to it; that way, the piping symbol can be translated as and then.\nWe could, for example, load data and then capitalize the column names and then group the data by status and attraction and then get the mean of money spend on deleting all observations except for the first one. A more formal way to write this would be:\n\nload %&gt;% capitalize %&gt;% group %&gt;% summarize.\n\nIn R this command would look like this:\n\n# example of a data processing pipeline\npipeddata &lt;- base::readRDS(\"tutorials/table/data/mld.rda\", \"rb\") %&gt;%\n    dplyr::rename(Status = status, Attraction = attraction, Money = money) %&gt;%\n    dplyr::group_by(Status, Attraction) %&gt;%\n    dplyr::summarise(Mean = mean(Money))\n# inspect summarized data\npipeddata\n\n# A tibble: 4 × 3\n# Groups:   Status [2]\n  Status       Attraction     Mean\n  &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt;\n1 Relationship Interested     99.2\n2 Relationship NotInterested  51.5\n3 Single       Interested    157. \n4 Single       NotInterested  46.0\n\n\nThe pipe has worked and we get the resulting summary which shows the mean of the money spend based on Attraction and Status."
  },
  {
    "objectID": "tutorials/table/table.html#selecting-and-filtering",
    "href": "tutorials/table/table.html#selecting-and-filtering",
    "title": "Handling tables in R",
    "section": "Selecting and filtering",
    "text": "Selecting and filtering\nAmong the most frequent procedures in data processing is selecting certain columns or extracting rows based on variable levels. In the tidyverse, this is done by using the select and filter functions. While select allows to extract columns, filter is used to extract rows, e.g. to get only observations that have a certain feature. Have a look at the example below.\n\n# select and filter\nreduceddata &lt;- newdata %&gt;%\n    # select the columns attraction and money\n    dplyr::select(attraction, money) %&gt;%\n    # extract rows which represent cases where the person was interested in someone\n    dplyr::filter(attraction == \"Interested\")\n# inspect new table\nnrow(reduceddata)\n\n[1] 50\n\ntable(reduceddata$attraction)\n\n\nInterested \n        50 \n\n\nWe have now reduced the data by excluding status (we have only selected attraction and money) and we have removed those 50 data rows of people who were not interested. The select function (like most other tidyverse functions) can also be used together with a minus sign which causes a column to be removed, thus dplyr::select(-money) would remove the money column (see below).\n\n# select and filter\ndatawithoutmoney &lt;- newdata %&gt;%\n    # remove money\n    dplyr::select(-money)\n# inspect data\nhead(datawithoutmoney)\n\n        status    attraction\n1 Relationship NotInterested\n2 Relationship NotInterested\n3 Relationship NotInterested\n4 Relationship NotInterested\n5 Relationship NotInterested\n6 Relationship NotInterested\n\n\nSelecting and filtering are extremely powerful functions that can also be combined with other functions. But before we discuss more complex issues, we will have a look at how we can change columns."
  },
  {
    "objectID": "tutorials/table/table.html#changing-data-and-adding-columns",
    "href": "tutorials/table/table.html#changing-data-and-adding-columns",
    "title": "Handling tables in R",
    "section": "Changing data and adding columns",
    "text": "Changing data and adding columns\nChanging and adding data is done with the mutate function. The mutate functions requires that we specify a column name - if we use the same name as the column we are changing, then we change the column but if we specify another column name, then a new column is created.\nWe will now create a new column (Spendalot) in which we encode if the person has spend a lot of money (100 AUD or more) on the present or not (less than 100 AUD).\n\n# creating a new column\nnewdata &lt;- newdata %&gt;%\n    dplyr::mutate(Spendalot = ifelse(money &gt;= 100, \"Alot\", \"Alittle\"))\n# inspect data\nhead(newdata)\n\n        status    attraction money Spendalot\n1 Relationship NotInterested 86.33   Alittle\n2 Relationship NotInterested 45.58   Alittle\n3 Relationship NotInterested 68.43   Alittle\n4 Relationship NotInterested 52.93   Alittle\n5 Relationship NotInterested 61.86   Alittle\n6 Relationship NotInterested 48.47   Alittle\n\n\nThe table now has a new column (Spendalot) because we have specified a column name that did not exist yet - had we written dplyr::mutate(money = ifelse(money &gt;= 100, \"Alot\", \"Alittle\")) then we would have changed the money column and replaced the money values with the labels Alot and Alittle."
  },
  {
    "objectID": "tutorials/table/table.html#renaming-columns",
    "href": "tutorials/table/table.html#renaming-columns",
    "title": "Handling tables in R",
    "section": "Renaming columns",
    "text": "Renaming columns\nOftentimes, column names are not really meaningful or incoherent which makes it easier to wrap your head around what the values in a column refer to. The easiest way around this is rename columns which is, fortunately very simple in the tidyverse. While the column names of our example table are meaningful, I want to capitalize the first letter of each column name. This can be done as follows.\n\n# renaming columns\nnewdata &lt;- newdata %&gt;%\n    dplyr::rename(Status = status, Attraction = attraction, Money = money)\n# inspect data\nhead(newdata)\n\n        Status    Attraction Money Spendalot\n1 Relationship NotInterested 86.33   Alittle\n2 Relationship NotInterested 45.58   Alittle\n3 Relationship NotInterested 68.43   Alittle\n4 Relationship NotInterested 52.93   Alittle\n5 Relationship NotInterested 61.86   Alittle\n6 Relationship NotInterested 48.47   Alittle\n\n\nThe renaming was successful as all column names now begin with a capital letter."
  },
  {
    "objectID": "tutorials/table/table.html#grouping-and-summarising",
    "href": "tutorials/table/table.html#grouping-and-summarising",
    "title": "Handling tables in R",
    "section": "Grouping and summarising",
    "text": "Grouping and summarising\nIn contrast to mutate, which retains the number of rows, summarizing creates new columns but collapses rows and only provides the summary value (or values if more than one summary is specified). Also, columns that are not grouping variables are removed.\nSummarizing is particularly useful when we want to get summaries of groups. We will modify the example from above and extract the mean and the standard deviation of the money spend on presents by relationship status and whether the giver was attracted to the giv-ee.\n\n# grouping and summarizing data\ndatasummary &lt;- newdata %&gt;%\n    dplyr::group_by(Status, Attraction) %&gt;%\n    dplyr::summarise(Mean = round(mean(Money), 2), SD = round(sd(Money), 1))\n# inspect summarized data\ndatasummary\n\n# A tibble: 4 × 4\n# Groups:   Status [2]\n  Status       Attraction     Mean    SD\n  &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 Relationship Interested     99.2  14.7\n2 Relationship NotInterested  51.5  17  \n3 Single       Interested    157.   23.2\n4 Single       NotInterested  46.0  19.9"
  },
  {
    "objectID": "tutorials/table/table.html#gathering-and-spreading",
    "href": "tutorials/table/table.html#gathering-and-spreading",
    "title": "Handling tables in R",
    "section": "Gathering and Spreading",
    "text": "Gathering and Spreading\nOne very common problem is that data - or at least parts of it - have to be transformed from long to wide format or vice versa. In the tidyverse, this is done using the gather and spread function. We will convert the summary table shown above into a wide format (we also remove the SD column as it is no longer needed)\n\n# converting data to wide format\nwidedata &lt;- datasummary %&gt;%\n    # remove SD column\n    dplyr::select(-SD) %&gt;%\n    # convert into wide format\n    tidyr::spread(Attraction, Mean)\n# inspect wide data\nwidedata\n\n# A tibble: 2 × 3\n# Groups:   Status [2]\n  Status       Interested NotInterested\n  &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n1 Relationship       99.2          51.5\n2 Single            157.           46.0\n\n\nWe can re-convert the wide into a long format using the gather function.\n\n# converting data to long format\nlongdata &lt;- widedata %&gt;%\n    # convert into long format\n    tidyr::gather(Attraction, Money, Interested:NotInterested)\n# inspect wide data\nlongdata\n\n# A tibble: 4 × 3\n# Groups:   Status [2]\n  Status       Attraction    Money\n  &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt;\n1 Relationship Interested     99.2\n2 Single       Interested    157. \n3 Relationship NotInterested  51.5\n4 Single       NotInterested  46.0\n\n\nThere are many more useful functions for processing, handling, and summarizing tables but this should suffice to get you started."
  },
  {
    "objectID": "tutorials/table/table.html#footnotes",
    "href": "tutorials/table/table.html#footnotes",
    "title": "Handling tables in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/llr/llr.html#visualizing-collocation-networks",
    "href": "tutorials/llr/llr.html#visualizing-collocation-networks",
    "title": "Analyzing learner language using R",
    "section": "Visualizing collocation networks",
    "text": "Visualizing collocation networks\nNetwork graphs are a very useful and flexible tool for visualizing relationships between elements such as words, personas, or authors. This section shows how to generate a network graph for collocations of the term transport using the quanteda package.\nIn a first step, we generate a document-feature matrix based on the sentences in the L1 data. A document-feature matrix shows how often elements (here these elements are the words that occur in the L1 data) occur in a selection of documents (here these documents are the sentences in the L1 data).\n\n# create document-feature matrix\nns_dfm &lt;- quanteda::dfm(quanteda::tokens(ns_sen)) %&gt;%\n  #quanteda::dfm_remove(remove_punct = TRUE) %&gt;%\n    quanteda::dfm_remove(pattern = stopwords('english'))\n\n\n\ndoc_idtransport01basicdilemafacingtext111000text210111text310000text400000text510000text610000\n\n\nAs we want to generate a network graph of words that collocate with the term organism, we use the calculateCoocStatistics function to determine which words most strongly collocate with our target term (organism).\n\n# load function for co-occurrence calculation\nsource(\"rscripts/calculateCoocStatistics.R\")\n# define term\ncoocTerm &lt;- \"transport\"\n# calculate co-occurrence statistics\ncoocs &lt;- calculateCoocStatistics(coocTerm, ns_dfm, measure=\"LOGLIK\")\n# inspect results\ncoocs[1:10]\n\n    public        use          .    traffic       rail     facing  commuters \n113.171974  19.437311  18.915658  10.508626   9.652830   9.382889   9.382889 \n   cheaper      roads       less \n  9.382889   9.080648   8.067363 \n\n\nWe now reduce the document-feature matrix to contain only the top 20 collocates of transport (plus our target word transport).\n\nredux_dfm &lt;- dfm_select(ns_dfm, \n                        pattern = c(names(coocs)[1:10], \"transport\"))\n\n\n\ndoc_idtransportfacingrail.commutersusepublicroadscheapertext1100000000text2111100000text3100110000text4000100000text5100111100text6100110110text7101110001text8100000000text9100100000text10000100000\n\n\nNow, we can transform the document-feature matrix into a feature-co-occurrence matrix as shown below. A feature-co-occurrence matrix shows how often each element in that matrix co-occurs with every other element in that matrix.\n\ntag_fcm &lt;- fcm(redux_dfm)\n\n\n\ndoc_idtransportfacingrail.commutersusepublicroadscheapertransport3417834183854facing002500000rail0054614242.0002254243845commuters000001211use0000001680public000000152roads000000040cheaper000000000traffic000000000\n\n\nUsing the feature-co-occurrence matrix, we can generate the network graph which shows the terms that collocate with the target term transport with the edges representing the co-occurrence frequency. To generate this network graph, we use the textplot_network function from the quanteda.textplots package.\n\n# generate network graph\nquanteda.textplots::textplot_network(tag_fcm, \n                                     min_freq = 1, \n                                     edge_alpha = 0.3, \n                                     edge_size = 5,\n                                     edge_color = \"gray80\",\n                                     vertex_labelsize = log(rowSums(tag_fcm)*15))"
  },
  {
    "objectID": "tutorials/lexsim/lexsim.html#jaccard-similarity",
    "href": "tutorials/lexsim/lexsim.html#jaccard-similarity",
    "title": "Introduction to Lexical Similarity",
    "section": "Jaccard Similarity",
    "text": "Jaccard Similarity\nThe Jaccard similarity is defined as an intersection of two texts divided by the union of that two documents. In other words it can be expressed as the number of common words over the total number of the words in the two texts or documents. The Jaccard similarity of two documents ranges from 0 to 1, where 0 signifies no similarity and 1 signifies complete overlap.The mathematical representation of the Jaccard Similarity is shown below: -\n\\[\\begin{equation}\nJ(A,B) = \\frac{|A \\bigcap B|}{|A \\bigcup B |} = \\frac{|A \\bigcap B|}{|A| + |B| - |A \\bigcap B|}\n\\end{equation}\\]"
  },
  {
    "objectID": "tutorials/lexsim/lexsim.html#cosine-similarity",
    "href": "tutorials/lexsim/lexsim.html#cosine-similarity",
    "title": "Introduction to Lexical Similarity",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nIn case of cosine similarity the two documents are represented in a n-dimensional vector space with each word represented in a vector form. Thus the cosine similarity metric measures the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space. The cosine similarity ranges from 0 to 1. A value closer to 0 indicates less similarity whereas a score closer to 1 indicates more similarity.The mathematical representation of the Cosine Similarity is shown below: -\n\\[\\begin{equation}\nsimilarity = cos(\\theta) = \\frac{A \\cdot B}{||A|| ||B||} = \\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\n\\end{equation}\\]"
  },
  {
    "objectID": "tutorials/lexsim/lexsim.html#levenshtein-distance",
    "href": "tutorials/lexsim/lexsim.html#levenshtein-distance",
    "title": "Introduction to Lexical Similarity",
    "section": "Levenshtein Distance",
    "text": "Levenshtein Distance\nLevenshtein distance comparison is generally carried out between two words. It determines the minimum number of single character edits required to change one word to another. The higher the number of edits more are the texts different from each other.An edit is defined by either an insertion of a character, a deletion of character or a replacement of a character. For two words a and b with lengths i and j the Levenshtein distance is defined as follows: -\n\\[\\begin{equation}\nlev_{a,b}(i,j) =\n\\begin{cases}\n    max(i,j) & \\quad \\text{if min(i,j) = 0,}\\\\\n    min \\begin{cases}\n      lev_{a,b}(i-1,j)+1  \\\\\n      lev_{a,b}(i, j-1)+1  & \\text{otherwise.}\\\\\n      lev_{a,b}(i-1,j-1)+1_{(a_{i} \\neq b_{j})} \\\\\n  \\end{cases}\n  \\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "tutorials/lexsim/lexsim.html#preparation-and-session-set-up",
    "href": "tutorials/lexsim/lexsim.html#preparation-and-session-set-up",
    "title": "Introduction to Lexical Similarity",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F)\n# install libraries\ninstall.packages(\"stringdist\")\ninstall.packages(\"hashr\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(stringdist)\nlibrary(hashr)\nlibrary(tidyverse)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/lexsim/lexsim.html#jaccard-similarity-1",
    "href": "tutorials/lexsim/lexsim.html#jaccard-similarity-1",
    "title": "Introduction to Lexical Similarity",
    "section": "Jaccard Similarity",
    "text": "Jaccard Similarity\n\n# Using the seq_dist function along with hash function to calculate the Jaccard similarity word-wise\njac_sim_score &lt;- seq_dist(hash(strsplit(text1, \"\\\\s+\")), hash(strsplit(text2, \"\\\\s+\")), method = \"jaccard\", q = 2)\nprint(paste0(\"The Jaccard similarity for the two texts is \", jac_sim_score))\n\n[1] \"The Jaccard similarity for the two texts is 0.727272727272727\""
  },
  {
    "objectID": "tutorials/lexsim/lexsim.html#cosine-similarity-1",
    "href": "tutorials/lexsim/lexsim.html#cosine-similarity-1",
    "title": "Introduction to Lexical Similarity",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\n\n# Using the seq_dist function along with hash function to calculate the Jaccard similarity word-wise\ncos_sim_score &lt;- seq_dist(hash(strsplit(text1, \"\\\\s+\")), hash(strsplit(text2, \"\\\\s+\")), method = \"cosine\", q = 2)\nprint(paste0(\"The Cosine similarity for the two texts is \", cos_sim_score))\n\n[1] \"The Cosine similarity for the two texts is 0.571428571428572\""
  },
  {
    "objectID": "tutorials/lexsim/lexsim.html#levenshtein-distance-1",
    "href": "tutorials/lexsim/lexsim.html#levenshtein-distance-1",
    "title": "Introduction to Lexical Similarity",
    "section": "Levenshtein distance",
    "text": "Levenshtein distance\n\n# Insert edit\nins_edit &lt;- stringdist(insert_ex[1], insert_ex[2], method = \"lv\")\nprint(paste0(\"The insert edit distance for \", insert_ex[1], \" and \", insert_ex[2], \" is \", ins_edit))\n\n[1] \"The insert edit distance for Marta and Martha is 1\"\n\n# Delete edit\ndel_edit &lt;- stringdist(del_ex[1], del_ex[2], method = \"lv\")\nprint(paste0(\"The delete edit distance for \", del_ex[1], \" and \", del_ex[2], \" is \", del_edit))\n\n[1] \"The delete edit distance for Genome and Gnome is 1\"\n\n# Replace edit\nrep_edit &lt;- stringdist(rep_ex[1], rep_ex[2], method = \"lv\")\nprint(paste0(\"The replace edit distance for \", rep_ex[1], \" and \", rep_ex[2], \" is \", rep_edit))\n\n[1] \"The replace edit distance for Tim and Tom is 1\""
  },
  {
    "objectID": "tutorials/repro/repro.html#what-is-data-management",
    "href": "tutorials/repro/repro.html#what-is-data-management",
    "title": "Data Management and Reproducibility",
    "section": "What is Data Management?",
    "text": "What is Data Management?\nData management refers to the comprehensive set of practices, policies, and processes used to manage data throughout its lifecycle (Corea, chap. 1). This involves ensuring that data is collected, stored, processed, and maintained in a way that it can be effectively used for analytical purposes. Good data management is crucial for enabling accurate, reliable, and meaningful analysis.\nKey components of data management in data analytics include:\n\nData Collection and Acquisition: Gathering data from various sources, which can include databases, APIs, sensors, web scraping, and more. The goal is to ensure the data is collected in a systematic and consistent manner.\nData Storage: Utilizing databases, data warehouses, data lakes, or cloud storage solutions to store collected data securely and efficiently. This ensures that data is easily accessible for analysis.\nData Cleaning and Preparation: Involves identifying and correcting errors or inconsistencies in the data to improve its quality. This step is critical for ensuring the accuracy of any subsequent analysis.\nData Integration: Combining data from different sources into a single, unified dataset. This often involves ETL (Extract, Transform, Load) processes where data is extracted from different sources, transformed into a consistent format, and loaded into a central repository.\nData Governance: Establishing policies and procedures to ensure data is managed properly. This includes defining roles and responsibilities, ensuring data privacy and security, and maintaining compliance with regulations.\nData Security: Implementing measures to protect data from unauthorized access, breaches, and other threats. This involves encryption, access controls, and regular security audits.\nData Analysis: Using statistical methods, algorithms, and software tools to analyze data and extract meaningful insights. This can involve descriptive, predictive, and prescriptive analytics.\nData Visualization: Presenting data in graphical or pictorial formats such as charts, graphs, and dashboards to help users understand trends, patterns, and insights more easily.\nData Quality Management: Continuously monitoring and maintaining the accuracy, consistency, and reliability of data. This involves data profiling, validation, and auditing.\nMetadata Management: Managing data about data, which includes documenting the data’s source, format, and usage. Metadata helps in understanding the context and provenance of the data.\nData Lifecycle Management: Managing data through its entire lifecycle, from initial creation and storage to eventual archiving and deletion. This ensures that data is managed in a way that supports its long-term usability and compliance with legal requirements.\n\nEffective data management practices ensure that data is high quality, well-organized, and accessible, which is essential for accurate and actionable data analytics. By implementing robust data management strategies, organizations can improve the reliability of their analyses, make better-informed decisions, and achieve greater operational efficiency.\nFor further reading and deeper insights, consider these resources: - Data Management Association International (DAMA) - Data Management Body of Knowledge (DAMA-DMBOK) - Gartner’s Data Management Solutions"
  },
  {
    "objectID": "tutorials/repro/repro.html#reproducibility-and-transparency",
    "href": "tutorials/repro/repro.html#reproducibility-and-transparency",
    "title": "Data Management and Reproducibility",
    "section": "Reproducibility and Transparency",
    "text": "Reproducibility and Transparency\nReproducibility is a cornerstone of scientific inquiry, demanding that two analyses empirical analyses yield consistent outcomes under equivalent conditions and with comparable populations under scrutiny (Gundersen; Goodman, Fanelli, and Ioannidis). Historically, the reproducibility of scientific findings was often assumed, but this assumption has been substantially challenged by the Replication Crisis (Moonesinghe, Khoury, and Janssens; Simons). The Replication Crisis, extensively documented Ioannidis, represents an ongoing methodological quandary stemming from the failure to reproduce critical medical studies and seminal experiments in social psychology during the late 1990s and early 2000s. By the early 2010s, the crisis had extended its reach to encompass segments of the social and life sciences Diener and Biswas-Diener, significantly eroding public trust in the results of studies from the humanities and social sciences (McRae; Yong).\nBelow are definitions of terms relevant for distinguishing in discussions around reproducibility and transparency. This clarification is necessary to avoid misunderstandings stemming from the common conflation of similar yet different terms in this discourse.\n\nReplication: Replication involves repeating a study’s procedure to determine if the prior findings can be reproduced. Unlike reproduction, which utilizes the same data and method, replication entails applying a similar method to different but comparable data. The aim is to ascertain if the results of the original study are robust across new data from the same or similar populations. Replication serves to advance theory by subjecting existing understanding to new evidence (Nosek and Errington; Moonesinghe, Khoury, and Janssens).\nReproduction: In contrast, reproduction (or computational replication) entails repeating a study by applying the exact same method to the exact same data (this is what McEnery and Brezina refer to as repeatability). The results should ideally be identical or highly similar to those of the original study. Reproduction relies on reproducibility, which assumes that the original study’s authors have provided sufficient information for others to replicate the study. This concept often pertains to the computational aspects of research, such as version control of software tools and environments (Nosek and Errington).\nRobustness: Robustness refers to the stability of results when studies are replicated using different procedures on either the same or different yet similar data. While replication studies may yield different results from the original study due to the use of different data, robust results demonstrate consistency in the direction and size of effects across varying procedures (Nosek and Errington).\nTriangulation: Recognizing the limitations of replication and reproducibility in addressing the issues highlighted by the Replication Crisis, researchers emphasize the importance of triangulation. Triangulation involves strategically employing multiple approaches to address a single research question, thereby enhancing the reliability and validity of findings (Munafò and Davey Smith).\nPractical versus theoretical/formal reproducibility: This distinction distinguishes between practical and theoretical or formal reproducibility (Schweinberger). Practical reproducibility emphasizes the provision of resources by authors to facilitate the replication of a study with minimal time and effort. These resources may include notebooks, code repositories, or detailed documentation, allowing for the reproducibility of studies across different computers and software environments (Grüning et al.).\nTransparency: Transparency in research entails clear and comprehensive reporting of research procedures, methods, data, and analytical techniques. It involves providing sufficient information about study design, data collection, analysis, and interpretation to enable others to understand and potentially replicate the study. Transparency is particularly relevant in qualitative and interpretive research in the social sciences, where data sharing may be limited due to ethical or copyright considerations (Moravcsik).\n\nWe now move on to some practical tips and tricks on how to implement transparent and well-documented research practices."
  },
  {
    "objectID": "tutorials/repro/repro.html#folder-structures",
    "href": "tutorials/repro/repro.html#folder-structures",
    "title": "Data Management and Reproducibility",
    "section": "Folder Structures",
    "text": "Folder Structures\n\nDifferent methods of organizing your folders have unique advantages and challenges, but they all share a reliance on a tree-structure hierarchy, where more general folders contain more specialized subfolders. For instance, if your goal is to locate any file with minimal clicks, an alphabetical folder structure might be effective. In this system, items are stored based on their initial letter (e.g., everything starting with a “T” like “travel” under “T”, or everything related to “courses” under “C”). However, this method can be unintuitive as it groups completely unrelated topics together simply because they share the same initial letter.\nA more common and intuitive approach is to organize your data into meaningful categories that reflect different aspects of your life. For example:\n\nWork: This can include subfolders like Teaching and Research.\nPersonal: This can encompass Rent, Finances, and Insurances.\nMedia: This might include Movies, Music, and Audiobooks.\n\nThis method not only makes it easier to locate files based on context but also keeps related topics grouped together, enhancing both accessibility and logical organization.\nTo further improve folder organization, consider the following best practices:\n\nConsistency: Use a consistent naming convention to avoid confusion.\nClarity: Use clear and descriptive names for folders and files.\nDate-Based Organization: For projects that evolve over time, include dates in the folder names.\nRegular Maintenance: Periodically review and reorganize your folders to keep them tidy and relevant.\n\nFolders and files should be labeled in a meaningful and consistent way to avoid ambiguity and confusion. Avoid generic names like Stuff or Document for folders, and doc2 or homework for files. Naming files consistently, logically, and predictably helps prevent disorganization, misplaced data, and potential project delays. A well-thought-out file naming convention ensures that files are:\n\nEasier to Process: Team members won’t have to overthink the file naming process, reducing cognitive load.\nEasier to Access, Retrieve, and Store: A consistent naming convention facilitates quick and easy access to files.\nEasier to Browse Through: Organized files save time and effort when searching through directories.\nHarder to Lose: A logical structure makes it less likely for files to be misplaced.\nEasier to Check for Obsolete or Duplicate Records: Systematic naming aids in identifying and managing outdated or redundant files.\n\nThe UQ Library offers the Digital Essentials module Working with Files. This module contains information on storage options, naming conventions, back up options, metadata, and file formats. Some of these issues are dealt with below but the materials provided by the library offer a more extensive introduction into these topics.\nBy implementing these strategies, you can create a folder structure that is not only efficient but also scalable, accommodating both your current needs and future expansions."
  },
  {
    "objectID": "tutorials/repro/repro.html#folders-for-research-and-teaching",
    "href": "tutorials/repro/repro.html#folders-for-research-and-teaching",
    "title": "Data Management and Reproducibility",
    "section": "Folders for Research and Teaching",
    "text": "Folders for Research and Teaching\nHaving a standard folder structure can keep your files neat and tidy and save you time looking for data. It can also help if you are sharing files with colleagues and having a standard place to put working data and documentation.\nStore your projects in a separate folder. For instance, if you are creating a folder for a research project, create the project folder within a separate project folder that is within a research folder. If you are creating a folder for a course, create the course folder within a courses folder within a teaching folder, etc.\n\nWhenever you create a folder for a new project, try to have a set of standard folders. For example, when I create research project folders, I always have folders called archive, data, docs, and images. When I create course folders, I always have folders called slides, assignments, exam, studentmaterials, and correspondence. However, you are, of course, free to modify and come up or create your own basic project design. Also, by prefixing the folder names with numbers, you can force your files to be ordered by the steps in your workflow.\n\nHaving different sub folders allows you to avoid having too many files and many different file types in a single folder. Folders with many different files and file types tend to be chaotic and can be confusing. In addition, I have one ReadMe file on the highest level (which only contains folders except for this one single file) in which I describe very briefly what the folder is about and which folders contain which documents as well as some basic information about the folder (e.g. why and when I created it). This ReadMe file is intended both for me as a reminder what this folder is about and what it contains but also for others in case I hand the project over to someone else who continues the project or for someone who takes over my course and needs to use my materials."
  },
  {
    "objectID": "tutorials/repro/repro.html#shared-folder-structures",
    "href": "tutorials/repro/repro.html#shared-folder-structures",
    "title": "Data Management and Reproducibility",
    "section": "Shared Folder Structures",
    "text": "Shared Folder Structures\nIf you work in a team or regularly share files and folders, establishing a logical structure for collaboration is essential. Here are key considerations for developing an effective team folder structure:\n\nPre-existing Agreements: Before implementing a new folder structure, ensure there are no existing agreements or conventions in place. Collaborate with team members to assess the current system and identify areas for improvement.\nMeaningful Naming: Name folders in a way that reflects their contents and purpose. Avoid using staff names and opt for descriptive labels that indicate the type of work or project. This ensures clarity and accessibility for all team members.\nConsistency: Maintain consistency across the folder hierarchy to facilitate navigation and organization. Adhere to the agreed-upon structure and naming conventions to streamline workflows and minimize confusion.\nHierarchical Structure: Organize folders hierarchically, starting with broad categories and gradually narrowing down to more specific topics. This hierarchical arrangement enhances organization and facilitates efficient file retrieval.\nDifferentiate Ongoing and Completed Work: Differentiate between ongoing and completed work by segregating folders accordingly. As projects progress and accumulate files, separating older documents from current ones helps maintain clarity and focus.\nBackup Solutions: Implement robust backup solutions to safeguard against data loss in the event of a disaster. Utilize university-provided storage solutions or external backup services to ensure files are securely backed up and retrievable.\nPost-Project Cleanup: Conduct regular cleanup activities to remove redundant or obsolete files and folders post-project completion. This declutters the workspace, improves efficiency, and ensures that relevant data remains easily accessible.\n\nBy following these guidelines, teams can establish a cohesive and efficient folder structure that promotes collaboration, organization, and data integrity.\n\n\nGOING FURTHER\n\nFor Beginners\n\nPick some of your projects and illustrate how you currently organize your files. See if you can devise a better naming convention or note one or two improvements you could make to how you name your files.\nThere are some really good folder template shapes around. Here is one you can download.\n\nFor Advanced Folder designers\n\nCome up with a policy for your team for folder structures. You could create a template and put it in a downloadable location for them to get them started."
  },
  {
    "objectID": "tutorials/repro/repro.html#file-naming-convention-fnc",
    "href": "tutorials/repro/repro.html#file-naming-convention-fnc",
    "title": "Data Management and Reproducibility",
    "section": "File Naming Convention (FNC)",
    "text": "File Naming Convention (FNC)\nOne of the most basic but also most important practices a researcher can do to improve the reproducibility and transparency of their research is to follow a consistent file naming convention. A File Naming Convention (FNC) is a systematic framework for naming your files in a way that clearly describes their contents and, importantly, how they relate to other files. Establishing an agreed-upon FNC before collecting data is essential as it ensures consistency, improves organization, and facilitates easier retrieval and collaboration. By establishing a clear and consistent File Naming Convention, you can significantly improve the efficiency and effectiveness of your data management practices, making it easier to handle, share, and preserve important information.\n\nKey elements to consider when creating a File Naming Convention include:\n\nDescriptive Names: Use clear and descriptive names that provide information about the file’s content, purpose, and date. Avoid vague or generic names.\nConsistency: Apply the same naming format across all files to maintain uniformity. This includes using consistent date formats, abbreviations, and capitalization.\nVersion Control: Incorporate version numbers or dates in the file names to track revisions and updates. For example, “ProjectReport_v1.0_2024-05-22.docx”.\nAvoid Special Characters: Use only alphanumeric characters and underscores or hyphens to avoid issues with different operating systems or software.\nLength and Readability: Keep file names concise yet informative. Avoid overly long names that may be difficult to read or cause problems with file path limitations.\nOrganizational Context: Reflect the file’s place within the broader project or system. For example, use a prefix that indicates the department or project phase.\n\nExample of a File Naming Convention:\n[ProjectName]_[DocumentType]_[Date]_[Version].[Extension]\nExample:\nClimateStudy_Report_20240522_v1.0.docx\n\nHere are some additional hints for optimizing file naming:\n\nAvoid Special Characters: Special characters like +, !, “, -, ., ö, ü, ä, %, &, (, ), [, ], &, $, =, ?, ’, #, or / should be avoided. They can cause issues with file sharing and compatibility across different systems. While underscores (_) are also special characters, they are commonly used for readability.\nNo White Spaces: Some software applications replace or collapse white spaces, which can lead to problems. A common practice is to capitalize initial letters in file names to avoid white spaces (e.g., TutorialIntroComputerSkills or Tutorial_IntroComputerSkills).\nInclude Time-Stamps: When adding dates to file names, use the YYYYMMDD format. This format ensures that files are easily sorted in chronological order. For example, use TutorialIntroComputerSkills20230522 or Tutorial_IntroComputerSkills_20230522.\n\nBenefits of a robust File Naming Convention include:\n\nEnhanced Organization: Files are easier to categorize and locate.\nImproved Collaboration: Team members can understand and navigate the file structure more efficiently.\nConsistency and Standardization: Reduces errors and confusion, ensuring that everyone follows the same system.\nStreamlined Data Management: Simplifies the process of managing large volumes of data.\n\nFor comprehensive guidance, the University of Edinburgh provides a detailed list of 13 Rules for File Naming Conventions with examples and explanations. Additionally, the Australian National Data Service (ANDS) offers a useful guide on file wrangling, summarized below."
  },
  {
    "objectID": "tutorials/repro/repro.html#keeping-copies-and-the-3-2-1-rule",
    "href": "tutorials/repro/repro.html#keeping-copies-and-the-3-2-1-rule",
    "title": "Data Management and Reproducibility",
    "section": "Keeping copies and the 3-2-1 Rule",
    "text": "Keeping copies and the 3-2-1 Rule\n\nKeeping a copy of all your data (working, raw, and completed) both in the cloud (recommended) and on your computer is incredibly important. This ensures that if you experience a computer failure, accidentally delete your data, or encounter data corruption, your research remains recoverable and restorable.\nWhen working with and processing data, it is also extremely important to always keep at least one copy of the original data. The original data should never be deleted; instead, you should copy the data and delete only sections of the copy while retaining the original data intact.\nThe 3-2-1 backup rule has been developed as a guide against data loss (Pratt). According to this rule, one should strive to have at least three copies of your project stored in different locations. Specifically, maintain at least three (3) copies of your data, storing backup copies on two (2) different storage media, with one (1) of them located offsite. While this guideline may vary depending on individual preferences, I personally adhere to this approach for safeguarding my projects.\n\n\non my personal notebook\non at least one additional hard drive (that you keep in a secure location)\nin an online repository (for example, UQ’s Research Data Management system (RDM) OneDrive, MyDrive, GitHub, or GitLab)\n\nUsing online repositories ensures that you do not lose any data in case your computer crashes (or in case you spill lemonade over it - don’t ask…) but it comes at the cost that your data can be more accessible to (criminal or other) third parties. Thus, if you are dealing with sensitive data, I suggest to store it on an additional external hard drive and do not keep cloud-based back-ups. If you trust tech companies with your data (or think that they are not interested in stealing your data), cloud-based solutions such as OneDrive, Google’s MyDrive, or Dropbox are ideal and easy to use options (however, UQ’s RDM is a safer option).\nThe UQ library also offers additional information on complying with ARC and NHMRC data management plan requirements and that UQ RDM meets these requirements for sensitive data (see here).\n\n\nGOING FURTHER\n\nFor Beginners\n\nGet your data into UQ’s RDM or Cloud Storage - If you need help, talk to the library or your tech/eResearch/QCIF Support\n\nFor Advanced backupers\n\nBuild a policy for your team or group on where things are stored. Make sure the location of your data is saved in your documentation"
  },
  {
    "objectID": "tutorials/repro/repro.html#dealing-with-sensitive-data",
    "href": "tutorials/repro/repro.html#dealing-with-sensitive-data",
    "title": "Data Management and Reproducibility",
    "section": "Dealing with Sensitive Data",
    "text": "Dealing with Sensitive Data\nThis section will elaborate on who to organize and handle (research) data and introduce some basic principles that may help you to keep your data tidy.\nTips for sensitive data\n\nSensitive data are data that can be used to identify an individual, species, object, or location that introduces a risk of discrimination, harm, or unwanted attention. Major, familiar categories of sensitive data are: personal data - health and medical data - ecological data that may place vulnerable species at risk.\n\nSeparating identifying variables from your data\n\nSeparating or deidentifying your data has the purpose to protect an individual’s privacy. According to the Australian Privacy Act 1988, “personal information is deidentified if the information is no longer about an identifiable individual or an individual who is reasonably identifiable”. Deidentified information is no longer considered personal information and can be shared. More information on the Commonwealth Privacy Act can be located here.\nDeidentifying aims to allow data to be used by others for publishing, sharing and reuse without the possibility of individuals/location being re-identified. It may also be used to protect the location of archaeological findings, cultural data of location of endangered species.\nAny identifiers (name, date of birth, address or geospatial locations etc) should be removed from main data set and replaced with a code/key. The code/key is then preferably encrypted and stored separately. By storing deidentified data in a secure solution, you are meeting safety, controlled, ethical, privacy and funding agency requirements.\nRe-identifying an individual is possible by recombining the deidentifiable data set and the identifiers."
  },
  {
    "objectID": "tutorials/repro/repro.html#managing-deidentification-ardc",
    "href": "tutorials/repro/repro.html#managing-deidentification-ardc",
    "title": "Data Management and Reproducibility",
    "section": "Managing Deidentification (ARDC)",
    "text": "Managing Deidentification (ARDC)\n\nPlan deidentification early in the research as part of your data management planning\nRetain original unedited versions of data for use within the research team and for preservation\nCreate a deidentification log of all replacements, aggregations or removals made\nStore the log separately from the deidentified data files\nIdentify replacements in text in a meaningful way, e.g. in transcribed interviews indicate replaced text with [brackets] or use XML mark-up tags.\n\nManagement of identifiable data (ARDC)\nData may often need to be identifiable (i.e. contains personal information) during the process of research, e.g. for analysis. If data is identifiable then ethical and privacy requirements can be met through access control and data security. This may take the form of:\n\nControl of access through physical or digital means (e.g. passwords)\nEncryption of data, particularly if it is being moved between locations\nEnsuring data is not stored in an identifiable and unencrypted format when on easily lost items such as USB keys, laptops and external hard drives.\nTaking reasonable actions to prevent the inadvertent disclosure, release or loss of sensitive personal information.\n\n\n\nSafely sharing sensitive data guide (ARDC)\n\n\nANDS’ Deidentification Guide collates a selection of Australian and international practical guidelines and resources on how to deidentify data sets. You can find more information about deidentification here and information about safely sharing sensitive data here.\n\n\n\n\nAustralian practical guidance for Deidentification (ARDC)\n\n\nAustralian Research Data Commons (ARDC) formerly known as Australian National Data Service (ANDS) released a fabulous guide on Deidentification. The Deidentification guide is intended for researchers who own a data set and wish to share safely with fellow researchers or for publishing of data. The guide can be located here.\n\n\n\n\nNationally available guidelines for handling sensitive data\n\n\nThe Australian Government’s Office of the Australian Information Commissioner (OAIC) and CSIRO Data61 have released a Deidentification Decision Making Framework, which is a “practical guide to deidentification, focusing on operational advice”. The guide will assist organisations that handle personal information to deidentify their data effectively.\nThe OAIC also provides high-level guidance on deidentification of data and information, outlining what deidentification is, and how it can be achieved.\nThe Australian Government’s guidelines for the disclosure of health information, includes techniques for making a data set non-identifiable and example case studies.\nAustralian Bureau of Statistics’ National Statistical Service Handbook. Chapter 11 contains a summary of methods to maintain privacy.\nmed.data.edu.au gives information about anonymisation\nThe Office of the Information Commissioner Queensland’s guidance on deidentification techniques can be found here"
  },
  {
    "objectID": "tutorials/repro/repro.html#data-as-publications",
    "href": "tutorials/repro/repro.html#data-as-publications",
    "title": "Data Management and Reproducibility",
    "section": "Data as publications",
    "text": "Data as publications\nMore recently, regarding data as a form of publications has gain a lot of traction. This has the advantage that it rewards researchers who put a lot of work into compiling data and it has created an incentive for making data available, e.g. for replication. The UQ RDM and UQ eSpace can help with the process of publishing a dataset.\nThere are many platforms where data can be published and made available in a sustainable manner. Below are listed just some options that are recommendable:\n\n\n\nUQ Research Data Manager\n\nThe UQ Research Data Manager (RDM) system is a robust, world-leading system designed and developed here at UQ. The UQ RDM provides the UQ research community with a collaborative, safe and secure large-scale storage facility to practice good stewardship of research data. The European Commission report “Turning FAIR into Reality” cites UQ’s RDM as an exemplar of, and approach to, good research data management practice. The disadvantage of RDM is that it is not available to everybody but restricted to UQ staff, affiliates, and collaborators.\n\n\n\nOpen Science Foundation\n\nThe Open Science Foundation (OSF) is a free, global open platform to support your research and enable collaboration.\n\n\n\nTROLLing\n\nTROLLing | DataverseNO (The Tromsø Repository of Language and Linguistics) is a repository of data, code, and other related materials used in linguistic research. The repository is open access, which means that all information is available to everyone. All postings are accompanied by searchable metadata that identify the researchers, the languages and linguistic phenomena involved, the statistical methods applied, and scholarly publications based on the data (where relevant).\n\n\n\nGit\n\nGitHub offers the distributed version control using Git. While GitHub is not designed to host research data, it can be used to share share small collections of research data and make them available to the public. The size restrictions and the fact that GitHub is a commercial enterprise owned by Microsoft are disadvantages of this as well as alternative, but comparable platforms such as GitLab."
  },
  {
    "objectID": "tutorials/repro/repro.html#software",
    "href": "tutorials/repro/repro.html#software",
    "title": "Data Management and Reproducibility",
    "section": "Software",
    "text": "Software\nUsing free, open-source software for data processing and analysis, such as Praat, R, Python, or Julia, promotes transparency and reproducibility by reducing financial access barriers and enabling broader audiences to conduct analyses. Open-source tools provide a transparent and accessible framework for conducting analyses, allowing other researchers to replicate and validate results while eliminating access limitations present in commercial tools, which may not be available to researchers from low-resource regions (see Heron, Hanson, and Ricketts for a case-study on the use of free imaging software).\nIn contrast, employing commercial tools or multiple tools in a single analysis can hinder transparency and reproducibility. Switching between tools often requires time-consuming manual input and may disadvantage researchers from low-resource regions who may lack access to licensed software tools. While free, open-source tools are recommended for training purposes, they may have limitations in functionality (Heron, Hanson, and Ricketts, 7/36)."
  },
  {
    "objectID": "tutorials/repro/repro.html#efficient-documentation",
    "href": "tutorials/repro/repro.html#efficient-documentation",
    "title": "Data Management and Reproducibility",
    "section": "Efficient Documentation",
    "text": "Efficient Documentation\n\nBe Clear and Concise: Write in a straightforward and concise manner. Avoid jargon and complex language to ensure that your documentation is accessible to a wide audience.\nInclude Context: Provide background information to help the reader understand the purpose and scope of the work. Explain why certain decisions were made.\nStep-by-Step Instructions: Break down processes into clear, sequential steps. This makes it easier for someone to follow your workflow.\nUse Consistent Formatting: Consistency in headings, fonts, and styles improves readability and helps readers quickly find the information they need.\nDocument Locations and Structures: Clearly describe where files are located and the structure of your directories. Include details on how to navigate through your file system.\nExplain File Naming Conventions: Detail your file naming conventions so others can understand the logic behind your organization and replicate it if necessary.\nUpdate Regularly: Documentation should be a living document. Regularly update it to reflect changes and new developments in your project.\n\nExample\nIf you were documenting a data analysis project, your documentation might include:\n\nProject Overview: A brief summary of the project’s objectives, scope, and outcomes.\nDirectory Structure: An explanation of the folder organization and the purpose of each directory.\nData Sources: Descriptions of where data is stored and how it can be accessed.\nProcessing Steps: Detailed steps on how data is processed, including code snippets and explanations.\nAnalysis Methods: An overview of the analytical methods used and the rationale behind their selection.\nResults: A summary of the results obtained and where they can be found.\nVersion Control: Information on how the project is version-controlled, including links to repositories and branches.\n\nBy following these best practices, your documentation will be comprehensive and user-friendly, ensuring that anyone who needs to understand your work can do so efficiently. This level of detail not only aids in collaboration but also enhances the reproducibility and transparency of your projects."
  },
  {
    "objectID": "tutorials/repro/repro.html#documentation-and-the-bus-factor",
    "href": "tutorials/repro/repro.html#documentation-and-the-bus-factor",
    "title": "Data Management and Reproducibility",
    "section": "Documentation and the Bus Factor",
    "text": "Documentation and the Bus Factor\nDocumentation is not just about where your results and data are saved; it encompasses a wide range of forms depending on your needs and work style. Documenting your processes can include photos, word documents with descriptions, or websites that detail how you work.\nThe concept of documentation is closely linked to the Bus Factor (Jabrayilzade et al.) — a measure of how many people on a project need to be unavailable (e.g., hit by a bus) for the project to fail. Many projects have a bus factor of one, meaning that if the key person is unavailable, the project halts. Effective documentation raises the bus factor, ensuring that the project can continue smoothly if someone suddenly leaves or is unavailable.\nIn collaborative projects, having a log of where to find relevant information and who to ask for help is particularly useful. Ideally, documentation should cover everything that a new team member needs to know. The perfect person to create this log is often the last person who joined the project, as they can provide fresh insights into what information is most needed."
  },
  {
    "objectID": "tutorials/repro/repro.html#creating-an-onboarding-log",
    "href": "tutorials/repro/repro.html#creating-an-onboarding-log",
    "title": "Data Management and Reproducibility",
    "section": "Creating an Onboarding Log",
    "text": "Creating an Onboarding Log\nIf you haven’t created a log for onboarding new team members, it’s highly recommended. This log should be stored in a ReadMe document or folder at the top level of the project directory. This ensures that essential information is easily accessible to anyone who needs it.\nBy documenting thoroughly and effectively, you improve the resilience and sustainability of your project, making it less dependent on any single individual and enhancing its overall robustness.\n\n\nGOING FURTHER\n\nFor Beginners\n\nRead this first: How to start Documenting and more by CESSDA ERIC\nStart with documenting in a text file or document- any start is a good start\nHave this document automatically synced to the cloud with your data or keep this in a shared place such as Google docs, Microsoft teams, or Owncloud. If you collaborate on a project and use UQ’s RDM, you should store a copy of your documentation there.\n\nFor Intermediates\n\nOnce you have the basics in place, go into detail on how your workflow goes from your raw data to the finished results. This can be anything from a detailed description of how you analyse your data, over R Notebooks, to downloaded function lists from Virtual Lab.\n\nFor Advanced documentarians\n\nLearn about Git Repositories and wikis."
  },
  {
    "objectID": "tutorials/repro/repro.html#reproducible-reports-and-notebooks",
    "href": "tutorials/repro/repro.html#reproducible-reports-and-notebooks",
    "title": "Data Management and Reproducibility",
    "section": "Reproducible reports and notebooks",
    "text": "Reproducible reports and notebooks\nNotebooks seamlessly combine formatted text with executable code (e.g., R or Python) and display the resulting outputs, enabling researchers to trace and understand every step of a code-based analysis. This integration is facilitated by markdown, a lightweight markup language that blends the functionalities of conventional text editors like Word with programming interfaces. Jupyter notebooks (Pérez and Granger) and R notebooks (Xie) exemplify this approach, allowing researchers to interleave explanatory text with code snippets and visualize outputs within the same document. This cohesive presentation enhances research reproducibility and transparency by providing a comprehensive record of the analytical process, from code execution to output generation.\n\nNotebooks offer several advantages for facilitating transparent and reproducible research in corpus linguistics. They have the capability to be rendered into PDF format, enabling easy sharing with reviewers and fellow researchers. This allows others to scrutinize the analysis process step by step. Additionally, the reporting feature of notebooks permits other researchers to replicate the same analysis with minimal effort, provided that the necessary data is accessible. As such, notebooks provide others with the means to thoroughly understand and replicate an analysis at the click of a button (Schweinberger and Haugh).\n\nFurthermore, while notebooks are commonly used for documenting quantitative and computational analyses, recent studies have demonstrated their efficacy in rendering qualitative and interpretative work in corpus pragmatics (Schweinberger and Haugh) and corpus-based discourse analysis (see Bednarek, Schweinberger, and Lee) more transparent. Especially interactive notebooks (but also traditional, non-interactive notebooks) enhance accountability by facilitating data exploration and enabling others to verify the reliability and accuracy of annotation schemes.\nSharing notebooks offers an additional advantage compared to sharing files containing only code. While code captures the logic and instructions for analysis, it lacks the output generated by the code, such as visualizations or statistical models. Reproducing analyses solely from code necessitates specific coding expertise and replicating the software environment used for the original analysis. This process can be challenging, particularly for analyses reliant on diverse software applications, versions, and libraries, especially for researchers lacking strong coding skills. In contrast, rendered notebooks display both the analysis steps and the corresponding code output, eliminating the need to recreate the output locally. Moreover, understanding the code in the notebook typically requires only basic comprehension of coding concepts, enabling broader accessibility to the analysis process."
  },
  {
    "objectID": "tutorials/repro/repro.html#version-control-git",
    "href": "tutorials/repro/repro.html#version-control-git",
    "title": "Data Management and Reproducibility",
    "section": "Version control (Git)",
    "text": "Version control (Git)\nImplementing version control systems, such as Git, helps track changes in code and data over time. The primary issue that version control applications address is the dependency of analyses on specific versions of software applications. What may have worked and produced a desired outcome with one version of a piece of software may no longer work with another version. Thus, keeping track of versions of software packages is crucial for sustainable reproducibility. Additionally, version control extends to tracking different versions of reports or analytic steps, particularly in collaborative settings (Blischak, Davenport, and Wilson).\n\nVersion control facilitates collaboration by allowing researchers to revert to previous versions if necessary and provides an audit trail of the data processing, analysis, and reporting steps. It enhances transparency by capturing the evolution of the research project. Version control systems, such as Git, can be utilized to track code changes and facilitate collaboration (Blischak, Davenport, and Wilson).\nRStudio has built-in version control and also allows direct connection of projects to GitHub repositories. GitHub is a web-based platform and service that provides a collaborative environment for software development projects. It offers version control using Git, a distributed version control system, allowing developers to track changes to their code, collaborate with others, and manage projects efficiently. GitHub also provides features such as issue tracking, code review, and project management tools, making it a popular choice for both individual developers and teams working on software projects.\nUploading and sharing resources (such as notebooks, code, annotation schemes, additional reports, etc.) on repositories like GitHub (https://github.com/) (Beer) ensures long-term preservation and accessibility, thereby ensuring that the research remains available for future analysis and verification. By openly sharing research materials on platforms like GitHub, researchers enable others to access and scrutinize their work, thereby promoting transparency and reproducibility."
  },
  {
    "objectID": "tutorials/repro/repro.html#digital-object-identifier-doi-and-persistent-identifier-pid",
    "href": "tutorials/repro/repro.html#digital-object-identifier-doi-and-persistent-identifier-pid",
    "title": "Data Management and Reproducibility",
    "section": "Digital Object Identifier (DOI) and Persistent identifier (PiD)",
    "text": "Digital Object Identifier (DOI) and Persistent identifier (PiD)\nOnce you’ve completed your project, help make your research data discoverable, accessible and possibly re-usable using a PiD such as a DOI! A Digital Object Identifier (DOI) is a unique alphanumeric string assigned by either a publisher, organisation or agency that identifies content and provides a PERSISTENT link to its location on the internet, whether the object is digital or physical. It might look something like this http://dx.doi.org/10.4225/01/4F8E15A1B4D89.\n\nDOIs are also considered a type of persistent identifiers (PiDs). An identifier is any label used to name some thing uniquely (whether digital or physical). URLs are an example of an identifier. So are serial numbers, and personal names. A persistent identifier is guaranteed to be managed and kept up to date over a defined time period.\nJournal publishers assign DOIs to electronic copies of individual articles. DOIs can also be assigned by an organisation, research institutes or agencies and are generally managed by the relevant organisation and relevant policies. DOIs not only uniquely identify research data collections, it also supports citation and citation metrics.\nA DOI will also be given to any data set published in UQ eSpace, whether added manually or uploaded from UQ RDM. For information on how cite data, have a look here.\nKey points\n\nDOIs are a persistent identifier and as such carry expectations of curation, persistent access and rich metadata\nDOIs can be created for DATA SETS and associated outputs (e.g. grey literature, workflows, algorithms, software etc) - DOIs for data are equivalent with DOIs for other scholarly publications\nDOIs enable accurate data citation and bibliometrics (both metrics and altmetrics)\nResolvable DOIs provide easy online access to research data for discovery, attribution and reuse\n\n\n\nGOING FURTHER\n\nFor Beginners\n\nEnsure data you associate with a publication has a DOI- your library is the best group to talk to for this.\n\nFor Intermediates\n\nLearn more about how your DOI can potentially increase your citation rates by watching this 4m:51s video\nLearn more about how your DOI can potentially increase your citation rate by reading the ANDS Data Citation Guide\n\nFor Advanced identifiers\n\nLearn more about PiDs and DOIs\nContact the Library for advice on how to obtain a DOI upon project completion.\nHave a look at ANDS/ARDC - Citation and Identifiers\nCheck out the DOI system for research data"
  },
  {
    "objectID": "tutorials/repro/repro.html#rproj",
    "href": "tutorials/repro/repro.html#rproj",
    "title": "Data Management and Reproducibility",
    "section": "Rproj",
    "text": "Rproj\nIf you’re utilizing RStudio, you can create a new R project, which is essentially a working directory identified by a .RProj file. When you open a project (either through ‘File/Open Project’ in RStudio or by double-clicking on the .RProj file outside of R), the working directory is automatically set to the location of the .RProj file.\n\nI highly recommend creating a new R Project at the outset of each research endeavor. Upon creating a new R project, promptly organize your directory by establishing folders to house your R code, data files, notes, and other project-related materials. This can be done outside of R on your computer or within the Files window of RStudio. For instance, consider creating a ‘R’ folder for your R code and a ‘data’ folder for your datasets.\nPrior to adopting R Projects, I used setwd() to set my working directory. However, this method has drawbacks as it requires specifying an absolute file path, which can lead to broken scripts and hinder collaboration and reproducibility efforts. Consequently, reliance on setwd() impedes the sharing and transparency of analyses and projects. By contrast, utilizing R Projects streamlines workflow management and facilitates reproducibility and collaboration."
  },
  {
    "objectID": "tutorials/repro/repro.html#dependency-control-renv",
    "href": "tutorials/repro/repro.html#dependency-control-renv",
    "title": "Data Management and Reproducibility",
    "section": "Dependency Control (renv)",
    "text": "Dependency Control (renv)\n\nThe renv package introduces a novel approach to enhancing the independence of R projects by eliminating external dependencies. By creating a dedicated library within your project, renv ensures that your R project operates autonomously from your personal library. Consequently, when sharing your project, the associated packages are automatically included.\nrenv aims to provide a robust and stable alternative to the packrat package, which, in my experience, fell short of expectations. Having utilized renv myself, I found it to be user-friendly and reliable. Although the initial generation of the local library may require some time, it seamlessly integrates into workflows without causing any disruptions. Overall, renv is highly recommended for simplifying the sharing of R projects, thereby enhancing transparency and reproducibility.\nOne of renv’s core principles is to preserve existing workflows, ensuring that they function as before while effectively isolating the R dependencies of your project, including package versioning.\nFor more information on renv and its functionalities, as well as guidance on its implementation, refer to the official documentation."
  },
  {
    "objectID": "tutorials/repro/repro.html#version-control-with-git",
    "href": "tutorials/repro/repro.html#version-control-with-git",
    "title": "Data Management and Reproducibility",
    "section": "Version Control with Git",
    "text": "Version Control with Git\n\nGetting started with Git\nTo connect your Rproject with GitHub, you need to have Git installed (if you have not yet downloaded and installed Git, simply search for download git in your favorite search engine and follow the instructions) and you need to have a GitHub account. If you do not have a GitHub account, here is a video tutorial showing how you can do this. If you have trouble with this, you can also check out Happy Git and GitHub for the useR at happygitwithr.com.\n\nJust as a word of warning: when I set up my connection to Git and GitHUb things worked very differently, so things may be a bit different on your machine. In any case, I highly recommend this YouTube tutorial which shows how to connect to Git and GitHub using the usethis package or this, slightly older, YouTube tutorial on how to get going with Git and GitHub when working with RStudio.\nOld school\nWhile many people use the usethis package to connect RStudio to GitHub, I still use a somewhat old school way to connect my projects with GitHub. I have decided to show you how to connect RStudio to GitHub using this method, as I actually think it is easier once you have installed Git and created a gitHub account.\nBefore you can use Git with R, you need to tell RStudio that you want to use Git. To do this, go to Tools, then Global Options and then to Git/SVN and make sure that the box labeled Enable version control interface for RStudio projects. is checked. You need to then browse to the Git executable file (for Window’s users this is called the Git.exe file).\n\n\n\nNow, we can connect our project to Git (not to GitHub yet). To do this, go to Tools, then to Project Options... and in the Git/SVN tab, change Version Control System to Git (from None). Once you have accepted these changes, a new tab called Git appears in the upper right pane of RStudio (you may need to / be asked to restart RStudio at this point). You can now commit files and changes in files to Git.\n\nTo commit files, go to the Git tab and check the boxes next to the files you would like to commit (this is called staging; meaning that these files are now ready to be committed). Then, click on Commit and enter a message in the pop-up window that appears. Finally, click on the commit button under the message window.\nConnecting your Rproj with GitHub\nTo integrate your R project with GitHub, start by navigating to your GitHub page and create a new repository (repo). You can name it anything you like; for instance, let’s call it test. To create a new repository on GitHub, simply click on the New icon and then select New Repository. While creating the repository, I recommend checking the option to ‘Add a README’, where you can provide a brief description of the contents, although it’s not mandatory.\nOnce you’ve created the GitHub repo, the next step is to connect it to your local computer. This is achieved by ‘cloning’ the repository. Click on the green Code icon on your GitHub repository page, and from the dropdown menu that appears, copy the URL provided under the clone with HTTPS section.\nNow, open your terminal (located between Console and Jobs in RStudio) and navigate to the directory where you want to store your project files. Use the cd command to change directories if needed. Once you’re in the correct directory, include the URL you copied from the git repository after the command git remote add origin. This sets up the connection between your local directory and the GitHub repository.\nNext, execute the command git branch -M main to rename the default branch to main. This step is necessary to align with recent changes in GitHub’s naming conventions, merging the previous master and main branches.\nFinally, push your local files to the remote GitHub repository by using the command git push -u origin main. This command uploads your files to GitHub, making them accessible to collaborators and ensuring version control for your project.\nFollowing these steps ensures seamless integration between your R project and GitHub, facilitating collaboration, version control, and project management.\n\n# initiate the upstream tracking of the project on the GitHub repo\ngit remote add origin https://github.com/YourGitHUbUserName/YouGitHubRepositoryName.git\n# set main as main branch (rather than master)\ngit branch -M main\n# push content to main\ngit push -u origin main\n\nWe can then commit changes and push them to the remote GitHub repo.\nYou can then go to your GitHub repo and check if the documents that we pushed are now in the remote repo.\nFrom now on, you can simply commit all changes that you make to the GitHub repo associated with that Rproject. Other projects can, of course, be connected and push to other GitHub repos."
  },
  {
    "objectID": "tutorials/repro/repro.html#solving-path-issues-here",
    "href": "tutorials/repro/repro.html#solving-path-issues-here",
    "title": "Data Management and Reproducibility",
    "section": "Solving path issues: here",
    "text": "Solving path issues: here\nThe primary objective of the here package is to streamline file referencing within project-oriented workflows. Unlike the conventional approach of using setwd(), which is susceptible to fragility and heavily reliant on file organization, here leverages the top-level directory of a project to construct file paths effortlessly.\nThis approach significantly enhances the robustness of your projects, ensuring that file paths remain functional even if the project is relocated or accessed from a different computer. Moreover, the here package mitigates compatibility issues when transitioning between different operating systems, such as Mac and Windows, which traditionally require distinct path specifications.\n\n# define path\nexample_path_full &lt;- \"D:\\\\Uni\\\\Konferenzen\\\\ISLE\\\\ISLE_2021\\\\isle6_reprows/repro.Rmd\"\n# show path\nexample_path_full\n\n[1] \"D:\\\\Uni\\\\Konferenzen\\\\ISLE\\\\ISLE_2021\\\\isle6_reprows/repro.Rmd\"\n\n\nWith the here package, the path starts in folder where the Rproj file is. As the Rmd file is in the same folder, we only need to specify the Rmd file and the here package will add the rest.\n\n# load package\nlibrary(here)\n# define path using here\nexample_path_here &lt;- here::here(\"repro.Rmd\")\n#show path\nexample_path_here\n\n[1] \"/Users/laurenceanthony/Documents/projects/LADALQ/repro.Rmd\""
  },
  {
    "objectID": "tutorials/repro/repro.html#reproducible-randomness-set.seed",
    "href": "tutorials/repro/repro.html#reproducible-randomness-set.seed",
    "title": "Data Management and Reproducibility",
    "section": "Reproducible randomness: set.seed",
    "text": "Reproducible randomness: set.seed\nThe set.seed function in R sets the seed of R‘s random number generator, which is useful for creating simulations or random objects that can be reproduced. This means that when you call a function that uses some form of randomness (e.g. when using random forests), using the set.seed function allows you to replicate results.\nBelow is an example of what I mean. First, we generate a random sample from a vector.\n\nnumbers &lt;- 1:10\nrandomnumbers1 &lt;- sample(numbers, 10)\nrandomnumbers1\n\n [1]  7  3  1  5  4  6  2  8  9 10\n\n\nWe now draw another random sample using the same sample call.\n\nrandomnumbers2 &lt;- sample(numbers, 10)\nrandomnumbers2\n\n [1]  4  9  7  6  5  3  2  8 10  1\n\n\nAs you can see, we now have a different string of numbers although we used the same call. However, when we set the seed and then generate a string of numbers as show below, we create a reproducible random sample.\n\nset.seed(123)\nrandomnumbers3 &lt;- sample(numbers, 10)\nrandomnumbers3\n\n [1]  3 10  2  8  6  9  1  7  5  4\n\n\nTo show that we can reproduce this sample, we call the same seed and then generate another random sample which will be the same as the previous one because we have set the seed.\n\nset.seed(123)\nrandomnumbers4 &lt;- sample(numbers, 10)\nrandomnumbers4\n\n [1]  3 10  2  8  6  9  1  7  5  4"
  },
  {
    "objectID": "tutorials/repro/repro.html#tidy-data-principles",
    "href": "tutorials/repro/repro.html#tidy-data-principles",
    "title": "Data Management and Reproducibility",
    "section": "Tidy data principles",
    "text": "Tidy data principles\nThe same (underlying) data can be represented in multiple ways. The following three tables are show the same data but in different ways.\n\n\n\n\nTable 1.\n\n\ncountry\ncontinent\n2002\n2007\n\n\n\n\nAfghanistan\nAsia\n42.129\n43.828\n\n\nAustralia\nOceania\n80.370\n81.235\n\n\nChina\nAsia\n72.028\n72.961\n\n\nGermany\nEurope\n78.670\n79.406\n\n\nTanzania\nAfrica\n49.651\n52.517\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.\n\n\nyear\nAfghanistan (Asia)\nAustralia (Oceania)\nChina (Asia)\nGermany (Europe)\nTanzania (Africa)\n\n\n\n\n2002\n42.129\n80.370\n72.028\n78.670\n49.651\n\n\n2007\n43.828\n81.235\n72.961\n79.406\n52.517\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.\n\n\ncountry\nyear\ncontinent\nlifeExp\n\n\n\n\nAfghanistan\n2002\nAsia\n42.129\n\n\nAfghanistan\n2007\nAsia\n43.828\n\n\nAustralia\n2002\nOceania\n80.370\n\n\nAustralia\n2007\nOceania\n81.235\n\n\nChina\n2002\nAsia\n72.028\n\n\nChina\n2007\nAsia\n72.961\n\n\nGermany\n2002\nEurope\n78.670\n\n\nGermany\n2007\nEurope\n79.406\n\n\nTanzania\n2002\nAfrica\n49.651\n\n\nTanzania\n2007\nAfrica\n52.517\n\n\n\n\n\n\n\n\nTable 3 should be the easiest to parse and understand. This is so because only Table 3 is tidy. Unfortunately, however, most data that you will encounter will be untidy. There are two main reasons:\n\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\nData is often organised to facilitate some use other than analysis. For example, data is often organised to make entry as easy as possible.\n\nThis means that for most real analyses, you’ll need to do some tidying. The first step is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems:\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\nTo avoid structuring in ways that make it harder to parse, there are three interrelated principles which make a data set tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nAn additional advantage of tidy data is that is can be transformed more easily into any other format when needed."
  },
  {
    "objectID": "tutorials/repro/repro.html#how-to-minimize-storage-space",
    "href": "tutorials/repro/repro.html#how-to-minimize-storage-space",
    "title": "Data Management and Reproducibility",
    "section": "How to minimize storage space",
    "text": "How to minimize storage space\nMost people use or rely on data that comes in spreadsheets and use software such as Microsoft Excel or OpenOffice Calc. However, spreadsheets produced by these software applications take up a lot of storage space.\nOne way to minimize the space, that your data takes up, is to copy the data and paste it into a simple editor or txt-file. The good thing about txt files is that they take up only very little space and they can be viewed easily so that you can open the file to see what the data looks like. You could then delete the spread sheet because you can copy and paste the content of the txt file right back into a spread sheet when you need it.\nIf you work with R, you may also consider to save your data as .rda files which is a very efficient way to save and story data in an R environment\nBelow is an example for how you can load, process, and save your data as .rda in RStudio.\n\n# load data\nlmm &lt;- read.delim(\"tutorials/repro/data/lmmdata.txt\", header = TRUE)\n# convert strings to factors\nlmm &lt;- lmm %&gt;%\n  mutate(Genre = factor(Genre),\n         Text = factor(Text),\n         Region = factor(Region))\n# save data\nbase::saveRDS(lmm, file = here::here(\"tutorials/repro/data\", \"lmm_out.rda\"))\n# remove lmm object\nrm(lmm)\n# load .rda data\nlmm  &lt;- base::readRDS(file = here::here(\"tutorials/repro/data\", \"lmm_out.rda\"))\n# or from web\nlmm  &lt;- base::readRDS(\"tutorials/repro/data/lmm.rda\", \"rb\")\n# inspect data\nstr(lmm)\n\n'data.frame':   537 obs. of  5 variables:\n $ Date        : int  1736 1711 1808 1878 1743 1908 1906 1897 1785 1776 ...\n $ Genre       : Factor w/ 16 levels \"Bible\",\"Biography\",..: 13 4 10 4 4 4 3 9 9 3 ...\n $ Text        : Factor w/ 271 levels \"abott\",\"albin\",..: 2 6 12 16 17 20 20 24 26 27 ...\n $ Prepositions: num  166 140 131 151 146 ...\n $ Region      : Factor w/ 2 levels \"North\",\"South\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nTo compare, the lmmdata.txt requires 19.2KB while the lmmdata.rda only requires 5.2KB (and only 4.1KB with xz compression). If stored as an Excel spreadsheet, the same file requires 28.6KB."
  },
  {
    "objectID": "tutorials/repro/repro.html#headings",
    "href": "tutorials/repro/repro.html#headings",
    "title": "Data Management and Reproducibility",
    "section": "Headings",
    "text": "Headings\nCommand:\n# Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4\n##### Heading 5\n###### Heading 6\nRendered:"
  },
  {
    "objectID": "tutorials/repro/repro.html#heading-2",
    "href": "tutorials/repro/repro.html#heading-2",
    "title": "Data Management and Reproducibility",
    "section": "Heading 2",
    "text": "Heading 2\n\nHeading 3\n\nHeading 4\n\nHeading 5\n\nHeading 6"
  },
  {
    "objectID": "tutorials/repro/repro.html#headings-with-links",
    "href": "tutorials/repro/repro.html#headings-with-links",
    "title": "Data Management and Reproducibility",
    "section": "Headings with Links",
    "text": "Headings with Links\nCommand:\n### [Heading with a Link](https://www.example.com)\nRendered:\n\nHeading with a Link"
  },
  {
    "objectID": "tutorials/repro/repro.html#custom-ids-for-headings",
    "href": "tutorials/repro/repro.html#custom-ids-for-headings",
    "title": "Data Management and Reproducibility",
    "section": "Custom IDs for Headings",
    "text": "Custom IDs for Headings\nCommand:\n### Custom ID Heading {#custom-id}\nRendered:\n\nCustom ID Heading"
  },
  {
    "objectID": "tutorials/repro/repro.html#table-of-contents",
    "href": "tutorials/repro/repro.html#table-of-contents",
    "title": "Data Management and Reproducibility",
    "section": "Table of Contents",
    "text": "Table of Contents\nCommand:\n## Table of Contents\n- [Headings](#headings)\n- [Blockquotes](#blockquotes)\n- [Images](#images)\n- [Tables](#tables)\nRendered:"
  },
  {
    "objectID": "tutorials/repro/repro.html#table-of-contents-1",
    "href": "tutorials/repro/repro.html#table-of-contents-1",
    "title": "Data Management and Reproducibility",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nHeadings\nBlockquotes\nImages\nTables"
  },
  {
    "objectID": "tutorials/repro/repro.html#emphasis",
    "href": "tutorials/repro/repro.html#emphasis",
    "title": "Data Management and Reproducibility",
    "section": "Emphasis",
    "text": "Emphasis\nCommand:\n*Italic text*\n_Italic text_\n\n**Bold text**\n__Bold text__\n\n***Bold and italic***\n___Bold and italic___\nRendered:\nItalic text\nItalic text\nBold text\nBold text\nBold and italic\nBold and italic"
  },
  {
    "objectID": "tutorials/repro/repro.html#strikethrough",
    "href": "tutorials/repro/repro.html#strikethrough",
    "title": "Data Management and Reproducibility",
    "section": "Strikethrough",
    "text": "Strikethrough\nCommand:\nThis is a ~~strikethrough~~ example.\nRendered:\nThis is a strikethrough example."
  },
  {
    "objectID": "tutorials/repro/repro.html#superscript-and-subscript",
    "href": "tutorials/repro/repro.html#superscript-and-subscript",
    "title": "Data Management and Reproducibility",
    "section": "Superscript and Subscript",
    "text": "Superscript and Subscript\nCommand:\nH~2~O and E=mc^2^\nRendered:\nH2O and E=mc2"
  },
  {
    "objectID": "tutorials/repro/repro.html#highlight",
    "href": "tutorials/repro/repro.html#highlight",
    "title": "Data Management and Reproducibility",
    "section": "Highlight",
    "text": "Highlight\nCommand:\nI need to ==highlight== this text.\nRendered:\nI need to ==highlight== this text."
  },
  {
    "objectID": "tutorials/repro/repro.html#emojis",
    "href": "tutorials/repro/repro.html#emojis",
    "title": "Data Management and Reproducibility",
    "section": "Emojis",
    "text": "Emojis\nCommand:\nHere is an emoji: :smile:\nRendered:\nHere is an emoji: :smile:"
  },
  {
    "objectID": "tutorials/repro/repro.html#emoji-shortcodes",
    "href": "tutorials/repro/repro.html#emoji-shortcodes",
    "title": "Data Management and Reproducibility",
    "section": "Emoji Shortcodes",
    "text": "Emoji Shortcodes\nCommand:\n:smile: :+1: :sparkles:\nRendered:\n:smile: :+1: :sparkles:"
  },
  {
    "objectID": "tutorials/repro/repro.html#lists",
    "href": "tutorials/repro/repro.html#lists",
    "title": "Data Management and Reproducibility",
    "section": "Lists",
    "text": "Lists\n\nUnordered List\nCommand:\n- Item 1\n- Item 2\n  - Subitem 2.1\n  - Subitem 2.2\n- Item 3\nRendered:\n\nItem 1\nItem 2\n\nSubitem 2.1\nSubitem 2.2\n\nItem 3\n\n\n\nOrdered List\nCommand:\n1. First item\n2. Second item\n3. Third item\n   1. Subitem 3.1\n   2. Subitem 3.2\nRendered:\n\nFirst item\nSecond item\nThird item\n\nSubitem 3.1\nSubitem 3.2\n\n\n\n\nTask Lists\nCommand:\n- [x] Completed task\n- [ ] Incomplete task\n- [ ] Another incomplete task\nRendered:\n\nCompleted task\nIncomplete task\nAnother incomplete task\n\n\n\nAdvanced Task Lists\nCommand:\n- [ ] Task 1\n  - [x] Subtask 1\n  - [ ] Subtask 2\n- [x] Task 2\nRendered:\n\nTask 1\n\nSubtask 1\nSubtask 2\n\nTask 2\n\n\n\nDefinition Lists\nCommand:\nFirst Term\n: This is the definition of the first term.\n\nSecond Term\n: This is the definition of the second term.\nRendered:\n\nFirst Term\n\nThis is the definition of the first term.\n\nSecond Term\n\nThis is the definition of the second term.\n\n\n\n\nNested Lists\nCommand:\n1. First item\n   - Subitem 1\n     - Sub-subitem 1\n   - Subitem 2\n2. Second item\nRendered:\n\nFirst item\n\nSubitem 1\n\nSub-subitem 1\n\nSubitem 2\n\nSecond item"
  },
  {
    "objectID": "tutorials/repro/repro.html#links",
    "href": "tutorials/repro/repro.html#links",
    "title": "Data Management and Reproducibility",
    "section": "Links",
    "text": "Links\nCommand:\n[LADAL](/)\nRendered:\nLADAL"
  },
  {
    "objectID": "tutorials/repro/repro.html#images",
    "href": "tutorials/repro/repro.html#images",
    "title": "Data Management and Reproducibility",
    "section": "Images",
    "text": "Images\nCommand:\n![Alt text](https://via.placeholder.com/150)\nRendered:\n\n\n\nAlt text"
  },
  {
    "objectID": "tutorials/repro/repro.html#images-with-links",
    "href": "tutorials/repro/repro.html#images-with-links",
    "title": "Data Management and Reproducibility",
    "section": "Images with Links",
    "text": "Images with Links\nCommand:\n[![Alt text](https://via.placeholder.com/150)](https://www.example.com)\nRendered:\n\n\n\nAlt text"
  },
  {
    "objectID": "tutorials/repro/repro.html#blockquotes",
    "href": "tutorials/repro/repro.html#blockquotes",
    "title": "Data Management and Reproducibility",
    "section": "Blockquotes",
    "text": "Blockquotes\nCommand:\n&gt; This is a blockquote.\n&gt; \n&gt; This is a second paragraph within the blockquote.\nRendered:\n\nThis is a blockquote.\nThis is a second paragraph within the blockquote."
  },
  {
    "objectID": "tutorials/repro/repro.html#blockquotes-with-multiple-paragraphs",
    "href": "tutorials/repro/repro.html#blockquotes-with-multiple-paragraphs",
    "title": "Data Management and Reproducibility",
    "section": "Blockquotes with Multiple Paragraphs",
    "text": "Blockquotes with Multiple Paragraphs\nCommand:\n&gt; This is a blockquote with multiple paragraphs.\n&gt;\n&gt; This is the second paragraph in the blockquote.\nRendered:\n\nThis is a blockquote with multiple paragraphs.\nThis is the second paragraph in the blockquote."
  },
  {
    "objectID": "tutorials/repro/repro.html#code",
    "href": "tutorials/repro/repro.html#code",
    "title": "Data Management and Reproducibility",
    "section": "Code",
    "text": "Code\n\nInline Code\nCommand:\n`inline code`\nRendered:\ninline code\n\n\nCode Block\nCommand:\n```markdown\n{\n  \"firstName\": \"Martin\",\n  \"lastName\": \"Schweinberger\",\n  \"age\": 43\n}\n```\n\nRendered:\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Smith\",\n  \"age\": 25\n}"
  },
  {
    "objectID": "tutorials/repro/repro.html#blockquotes-with-nested-elements",
    "href": "tutorials/repro/repro.html#blockquotes-with-nested-elements",
    "title": "Data Management and Reproducibility",
    "section": "Blockquotes with Nested Elements",
    "text": "Blockquotes with Nested Elements\nCommand:\n&gt; ### This is a heading\n&gt; - This is a list item within a blockquote\n&gt; - Another item\n&gt;\n&gt; &gt; This is a nested blockquote\nRendered:\n\nThis is a heading\n\nThis is a list item within a blockquote\nAnother item\n\n\nThis is a nested blockquote"
  },
  {
    "objectID": "tutorials/repro/repro.html#inline-html",
    "href": "tutorials/repro/repro.html#inline-html",
    "title": "Data Management and Reproducibility",
    "section": "Inline HTML",
    "text": "Inline HTML\nCommand:\n&lt;p&gt;This is an inline HTML paragraph.&lt;/p&gt;\nRendered:\n\nThis is an inline HTML paragraph."
  },
  {
    "objectID": "tutorials/repro/repro.html#html-entities",
    "href": "tutorials/repro/repro.html#html-entities",
    "title": "Data Management and Reproducibility",
    "section": "HTML Entities",
    "text": "HTML Entities\nCommand:\nAT&T &copy; 2024\nRendered:\nAT&T © 2024"
  },
  {
    "objectID": "tutorials/repro/repro.html#expandable-sections-details-tag",
    "href": "tutorials/repro/repro.html#expandable-sections-details-tag",
    "title": "Data Management and Reproducibility",
    "section": "Expandable Sections (Details Tag)",
    "text": "Expandable Sections (Details Tag)\nCommand:\n&lt;details&gt;\n  &lt;summary&gt;Click to expand&lt;/summary&gt;\n  This is the detailed content that is hidden until expanded.\n&lt;/details&gt;\nRendered:\n\n\nClick to expand\n\nThis is the detailed content that is hidden until expanded."
  },
  {
    "objectID": "tutorials/repro/repro.html#horizontal-rule",
    "href": "tutorials/repro/repro.html#horizontal-rule",
    "title": "Data Management and Reproducibility",
    "section": "Horizontal Rule",
    "text": "Horizontal Rule\nCommand:\n\n## Tables {-}\n\n**Command:**\n\n```markdown\n| Header 1 | Header 2 |\n|----------|----------|\n| Cell 1   | Cell 2   |\n| Cell 3   | Cell 4   |\nRendered:\n\n\n\nHeader 1\nHeader 2\n\n\n\n\nCell 1\nCell 2\n\n\nCell 3\nCell 4"
  },
  {
    "objectID": "tutorials/repro/repro.html#advanced-tables",
    "href": "tutorials/repro/repro.html#advanced-tables",
    "title": "Data Management and Reproducibility",
    "section": "Advanced Tables",
    "text": "Advanced Tables\nCommand:\n| Header 1 | Header 2 | Header 3 |\n|----------|----------|----------|\n| Row 1 Col 1 | Row 1 Col 2 | Row 1 Col 3 |\n| Row 2 Col 1 | Row 2 Col 2 | Row 2 Col 3 |\nRendered:\n\n\n\nHeader 1\nHeader 2\nHeader 3\n\n\n\n\nRow 1 Col 1\nRow 1 Col 2\nRow 1 Col 3\n\n\nRow 2 Col 1\nRow 2 Col 2\nRow 2 Col 3"
  },
  {
    "objectID": "tutorials/repro/repro.html#footnotes",
    "href": "tutorials/repro/repro.html#footnotes",
    "title": "Data Management and Reproducibility",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the footnote.↩︎"
  },
  {
    "objectID": "tutorials/repro/repro.html#syntax-highlighting",
    "href": "tutorials/repro/repro.html#syntax-highlighting",
    "title": "Data Management and Reproducibility",
    "section": "Syntax Highlighting",
    "text": "Syntax Highlighting\nCommand:\n```python\ndef hello_world():\n    print(\"Hello, world!\")\n```\n\nRendered:\ndef hello_world():\n    print(\"Hello, world!\")"
  },
  {
    "objectID": "tutorials/repro/repro.html#math-expressions",
    "href": "tutorials/repro/repro.html#math-expressions",
    "title": "Data Management and Reproducibility",
    "section": "Math Expressions",
    "text": "Math Expressions\nCommand:\nInline math: $E = mc^2$\n\nBlock math:\n$$\n\\frac{a}{b} = c\n$$\nRendered:\nInline math: ( E = mc^2 )\nBlock math:\n\\[\n\\frac{a}{b} = c\n\\]"
  },
  {
    "objectID": "tutorials/repro/repro.html#escaping-characters",
    "href": "tutorials/repro/repro.html#escaping-characters",
    "title": "Data Management and Reproducibility",
    "section": "Escaping Characters",
    "text": "Escaping Characters\nCommand:\nUse the backslash to escape special characters: \\*literal asterisks\\*\nRendered:\nUse the backslash to escape special characters: *literal asterisks*"
  },
  {
    "objectID": "tutorials/repro/repro.html#mermaid-diagrams",
    "href": "tutorials/repro/repro.html#mermaid-diagrams",
    "title": "Data Management and Reproducibility",
    "section": "Mermaid Diagrams",
    "text": "Mermaid Diagrams\nCommand:\n```mermaid\ngraph TD;\n    A--&gt;B;\n    A--&gt;C;\n    B--&gt;D;\n    C--&gt;D;\n\n**Rendered:**\n\n```mermaid\ngraph TD;\n    A--&gt;B;\n    A--&gt;C;\n    B--&gt;D;\n    C--&gt;D;\n\nThese additional advanced Markdown features allow you to create even more complex and sophisticated documents. Practice using these commands to further enhance your Markdown proficiency!"
  },
  {
    "objectID": "tutorials/repro/repro.html#preparation-of-the-rmd",
    "href": "tutorials/repro/repro.html#preparation-of-the-rmd",
    "title": "Data Management and Reproducibility",
    "section": "Preparation of the Rmd",
    "text": "Preparation of the Rmd\n\nBegin by duplicating the Rmd file. A simple convention is to append “_cb” to the filename, indicating that this version will be converted into an interactive notebook.\nUnlike R Notebooks, interactive Jupyter notebooks do not utilize a YAML metadata header. Consequently, remove the YAML metadata header from the duplicated Rmd file and replace it with a first-level header to denote the notebook’s title. It’s advisable to adjust all headers in the Rmd file to maintain the original structure.\nCreate a separate script file to list all required packages for installation. Remove any code chunks responsible for installing packages, as Jupyter notebooks have limited options for hiding code chunks."
  },
  {
    "objectID": "tutorials/repro/repro.html#converting-your-rmd-into-a-jupyter-notebook",
    "href": "tutorials/repro/repro.html#converting-your-rmd-into-a-jupyter-notebook",
    "title": "Data Management and Reproducibility",
    "section": "Converting your Rmd into a Jupyter notebook",
    "text": "Converting your Rmd into a Jupyter notebook\n\nOpen the “rmd2jupyter.Rmd” file and proceed to install and activate the “rmd2jupyter” package, which contains the necessary function for Rmd to Jupyter conversion. Since this package isn’t available via CRAN, utilize the “devtools” package for installation.\n\n\nlibrary(devtools)\ndevtools::install_github(\"mkearney/rmd2jupyter\")\nlibrary(rmd2jupyter)\n\n\nOnce the “rmd2jupyter” package is activated, use the “rmd2jupyter” function to convert the Rmd file to a Jupyter notebook.\n\n\n# Convert one notebook\nrmd2jupyter::rmd2jupyter(here::here(\"acvqainter_cb.Rmd\"))"
  },
  {
    "objectID": "tutorials/repro/repro.html#creating-a-github-repository-that-connects-to-binder",
    "href": "tutorials/repro/repro.html#creating-a-github-repository-that-connects-to-binder",
    "title": "Data Management and Reproducibility",
    "section": "Creating a GitHub repository that connects to Binder",
    "text": "Creating a GitHub repository that connects to Binder\n\nLog in to your GitHub account and visit: https://github.com/Australian-Text-Analytics-Platform/r-binder-template.\nOn the GitHub page, click on Use this template and select Create a new repository from the dropdown menu. Assign a name to the repository and optionally provide a brief description.\nOpen the “install.R” file and specify the packages to be pre-installed by adding commands like install.packages(“dplyr”) on separate lines.\nUpload your Jupyter notebook by selecting Upload file from the drop-down menu. Additionally, if you need to create folders, create a dummy file first and specify its location as a subfolder.\n\nOnce all required packages, the Jupyter notebook, and any necessary data files are uploaded, initiate the notebook by following the provided link. Adapt the URL to point to your interactive notebook by adjusting the username, repository name, and notebook filename.\nAlternatively, consider uploading your Jupyter notebook to Google Colab for sharing. However, note that Google Colab may terminate sessions if computations take too long or exceed resource limits.\nPlease anticipate an extended setup time for the interactive notebook’s first launch, as Docker image creation from the GitHub repository may take up to an hour for complex notebooks with numerous dependencies. Simpler notebooks typically start faster but may still require at least 10 minutes for the initial setup."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#mean",
    "href": "tutorials/dstats/dstats.html#mean",
    "title": "Descriptive Statistics with R",
    "section": "Mean",
    "text": "Mean\nThe mean is used when the data is numeric and normally distributed. The mean is calculated by applying the formula shown below.\n\\[\\begin{equation}\n  \\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i = \\frac{x_{1}+x_{2}+ \\dots + x_{n}}{n}\n\\end{equation}\\]\nTo calculate the mean, sum up all values and divide by the number of values. See the example below for clarification.\n\n\n\n\n\n\n\n\n\nConsider, we are interested in the mean length of sentences in a short text, then the first thing we could do would be to list the sentences and their length in a table.\n\n\nSentencesWordsCall me Ishmael3Some years ago -- never mind how long precisely -- having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.40It is a way I have of driving off the spleen, and regulating the circulation.15Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people's hats off--then, I account it high time to get to sea as soon as I can.87\n\n\nTo calculate the mean, we need to divide the sum of the number of words per sentence (145) by the number of sentences (7) (see the equation below).\n\\[\\begin{equation}\n\\frac{3+40+15+87}{4} = \\frac{145}{4} = 36.25\n\\label{eq:mittel2}\n\\end{equation}\\]\nThe mean sentences length in our example is 36.25 words\nIn R, the mean is calculated as follows.\n\n# create numeric vector\nfrequencies &lt;- c(3, 40, 15, 87)\n# calculate mean\nmean(frequencies)\n\n[1] 36.25\n\n\nThe mean is the most common way to summarize numeric variables and it is very easy and intuitive to understand. A disadvantage of the mean is that it is very strongly affected by outliers which is why the median is the preferable measure of centrality when dealing with data that is not normal or that contains outliers.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCalculate the arithmetic mean: 1, 2, 3, 4, 5, 6\n\n\n\nAnswer\n\n\n(1 + 2 + 3 + 4 + 5 + 6) / 6\n\n[1] 3.5\n\n\n\n\nCalculate the arithmetic mean for the following values using the mean function: 4, 3, 6, 2, 1, 5, 6, 8\n\n\n\nAnswer\n\n\nmean(c(4, 3, 6, 2, 1, 5, 6, 8))\n\n[1] 4.375\n\n\n\n\nCreate a vector out of the following values and calculate the arithmetic mean using the mean function: 1, 5, 5, 9\n\n\n\nAnswer\n\n\nvec &lt;- c(1, 5, 5, 9)\nmean(vec)\n\n[1] 5"
  },
  {
    "objectID": "tutorials/dstats/dstats.html#median",
    "href": "tutorials/dstats/dstats.html#median",
    "title": "Descriptive Statistics with R",
    "section": "Median",
    "text": "Median\nThe median can be used for both numeric and ordinal variables. In contract to the mean, it is more robust and not as easily affected by outliers. While the mean is commonly associated with numeric data that is normally distributed, the median is typically used when dealing with non-normal numeric or ordinal variables, i.e. variables that are ordered but not truly numeric. The median is the central value in a de- or increasing ordering of values in a vector. In other words, 50 percent of values are above and 50 percent of values are below the median in a given vector.\nIf the vector contains an even number of elements, then the two central values are summed up and divided by 2. If the vector contains an uneven number of elements, the median represents the central value.\n\\[\\begin{equation}\nmedian_{x}=\n\\begin{cases}\nx_{\\frac{n+1}{2}} & n\\text{ uneven} \\\\\n\\frac{1}{2}\\bigl(x_{\\frac{n}{2}}+x_{\\frac{n+1}{2}}\\bigr) & n\\text{ even}\n\\end{cases}\n\\label{eq:median}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\n\nLet’s have a look at an example. Consider you are interested in the age stratification of speakers in the private dialogue section of the Irish component of the International Corpus of English (ICE). When tabulating and plotting the age variable you get the following table and graph.\n\n\nAgeCounts0-18919-2516026-337034-411542-49950+57\n\n\n\n\n\n\n\n\n\n\n\nThe age groups represent an order factor which means that there are categories with a natural order (here from old to young or vice versa). If we order speakers according to their age from young to old, we get a vector of length 320. If we then take the central value, i.e. the value of the 160th speaker, we get the median age in the private dialogue section of the Irish component of the International Corpus of English (ICE).\nIn R, the median is calculated as shown below.\n\n# create a vector consisting out of ranks\nranks &lt;- c(rep(1, 9), rep(2, 160), rep(3, 70), rep(4, 15), rep(5, 9), rep(6, 57))\n# calculate median\nmedian(ranks)\n\n[1] 2\n\n\nIn our case, the median age is 19-25 because the 160th speaker belongs to the 2nd age group, i.e. the age group with speakers between 19 and 25 years old.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCalculate the median: 1, 2, 3, 4, 5, 6\n\n\n\nAnswer\n\n\n(3 + 4) / 2\n\n[1] 3.5\n\n\n\n\nCalculate the median for the following values using the median function: 4, 3, 6, 2, 1, 5, 6, 8\n\n\n\nAnswer\n\n\nmedian(c(4, 3, 6, 2, 1, 5, 6, 8))\n\n[1] 4.5\n\n\n\n\nCreate a vector out of the following values and calculate the median using the median function: 1, 5, 5, 9\n\n\n\nAnswer\n\n\nvec &lt;- c(1, 5, 5, 9)\nmedian(vec)\n\n[1] 5"
  },
  {
    "objectID": "tutorials/dstats/dstats.html#mode",
    "href": "tutorials/dstats/dstats.html#mode",
    "title": "Descriptive Statistics with R",
    "section": "Mode",
    "text": "Mode\nThe mode is typically used when dealing with categorical variables and it reports which level of a factor or a categorical variable is the most frequent.\n\n\n\n\n\n\n\n\n\nHere is an example to illustrate the mode. Consider you are interested where most speakers in the private dialogue section of the Irish component of the International Corpus of English are currently residing and you get the following distribution.\n\n\nCurrentResidenceSpeakersBelfast98Down20Dublin (city)110Limerick13Tipperary19\n\n\n\n\n\n\n\n\n\n\n\nThe tabulated and visualized data show that the mode is Dublin (City), because the largest group (110 speakers) of speakers in the corpus are speakers from the city of Dublin. This means that the average speaker in in the private dialogue section of the Irish component of the International Corpus of English (ICE) is from Dublin city.\nIn R the mode is calculated as shown below:\n\n# create a factor with the current residence of speakers\nCurrentResidence &lt;- c(\n  rep(\"Belfast\", 98), # repeat \"Belfast\" 98 times\n  rep(\"Down\", 20), # repeat \"Down\" 20 times\n  rep(\"Dublin (city)\", 110), # repeat \"Dublin (city)\" 110 times\n  rep(\"Limerick\", 13), # repeat \"Limerick\" 13 times\n  rep(\"Tipperary\", 19)\n) # repeat \"Tipperary\" 19 times\n# calculate mode\nnames(which.max(table(CurrentResidence))) # extract which level occurs most frequently\n\n[1] \"Dublin (city)\"\n\n\nA word of warning is in order here as only the first(!) maximal value is provided by R even if several categories have the same frequency."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#geometric-mean",
    "href": "tutorials/dstats/dstats.html#geometric-mean",
    "title": "Descriptive Statistics with R",
    "section": "Geometric mean",
    "text": "Geometric mean\nThe geometric mean represents a measure of central tendency that is used when dealing with dynamic processes where the later elements are dependent on the previous elements. The geometric mean is calculated according to the equation below.\n\\[\\begin{equation}\n\\bar{x}_{geometric} = \\sqrt[n]{x_1 \\times x_{i+1} \\times \\dots \\times x_n}\n\\end{equation}\\]\nImagine you have the option to buy two different stock packages and you have to buy one of them. Which one would you buy?\n\n\nYearPackage1Package2Year 1+5%+20%Year 2-5%-20%Year 3+5%+20%Year 4-5%-20%\n\n\nIs one package better than the other? Did one package perform better than the other?\n\nPackage 1:\n\nReturn: \\(1.05 \\times .95 \\times 1.05 \\times .95  = .995\\) (0.5% loss)\nYear-over-year average: \\(.995^{1/4}\\) = ~0.125% loss per year\n\nPackage 2:\n\nReturn: \\(1.2 \\times .8 \\times 1.2 \\times .8 = 0.9216\\) (7.84% loss)\nYear-over-year average: \\(.9216^{1/4}\\) = ~2% loss per year.\n\n\nPackage 2 performs substantially worse because here, the changes in growth depend on the previous growth rates."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#harmonic-mean",
    "href": "tutorials/dstats/dstats.html#harmonic-mean",
    "title": "Descriptive Statistics with R",
    "section": "Harmonic mean",
    "text": "Harmonic mean\nThe harmonic mean is a measure of central tendency that provides us with the average rate and is used when dealing with dynamic processes that involve velocities and distances. The harmonic mean is calculated according to the equation below.\n\\[\\begin{equation}\n\\bar{x}_{harmonic} =\n\\frac{n}{\\frac{1}{x_i} + \\frac{1}{x_{i+1}} + \\frac{1}{x_{i+\\dots}} + \\frac{1}{x_n}}\n\\end{equation}\\]\nThe harmonic mean is used when two rates contribute to the same workload (for instance when we download a file). Each installment is in a relay race and contributes the same amount to the issue. For example, we make a round trip to work and back. The way to work is 60 kilometers. On the way to work, we can only travel at 30 kph while we can go 60 kph on the way back. The distance is the same. Half of the results (distance traveled) comes from the first rate (30 kilometers per hour) and the other half from the second rate (60 kilometers per hour). The result is that is takes us 3 hours to get to work and back.\n\\[\\begin{equation}\n\\bar{x}_{harmonic} =\n\\frac{2}{\\frac{1}{30} + \\frac{1}{60}} = \\frac{2}{\\frac{2}{60} + \\frac{1}{60}} = \\frac{2}{\\frac{3}{60}} = \\frac{2}{1} \\times \\frac{60}{3} = \\frac{120}{3} = 40\n\\end{equation}\\]\nThe reason why using the arithmetic mean is inappropriate in such cases is the following: The idea behind the arithmetic mean is that we calculate a single value that can replace all values in a given distribution and the sum of the mean values is identical to the sum of the observed values. So, the average is a single element that replaces each element. In our example, we have to drive at 40 kilometers per hour (instead of 30) to work and 40 kilometers per hour (instead of 60) to get back from work in the same amount of time. If we went with 45 kilometers per hour, then the result would not be 3 hours but 2 hours and 40 minutes so that the result would not be the same."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#notes-on-measures-of-centrality",
    "href": "tutorials/dstats/dstats.html#notes-on-measures-of-centrality",
    "title": "Descriptive Statistics with R",
    "section": "Notes on Measures of Centrality",
    "text": "Notes on Measures of Centrality\nAs suggested above, the mean is strongly affected by outliers (which is why in sch cases, the median is the more appropriate measure fo central tendency). To illustrate this, imagine you are interested whether the use of discourse particles differs across two corpora. The two corpora represent the speech of the same five speakers but in different situations and the speech thus represents different registers. In a first step, you calculate the relative frequency of discourse particle use and both corpora have a mean of 13.4 particles per 1,000 words. Given the mean, the two corpora do not seem to differ. However, when tabulating and plotting the use of particles by speaker and across these two corpora, it becomes immediately clear that the mean is not the appropriate measure of central tendency as the distributions are very dissimilar.\n\n\nCorpusSpeakerFrequencyC1A11.4C1B5.2C1C27.1C1D9.6C1E13.7C2A0.2C2B0.0C2C1.1C2D65.3C2E0.4\n\n\n\n\n\n\n\n\n\n\n\nThe Figure above shows that the use of discourse particles is distributed rather evenly across speakers in Corpus 1 while the distribution is very uneven in corpus 2. In corpus 2, 4 out of 5 speakers use almost no discourse particles and only one speaker, speaker D, makes excessive use of discourse particles in corpus 2. The high usage frequency of discourse particles by speaker D in corpus 2 causes the mean of corpus 2 to be identical to the mean reported for corpus 1 although the distribution of usage rates differs drastically. This means that reporting the median in addition to the mean can be useful if the distribution of values is very uneven (or non-normal or skewed).\nTo exemplify, we will summarize the distribution of discourse particles in the two corpora: the use of discourse particles in corpus 1 (mean = 13.4, median = 11.4) is substantially different from the use of discourse particles in corpus 2 (mean = 13.4, median = 0.4)."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#range",
    "href": "tutorials/dstats/dstats.html#range",
    "title": "Descriptive Statistics with R",
    "section": "Range",
    "text": "Range\nThe range is the simplest measure of variability and reports the lowest and highest value of a distribution. That is, the range provides minimum and maximum of a vector to show the span of values within a distribution.\nIn R, the range is extracted as shown below.\n\n# create a numeric vector\nMoscow &lt;- c(-5, -12, 5, 12, 15, 18, 22, 23, 20, 16, 8, 1)\nmin(Moscow)\n\n[1] -12\n\nmax(Moscow) # extract range\n\n[1] 23\n\n\nThe lowest temperature value for Moscow is -12 degrees Celsius and the highest value is 23 degrees Celsius. The range thus spans from -12 to 23."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#interquartile-range-iqr",
    "href": "tutorials/dstats/dstats.html#interquartile-range-iqr",
    "title": "Descriptive Statistics with R",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\nThe interquartile range (IQR) denotes the range that encompasses the central 50 percent of data points and thus informs about how values are distributed. This means that the IQR spans from the first quartile that encompasses 25 percent of the data to the third quartile that encompasses 75 percent of the data.\nThe easiest way to extract the IQR in R is to apply the summary function to a vector as shown below and then subtract the value of the 1st quartile from the value of the 3rd quartile.\n\nsummary(Moscow) # extract IQR\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -12.00    4.00   13.50   10.25   18.50   23.00 \n\n\nThe summary function reports that the minimum temperature is -12 degrees Celsius and that the maximum temperature is 23 degrees Celsius. Also, the lower 25 percent of the data fall within -12 and 4 degrees Celsius (from the minimum value to the 1st quartile) and the upper 25 percent fall within 18.5 and 23 degrees Celsius (from the 3rd quartile to the maximum value). The IQR range represents a range that encompasses the central 50% of the data and thus represents the value that can be calculated by subtracting the value of the 1st from the value of the 3rd quartile..\nThus, the IQR is 18.5 - 4 = 14.5\n."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#variance",
    "href": "tutorials/dstats/dstats.html#variance",
    "title": "Descriptive Statistics with R",
    "section": "Variance",
    "text": "Variance\nThe variance is a measure of the spread of a set of data around its mean. It is a key concept in statistics and is used to quantify the dispersion of a set of observations. The variance is defined as the average of the squared differences between each observation and the mean of the data set.\nThe variance is calculated according to the formula below. To calculate the variance, each value is subtracted from the mean and the result is squared. The squared values are then added and the resulting sum is divided by the number of values minus 1.\n\\(s = \\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^{2}\\)\nFor our example, the variance of temperatures for Moscow is 123.6591 and 9.477273 for Hamburg.\nIn R, the variance is calculated as shown below.\n\nsd(Moscow)^2\n\n[1] 123.659090909"
  },
  {
    "objectID": "tutorials/dstats/dstats.html#standard-deviation",
    "href": "tutorials/dstats/dstats.html#standard-deviation",
    "title": "Descriptive Statistics with R",
    "section": "Standard deviation",
    "text": "Standard deviation\nThe standard deviation (abbreviated with capital \\(sigma\\) \\(\\sigma\\)) is calculated according to first equation shown below or, alternatively, according to second equation shown below and it is the square root of the squared variance.\n\\(\\sigma = \\sqrt{s} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\)\n\\(\\sigma = \\sqrt{\\frac{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\\)\nFor our example, the first equation shown above provides a standard deviation of 11.12 for Moscow and a standard deviation of 3.08 for Hamburg.\nIn R, the standard deviation is calculated as shown below.\n\n# calculate standard deviation\nsd(Moscow)\n\n[1] 11.1202109202\n\n\nThe standard deviation of temperature values of Moscow is 11.12.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCalculate the mean, median, and mode as well as the standard deviation for the following two vectorsA: 1, 3, 6, 2, 1, 1, 6, 8, 4, 2, 3, 5, 0, 0, 2, 1, 2, 1, 0B: 3, 2, 5, 1, 1, 4, 0, 0, 2, 3, 0, 3, 0, 5, 4, 5, 3, 3, 4\n\n\n\nAnswer\n\n\nA &lt;- c(1, 3, 6, 2, 1, 1, 6, 8, 4, 2, 3, 5, 0, 0, 2, 1, 2, 1, 0)\nB &lt;- c(3, 2, 5, 1, 1, 4, 0, 0, 2, 3, 0, 3, 0, 5, 4, 5, 3, 3, 4)\nmean(A)\nmedian(A)\nmax(A)\nsd(A)\nmean(B)\nmedian(B)\nmax(B)\nsd(B)\n\n\n\nFind a partner and discuss which measure of central tendency is appropriate when dealing with grades. Then, find another partner and see whether they have come to the same conclusion or discuss why if not. Finally, discuss the advantages and disadvantages of calculating the mean when dealing with grades.\n\n\n\nAnswer\n\nThe problem is that - strictly speaking - grades are ordinal and not numeric (i.e., interval- or ratio-scaled). This means that calculating the arithmetic mean is somewhat controversial. To resolve this issue, it is recommendable to either calculate the median (rather than the mean) or to be transparent about this issue and inform readers that the mean was used despite dealing with an ordinal variable.\n\n\nWhere are mean, median, and mode when dealing with normal data (i.e., when the data approximate a normal distribution)?\n\n\n\nAnswer\n\nWhen dealing with normally distributed data, the arithmetic mean, median, and mode would ideally be identical (which is extremely rare when working with empirical data) or, at least, very similar.\n\n\nGo and find a partner and discuss what it means - on a conceptual level rather than on a statistical/mathematical level - that two groups have different ranges for a certain feature (be careful, this is not as trivial as it may seem!)."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#standard-error",
    "href": "tutorials/dstats/dstats.html#standard-error",
    "title": "Descriptive Statistics with R",
    "section": "Standard Error",
    "text": "Standard Error\nThe standard error is a measure of variability and it reports the average distance from some parameters (most often from the mean). It is calculated as the standard deviation of the residuals of the parameter in question. To exemplify the standard error, we will have a look at reaction times which show how fast participants realized that a sequence of letters were either existing words or just a sequence of letters.\n\n\nRTStateGender429.276SoberMale435.473SoberMale394.535SoberMale377.325SoberMale430.294SoberMale289.102SoberFemale411.505SoberFemale366.191SoberFemale365.792SoberFemale334.034SoberFemale444.188DrunkMale540.866DrunkMale468.531DrunkMale476.011DrunkMale412.473DrunkMale520.845DrunkFemale435.682DrunkFemale463.421DrunkFemale536.036DrunkFemale494.936DrunkFemale\n\n\nThe standard error of the mean is calculated using the equation below.\n\\[\\begin{equation}\n\\sigma~{\\bar{x}}~ =\\frac{\\sigma}{\\sqrt{n}}\n\\end{equation}\\]\nThe standard error can be calculated manually (see below) by implementing the equation from above.\n\nsd(rts$RT, na.rm = TRUE) /\n  sqrt(length(rts$RT[!is.na(rts$RT)]))\n\n[1] 14.7692485022\n\n\nAn easier way to extract standard errors is to use the describe function from the psych package (see below)\n\n# describe data\npsych::describe(rts$RT, type = 2)\n\n   vars  n   mean    sd median trimmed  mad   min    max  range skew kurtosis\nX1    1 20 431.33 66.05 432.88   432.9 60.4 289.1 540.87 251.76 -0.2    -0.13\n      se\nX1 14.77"
  },
  {
    "objectID": "tutorials/dstats/dstats.html#confidence-intervals-for-simple-vectors",
    "href": "tutorials/dstats/dstats.html#confidence-intervals-for-simple-vectors",
    "title": "Descriptive Statistics with R",
    "section": "Confidence Intervals for Simple Vectors",
    "text": "Confidence Intervals for Simple Vectors\nConfidence intervals (CIs) give a range that’s likely to include a population value with a certain degree of confidence. As such, CIs tell us how likely it is to get a value within a certain range if we drew another sample from the same population.\nOne easy method for extracting confidence intervals is to apply the CI function from the Rmisc package.\n\n# extract mean and confidence intervals\nRmisc::CI(rts$RT, ci = 0.95)\n\n        upper          mean         lower \n462.238192381 431.325800000 400.413407619 \n\n\nThe ´CI´ function provides the mean reaction time (431.3258) and the 95 percent confidence band. With 95 percent confidence, the mean reaction time will have a mean between 400.41 and 462.24 milliseconds (ms).\nAnother way to extract the mean and its confidence intervals is by using t.test function.\n\n# extract mean and confidence intervals\nstats::t.test(rts$RT, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  rts$RT\nt = 29.20431598, df = 19, p-value &lt; 0.0000000000000002220446\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 400.413407619 462.238192381\nsample estimates:\nmean of x \n 431.3258 \n\n\nAnother alternative to extract the man ans the confidence interval from a range of values is to use the MeanCI function from the DescTools package.\n\n# extract mean and confidence intervals\nDescTools::MeanCI(rts$RT, conf.level = 0.95)\n\n         mean        lwr.ci        upr.ci \n431.325800000 400.413407619 462.238192381 \n\n\nThis method is particularly interesting because it uses bootstrapping or resampling the data. As such, it is an empirical method to extract the mean and the confidence intervals. The values will differ given how many samples are drawn and we can get very precise estimates using this method.\n\n# extract mean CIs\nDescTools::MeanCI(rts$RT, method = \"boot\", type = \"norm\", R = 1000)\n\n         mean        lwr.ci        upr.ci \n431.325800000 402.065920019 459.599597681 \n\n\nBecause this is a data-driven approach, the results will vary, depending on the characteristics of the resampled data. To illustrate, compare the values provided above to the values generated below.\n\n# extract mean CIs\nDescTools::MeanCI(rts$RT, method = \"boot\", type = \"norm\", R = 1000)\n\n         mean        lwr.ci        upr.ci \n431.325800000 403.272504877 458.846973023 \n\n\nAnother method for extracting the mean and the confidence intervals from a range of values using bootstrapping is to use the boot function from the boot package.\n\n# function to extract values\nBootFunction &lt;- function(x, index) {\n  return(c(\n    mean(x[index]),\n    var(x[index]) / length(index)\n  ))\n}\n# apply function to data\nBootstrapped &lt;- boot(\n  data = rts$RT,\n  statistic = BootFunction,\n  R = 1000\n)\n# extract values\nmean(Bootstrapped$t[, 1])\n\n[1] 431.4550746\n\n# alternative to extract values\nboot.ci(Bootstrapped, conf = 0.95)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = Bootstrapped, conf = 0.95)\n\nIntervals : \nLevel      Normal              Basic             Studentized     \n95%   (402.9, 459.5 )   (402.8, 460.4 )   (397.6, 463.3 )  \n\nLevel     Percentile            BCa          \n95%   (402.2, 459.9 )   (400.1, 457.6 )  \nCalculations and Intervals on Original Scale\n\n\nThe advantage of using bootstrapping methods lies in the fact that the data is (frequently) not distributed normally which is not an issue for the bootstrapping and it will thus provide more reliable results as it does not rely on distributional assumptions about the data."
  },
  {
    "objectID": "tutorials/dstats/dstats.html#confidence-intervals-for-grouped-data",
    "href": "tutorials/dstats/dstats.html#confidence-intervals-for-grouped-data",
    "title": "Descriptive Statistics with R",
    "section": "Confidence Intervals for Grouped Data",
    "text": "Confidence Intervals for Grouped Data\nTo extract the confidence intervals for grouped data, we can sue the summarySE function from the Rmisc package.\n\n# apply summarySE function to data\nRmisc::summarySE(\n  data = rts,\n  # define variable representing frequencies\n  measurevar = \"RT\",\n  # define grouping variable\n  groupvars = \"Gender\",\n  # extract standard deviation, standard error, and confidence intervals\n  conf.interval = 0.95\n)\n\n  Gender  N       RT            sd            se            ci\n1 Female 10 421.7544 82.8522922089 26.2001952746 59.2689594071\n2   Male 10 440.8972 46.2804393809 14.6351599557 33.1070319225"
  },
  {
    "objectID": "tutorials/dstats/dstats.html#confidence-intervals-for-nominal-data",
    "href": "tutorials/dstats/dstats.html#confidence-intervals-for-nominal-data",
    "title": "Descriptive Statistics with R",
    "section": "Confidence Intervals for Nominal Data",
    "text": "Confidence Intervals for Nominal Data\nWe now turn to confidence intervals for nominal data (see also Thomas and Grunkemeier). When dealing with nominal data, confidence intervals can be determined with the binom.test function in the in-built stats package. Alternative methods are available via the BinomCI and MultinomCI functions from the DescTools package. More advanced techniques for confidence intervals on nominal data are available via the PropCIs package.\n\nstats::binom.test(2, 20, 0.5, # binom.test(x, n, p = 0.5, ...)\n  alternative = \"two.sided\", # define sidedness\n  conf.level = 0.95\n) # define confidence level\n\n\n    Exact binomial test\n\ndata:  2 and 20\nnumber of successes = 2, number of trials = 20, p-value =\n0.000402450562\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.0123485271703 0.3169827140191\nsample estimates:\nprobability of success \n                   0.1 \n\n\nAnother way to use the BinomCI function is shown below.\n\n# extract CIs\nBinomCI(2, 20, # apply BinomCI function\n  conf.level = 0.95, # define ci\n  method = \"modified wilson\"\n) # define method for ci extraction\n\n     est          lwr.ci         upr.ci\n[1,] 0.1 0.0177680755349 0.301033645228"
  },
  {
    "objectID": "tutorials/dstats/dstats.html#confidence-intervals-for-multinomial-data",
    "href": "tutorials/dstats/dstats.html#confidence-intervals-for-multinomial-data",
    "title": "Descriptive Statistics with R",
    "section": "Confidence Intervals for Multinomial Data",
    "text": "Confidence Intervals for Multinomial Data\nWe use the MultinomCI function to extract the confidence intervals form multinominal data.\n\nobserved &lt;- c(35, 74, 22, 69) # define multinominal vector\nMultinomCI(observed, # apply MultinomCI function\n  conf.level = 0.95, # define ci\n  method = \"goodman\"\n) # define method for ci extraction\n\n       est          lwr.ci         upr.ci\n[1,] 0.175 0.1180188110631 0.251643112255\n[2,] 0.370 0.2898697185019 0.457995050825\n[3,] 0.110 0.0661144727952 0.177479835187\n[4,] 0.345 0.2668784298282 0.432498795139"
  },
  {
    "objectID": "tutorials/dstats/dstats.html#footnotes",
    "href": "tutorials/dstats/dstats.html#footnotes",
    "title": "Descriptive Statistics with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/pwr/pwr.html#what-determines-if-you-find-an-effect",
    "href": "tutorials/pwr/pwr.html#what-determines-if-you-find-an-effect",
    "title": "Power Analysis in R",
    "section": "What determines if you find an effect?",
    "text": "What determines if you find an effect?\nTo explore this issue, let us have a look at some distributions of samples with varying features sampled from either one or two distributions.\nLet us start with the distribution of two samples (N = 30) sampled from the same population.\n\n\n\n\n\n\n\n\n\nThe means of the samples are very similar and a t-test confirms that we can assume that the samples are drawn from the same population (as the p-value is greater than .05).\nLet us now draw another two samples (N = 30) but from different populations where the effect of group is weak (the population difference is small).\n\n\n\n\n\n\n\n\n\nLet us briefly check if the effect size, Cohen’s d of 0.2, is correct (for this, we increase the sample size dramatically to get very accurate estimates). If the above effect size is correct (Cohen’s d = 0.2), then the reported effect size should be 0.2 (or -0.2). This is so because Cohen’s d represents distances between group means measured in standard deviations (in our example, the standard deviation is 10 and the difference between the means is 2, i.e., 20 percent of a standard deviation; which is equal to a Cohen’s d value of 0.2). Let’s check the effect size now (we will only do this for this distribution but you can easily check for yourself that the effect sizes provided in the plot headers below are correct by adapting the code chunk below and using the numbers provided in the plot headers).\n\n# generate two vectors with numbers using the means and sd from above\n# means = 101, 99, sd = 10\nnum1 &lt;- rnorm(1000000, mean = 101, sd = 10)\nnum2 &lt;- rnorm(1000000, mean = 99, sd = 10)\n# perform t-test\ntt &lt;- t.test(num1, num2, alternative = \"two.sided\")\n# extract effect size\neffectsize::effectsize(tt)\n\nCohen's d | 95% CI\n------------------\n0.20      |       \n\n- Estimated using un-pooled SD.\n\n\nBased on the t-test results in the distribution plot, we assume that the data represents two samples from the same population (which is false) because the p-value is higher than .05. This means that the sample size is insufficient or does not enough power to show that they are actually from two different populations given the variability in the data nd the size of the effect (which is weak).\nWhat happens if we increase the effect size to medium? This means that we again draw two samples from two different populations but the difference between the populations is a bit larger (group has a medium effect, Cohen’s d = .5)\n\n\n\n\n\n\n\n\n\nNow, lets have a look at at the distribution of two different groups (group has a strong effect, Cohen’s d = .8)\n\n\n\n\n\n\n\n\n\nNow, lets have a look at at the distribution of two different groups (group has a very strong effect, Cohen’s d = 1.2)\n\n\n\n\n\n\n\n\n\nIf variability and sample size remain constant, larger effects are easier to detect than smaller effects!"
  },
  {
    "objectID": "tutorials/pwr/pwr.html#sample-size",
    "href": "tutorials/pwr/pwr.html#sample-size",
    "title": "Power Analysis in R",
    "section": "Sample size",
    "text": "Sample size\nAnd let’s now look at sample size.\n\n\n\n\n\n\n\n\n\nLet us now increase the sample size to N = 50.\n\n\n\n\n\n\n\n\n\nIf variability and effect size remain constant, effects are easier to detect with increasing sample size!"
  },
  {
    "objectID": "tutorials/pwr/pwr.html#variability",
    "href": "tutorials/pwr/pwr.html#variability",
    "title": "Power Analysis in R",
    "section": "Variability",
    "text": "Variability\nAnd let’s now look at variability.\n\n\n\n\n\n\n\n\n\nLet’s decrease the variability to sd = 5.\n\n\n\n\n\n\n\n\n\nIf the sample and effect size remain constant, effects are easier to detect with decreasing variability!\nIn summary, there are three main factors that determine if a model finds an effect. The accuracy (i.e., the probability of finding an effect):\n\nthe size of the effect (bigger effects are easier to detect)\nthe variability of the effect (less variability makes it easier to detect an effect), and\nthe sample size (the bigger the sample size, the easier it is to detect an effect);\n\nnumber of subjects/participants\nnumber of items/questions\nnumber of observations per item within subjects/participants\n\n\nNow, if a) we dealing with a very big effect, then we need only few participants and few items to accurately find this effect.\nOr b) if we dealing with an effect that has low variability (it is observable for all subjects with the same strength), then we need only few participants and few items to accurately find this effect.\nBefore we conduct a study, we should figure out, what sample we need to detect a small/medium effect with medium variability so that our model is sufficient to detect this kind of effect. In order to do this, we would generate a data set that mirrors the kind of data that we expect to get (with the properties that we expect to get). We can then fit a model to this data and check if a model would be able to detect the expected effect. However, because a single model does not tell us that much (it could simply be luck that it happened to find the effect), we run many different models on variations of the data and see how many of them find the effect. As a general rule of thumb, we want a data set that allows a model to find a medium sized effect with at least an accuracy of 80 percent (Field et al.).\nIn the following, we will go through how to determine what sample size we need for an example analysis."
  },
  {
    "objectID": "tutorials/pwr/pwr.html#preparation-and-session-set-up",
    "href": "tutorials/pwr/pwr.html#preparation-and-session-set-up",
    "title": "Power Analysis in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages into the R library on your computer so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install libraries\ninstall.packages(c(\"tidyverse\", \"pwr\", \"lme4\", \"sjPlot\", \"simr\", \"effectsize\"))\ninstall.packages(c(\"DT\", \"knitr\", \"flextable\"))\ninstall.packages(\"DescTools\")\ninstall.packages(\"pacman\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we can activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F) # no automatic conversion of factors\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\noptions(max.print = 1000) # print max 1000 results\n# load packages\nlibrary(tidyverse)\nlibrary(pwr)\nlibrary(lme4)\nlibrary(sjPlot)\nlibrary(simr)\nlibrary(effectsize)\nlibrary(DT)\nlibrary(knitr)\nlibrary(flextable)\npacman::p_load_gh(\"trinker/entity\")\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/pwr/pwr.html#one-way-anova",
    "href": "tutorials/pwr/pwr.html#one-way-anova",
    "title": "Power Analysis in R",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nLet check how to calculate the necessary sample size for each group for a one-way ANOVA that compares 5 groups (k) and that has a power of 0.80 (80 percent), when the effect size is moderate (f = 0.25) and the significance level is 0.05 (5 percent)..\n\n# load package\nlibrary(pwr)\n# calculate minimal sample size\npwr.anova.test(\n  k = 5, # 5 groups are compared\n  f = .25, # moderate effect size\n  sig.level = .05, # alpha/sig. level = .05\n  power = .8\n) # confint./power = .8\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 5\n              n = 39.15\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nIn this case, the minimum number of participants in each group would be 40.\nLet’s check how we could calculate the power if we had already collected data (with 30 participants in each group) and we want to report the power of our analysis (and let us assume that the effect size was medium).\n\n# calculate minimal sample size\npwr.anova.test(\n  k = 5, # 5 groups are compared\n  f = .25, # moderate effect size\n  sig.level = .05, # alpha/sig. level = .05\n  n = 30\n) # n of participants\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 5\n              n = 30\n              f = 0.25\n      sig.level = 0.05\n          power = 0.6676\n\nNOTE: n is number in each group\n\n\nIn this case our analysis would only have a power or 66.8 percent. This means that we would only detect a medium sized effect in 66.7 percent of cases (which is considered insufficient)."
  },
  {
    "objectID": "tutorials/pwr/pwr.html#power-analysis-for-glms",
    "href": "tutorials/pwr/pwr.html#power-analysis-for-glms",
    "title": "Power Analysis in R",
    "section": "Power Analysis for GLMs",
    "text": "Power Analysis for GLMs\nWhen determining the power of a generalized linear model, we need to provide\n\nthe degrees of freedom for numerator (´u´)\nthe degrees of freedom for denominator (v)\nthe effect size (the estimate of the intercept and the slope/estimates of the predictors)\nthe level of significance (i.e., the type I error probability)\n\nThe values of the parameters in the example below are adapted from the fixed-effects regression example that was used to analyze different teaching styles (see here).\nThe effect size used here is \\(f^2^\\) that has be categorized as follows (see Cohen): small \\(≥\\) 0.02, medium \\(≥\\) 0.15, and large \\(≥\\) 0.35. So in order to determine if the data is sufficient to find a weak effect when comparing 2 groups with 30 participants in both groups (df_numerator_: 2-1; df_denominator_: (30-1) + (30-1)) and a significance level at \\(\\alpha\\) = .05, we can use the following code.\n\n# general linear model\npwrglm &lt;- pwr.f2.test(\n  u = 1,\n  v = 58,\n  f2 = .02,\n  sig.level = 0.05\n)\n# inspect results\npwrglm\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 58\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.1899\n\n\nThe results show that the regression analyses used to evaluate the effectiveness of different teaching styles only had a power of 0.1899206."
  },
  {
    "objectID": "tutorials/pwr/pwr.html#power-analysis-for-t-tests",
    "href": "tutorials/pwr/pwr.html#power-analysis-for-t-tests",
    "title": "Power Analysis in R",
    "section": "Power Analysis for t-tests",
    "text": "Power Analysis for t-tests\nFor t-tests (both paired and 2-sample t-tests), the effect size measure is Cohen’s \\(d\\) that has be categorized as follows (see Cohen): small \\(≥\\) 0.2, medium \\(≥\\) 0.5, and large \\(≥\\) 0.8.\nPaired t-test\nSo in order to determine if the data is sufficient to find a weak effect when comparing the pre- and post-test results of a group with 30 participants, evaluating an undirected hypothesis (thus the two-tailed approach), and a significance level at \\(\\alpha\\) = .05, we can use the following code.\n\n# paired t-test\npwrpt &lt;- pwr.t.test(\n  d = 0.2,\n  n = 30,\n  sig.level = 0.05,\n  type = \"paired\",\n  alternative = \"two.sided\"\n)\n# inspect\npwrpt\n\n\n     Paired t test power calculation \n\n              n = 30\n              d = 0.2\n      sig.level = 0.05\n          power = 0.1852\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nGiven the data, a weak effect in this design can only be detected with a certainty of 0.1851834 percent. This means that we would need to substantively increase the sample size to detect a small effect with this design.\nTwo-sample (independent) t-test\nThe power in a similar scenario but with two different groups (with 25 and 35 subjects) can be determined as follows (in this case we test a directed hypothesis that checks if the intervention leads to an increase in the outcome - hence the greater in the alternative argument):\n\n# independent t-test\npwr2t &lt;- pwr.t2n.test(\n  d = 0.2,\n  n1 = 35,\n  n2 = 25,\n  sig.level = 0.05,\n  alternative = \"greater\"\n)\n# inspect\npwr2t\n\n\n     t test power calculation \n\n             n1 = 35\n             n2 = 25\n              d = 0.2\n      sig.level = 0.05\n          power = 0.1867\n    alternative = greater\n\n\nGiven the data, a weak effect in this design can only be detected with a certainty of 0.1867382 percent. This means that we would need to substantively increase the sample size to detect a small effect with this design."
  },
  {
    "objectID": "tutorials/pwr/pwr.html#power-analysis-for-chi2-tests",
    "href": "tutorials/pwr/pwr.html#power-analysis-for-chi2-tests",
    "title": "Power Analysis in R",
    "section": "Power Analysis for \\(\\chi\\)2-tests",
    "text": "Power Analysis for \\(\\chi\\)2-tests\nLet us now check the power of a \\(\\chi^2\\)^-test. For \\(\\chi^2\\)^-test, the effect size measure used in the power analysis is \\(w\\) that has be categorized as follows (see Cohen): small \\(≥\\) 0.1, medium \\(≥\\) 0.3, and large \\(≥\\) 0.5. Also, keep in mind that for \\(\\chi^2\\)^-tests, at least 80 percent of cells need to have values \\(≥\\) 5 and none of the cells should have expected values smaller than 1 (see Bortz, Lienert, and Boehnke).\n\n# x2-test\npwrx2 &lt;- pwr.chisq.test(\n  w = 0.2,\n  N = 25, # total number of observations\n  df = 1,\n  sig.level = 0.05\n)\n# inspect\npwrx2\n\n\n     Chi squared power calculation \n\n              w = 0.2\n              N = 25\n             df = 1\n      sig.level = 0.05\n          power = 0.1701\n\nNOTE: N is the number of observations\n\n\nGiven the data, a weak effect in this design can only be detected with a certainty of 0.170075 percent. This means that we would need to substantively increase the sample size to detect a small effect with this design.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nFor the tests above (anova, glm, paired and independent t-test, and the \\(\\chi^2\\)-test), how many participants would you need to have a power of 80 percent?\n\nHere are the commands we used to help you:\n\nANOVA: pwr.anova.test(k=5, f=.25, sig.level=.05, n=30)\nGLM: pwr.f2.test(u = 1, v = 58, f2 = .02, sig.level = 0.05)\npaired t-test: pwr.t.test(d=0.2, n=30, sig.level=0.05, type=\"paired\", alternative=\"two.sided\")\nindependent t-test: pwr.t2n.test(d=0.2,n1=35, n2 = 25, sig.level=0.05, alternative=\"greater\")\n\\(\\chi^2\\)-test: pwr.chisq.test(w=0.2, N = 25,  df = 1, sig.level=0.05) \n\n\nAnswer\n\n\n\npwr.anova.test(k = 5, f = .25, sig.level = .05, p = .8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 5\n              n = 39.15\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\npwr.f2.test(u = 1, f2 = .02, sig.level = 0.05, p = .8)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 392.4\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.8\n\npwr.t.test(d = 0.2, p = .8, sig.level = 0.05, type = \"paired\", alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 198.2\n              d = 0.2\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\npwr.t2n.test(d = 0.2, n1 = 310, n2 = 310, sig.level = 0.05, alternative = \"greater\") # by checking N values\n\n\n     t test power calculation \n\n             n1 = 310\n             n2 = 310\n              d = 0.2\n      sig.level = 0.05\n          power = 0.8002\n    alternative = greater\n\npwr.chisq.test(w = 0.2, p = .8, df = 1, sig.level = 0.05)\n\n\n     Chi squared power calculation \n\n              w = 0.2\n              N = 196.2\n             df = 1\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations"
  },
  {
    "objectID": "tutorials/pwr/pwr.html#excursion-language-is-never-ever-random",
    "href": "tutorials/pwr/pwr.html#excursion-language-is-never-ever-random",
    "title": "Power Analysis in R",
    "section": "Excursion: Language is never ever random",
    "text": "Excursion: Language is never ever random\nIn 2005, Adam Kilgarriff (2005) made a point that Language is never, ever, ever, random. Here is part of the abstract:\n\nLanguage users never choose words randomly, and language is essentially non-random. Statistical hypothesis testing uses a null hypothesis, which posits randomness. Hence, when we look at linguistic phenomena in corpora, the null hypothesis will never be true. Moreover, where there is enough data, we shall (almost) always be able to establish that it is not true. In corpus studies, we frequently do have enough data, so the fact that a relation between two phenomena is demonstrably non-random, does not support the inference that it is not arbitrary. We present experimental evidence of how arbitrary associations between word frequencies and corpora are systematically non-random.\n\nThis is a problem because if we are using ever bigger corpora, even the tiniest of difference will become significant. have a look at the following example.\n\n# first let us generate some data\nfreqs1 &lt;- matrix(c(10, 28, 30, 92), byrow = T, ncol = 2)\n# inspect data\nfreqs1\n\n     [,1] [,2]\n[1,]   10   28\n[2,]   30   92\n\n\nNow, we perform a simple \\(\\chi^2\\)-test and extract the effect size.\n\n# x2-test\nx21 &lt;- chisq.test(freqs1)\n# inspect results\nx21\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  freqs1\nX-squared = 0, df = 1, p-value = 1\n\n# effect size\neffectsize::effectsize(x21)\n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.00              | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nthe output shows that the difference is not significant and that the effect size is extremely(!) small.\nNow, let us simply increase the sample size by a factor of 1000 and also perform a \\(\\chi^2\\)-test on this extended data set and extract the effect size.\n\n#\nfreqs2 &lt;- matrix(c(10000, 28000, 30000, 92000), byrow = T, ncol = 2)\n# first let us generate some data\nx22 &lt;- chisq.test(freqs2)\n# inspect results\nx22\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  freqs2\nX-squared = 46, df = 1, p-value = 0.00000000001\n\n# effect size\neffectsize::effectsize(x22)\n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.02              | [0.01, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe output shows that the difference is not significant but that the effect size has remain the same. For this reason, in a response to Kilgariff, Stefan (Gries 2005) suggested to always also report effect size in addition to significance so that the reader has an understanding of whether a significant effect is meaningful.\nThis is relevant here because we have focused on the power for finding small effects as these can be considered the smallest meaningful effects. However, even tiny effects can be meaningful under certain circumstances - as such, focusing on small effects is only a rule of thumb, should be taken with a pinch of salt, and should be re-evaluated in the context of the study at hand."
  },
  {
    "objectID": "tutorials/pwr/pwr.html#using-piloted-data-and-a-lmer",
    "href": "tutorials/pwr/pwr.html#using-piloted-data-and-a-lmer",
    "title": "Power Analysis in R",
    "section": "Using piloted data and a lmer",
    "text": "Using piloted data and a lmer\nFor this analysis, we load an existing data set resulting from a piloted study that only contains the predictors (not the response variable).\n\n# load data\nregdat &lt;- base::readRDS(\"tutorials/pwr/data/regdat.rda\", \"rb\")\n# inspect\nhead(regdat, 10)\n\n       ID   Sentence     Group WordOrder SentenceType\n1  Part01 Sentence01 L1English        V2  NoAdverbial\n2  Part01 Sentence02 L1English        V3  NoAdverbial\n3  Part01 Sentence03 L1English        V3  NoAdverbial\n4  Part01 Sentence04 L1English        V2  NoAdverbial\n5  Part01 Sentence05 L1English        V2  NoAdverbial\n6  Part01 Sentence06 L1English        V3  NoAdverbial\n7  Part01 Sentence07 L1English        V2  NoAdverbial\n8  Part01 Sentence08 L1English        V3  NoAdverbial\n9  Part01 Sentence09 L1English        V3  NoAdverbial\n10 Part01 Sentence10 L1English        V2  NoAdverbial\n\nstr(regdat)\n\n'data.frame':   480 obs. of  5 variables:\n $ ID          : Factor w/ 20 levels \"Part01\",\"Part02\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Sentence    : Factor w/ 24 levels \"Sentence01\",\"Sentence02\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ Group       : Factor w/ 2 levels \"L1English\",\"L2English\": 1 1 1 1 1 1 1 1 1 1 ...\n $ WordOrder   : Factor w/ 2 levels \"V2\",\"V3\": 1 2 2 1 1 2 1 2 2 1 ...\n $ SentenceType: Factor w/ 2 levels \"NoAdverbial\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe inspect the data and check how many levels we have for each predictor and if the levels are distributed correctly (so that we do not have incomplete information).\n\ntable(regdat$Group, regdat$WordOrder)\n\n           \n             V2  V3\n  L1English 120 120\n  L2English 120 120\n\ntable(regdat$Group, regdat$SentenceType)\n\n           \n            NoAdverbial SentenceAdverbial\n  L1English         120               120\n  L2English         120               120\n\ntable(regdat$ID)\n\n\nPart01 Part02 Part03 Part04 Part05 Part06 Part07 Part08 Part09 Part10 Part11 \n    24     24     24     24     24     24     24     24     24     24     24 \nPart12 Part13 Part14 Part15 Part16 Part17 Part18 Part19 Part20 \n    24     24     24     24     24     24     24     24     24 \n\ntable(regdat$Sentence)\n\n\nSentence01 Sentence02 Sentence03 Sentence04 Sentence05 Sentence06 Sentence07 \n        20         20         20         20         20         20         20 \nSentence08 Sentence09 Sentence10 Sentence11 Sentence12 Sentence13 Sentence14 \n        20         20         20         20         20         20         20 \nSentence15 Sentence16 Sentence17 Sentence18 Sentence19 Sentence20 Sentence21 \n        20         20         20         20         20         20         20 \nSentence22 Sentence23 Sentence24 \n        20         20         20 \n\n\nWe could also add a response variable (but we will do this later when we deal with post-hoc power analyses).\n\nGenerating the model\nWe now generate model that has per-defined parameters. We begin by specifying the parameters by setting effect sizes of the fixed effects and the intercept, the variability accounted fro by the random effects and the residuals.\n\n# Intercept + slopes for fixed effects\n# (Intercept + Group, SentenceType, WordOrder, and an interaction between Group * SentenceType)\nfixed &lt;- c(.52, .52, .52, .52, .52)\n# Random intercepts for Sentence and ID\nrand &lt;- list(0.5, 0.1)\n# res. variance\nres &lt;- 2\n\nWe now generate the model and fit it to our data.\n\nm1 &lt;- makeLmer(y ~ (1 | Sentence) + (1 | ID) + Group * SentenceType + WordOrder,\n  fixef = fixed,\n  VarCorr = rand,\n  sigma = res,\n  data = regdat\n)\n\n# inspect\nm1\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ (1 | Sentence) + (1 | ID) + Group * SentenceType + WordOrder\n   Data: regdat\nREML criterion at convergence: 2100\nRandom effects:\n Groups   Name        Std.Dev.\n Sentence (Intercept) 0.707   \n ID       (Intercept) 0.316   \n Residual             2.000   \nNumber of obs: 480, groups:  Sentence, 24; ID, 20\nFixed Effects:\n                                 (Intercept)  \n                                        0.52  \n                              GroupL2English  \n                                        0.52  \n               SentenceTypeSentenceAdverbial  \n                                        0.52  \n                                 WordOrderV3  \n                                        0.52  \nGroupL2English:SentenceTypeSentenceAdverbial  \n                                        0.52  \n\n\nInspect summary table\n\nsjPlot::tab_model(m1)\n\n\n\n \ny\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n0.52\n-0.14 – 1.18\n0.124\n\n\nGroup [L2English]\n0.52\n-0.06 – 1.10\n0.078\n\n\nSentenceType[SentenceAdverbial]\n0.52\n-0.24 – 1.28\n0.180\n\n\nWordOrder [V3]\n0.52\n-0.15 – 1.19\n0.129\n\n\nGroup [L2English] ×SentenceType[SentenceAdverbial]\n0.52\n-0.20 – 1.24\n0.155\n\n\nRandom Effects\n\n\n\nσ2\n4.00\n\n\n\nτ00 Sentence\n0.50\n\n\nτ00 ID\n0.10\n\n\nICC\n0.13\n\n\nN Sentence\n24\n\n\nN ID\n20\n\nObservations\n480\n\n\nMarginal R2 / Conditional R2\n0.078 / 0.198\n\n\n\n\n\n\nThe summary table shows that the effects are correct but none of them are reported as being significant!\n\n\nPower analysis\nLet us now check if the data is sufficient to detect the main effect of WordOrder. In the test argument we use fcompare which allows us to compare a model with that effect (our m1 model) to a model without that effect. Fortunately, we only need to specify the fixed effects structure.\n\nsim_wo &lt;- simr::powerSim(m1,\n  nsim = 20,\n  test = fcompare(y ~ Group * SentenceType)\n)\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect results\nsim_wo\n\nPower for model comparison, (95% confidence interval):\n      35.00% (15.39, 59.22)\n\nTest: Likelihood ratio\n      Comparison to y ~ Group * SentenceType + [re]\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 480\n\nTime elapsed: 0 h 0 m 1 s\n\n\nThe data is not sufficient and would only detect a weak effect of WordOrder with 7 percent accuracy!\nBefore we continue to see how to test the power of data to find interactions, let us briefly think about why I chose 0.52 when we specified the effect sizes.\n\n\nWhy did I set the estimates to 0.52?\nLet us now inspect the effect sizes and see why I used 0.52 as an effect size in the model. We need to determine the odds ratios of the fixed effects and then convert them into Cohen’s d values for which we have associations between traditional denominations (small, medium, and large) and effect size values.\n\n# extract fixed effect estimates\nestimatesfixedeffects &lt;- fixef(m1)\n# convert estimates into odds ratios\nexp(estimatesfixedeffects)\n\n                                 (Intercept) \n                                       1.682 \n                              GroupL2English \n                                       1.682 \n               SentenceTypeSentenceAdverbial \n                                       1.682 \n                                 WordOrderV3 \n                                       1.682 \nGroupL2English:SentenceTypeSentenceAdverbial \n                                       1.682 \n\n\nWe will now check the effect size which can be interpreted according to Chen, Cohen, and Chen (see also Cohen; Perugini, Gallucci, and Costantini, 2).\n\nsmall effect (Cohen’s d 0.2, OR = 1.68)\nmedium effect (Cohen’s d 0.5, OR = 3.47)\nstrong effect (Cohen’s d 0.8, OR = 6.71)\n\n\n\nPower analysis for interactions\nLet us now check if the data set has enough power to detect a weak effect for the interaction between Group:SentenceType.\n\nsim_gst &lt;- simr::powerSim(m1,\n  nsim = 20,\n  # compare model with interaction to model without interaction\n  test = fcompare(y ~ WordOrder + Group + SentenceType)\n)\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect\nsim_gst\n\nPower for model comparison, (95% confidence interval):\n      25.00% ( 8.66, 49.10)\n\nTest: Likelihood ratio\n      Comparison to y ~ WordOrder + Group + SentenceType + [re]\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 480\n\nTime elapsed: 0 h 0 m 1 s\n\n\nThe data is not sufficient to detect a weak effect of Group:SentenceType with 5 percent accuracy!\n\n\n\n\nNOTEOh no… What can we do?"
  },
  {
    "objectID": "tutorials/pwr/pwr.html#extending-data",
    "href": "tutorials/pwr/pwr.html#extending-data",
    "title": "Power Analysis in R",
    "section": "Extending Data",
    "text": "Extending Data\nWe will now extend the data to see what sample size is needed to get to the 80 percent accuracy threshold. We begin by increasing the number of sentences from 10 to 30 to see if this would lead to a sufficient sample size. After increasing the number of sentences, we will extend the data to see how many participants we would need.\n\nAdding sentences\n\nm1_as &lt;- simr::extend(m1,\n  along = \"Sentence\",\n  n = 120\n)\n# inspect model\nm1_as\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ (1 | Sentence) + (1 | ID) + Group * SentenceType + WordOrder\n   Data: regdat\nREML criterion at convergence: 2100\nRandom effects:\n Groups   Name        Std.Dev.\n Sentence (Intercept) 0.707   \n ID       (Intercept) 0.316   \n Residual             2.000   \nNumber of obs: 480, groups:  Sentence, 24; ID, 20\nFixed Effects:\n                                 (Intercept)  \n                                        0.52  \n                              GroupL2English  \n                                        0.52  \n               SentenceTypeSentenceAdverbial  \n                                        0.52  \n                                 WordOrderV3  \n                                        0.52  \nGroupL2English:SentenceTypeSentenceAdverbial  \n                                        0.52  \n\n\nCheck power when using 120 sentences\n\nsim_m1_as_gst &lt;- powerSim(m1_as,\n  nsim = 20,\n  test = fcompare(y ~ WordOrder + Group + SentenceType)\n)\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect\nsim_m1_as_gst\n\nPower for model comparison, (95% confidence interval):\n      90.00% (68.30, 98.77)\n\nTest: Likelihood ratio\n      Comparison to y ~ WordOrder + Group + SentenceType + [re]\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 2400\n\nTime elapsed: 0 h 0 m 2 s\n\n\nThe data with 120 sentences is sufficient and would detect a weak effect of Group:SentenceType with 18 percent accuracy!\nLet us now plot a power curve to see where we cross the 80 percent threshold.\n\npcurve_m1_as_gst &lt;- simr::powerCurve(m1_as,\n  test = fcompare(y ~ WordOrder + Group + SentenceType),\n  along = \"Sentence\",\n  nsim = 20,\n  breaks = seq(20, 120, 20)\n)\n# show plot\nplot(pcurve_m1_as_gst)\n\n\n\n\n\n\n\n\nUsing more items (or in this case sentences) is rather easy but it can make experiments longer which may lead to participants becoming tired or annoyed. An alternative would be to use more participants. Let us thus check how we can determine how many subjects we would need to reach a power of at least 80 percent.\n\n\nChecking participants\nWhat about increasing the number of participants?\nWe increase the number of participants to 120.\n\nm1_ap &lt;- simr::extend(m1,\n  along = \"ID\",\n  n = 120\n)\n# inspect model\nm1_ap\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ (1 | Sentence) + (1 | ID) + Group * SentenceType + WordOrder\n   Data: regdat\nREML criterion at convergence: 2100\nRandom effects:\n Groups   Name        Std.Dev.\n Sentence (Intercept) 0.707   \n ID       (Intercept) 0.316   \n Residual             2.000   \nNumber of obs: 480, groups:  Sentence, 24; ID, 20\nFixed Effects:\n                                 (Intercept)  \n                                        0.52  \n                              GroupL2English  \n                                        0.52  \n               SentenceTypeSentenceAdverbial  \n                                        0.52  \n                                 WordOrderV3  \n                                        0.52  \nGroupL2English:SentenceTypeSentenceAdverbial  \n                                        0.52  \n\n\nCheck power when using 120 participants\n\nsim_m1_ap_gst &lt;- powerSim(m1_ap,\n  nsim = 20,\n  test = fcompare(y ~ WordOrder + Group + SentenceType)\n)\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect\nsim_m1_ap_gst\n\nPower for model comparison, (95% confidence interval):\n      80.00% (56.34, 94.27)\n\nTest: Likelihood ratio\n      Comparison to y ~ WordOrder + Group + SentenceType + [re]\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 2880\n\nTime elapsed: 0 h 0 m 2 s\n\n\nThe data with 120 participants is sufficient and would detect a weak effect of Group * SentenceType with 16 percent accuracy\n\npcurve_m1_ap_gst &lt;- simr::powerCurve(m1_ap,\n  test = fcompare(y ~ Group + SentenceType + WordOrder),\n  along = \"ID\",\n  nsim = 20\n)\n\nCalculating power at 10 sample sizes along ID\n\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n# show plot\nplot(pcurve_m1_ap_gst)"
  },
  {
    "objectID": "tutorials/pwr/pwr.html#generating-data-and-a-glmer",
    "href": "tutorials/pwr/pwr.html#generating-data-and-a-glmer",
    "title": "Power Analysis in R",
    "section": "Generating data and a glmer",
    "text": "Generating data and a glmer\nIn order to perform a power analysis, we will start by loading the tidyverse package to process the data and by generating a data that we will use to determine the power of a regression model.\nThis simulated data set has\n\n200 data points\n2 Conditions (Control, Test)\n10 Subjects\n10 Items\n\n\n# generate data\nsimdat &lt;- data.frame(\n  sub &lt;- rep(paste0(\"Sub\", 1:10), each = 20),\n  cond &lt;- rep(\n    c(\n      rep(\"Control\", 10),\n      rep(\"Test\", 10)\n    ),\n    10\n  ),\n  itm &lt;- as.character(rep(1:10, 20))\n) %&gt;%\n  dplyr::rename(\n    Subject = 1,\n    Condition = 2,\n    Item = 3\n  ) %&gt;%\n  dplyr::mutate_if(is.character, factor)\n# inspect\nhead(simdat, 15)\n\n   Subject Condition Item\n1     Sub1   Control    1\n2     Sub1   Control    2\n3     Sub1   Control    3\n4     Sub1   Control    4\n5     Sub1   Control    5\n6     Sub1   Control    6\n7     Sub1   Control    7\n8     Sub1   Control    8\n9     Sub1   Control    9\n10    Sub1   Control   10\n11    Sub1      Test    1\n12    Sub1      Test    2\n13    Sub1      Test    3\n14    Sub1      Test    4\n15    Sub1      Test    5\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nCan you create a data set with 5 Subjects, 5 Items (each) for 4 Conditions? \n\n\n\nAnswer\n\n\n\n   Subjects Items  Condition\n1  Subject1 Item1 Condition1\n2  Subject1 Item2 Condition1\n3  Subject1 Item3 Condition1\n4  Subject1 Item4 Condition1\n5  Subject1 Item5 Condition1\n6  Subject1 Item1 Condition2\n7  Subject1 Item2 Condition2\n8  Subject1 Item3 Condition2\n9  Subject1 Item4 Condition2\n10 Subject1 Item5 Condition2\n11 Subject1 Item1 Condition3\n12 Subject1 Item2 Condition3\n13 Subject1 Item3 Condition3\n14 Subject1 Item4 Condition3\n15 Subject1 Item5 Condition3\n16 Subject1 Item1 Condition4\n17 Subject1 Item2 Condition4\n18 Subject1 Item3 Condition4\n19 Subject1 Item4 Condition4\n20 Subject1 Item5 Condition4\n\n\n\n\n\n\nGenerating the model\nAs before with the lmer, we specify the model parameters - but when generating glmers, we only need to specify the effects for the fixed effects and the intercept and define the variability in the random effects (we do not need to specify the residuals).\n\n# Intercept + slopes for fixed effects\n# (Group, SentenceType, WordOrder, and an interaction between Group * SentenceType)\nfixed &lt;- c(.52, .52)\n# Random intercepts for Sentence and ID\nrand &lt;- list(0.5, 0.1)\n\nWe now generate the model and fit it to the data.\n\nm2 &lt;- simr::makeGlmer(y ~ (1 | Subject) + (1 | Item) + Condition,\n  family = \"binomial\",\n  fixef = fixed,\n  VarCorr = rand,\n  data = simdat\n)\n# inspect\nsjPlot::tab_model(m2)\n\n\n\n \ny\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.68\n0.89 – 3.17\n0.108\n\n\nCondition [Test]\n1.68\n0.94 – 3.02\n0.081\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Subject\n0.50\n\n\nτ00 Item\n0.10\n\n\nICC\n0.15\n\n\nN Subject\n10\n\n\nN Item\n10\n\nObservations\n200\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.169\n\n\n\n\n\n\nNext, we extract power. In this case, we use fixed in the test argument which allows us to test a specific predictor.\n\n# set seed for replicability\nset.seed(12345)\n# perform power analysis for present model\nrsim_m2_c &lt;- powerSim(m2, fixed(\"ConditionTest\", \"z\"),\n  nsim = 20\n)\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\n# inspect\nrsim_m2_c\n\nPower for predictor 'ConditionTest', (95% confidence interval):\n      35.00% (15.39, 59.22)\n\nTest: z-test\n      Effect size for ConditionTest is 0.52\n\nBased on 20 simulations, (0 warnings, 0 errors)\nalpha = 0.05, nrow = 200\n\nTime elapsed: 0 h 0 m 1 s\n\n\nThe data is sufficient and would detect a weak effect of ConditionTest with only 7 percent accuracy"
  },
  {
    "objectID": "tutorials/pwr/pwr.html#extending-the-data",
    "href": "tutorials/pwr/pwr.html#extending-the-data",
    "title": "Power Analysis in R",
    "section": "Extending the data",
    "text": "Extending the data\nLike before, we can now extend the data to see how many participants or items we would need to reach the 80 percent confidence threshold.\n\nAdding Items\nWe start of by increasing the number of items from 10 to 40. This means that our new data/model has the following characteristics.\n\n2 Conditions\n10 Subjects\n40 Items (from 10)\n\nKeep in mind though that when extending the data/model in this way, each combination occurs only once!\n\nm2_ai &lt;- simr::extend(m2,\n  along = \"Item\",\n  n = 40\n)\n\nNext, we plot the power curve.\n\npcurve_m2_ai_c &lt;- powerCurve(m2_ai,\n  fixed(\"ConditionTest\", \"z\"),\n  along = \"Item\",\n  nsim = 20,\n  breaks = seq(10, 40, 5)\n)\n\nCalculating power at 7 sample sizes along Item\n\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) \b\b\b\b\b\b(1/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==================                                          |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===========================                                 |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==============================                              |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |================================================            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |======================================================      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=========================================================   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) \b\b\b\b\b\b(2/7) \b\b\b\b\b\b(2/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==================                                          |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===========================                                 |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==============================                              |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=========================================================   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) \b\b\b\b\b\b(3/7) \b\b\b\b\b\b(3/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===============                                             |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) \b\b\b\b\b\b(4/7) \b\b\b\b\b\b(4/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===============                                             |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) \b\b\b\b\b\b(5/7) \b\b\b\b\b\b(5/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===============                                             |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) \b\b\b\b\b\b(6/7) \b\b\b\b\b\b(6/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) \b\b\b\b\b\b(7/7) \b\b\b\b\b\b(7/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===============                                             |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m2_ai_c)\n\n\n\n\n\n\n\n\nThe power curve shows that we breach the 80 percent threshold with about 35 items.\n\n\nAdding subjects\nAn alternative to adding items is, of course, to use more subjects or participants. We thus continue by increasing the number of participants from 10 to 40. This means that our new data/model has the following characteristics.\n\n2 Conditions\n10 Items\n40 Subjects (from 10)\n\nAgain, keep in mind though that when extending the data/model in this way, each combination occurs only once!\n\nm2_as &lt;- simr::extend(m2,\n  along = \"Subject\",\n  n = 40\n)\n# inspect model\nsjPlot::tab_model(m2_as)\n\n\n\n \ny\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.68\n0.89 – 3.17\n0.108\n\n\nCondition [Test]\n1.68\n0.94 – 3.02\n0.081\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Subject\n0.50\n\n\nτ00 Item\n0.10\n\n\nICC\n0.15\n\n\nN Subject\n10\n\n\nN Item\n10\n\nObservations\n200\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.169\n\n\n\n\n\n\nAs before, we plot power curve.\n\npcurve_m2_as_c &lt;- powerCurve(m2_as,\n  fixed(\"ConditionTest\", \"z\"),\n  along = \"Subject\",\n  nsim = 20,\n  breaks = seq(10, 40, 5)\n)\n\nCalculating power at 7 sample sizes along Subject\n\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) \b\b\b\b\b\b(1/7) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |======                                                      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==================                                          |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=====================                                       |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |========================                                    |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |======================================================      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |=========================================================   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/7) \b\b\b\b\b\b(2/7) \b\b\b\b\b\b(2/7) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |======                                                      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |============                                                |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==================                                          |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |======================================================      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/7) \b\b\b\b\b\b(3/7) \b\b\b\b\b\b(3/7) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |=========================================================   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/7) \b\b\b\b\b\b(4/7) \b\b\b\b\b\b(4/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/7) \b\b\b\b\b\b(5/7) \b\b\b\b\b\b(5/7) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |======                                                      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/7) \b\b\b\b\b\b(6/7) \b\b\b\b\b\b(6/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/7) \b\b\b\b\b\b(7/7) \b\b\b\b\b\b(7/7) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(7/7) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m2_as_c)\n\n\n\n\n\n\n\n\nHow often does each combination occur?\nOnly once!\nSo, what if we increase the number of combinations (this is particularly important when using a *repeated measures** design)? Below, we increase the number of configuration from 1 to 10 so that each item is shown 10 times to the same participant. This means that our new data/model has the following characteristics.\n\n2 Conditions\n10 Items\n10 Subjects\n\nNow each combination of item and subject occurs 10 times!\n\nm2_asi_c &lt;- extend(m2,\n  within = \"Subject+Item\",\n  n = 10\n)\n# perform power calculation\npcurve_m2_asi_c &lt;- powerCurve(m2_asi_c,\n  fixed(\"ConditionTest\", \"z\"),\n  within = \"Subject+Item\",\n  nsim = 20,\n  breaks = seq(2, 10, 2)\n)\n\nCalculating power at 5 sample sizes within Subject+Item\n\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) \b\b\b\b\b\b(1/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |==========================================                  |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=============================================               |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |================================================            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/5) \b\b\b\b\b\b(2/5) \b\b\b\b\b\b(2/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=========                                                   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/5) \b\b\b\b\b\b(3/5) \b\b\b\b\b\b(3/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/5) \b\b\b\b\b\b(4/5) \b\b\b\b\b\b(4/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=================================                           |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/5) \b\b\b\b\b\b(5/5) \b\b\b\b\b\b(5/5) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/5) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m2_asi_c)\n\n\n\n\n\n\n\n\nIf we did this, then even 5 subjects may be enough to reach the 80 percent threshold.\n\n\nAdding subjects and items\nWe can also add subjects and items simultaneously to address questions like How many subjects would I need if I had 30 items?. Hence, we increase both subjects and items from 10 to 30. This means that our new data/model has the following characteristics.\n\n2 Conditions\n30 Items (from 10)\n30 Subjects (from 10)\n\nIn this case, we return to each combination only occurring once.\n\nm2_as &lt;- simr::extend(m2,\n  along = \"Subject\",\n  n = 30\n)\nm2_asi &lt;- simr::extend(m2_as,\n  along = \"Item\",\n  n = 30\n)\n# inspect model\nsjPlot::tab_model(m2_asi)\n\n\n\n \ny\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.68\n0.89 – 3.17\n0.108\n\n\nCondition [Test]\n1.68\n0.94 – 3.02\n0.081\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Subject\n0.50\n\n\nτ00 Item\n0.10\n\n\nICC\n0.15\n\n\nN Subject\n10\n\n\nN Item\n10\n\nObservations\n200\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.169\n\n\n\n\n\n\nWe can now plot power curve to answer the question How many subjects do you need if you have 30 Items?.\n\npcurve_m2_asi_c &lt;- powerCurve(m2_asi,\n  fixed(\"ConditionTest\", \"z\"),\n  along = \"Subject\",\n  breaks = seq(5, 30, 5),\n  nsim = 20\n)\n\nCalculating power at 6 sample sizes along Subject\n\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) \b\b\b\b\b\b(1/6) Simulating: |                                                            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |====================================                        |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |================================================            |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |======================================================      |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) \b\b\b\b\b\b(2/6) \b\b\b\b\b\b(2/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=========                                                   |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=======================================                     |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===================================================         |\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) \b\b\b\b\b\b(3/6) \b\b\b\b\b\b(3/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) \b\b\b\b\b\b(4/6) \b\b\b\b\b\b(4/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) \b\b\b\b\b\b(5/6) \b\b\b\b\b\b(5/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) \b\b\b\b\b\b(6/6) \b\b\b\b\b\b(6/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |======                                                      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=========                                                   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===============                                             |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=====================                                       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===========================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==============================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==========================================                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===================================================         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |======================================================      |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=========================================================   |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m2_asi_c)\n\n\n\n\n\n\n\n\nWe can see that with 30 items, we would need only about 15 subjects to reach the 80 percent threshold.\nWe can also check the results in tabular form as shown below.\nThe results are shown below.\n\n# print results\nprint(pcurve_m2_asi_c)\n\nPower for predictor 'ConditionTest', (95% confidence interval),\nby number of levels in Subject:\n      5: 65.00% (40.78, 84.61) - 300 rows\n     10: 95.00% (75.13, 99.87) - 600 rows\n     15: 100.0% (83.16, 100.0) - 900 rows\n     20: 100.0% (83.16, 100.0) - 1200 rows\n     25: 100.0% (83.16, 100.0) - 1500 rows\n     30: 100.0% (83.16, 100.0) - 1800 rows\n\nTime elapsed: 0 h 0 m 12 s"
  },
  {
    "objectID": "tutorials/pwr/pwr.html#post-hoc-analyses",
    "href": "tutorials/pwr/pwr.html#post-hoc-analyses",
    "title": "Power Analysis in R",
    "section": "Post-Hoc Analyses",
    "text": "Post-Hoc Analyses\n\n\n\n\nNOTEPower analysis have also been used post-hoc to test if the sample size of studies was sufficient to detect meaningful effects. However, such post-hoc power calculations where the target effect size comes from the data, give misleading results (Hoenig and Heisey; Perugini, Gallucci, and Costantini) and should thus be treated with extreme care!\n\n\n\n\n\n\nWe begin by adding a response variable to our data. In this case, we vary the response variable ti a higher likelihood of obtaining gazes in the area of interests (AOI) in the test condition.\n\nsimdat2 &lt;- simdat %&gt;%\n  dplyr::arrange(Condition) %&gt;%\n  dplyr::mutate(\n    dep &lt;- c(\n      sample(c(\"yes\", \"no\"), 100, replace = T, prob = c(.5, .5)),\n      sample(c(\"yes\", \"no\"), 100, replace = T, prob = c(.7, .3))\n    )\n  ) %&gt;%\n  dplyr::mutate_if(is.character, factor) %&gt;%\n  dplyr::rename(AOI = 4)\n# inspect\nhead(simdat2, 20)\n\n   Subject Condition Item AOI\n1     Sub1   Control    1  no\n2     Sub1   Control    2 yes\n3     Sub1   Control    3  no\n4     Sub1   Control    4 yes\n5     Sub1   Control    5  no\n6     Sub1   Control    6 yes\n7     Sub1   Control    7  no\n8     Sub1   Control    8 yes\n9     Sub1   Control    9  no\n10    Sub1   Control   10 yes\n11    Sub2   Control    1  no\n12    Sub2   Control    2  no\n13    Sub2   Control    3 yes\n14    Sub2   Control    4  no\n15    Sub2   Control    5  no\n16    Sub2   Control    6 yes\n17    Sub2   Control    7  no\n18    Sub2   Control    8 yes\n19    Sub2   Control    9 yes\n20    Sub2   Control   10 yes\n\n\nNow that we have generated some data, we will fit a model to it and perform a power analysis on the observed effects.\nWe will fit a first model to the data. Thus, in a first step, we load the lme4 package to create a model, set a seed (to save the results and so that the results can be replicated), and then create an initial mixed-effects model.\n\n# set seed for replicability\nset.seed(12345)\n# fit model\nm3 &lt;- glmer(AOI ~ (1 | Subject) + (1 | Item) + Condition,\n  family = \"binomial\",\n  data = simdat2\n)\n# inspect results\nsummary(m3)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: AOI ~ (1 | Subject) + (1 | Item) + Condition\n   Data: simdat2\n\n     AIC      BIC   logLik deviance df.resid \n   259.3    272.5   -125.6    251.3      196 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.687 -1.151  0.593  0.869  0.869 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n Subject (Intercept) 0        0       \n Item    (Intercept) 0        0       \nNumber of obs: 200, groups:  Subject, 10; Item, 10\n\nFixed effects:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)      0.282      0.202    1.40    0.163  \nConditionTest    0.764      0.305    2.51    0.012 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nConditinTst -0.663\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nWe now check the effect sizes of the predictors in the model. We can do this by displaying the results of the model using the tab_model function from the sjPlot package.\n\n# tabulate results\nsjPlot::tab_model(m3)\n\n\n\n \nAOI\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.33\n0.89 – 1.97\n0.163\n\n\nCondition [Test]\n2.15\n1.18 – 3.90\n0.012\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Subject\n0.00\n\n\nτ00 Item\n0.00\n\n\nN Subject\n10\n\n\nN Item\n10\n\nObservations\n200\n\n\nMarginal R2 / Conditional R2\n0.043 / NA\n\n\n\n\n\n\nNow, we perform power analysis on an observed effect. This analysis tells us how likely the model is to find an observed effect given the data.\n\n\n\n\nNOTEWe use a very low number of simulations (10) and we use the default z-test which is suboptimal for small samples (Bolker et al.). In a proper study, you should increase the number of simulations (to at least 1000) and you should use a bootstrapping rather than a z-test (cf. Halekoh, Højsgaard, et al.).\n\n\n\n\n\n\nWhat is the probability of finding the observed effect given the data?\n\n# set seed for replicability\nset.seed(12345)\n# perform power analysis for present model\nm3_pwr &lt;- powerSim(m3,\n  fixed(\"ConditionTest\", \"z\"),\n  nsim = 20\n)\n# inspect results\nm3_pwr\n\nThe results of the power analysis show that, given the data at hand, the model would have detected the effect of Conidition:Test with a probability of m3_pwr$x percent. However, and as stated above, the results of such post-hoc power calculations (where the target effect size comes from the data) give misleading results (Hoenig and Heisey) and should thus be treated with extreme care!\n\nFixing effects\nWe will set the effects that we obtained based on our “observed” data to check if, given the size of the data, we have enough power to detect a small effect of Condition. In a first step, we check the observed effects.\n\nestimatesfixedeffects &lt;- fixef(m3)\nexp(estimatesfixedeffects)\n\n  (Intercept) ConditionTest \n        1.326         2.147 \n\n\nWe can see that the effect of Condition is rather small which makes it very hard to detect an effect. We will now change the size of the effect of ConditionTest to represent a truly small effect, i.e. on the brink of being noise but being just strong enough to be considered small. In other words, we will set the effect so that its odds ratio is exactly 1.68.\n\n# set seed for replicability\nset.seed(12345)\n# perform power analysis for small effect\nfixef(m3)[\"ConditionTest\"] &lt;- 0.519\nestimatesfixedeffects &lt;- fixef(m3)\nexp(estimatesfixedeffects)\n\n  (Intercept) ConditionTest \n        1.326         1.680 \n\n\nWhat is the probability of finding a weak effect given the data?\nWe now perform the power analysis.\n\n# set seed for replicability\nset.seed(12345)\n# perform power analysis for present model\nm3_pwr_se &lt;- powerSim(m3,\n  fixed(\"ConditionTest\", \"z\"),\n  nsim = 20\n)\n# inspect results\nm3_pwr_se\n\nThe data is not sufficient and would detect a weak effect of Condition with only 8 percent accuracy\n\n\nPower Analysis of Extended Data\nWe will now extend the data to see what sample size is needed to get to the 80 percent accuracy threshold. We begin by increasing the number of items from 10 to 30 to see if this would lead to a sufficient sample size.\n\n# increase sample size\nm3_ai &lt;- extend(m3,\n  along = \"Item\",\n  n = 30\n)\n# perform power simulation\nm3_ai_pwr &lt;- powerSim(m3_ai,\n  fixed(\"ConditionTest\", \"z\"),\n  nsim = 20\n)\n# show results\nm3_ai_pwr\n\nThe data with 30 Items is sufficient and would detect a weak effect of Condition with 18 percent accuracy\nAt what number of sentences are the data sufficient to detect an effect?\n\npcurve_m3_asi_c &lt;- powerCurve(m3_ai,\n  fixed(\"ConditionTest\", \"z\"),\n  along = \"Item\",\n  breaks = seq(5, 30, 5),\n  nsim = 20\n)\n\nSimulating: |                                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===                                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |======                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=========                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============                                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |================                                                  |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===================                                               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================                                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================                                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=============================                                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================                                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================                              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=======================================                           |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==========================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |=================================================                 |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |====================================================              |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |========================================================          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |===========================================================       |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==============================================================    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bSimulating: |==================================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) \b\b\b\b\b\b(1/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===                                                         |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |======                                                      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=========                                                   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |============                                                |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===============                                             |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==================                                          |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=====================                                       |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |========================                                    |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===========================                                 |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==============================                              |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=================================                           |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |====================================                        |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=======================================                     |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |==========================================                  |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |================================================            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |===================================================         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |======================================================      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |=========================================================   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(1/6) \b\b\b\b\b\b(2/6) \b\b\b\b\b\b(2/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===                                                         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |======                                                      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=========                                                   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===============                                             |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=====================                                       |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |========================                                    |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===========================                                 |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==============================                              |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=================================                           |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |====================================                        |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=======================================                     |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |==========================================                  |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |================================================            |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |===================================================         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |======================================================      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |=========================================================   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(2/6) \b\b\b\b\b\b(3/6) \b\b\b\b\b\b(3/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===                                                         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |======                                                      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=========                                                   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |============                                                |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===============                                             |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=====================                                       |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |========================                                    |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===========================                                 |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==============================                              |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=================================                           |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |====================================                        |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=======================================                     |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |==========================================                  |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=============================================               |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |================================================            |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |===================================================         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |======================================================      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |=========================================================   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(3/6) \b\b\b\b\b\b(4/6) \b\b\b\b\b\b(4/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===                                                         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |======                                                      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=========                                                   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |============                                                |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===============                                             |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=====================                                       |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |========================                                    |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===========================                                 |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==============================                              |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=================================                           |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |====================================                        |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=======================================                     |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |==========================================                  |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=============================================               |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |================================================            |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |===================================================         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |======================================================      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |=========================================================   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(4/6) \b\b\b\b\b\b(5/6) \b\b\b\b\b\b(5/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===                                                         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |======                                                      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=========                                                   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |============                                                |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===============                                             |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=====================                                       |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |========================                                    |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===========================                                 |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==============================                              |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=================================                           |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |====================================                        |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=======================================                     |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |==========================================                  |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=============================================               |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |================================================            |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |===================================================         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |======================================================      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |=========================================================   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(5/6) \b\b\b\b\b\b(6/6) \b\b\b\b\b\b(6/6) Simulating: |                                                            |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===                                                         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |======                                                      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=========                                                   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |============                                                |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===============                                             |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==================                                          |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=====================                                       |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |========================                                    |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===========================                                 |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==============================                              |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=================================                           |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |====================================                        |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=======================================                     |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |==========================================                  |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=============================================               |\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |================================================            |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |===================================================         |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |======================================================      |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |=========================================================   |\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) Simulating: |============================================================|\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b(6/6) \b\b\b\b\b\b\n\n# show plot\nplot(pcurve_m3_asi_c)\n\n\n\n\n\n\n\n\nWe reach the 80 percent threshold with about 25 subjects."
  },
  {
    "objectID": "tutorials/surveys/surveys.html#line-graphs-for-likert-scaled-data",
    "href": "tutorials/surveys/surveys.html#line-graphs-for-likert-scaled-data",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Line graphs for Likert-scaled data",
    "text": "Line graphs for Likert-scaled data\nA special case of line graphs is used when dealing with Likert-scaled variables (we will talk about Likert scales in more detail below). In such cases, the line graph displays the density of cumulative frequencies of responses. The difference between the cumulative frequencies of responses displays differences in preferences. We will only focus on how to create such graphs using the ggplot environment here as it has an in-build function (ecdf) which is designed to handle such data.\nIn a first step, we load a data set (ldat) which contains Likert-scaled variables. This data set represents fictitious rating of students from courses about how satisfied they were with their learning experience. The response to the Likert item is numeric so that strongly disagree/very dissatisfied would get the lowest (1) and strongly agree/very satisfied the highest numeric value (5).\n\n# define color vectors\nclrs3 &lt;- c(\"firebrick4\", \"gray70\", \"darkblue\")\nclrs5 &lt;- c(\"firebrick4\", \"firebrick1\", \"gray70\", \"blue\", \"darkblue\")\n# load data\nldat &lt;- base::readRDS(\"tutorials/surveys/data/lid.rda\", \"rb\")\n\nLet’s briefly inspect the ldat data set.\n\n\nCourseSatisfactionChinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1Chinese1\n\n\nThe ldat data set has only two columns: a column labeled Course which has three levels (German, Japanese, and Chinese) and a column labeled Satisfaction which contains values from 1 to 5 which represent values ranging from very dissatisfied to very satisfied. Now that we have data resembling a Likert-scaled item from a questionnaire, we will display the data in a cumulative line graph.\n\n# create cumulative density plot\nldat %&gt;%\n  ggplot(aes(x = Satisfaction, color = Course)) +\n  geom_step(aes(y = ..y..), stat = \"ecdf\") +\n  labs(y = \"Cumulative Density\") +\n  scale_x_discrete(\n    limits = c(\"1\", \"2\", \"3\", \"4\", \"5\"),\n    breaks = c(1, 2, 3, 4, 5),\n    labels = c(\n      \"very dissatisfied\", \"dissatisfied\",\n      \"neutral\", \"satisfied\", \"very satisfied\"\n    )\n  ) +\n  scale_colour_manual(values = clrs3) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe satisfaction of the German course was the lowest as the red line shows the highest density (frequency of responses) of very dissatisfied and dissatisfied ratings. The students in our fictitious data set were most satisfied with the Chinese course as the blue line is the lowest for very dissatisfied and “dissatisfied” ratings while the difference between the courses shrinks for “satisfied” and very satisfied. The Japanese language course is in-between the German and the Chinese course."
  },
  {
    "objectID": "tutorials/surveys/surveys.html#pie-charts",
    "href": "tutorials/surveys/surveys.html#pie-charts",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Pie charts",
    "text": "Pie charts\nMost commonly, the data for visualization comes from tables of absolute frequencies associated with a categorical or nominal variable. The default way to visualize such frequency tables are pie charts and bar plots. In a first step, we modify the data to get counts and percentages.\n\n# create bar plot data\nbdat &lt;- ldat %&gt;%\n  dplyr::group_by(Satisfaction) %&gt;%\n  dplyr::summarise(Frequency = n()) %&gt;%\n  dplyr::mutate(Percent = round(Frequency / sum(Frequency) * 100, 1)) %&gt;%\n  # order the levels of Satisfaction manually so that the order is not alphabetical\n  dplyr::mutate(Satisfaction = factor(Satisfaction,\n    levels = 1:5,\n    labels = c(\n      \"very dissatisfied\",\n      \"dissatisfied\",\n      \"neutral\",\n      \"satisfied\",\n      \"very satisfied\"\n    )\n  ))\n\nLet’s briefly inspect the new data set.\n\n\nSatisfactionFrequencyPercentvery dissatisfied7023.3dissatisfied7023.3neutral6020.0satisfied5016.7very satisfied5016.7\n\n\nBefore creating bar plots, we will briefly turn to pie charts because pie charts are very common despite suffering from certain shortcomings. Consider the following example which highlights some of the issues that arise when using pie charts.\n# create pie chart\nbdat %&gt;%\n  ggplot(aes(\"\", Percent, fill = Satisfaction)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  scale_fill_manual(values = clrs5) +\n  theme_void()\n\nIf the slices of the pie chart are not labelled, it is difficult to see which slices are smaller or bigger compared to other slices. This problem can easily be avoided when using a bar plot instead. This issue can be avoided by adding labels to pie charts. The labeling of pie charts is, however, somewhat tedious as the positioning is tricky. Below is an example for adding labels without specification.\n# create pie chart\nbdat %&gt;%\n  ggplot(aes(\"\", Percent, fill = Satisfaction)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  scale_fill_manual(values = clrs5) +\n  theme_void() +\n  geom_text(aes(y = Percent, label = Percent), color = \"white\", size = 6)\n\nTo place the labels where they make sense, we will add another variable to the data called “Position”.\npdat &lt;- bdat %&gt;%\n  dplyr::arrange(desc(Satisfaction)) %&gt;%\n  dplyr::mutate(Position = cumsum(Percent) - 0.5 * Percent)\nLet’s briefly inspect the new data set.\n\n\nSatisfactionFrequencyPercentPositionvery satisfied5016.78.35satisfied5016.725.05neutral6020.043.40dissatisfied7023.365.05very dissatisfied7023.388.35\n\n\nNow that we have specified the position, we can include it into the pie chart.\n# create pie chart\npdat %&gt;%\n  ggplot(aes(\"\", Percent, fill = Satisfaction)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  scale_fill_manual(values = clrs5) +\n  theme_void() +\n  geom_text(aes(y = Position, label = Percent), color = \"white\", size = 6)\n\nWe will now create separate pie charts for each course. In a first step, we create a data set that does not only contain the Satisfaction levels and their frequency but also the course.\n\n# create grouped pie data\ngldat &lt;- ldat %&gt;%\n  dplyr::group_by(Course, Satisfaction) %&gt;%\n  dplyr::summarise(Frequency = n()) %&gt;%\n  dplyr::mutate(\n    Percent = round(Frequency / sum(Frequency) * 100, 1),\n    Satisfaction = factor(Satisfaction,\n      levels = 1:5,\n      labels = c(\n        \"very dissatisfied\",\n        \"dissatisfied\",\n        \"neutral\",\n        \"satisfied\",\n        \"very satisfied\"\n      )\n    )\n  ) %&gt;%\n  dplyr::arrange(desc(Satisfaction)) %&gt;%\n  dplyr::mutate(Position = cumsum(Percent) - 0.5 * Percent)\n\nLet’s briefly inspect the new data set.\n\n\nCourseSatisfactionFrequencyPercentPositionChinesevery satisfied15157.5Germanvery satisfied552.5Japanesevery satisfied303015.0Chinesesatisfied101020.0Germansatisfied151512.5Japanesesatisfied252542.5Chineseneutral252537.5Germanneutral151527.5Japaneseneutral202065.0Chinesedissatisfied303065.0Germandissatisfied252547.5Japanesedissatisfied151582.5Chinesevery dissatisfied202090.0Germanvery dissatisfied404080.0Japanesevery dissatisfied101095.0\n\n\nNow that we have created the data, we can plot separate pie charts for each course.\n# create pie chart\ngldat %&gt;%\n  ggplot(aes(\"\", Percent, fill = Satisfaction)) +\n  facet_wrap(~Course) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  scale_fill_manual(values = clrs5) +\n  theme_void() +\n  geom_text(aes(y = Position, label = Percent), color = \"white\", size = 4)"
  },
  {
    "objectID": "tutorials/surveys/surveys.html#bar-plots",
    "href": "tutorials/surveys/surveys.html#bar-plots",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Bar plots",
    "text": "Bar plots\nLike pie charts, bar plot display frequency information across categorical variable levels.\n\n# bar plot\nbdat %&gt;%\n  ggplot(aes(Satisfaction, Percent, fill = Satisfaction)) +\n  # determine type of plot\n  geom_bar(stat = \"identity\") +\n  # use black & white theme\n  theme_bw() +\n  # add and define text\n  geom_text(aes(y = Percent - 5, label = Percent), color = \"white\", size = 3) +\n  # add colors\n  scale_fill_manual(values = clrs5) +\n  # suppress legend\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nCompared with the pie chart, it is much easier to grasp the relative size and order of the percentage values which shows that pie charts are unfit to show relationships between elements in a graph and, as a general rule of thumb, should be avoided.\nBar plots can be grouped which adds another layer of information that is particularly useful when dealing with frequency counts across multiple categorical variables. But before we can create grouped bar plots, we need to create an appropriate data set.\n\n# create bar plot data\ngldat &lt;- ldat %&gt;%\n  dplyr::group_by(Course, Satisfaction) %&gt;%\n  dplyr::summarise(Frequency = n()) %&gt;%\n  dplyr::mutate(Percent = round(Frequency / sum(Frequency) * 100, 1)) %&gt;%\n  dplyr::mutate(Satisfaction = factor(Satisfaction,\n    levels = 1:5,\n    labels = c(\n      \"very dissatisfied\",\n      \"dissatisfied\",\n      \"neutral\",\n      \"satisfied\",\n      \"very satisfied\"\n    )\n  ))\n\nLet’s briefly inspect the data set.\n\n\nCourseSatisfactionFrequencyPercentChinesevery dissatisfied2020Chinesedissatisfied3030Chineseneutral2525Chinesesatisfied1010Chinesevery satisfied1515Germanvery dissatisfied4040Germandissatisfied2525Germanneutral1515Germansatisfied1515Germanvery satisfied55Japanesevery dissatisfied1010Japanesedissatisfied1515Japaneseneutral2020Japanesesatisfied2525Japanesevery satisfied3030\n\n\nWe have now added Course as an additional categorical variable and will include Course as the “fill” argument in our bar plot. To group the bars, we use the command “position=position_dodge()”.\n\n# bar plot\ngldat %&gt;%\n  ggplot(aes(Satisfaction, Frequency, fill = Course)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  # define colors\n  scale_fill_manual(values = clrs3) +\n  # add text\n  geom_text(aes(label = Frequency),\n    vjust = 1.6, color = \"white\",\n    # define text position and size\n    position = position_dodge(0.9), size = 3.5\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nBar plots are particularly useful when visualizing data obtained through Likert items. As this is a very common issue that empirical researchers face. There are two basic ways to display Likert items using bar plots: grouped bar plots and more elaborate scaled bar plots.\nAlthough we have seen above how to create grouped bar plots, we will repeat it here with the language course example used above when we used cumulative density line graphs to visualize how to display Likert data.\nIn a first step, we recreate the data set which we have used above. The data set consists of a Likert-scaled variable (Satisfaction) which represents rating of students from three courses about how satisfied they were with their language-learning course. The response to the Likert item is numeric so that “strongly disagree/very dissatisfied” would get the lowest and “strongly agree/very satisfied” the highest numeric value.\nAgain, we can also plot separate bar graphs for each class by specifying “facets”.\n\n# create grouped bar plot\ngldat %&gt;%\n  ggplot(aes(Satisfaction, Frequency,\n    fill = Satisfaction,\n    color = Satisfaction\n  )) +\n  facet_grid(~Course) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_line() +\n  # define colors\n  scale_fill_manual(values = clrs5) +\n  scale_color_manual(values = clrs5) +\n  # add text and define color\n  geom_text(aes(label = Frequency),\n    vjust = 1.6, color = \"white\",\n    # define text position and size\n    position = position_dodge(0.9), size = 3.5\n  ) +\n  theme_bw() +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nAnother and very interesting way to display such data is by using the Likert package. In a first step, we need to activate the package, clean the data, and extract a subset for the data visualization example.\nOne aspect that is different to previous visualizations is that, when using the Likert package, we need to transform the data into a “likert” object (which is, however, very easy and is done by using the “likert()” function as shown below).\n\nsdat &lt;- base::readRDS(\"tutorials/surveys/data/sdd.rda\", \"rb\")\n\n\n\nGroupRespondentHow.did.you.like.the.course.How.did.you.like.the.teacher.Was.the.content.intersting.Was.the.content.adequate.for.the.course.Were.there.enough.discussions.Was.the.use.of.online.materials.appropriate.Was.the.teacher.appropriately.prepared.Was.the.workload.of.the.course.appropriate.Was.the.course.content.enganging.Were.there.enough.interactive.exerceises.included.in.the.sessions.GermanG14444444444GermanG24534445313GermanG35342434431GermanG43333333333GermanG51111111111GermanG63132333333GermanG75342433445GermanG85555555555GermanG95133445531GermanG103333333333\n\n\nAs you can see, we need to clean and adapt the column names. To do this, we will\n\nadd an identifier which shows which question we are dealing with (e.g. Q 1: question text)\nremove the dots between words with spaces\nadd a question mark at the end of questions\nremove superfluous white spaces\n\n\n# clean column names\ncolnames(sdat)[3:ncol(sdat)] &lt;- paste0(\"Q \", str_pad(1:10, 2, \"left\", \"0\"), \": \", colnames(sdat)[3:ncol(sdat)]) %&gt;%\n  stringr::str_replace_all(\"\\\\.\", \" \") %&gt;%\n  stringr::str_squish() %&gt;%\n  stringr::str_replace_all(\"$\", \"?\")\n# inspect column names\ncolnames(sdat)\n\n [1] \"Group\"                                                                   \n [2] \"Respondent\"                                                              \n [3] \"Q 01: How did you like the course?\"                                      \n [4] \"Q 02: How did you like the teacher?\"                                     \n [5] \"Q 03: Was the content intersting?\"                                       \n [6] \"Q 04: Was the content adequate for the course?\"                          \n [7] \"Q 05: Were there enough discussions?\"                                    \n [8] \"Q 06: Was the use of online materials appropriate?\"                      \n [9] \"Q 07: Was the teacher appropriately prepared?\"                           \n[10] \"Q 08: Was the workload of the course appropriate?\"                       \n[11] \"Q 09: Was the course content enganging?\"                                 \n[12] \"Q 10: Were there enough interactive exerceises included in the sessions?\"\n\n\nNow, that we have nice column names, we will replace the numeric values (1 to 5) with labels ranging from disagree to agree and convert our data into a data frame.\n\nlbs &lt;- c(\"disagree\", \"somewhat disagree\", \"neither agree nor disagree\", \"somewhat agree\", \"agree\")\nsurvey &lt;- sdat %&gt;%\n  dplyr::mutate_if(is.character, factor) %&gt;%\n  dplyr::mutate_if(is.numeric, factor, levels = 1:5, labels = lbs) %&gt;%\n  drop_na() %&gt;%\n  as.data.frame()\n\n\n\nGroupRespondentQ 01: How did you like the course?Q 02: How did you like the teacher?Q 03: Was the content intersting?Q 04: Was the content adequate for the course?Q 05: Were there enough discussions?Q 06: Was the use of online materials appropriate?Q 07: Was the teacher appropriately prepared?Q 08: Was the workload of the course appropriate?Q 09: Was the course content enganging?Q 10: Were there enough interactive exerceises included in the sessions?GermanG1somewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreesomewhat agreeGermanG2somewhat agreeagreeneither agree nor disagreesomewhat agreesomewhat agreesomewhat agreeagreeneither agree nor disagreedisagreeneither agree nor disagreeGermanG3agreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeneither agree nor disagreesomewhat agreesomewhat agreeneither agree nor disagreedisagreeGermanG4neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG5disagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreedisagreeGermanG6neither agree nor disagreedisagreeneither agree nor disagreesomewhat disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeGermanG7agreeneither agree nor disagreesomewhat agreesomewhat disagreesomewhat agreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat agreeagreeGermanG8agreeagreeagreeagreeagreeagreeagreeagreeagreeagreeGermanG9agreedisagreeneither agree nor disagreeneither agree nor disagreesomewhat agreesomewhat agreeagreeagreeneither agree nor disagreedisagreeGermanG10neither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagreeneither agree nor disagree\n\n\nNow, we can use the plot and the likert function to visualize the survey data.\n\nplot(likert(survey[, 3:12]), ordered = F, wrap = 60)\n\n\n\n\n\n\n\n\nTo save this plot, you can use the save_plot function from the cowplot package as shown below.\n\nsurvey_p1 &lt;- plot(likert(survey[, 3:12]), ordered = F, wrap = 60)\n# save plot\ncowplot::save_plot(here(\"images\", \"stu_p1.png\"), # where to save the plot\n  survey_p1, # object to plot\n  base_asp = 1.5, # ratio of space fro questions vs space for plot\n  base_height = 8\n) # size! higher for smaller font size\n\nAn additional and very helpful feature is that the likert package enables grouping the data as shown below. The display columns 3 to 8 and use column 1 for grouping.\n\n# create plot\nplot(likert(survey[, 3:8], grouping = survey[, 1]))"
  },
  {
    "objectID": "tutorials/surveys/surveys.html#evaluating-the-reliability-of-questions",
    "href": "tutorials/surveys/surveys.html#evaluating-the-reliability-of-questions",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Evaluating the reliability of questions",
    "text": "Evaluating the reliability of questions\n\nCronbach’s Alpha\nOftentimes several questions in one questionnaire aim to tap into the same cognitive concept or attitude or whatever we are interested in. The answers to these related questions should be internally consistent, i.e. the responses should correlate strongly and positively.\nCronbach’s \\(\\alpha\\) (Cronbach) is measure of internal consistency or reliability that provides information on how strongly the responses to a set of questions correlate. The formula for Cronbach’s \\(\\alpha\\) is shown below (N: number of items, \\(\\bar c\\): average inter-item co-variance among items, \\(\\bar v\\): average variance).\n\\(\\alpha = \\frac{N*\\bar c}{\\bar v + (N-1)\\bar c}\\)\nIf the values for Cronbach’s \\(\\alpha\\) are low (below .7), then this indicates that the questions are not internally consistent (and do not tap into the same concept) or that the questions are not uni-dimensional (as they should be).\nWhile Cronbach’s \\(\\alpha\\) is the most frequently used measures of reliability (probably because it is conceptually simple and can be computed very easily), it underestimates the reliability of a test and overestimates the first factor saturation. This can be a problem is the data is lumpy. Thus, various other measures of reliability have been proposed. Also,Cronbach’s \\(\\alpha\\) assumes that scale items are repeated measurements, an assumption that is often violated.\nAn alternative reliability measure that takes the amount of variance per item into account and thus performs better when dealing with lumpy data (although it is still affected by lumpiness) is Guttman’s Lambda 6 (G6) (Guttman). In contrast to Cronbach’s \\(\\alpha\\), G6 is mostly used to evaluate the reliability of individual test items though. This means that it provides information about how well individual questions reflect the concept that they aim to tap into.\nProbably the best measures of reliability are \\(\\omega\\) (omega) measures. Hierarchical \\(\\omega\\) provides more appropriate estimates of the general factor saturation while total \\(\\omega\\) is a better estimate of the reliability of the total test compared to both Cronbach’s \\(\\alpha\\) and G6 (Revelle and Zinbarg).\nCalculating Cronbach’s alpha in R\nWe will now calculate Cronbach’s \\(\\alpha\\) in R. In a first step, we activate the “psych” package and load as well as inspect the data.\n\n# load data\nsurveydata &lt;- base::readRDS(\"tutorials/surveys/data/sud.rda\", \"rb\")\n\n\n\nRespondentQ01_OutgoingQ02_OutgoingQ03_OutgoingQ04_OutgoingQ05_OutgoingQ06_IntelligenceQ07_IntelligenceQ08_IntelligenceQ09_IntelligenceQ10_IntelligenceQ11_AttitudeQ12_AttitudeQ13_AttitudeQ14_AttitudeQ15_AttitudeRespondent_01454452332233233Respondent_02545442221244454Respondent_03544552112254444Respondent_04555451111154555Respondent_05454552212145455Respondent_06555545452212121Respondent_07454554544521121Respondent_08445455454512112Respondent_09554445544512212Respondent_10455444454521221Respondent_11222114555412221Respondent_12332325555412111Respondent_13322234345422122Respondent_14222113435412111Respondent_15122211222543445Respondent_16111122321434434Respondent_17222222222211122Respondent_18111111222555555Respondent_19223232111545545Respondent_20111122322555555\n\n\nThe inspection of the data shows that the responses of participants represent the rows and that the questions represent columns. The column names show that we have 15 questions and that the first five questions aim to test how outgoing respondents are. To check if the first five questions reliably test “outgoingness” (or “extraversion”), we calculate Cronbach’s alpha for these five questions.\nThus, we use the “alpha()” function and provide the questions that tap into the concept we want to assess. In addition to Cronbach’s \\(\\alpha\\), the “alpha()” function also reports Guttman’s lambda_6 which is an alternative measure for reliability. This is an advantage because Cronbach’s \\(\\alpha\\) underestimates the reliability of a test and overestimates the first factor saturation.\n\n# calculate cronbach's alpha\nCronbach &lt;- psych::alpha(surveydata[c(\n  \"Q01_Outgoing\",\n  \"Q02_Outgoing\",\n  \"Q03_Outgoing\",\n  \"Q04_Outgoing\",\n  \"Q05_Outgoing\"\n)], check.keys = F)\n# inspect results\nCronbach\n\n\nReliability analysis   \nCall: psych::alpha(x = surveydata[c(\"Q01_Outgoing\", \"Q02_Outgoing\", \n    \"Q03_Outgoing\", \"Q04_Outgoing\", \"Q05_Outgoing\")], check.keys = F)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.98      0.98    0.97      0.89  42 0.0083  3.1 1.5      0.9\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.96  0.98  0.99\nDuhachek  0.96  0.98  0.99\n\n Reliability if an item is dropped:\n             raw_alpha std.alpha G6(smc) average_r S/N alpha se   var.r med.r\nQ01_Outgoing      0.97      0.97    0.97      0.89  33   0.0108 0.00099  0.89\nQ02_Outgoing      0.97      0.97    0.96      0.89  31   0.0116 0.00054  0.89\nQ03_Outgoing      0.97      0.97    0.97      0.90  35   0.0104 0.00095  0.90\nQ04_Outgoing      0.97      0.97    0.96      0.89  31   0.0115 0.00086  0.89\nQ05_Outgoing      0.98      0.98    0.97      0.91  41   0.0088 0.00034  0.91\n\n Item statistics \n              n raw.r std.r r.cor r.drop mean  sd\nQ01_Outgoing 20  0.96  0.96  0.95   0.94  3.1 1.5\nQ02_Outgoing 20  0.97  0.97  0.96   0.95  3.2 1.6\nQ03_Outgoing 20  0.95  0.95  0.94   0.93  3.1 1.5\nQ04_Outgoing 20  0.97  0.97  0.96   0.95  3.0 1.6\nQ05_Outgoing 20  0.94  0.94  0.91   0.90  3.2 1.6\n\nNon missing response frequency for each item\n                1   2    3    4    5 miss\nQ01_Outgoing 0.20 0.2 0.10 0.25 0.25    0\nQ02_Outgoing 0.15 0.3 0.05 0.15 0.35    0\nQ03_Outgoing 0.15 0.3 0.05 0.25 0.25    0\nQ04_Outgoing 0.25 0.2 0.05 0.30 0.20    0\nQ05_Outgoing 0.20 0.2 0.10 0.20 0.30    0\n\n\nThe output of the “alpha()” function is rather extensive and we will only interpret selected output here.\nThe value under alpha is Cronbach’s \\(\\alpha\\) and it should be above 0.7. The values to its left and right are the lower and upper bound of its confidence interval. The values in the column with the header “G6” show how well each question represents the concept it aims to reflect. Low values indicate that the question does not reflect the underlying concept while high values (.7 and higher) indicate that the question captures that concept well (or to an acceptable degree).\n\n\nOmega\nThe omega (\\(\\omega\\)) coefficient is also a reliability measure of internal consistency. \\(\\omega\\) represents an estimate of the general factor saturation of a test that was proposed by McDonald. (R. E. Zinbarg et al.) compare McDonald’s Omega to Cronbach’s \\(\\alpha\\) and Revelle’s \\(\\beta\\). They conclude that omega is the best estimate (R. Zinbarg et al.).\nA very handy way to calculate McDonald’s \\(\\omega\\) is to use the scaleReliability() function from the userfriendlyscience package (which also provides Cronbach’s \\(\\alpha\\) and the Greatest Lower Bound (GLB) estimate which is also a very good and innovative measure of reliability) (see also Peters).\n\n# activate package\nlibrary(ufs)\n# extract reliability measures\nreliability &lt;- ufs::scaleStructure(surveydata[c(\n  \"Q01_Outgoing\",\n  \"Q02_Outgoing\",\n  \"Q03_Outgoing\",\n  \"Q04_Outgoing\",\n  \"Q05_Outgoing\"\n)])\n# inspect results\nprint(reliability)\n\n\nInformation about this analysis:\n\n                 Dataframe: surveydata[c(\"Q01_Outgoing\", \"Q02_Outgoing\", \"Q03_Outgoing\", \n                     Items: all\n              Observations: 20\n     Positive correlations: 10 out of 10 (100%)\n\nEstimates assuming interval level:\n \nInformation about this analysis:\n\n                 Dataframe:     \"Q04_Outgoing\", \"Q05_Outgoing\")]\n                     Items: all\n              Observations: 20\n     Positive correlations: 10 out of 10 (100%)\n\nEstimates assuming interval level:\n\n             Omega (total): 0.98\n      Omega (hierarchical): 0.95\n   Revelle's omega (total): 0.98\nGreatest Lower Bound (GLB): NA\n             Coefficient H: 0.98\n         Coefficient alpha: 0.98\n\n(Estimates assuming ordinal level not computed, as the polychoric correlation matrix has missing values.)\n\nNote: the normal point estimate and confidence interval for omega are based on the procedure suggested by Dunn, Baguley & Brunsden (2013) using the MBESS function ci.reliability, whereas the psych package point estimate was suggested in Revelle & Zinbarg (2008). See the help ('?scaleStructure') for more information."
  },
  {
    "objectID": "tutorials/surveys/surveys.html#factor-analysis",
    "href": "tutorials/surveys/surveys.html#factor-analysis",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Factor analysis",
    "text": "Factor analysis\nWhen dealing with many variables it is often the case that several variables are related and represent a common, underlying factor. To find such underlying factors, we can use a factor analysis.\nFactor analysis is a method that allows to find commonalities or structure in data. This is particularly useful when dealing with many variables. Factors can be considered hidden latent variables or driving forces that affect or underlie several variables at once.\nThis becomes particularly apparent when considering socio-demographic variables as behaviors are not only dependent on single variables, e.g., economic status, but on the interaction of several additional variables such as education level, marital status, number of children, etc. All of these variables can be combined into a single factor (or hidden latent variable).\n\n# remove respondent\nsurveydata &lt;- surveydata %&gt;%\n  dplyr::select(-Respondent)\nfactoranalysis &lt;- factanal(surveydata, 3, rotation = \"varimax\")\nprint(factoranalysis, digits = 2, cutoff = .2, sort = TRUE)\n\n\nCall:\nfactanal(x = surveydata, factors = 3, rotation = \"varimax\")\n\nUniquenesses:\n    Q01_Outgoing     Q02_Outgoing     Q03_Outgoing     Q04_Outgoing \n            0.09             0.06             0.12             0.07 \n    Q05_Outgoing Q06_Intelligence Q07_Intelligence Q08_Intelligence \n            0.14             0.10             0.13             0.10 \nQ09_Intelligence Q10_Intelligence     Q11_Attitude     Q12_Attitude \n            0.28             0.41             0.08             0.14 \n    Q13_Attitude     Q14_Attitude     Q15_Attitude \n            0.04             0.09             0.06 \n\nLoadings:\n                 Factor1 Factor2 Factor3\nQ06_Intelligence -0.82    0.25    0.41  \nQ07_Intelligence -0.80            0.47  \nQ08_Intelligence -0.85            0.42  \nQ09_Intelligence -0.79            0.29  \nQ11_Attitude      0.96                  \nQ12_Attitude      0.92                  \nQ13_Attitude      0.97                  \nQ14_Attitude      0.95                  \nQ15_Attitude      0.96                  \nQ01_Outgoing              0.94          \nQ02_Outgoing              0.96          \nQ03_Outgoing              0.93          \nQ04_Outgoing              0.96          \nQ05_Outgoing              0.92          \nQ10_Intelligence -0.22   -0.46    0.57  \n\n               Factor1 Factor2 Factor3\nSS loadings       7.29    4.78    1.02\nProportion Var    0.49    0.32    0.07\nCumulative Var    0.49    0.80    0.87\n\nTest of the hypothesis that 3 factors are sufficient.\nThe chi square statistic is 62.79 on 63 degrees of freedom.\nThe p-value is 0.484 \n\n\nThe results of a factor analysis can be visualized so that questions which reflect the same underlying factor are grouped together.\n\n# plot factor 1 by factor 2\nload &lt;- factoranalysis$loadings[, 1:2]\n# set up plot\nplot(load, type = \"n\", xlim = c(-1.5, 1.5))\n# add variable names\ntext(load,\n  # define labels\n  labels = names(surveydata),\n  # define font size\n  # (smaller than default = values smaller than 1)\n  cex = .7\n)\n\n\n\n\n\n\n\n\nThe plot shows that the questions form groups which indicates that the questions do a rather good job at reflecting the concepts that they aim to tap into. The only problematic question is question 10 (Q10) which aimed to tap into the intelligence of respondents but appears not to correlate strongly with the other questions that aim to extract information about the respondents intelligence. In such cases, it makes sense, to remove a question (in this case Q10) from the survey as it does not appear to reflect what we wanted it to."
  },
  {
    "objectID": "tutorials/surveys/surveys.html#principle-component-analysis",
    "href": "tutorials/surveys/surveys.html#principle-component-analysis",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Principle component analysis",
    "text": "Principle component analysis\nPrincipal component analysis is used when several questions or variables reflect a common factor and they should be combined into a single variable, e.g. during the statistical analysis of the data. Thus, principal component analysis can be used to collapse different variables (or questions) into one.\nImagine you have measured lengths of sentences in different ways (in words, syllables, characters, time it takes to pronounce, etc.). You could combine all these different measures of length by applying a PCA to those measures and using the first principal component as a single proxy for all these different measures.\n\n# entering raw data and extracting PCs  from the correlation matrix\nPrincipalComponents &lt;- princomp(surveydata[c(\n  \"Q01_Outgoing\",\n  \"Q02_Outgoing\",\n  \"Q03_Outgoing\",\n  \"Q04_Outgoing\",\n  \"Q05_Outgoing\"\n)], cor = TRUE)\nsummary(PrincipalComponents) # print variance accounted for\n\nImportance of components:\n                       Comp.1  Comp.2  Comp.3  Comp.4  Comp.5\nStandard deviation     2.1399 0.41221 0.33748 0.29870 0.21818\nProportion of Variance 0.9159 0.03398 0.02278 0.01784 0.00952\nCumulative Proportion  0.9159 0.94986 0.97264 0.99048 1.00000\n\n\nThe output shows that the first component (Comp.1) explains 91.58 percent of the variance. This shows that we only lose 8.42 percent of the variance if we use this component as a proxy for “outgoingness” if we use the collapsed component rather than the five individual items.\n\nloadings(PrincipalComponents) # pc loadings\n\n\nLoadings:\n             Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nQ01_Outgoing  0.448  0.324         0.831       \nQ02_Outgoing  0.453  0.242 -0.408 -0.360  0.663\nQ03_Outgoing  0.446  0.405  0.626 -0.405 -0.286\nQ04_Outgoing  0.452 -0.191 -0.568 -0.114 -0.650\nQ05_Outgoing  0.437 -0.798  0.342         0.230\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n\n\nWe now check if the five questions that are intended to tap into “outgoingness” represent one (and not more) underlying factors. Do check this, we create a scree plot.\n\nplot(PrincipalComponents, type = \"lines\") # scree plot\n\n\n\n\n\n\n\n\nThe scree plot shown above indicates that we only need a single component to explain the variance as there is a steep decline from the first to the second component. This confirms that the questions that tap into “outgoingness” represent one (and not more) underlying factors.\n\nPrincipalComponents$scores # the principal components\n\n       Comp.1   Comp.2   Comp.3    Comp.4   Comp.5\n [1,]  1.8382 -0.36615 -0.05472 -0.185983  0.45152\n [2,]  1.8663  0.49141  0.43588  0.293521 -0.29327\n [3,]  2.1436 -0.43110 -0.14566  0.529200 -0.37631\n [4,]  2.4440  0.12865  0.39433  0.093406  0.28596\n [5,]  2.1362 -0.49210 -0.42957 -0.260901  0.02262\n [6,]  2.4574  0.52188 -0.20319 -0.014602 -0.29283\n [7,]  2.1362 -0.49210 -0.42957 -0.260901  0.02262\n [8,]  1.8506 -0.24517  0.63889 -0.230286 -0.17380\n [9,]  1.8538  0.37043 -0.25773  0.337824  0.33205\n[10,]  1.8589  0.43041  0.15198 -0.496581  0.10566\n[11,] -2.2853  0.62953  0.07366  0.047246  0.18177\n[12,] -0.8112  0.23229 -0.69790  0.254192 -0.06639\n[13,] -1.1176 -0.31735  0.16386  0.595405  0.08306\n[14,] -2.2853  0.62953  0.07366  0.047246  0.18177\n[15,] -2.2876  0.28617 -0.32086 -0.584569 -0.27755\n[16,] -2.8995 -0.54086  0.11152  0.034152  0.06787\n[17,] -1.7026 -0.01559 -0.07850  0.005418 -0.09725\n[18,] -3.1841 -0.02169 -0.11116  0.001061 -0.08202\n[19,] -1.1125 -0.25737  0.57357 -0.238999 -0.14334\n[20,] -2.8995 -0.54086  0.11152  0.034152  0.06787\n\n\nYou could now replace the five items which tap into “outgoingness” with the single first component shown in the table above."
  },
  {
    "objectID": "tutorials/surveys/surveys.html#ordinal-regression",
    "href": "tutorials/surveys/surveys.html#ordinal-regression",
    "title": "Questionnaires and Surveys: Analyses with R",
    "section": "Ordinal Regression",
    "text": "Ordinal Regression\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable (Agresti). For this reason, ordinal regression is one of the key methods in analyzing Likert data.\nTo see how an ordinal regression is implemented in R, we load and inspect the “ordinaldata” data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students’ final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (“very likely”, “somewhat likely”, and “unlikely”) and reflects the committees’ assessment of whether the student is likely to succeed in the program.\n\n# load data\nordata &lt;- base::readRDS(\"tutorials/surveys/data/oda.rda\", \"rb\")\n# inspect data\nstr(ordata)\n\n'data.frame':   400 obs. of  4 variables:\n $ Recommend : chr  \"very likely\" \"somewhat likely\" \"unlikely\" \"somewhat likely\" ...\n $ Internal  : int  0 1 1 0 0 0 0 0 0 1 ...\n $ Exchange  : int  0 0 1 0 0 1 0 0 0 0 ...\n $ FinalScore: num  3.26 3.21 3.94 2.81 2.53 ...\n\n\nIn a first step, we need to re-level the ordinal variable to represent an ordinal factor (or a progression from “unlikely” over “somewhat likely” to “very likely”. And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n# relevel data\nordata &lt;- ordata %&gt;%\n  dplyr::mutate(Recommend = factor(Recommend,\n    levels = c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n    labels = c(\"unlikely\", \"somewhat likely\", \"very likely\")\n  )) %&gt;%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %&gt;%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n\nNow that the dependent variable is re-leveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\nsummary(ordata$FinalScore)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.90    2.72    2.99    3.00    3.27    4.00 \n\nsd(ordata$FinalScore)\n\n[1] 0.3979\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n# visualize data\nordata %&gt;%\n  ggplot(aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\n\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modelling by using the “polr” function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course.\n\n## fit ordered logit model and store results 'm'\nm &lt;- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess = TRUE)\n## view a summary of the model\nsummary(m)\n\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                    Value Std. Error t value\nInternalInternal   1.0477      0.266   3.942\nExchangeNoExchange 0.0587      0.298   0.197\nFinalScore         0.6157      0.261   2.363\n\nIntercepts:\n                            Value Std. Error t value\nunlikely|somewhat likely    2.262 0.882      2.564  \nsomewhat likely|very likely 4.357 0.904      4.818  \n\nResidual Deviance: 717.02 \nAIC: 727.02 \n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n## store table\n(ctable &lt;- coef(summary(m)))\n\n                              Value Std. Error t value\nInternalInternal            1.04766     0.2658   3.942\nExchangeNoExchange          0.05868     0.2979   0.197\nFinalScore                  0.61574     0.2606   2.363\nunlikely|somewhat likely    2.26200     0.8822   2.564\nsomewhat likely|very likely 4.35744     0.9045   4.818\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n## calculate and store p values\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable &lt;- cbind(ctable, \"p value\" = p))\n\n                              Value Std. Error t value     p value\nInternalInternal            1.04766     0.2658   3.942 0.000080902\nExchangeNoExchange          0.05868     0.2979   0.197 0.843819939\nFinalScore                  0.61574     0.2606   2.363 0.018151727\nunlikely|somewhat likely    2.26200     0.8822   2.564 0.010343823\nsomewhat likely|very likely 4.35744     0.9045   4.818 0.000001452\n\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n# extract profiled confidence intervals\nci &lt;- confint(m)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m), ci))\n\n                      OR 2.5 % 97.5 %\nInternalInternal   2.851 1.696  4.817\nExchangeNoExchange 1.060 0.595  1.920\nFinalScore         1.851 1.114  3.098\n\n\nThe odds ratios show that internal students are 2.85 or 285 percent more likely compared to non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant.\nIn a final step, we will visualize the results of the ordinal regression model. To do that, we need to reformat the data and add the predictions.\n\n# extract predictions\npredictions &lt;- predict(m, data = ordata, type = \"prob\")\n# add predictions to the data\nnewordata &lt;- cbind(ordata, predictions)\n# rename columns\ncolnames(newordata)[6:7] &lt;- c(\"somewhat_likely\", \"very_likely\")\n# reformat data\nnewordata &lt;- newordata %&gt;%\n  dplyr::select(-Recommend) %&gt;%\n  tidyr::gather(Recommendation, Probability, unlikely:very_likely) %&gt;%\n  dplyr::mutate(Recommendation = factor(Recommendation,\n    levels = c(\n      \"unlikely\",\n      \"somewhat_likely\",\n      \"very_likely\"\n    )\n  ))\n\n\nnewordata %&gt;%\n  as.data.frame() %&gt;%\n  head(10) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .5, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 10 rows of the newordata.\") %&gt;%\n  flextable::border_outer()\n\nInternalExchangeFinalScoreRecommendationProbabilityExternalNoExchange3.26unlikely0.5488InternalNoExchange3.21unlikely0.3056InternalExchange3.94unlikely0.2294ExternalNoExchange2.81unlikely0.6161ExternalNoExchange2.53unlikely0.6560ExternalExchange2.59unlikely0.6609ExternalNoExchange2.56unlikely0.6518ExternalNoExchange2.73unlikely0.6277ExternalNoExchange3.00unlikely0.5881InternalNoExchange3.50unlikely0.2690\n\n\nWe can now visualize the predictions of the model.\n\n# bar plot\nnewordata %&gt;%\n  ggplot(aes(\n    x = FinalScore, Probability,\n    color = Recommendation,\n    group = Recommendation\n  )) +\n  facet_grid(Exchange ~ Internal) +\n  geom_smooth() +\n  # define colors\n  scale_fill_manual(values = clrs3) +\n  scale_color_manual(values = clrs3) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFor more information about regression modeling, model fitting, and model diagnostics, please have a look at the tutorial on fixed-effects regressions."
  },
  {
    "objectID": "tutorials/kwics/kwics.html#loading-and-processing-text",
    "href": "tutorials/kwics/kwics.html#loading-and-processing-text",
    "title": "Concordancing with R",
    "section": "Loading and Processing Text",
    "text": "Loading and Processing Text\nFor this tutorial, we will use Lewis Carroll’s classic novel Alice’s Adventures in Wonderland as our primary text dataset. This whimsical tale follows the adventures of Alice as she navigates a fantastical world filled with peculiar characters and surreal landscapes.\nLoading Text (from Project Gutenberg)\nTo access the text within R, you can use the following code snippet, ensuring you have an active internet connection:\n\n# Load Alice's Adventures in Wonderland text into R\nrawtext &lt;- readLines(\"https://www.gutenberg.org/files/11/11-0.txt\")\n\n\n\n.*** START OF THE PROJECT GUTENBERG EBOOK 11 ***[Illustration]Alice’s Adventures in Wonderlandby Lewis CarrollTHE MILLENNIUM FULCRUM EDITION 3.0Contents CHAPTER I.     Down the Rabbit-Hole CHAPTER II.    The Pool of Tears CHAPTER III.   A Caucus-Race and a Long Tale CHAPTER IV.    The Rabbit Sends in a Little Bill CHAPTER V.     Advice from a Caterpillar CHAPTER VI.    Pig and Pepper CHAPTER VII.   A Mad Tea-Party CHAPTER VIII.  The Queen’s Croquet-Ground CHAPTER IX.    The Mock Turtle’s Story CHAPTER X.     The Lobster Quadrille CHAPTER XI.    Who Stole the Tarts? CHAPTER XII.   Alice’s EvidenceCHAPTER I.Down the Rabbit-HoleAlice was beginning to get very tired of sitting by her sister on thebank, and of having nothing to do: once or twice she had peeped intothe book her sister was reading, but it had no pictures orconversations in it, “and what is the use of a book,” thought Alice“without pictures or conversations?”\n\n\nAfter retrieving the text from Project Gutenberg, it becomes available for analysis within R. However, upon loading the text into our environment, we notice that it requires some processing. This includes removing extraneous elements such as the table of contents to isolate the main body of the text. Therefore, in the next step, we process the text. This will include consolidating the text into a single object and eliminating any non-essential content. Additionally, we clean up the text by removing superfluous white spaces to ensure a tidy dataset for our analysis.\nData Preparation\nData processing and preparation play a crucial role in text analysis, as they directly impact the quality and accuracy of the results obtained. When working with text data, it’s essential to ensure that the data is clean, structured, and formatted appropriately for analysis. This involves tasks such as removing irrelevant information, standardizing text formats, and handling missing or erroneous data.\nThe importance of data processing and preparation lies in its ability to transform raw text into a usable format that can be effectively analyzed. By cleaning and pre-processing the data, researchers can mitigate the impact of noise and inconsistencies, enabling more accurate and meaningful insights to be extracted from the text.\nHowever, it’s worth noting that data preparation can often be a time-consuming process, sometimes requiring more time and effort than the actual analysis task itself. The extent of data preparation required can vary significantly depending on the complexity of the data and the specific research objectives. While some datasets may require minimal processing, others may necessitate extensive cleaning and transformation.\nUltimately, the time and effort invested in data preparation are essential for ensuring the reliability and validity of the analysis results. By dedicating sufficient attention to data processing and preparation, researchers can enhance the quality of their analyses and derive more robust insights from the text data at hand.\n\ntext &lt;- rawtext %&gt;%\n  # collapse lines into a single  text\n  paste0(collapse = \" \") %&gt;%\n  # remove superfluous white spaces\n  stringr::str_squish() %&gt;%\n  # remove everything before \"CHAPTER I.\"\n  stringr::str_remove(\".*CHAPTER I\\\\.\")\n\n\n\n. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?” So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her. There was nothing so _very_ remarkable in that; nor did Alice think it so _very_ much out of the way to hear the Rabbit say to itself, “Oh dear! Oh dear! I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually _took a watch out of its waistcoat-pocke\n\n\n\n\n\nADDITIONAL INFO: the regular expression “.CHAPTER I\\.” can be interpreted as follows: Match any sequence of characters (.) followed by the exact text”CHAPTER I” and ending with a period (.). This pattern is commonly used to locate occurrences of a chapter heading labeled “CHAPTER I” within a larger body of text.\n\n\n\n\nThe entire content of Lewis Carroll’s Alice’s Adventures in Wonderland is now combined into a single character object and we can begin with generating concordances (KWICs)."
  },
  {
    "objectID": "tutorials/kwics/kwics.html#generating-basic-kwics",
    "href": "tutorials/kwics/kwics.html#generating-basic-kwics",
    "title": "Concordancing with R",
    "section": "Generating Basic KWICs",
    "text": "Generating Basic KWICs\nNow, extracting concordances becomes straightforward with the kwic function from the quanteda package. This function is designed to enable the extraction of keyword-in-context (KWIC) displays, a common format for displaying concordance lines.\nTo prepare the text for concordance extraction, we first need to tokenize it, which involves splitting it into individual words or tokens. Additionally, we specify the phrase argument in the kwic function, allowing us to extract phrases consisting of more than one token, such as “poor alice”.\nThe kwic function primarily requires two arguments: the tokenized text (x) and the search pattern (pattern). Additionally, it offers flexibility by allowing users to specify the context window, determining the number of words or elements displayed to the left and right of the nodeword. We’ll delve deeper into customizing this context window later on.\n\nmykwic &lt;- quanteda::kwic(\n  # define and tokenise text\n  quanteda::tokens(text),\n  # define search pattern and add the phrase function\n  pattern = phrase(\"Alice\")\n) %&gt;%\n  # convert it into a data frame\n  as.data.frame()\n\n\n\ndocnamefromtoprekeywordpostpatterntext144Down the Rabbit-HoleAlicewas beginning to get veryAlicetext16363a book , ” thoughtAlice“ without pictures or conversationsAlicetext1143143in that ; nor didAlicethink it so _very_ muchAlicetext1229229and then hurried on ,Alicestarted to her feet ,Alicetext1299299In another moment down wentAliceafter it , never onceAlicetext1338338down , so suddenly thatAlicehad not a moment toAlice\n\n\nThe resulting table showcases how “Alice” is used within our example text. However, since we use the head function, the table only displays the first six instances.\nAfter extracting a concordance table, we can easily determine the frequency of the search term (“alice”) using either the nrow or length functions. These functions provide the number of rows in a table (nrow) or the length of a vector (length).\n\nnrow(mykwic)\n\n[1] 386\n\n\n\nlength(mykwic$keyword)\n\n[1] 386\n\n\nThe results indicate that there are 386 instances of the search term (“alice”). Moreover, we can also explore how often different variants of the search term were found using the table function. While this may be particularly useful for searches involving various search terms (although less so in the present example).\n\ntable(mykwic$keyword)\n\n\nAlice \n  386 \n\n\nTo gain a deeper understanding of how a word is used, it can be beneficial to extract more context. This can be achieved by adjusting the size of the context window. To do so, we simply specify the window argument of the kwic function. In the following example, we set the context window size to 10 words/elements, deviating from the default size of 5 words/elements.\n\nmykwic_long &lt;- quanteda::kwic(\n  # define text\n  quanteda::tokens(text),\n  # define search pattern\n  pattern = phrase(\"alice\"),\n  # define context window size\n  window = 10\n) %&gt;%\n  # make it a data frame\n  as.data.frame()\n\n\n\ndocnamefromtoprekeywordpostpatterntext144Down the Rabbit-HoleAlicewas beginning to get very tired of sitting by heralicetext16363what is the use of a book , ” thoughtAlice“ without pictures or conversations ? ” So she wasalicetext1143143was nothing so _very_ remarkable in that ; nor didAlicethink it so _very_ much out of the way toalicetext1229229and looked at it , and then hurried on ,Alicestarted to her feet , for it flashed across heralicetext1299299rabbit-hole under the hedge . In another moment down wentAliceafter it , never once considering how in the worldalicetext1338338, and then dipped suddenly down , so suddenly thatAlicehad not a moment to think about stopping herself beforealice\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nExtract the first 10 concordances for the word confused.\n\n\n\nAnswer\n\n\nkwic_confused &lt;- quanteda::kwic(x = quanteda::tokens(text), pattern = phrase(\"confused\"))\n# inspect\nkwic_confused %&gt;%\n  as.data.frame() %&gt;%\n  head(10)\n\n  docname  from    to                    pre  keyword\n1   text1  6217  6217     , calling out in a confused\n2   text1 19140 19140     . ” This answer so confused\n3   text1 19325 19325 said Alice , very much confused\n4   text1 33422 33422      she knew ) to the confused\n                                 post  pattern\n1                    way , “ Prizes ! confused\n2               poor Alice , that she confused\n3                   , “ I don’t think confused\n4 clamour of the busy farm-yard—while confused\n\n\n\n\nHow many instances are there of the word wondering?\n\n\n\nAnswer\n\n\nquanteda::kwic(x = quanteda::tokens(text), pattern = phrase(\"wondering\")) %&gt;%\n  as.data.frame() %&gt;%\n  nrow()\n\n[1] 7\n\n\n\n\nExtract concordances for the word strange and show the first 5 concordance lines.\n\n\n\nAnswer\n\n\nkwic_strange &lt;- quanteda::kwic(x = quanteda::tokens(text), pattern = phrase(\"strange\"))\n# inspect\nkwic_strange %&gt;%\n  as.data.frame() %&gt;%\n  head(5)\n\n  docname  from    to                          pre keyword\n1   text1  3527  3527 her voice sounded hoarse and strange\n2   text1 13147 13147         , that it felt quite strange\n3   text1 32997 32997    remember them , all these strange\n4   text1 33204 33204    her became alive with the strange\n5   text1 33514 33514        and eager with many a strange\n                              post pattern\n1              , and the words did strange\n2               at first ; but she strange\n3      Adventures of hers that you strange\n4 creatures of her little sister’s strange\n5         tale , perhaps even with strange"
  },
  {
    "objectID": "tutorials/kwics/kwics.html#exporting-kwics",
    "href": "tutorials/kwics/kwics.html#exporting-kwics",
    "title": "Concordancing with R",
    "section": "Exporting KWICs",
    "text": "Exporting KWICs\nTo export or save a concordance table as an MS Excel spreadsheet, you can utilize the write_xlsx function from the writexl package, as demonstrated below. It’s important to note that we employ the here function from the here package to specify the location where we want to save the file. In this instance, we save the file in the current working directory. If you’re working with Rproj files in RStudio, which is recommended, then the current working directory corresponds to the directory or folder where your Rproj file is located.\n\nwrite_xlsx(mykwic, here::here(\"mykwic.xlsx\"))"
  },
  {
    "objectID": "tutorials/kwics/kwics.html#extracting-multi-word-expressions",
    "href": "tutorials/kwics/kwics.html#extracting-multi-word-expressions",
    "title": "Concordancing with R",
    "section": "Extracting Multi-Word Expressions",
    "text": "Extracting Multi-Word Expressions\nWhile extracting single words is a common practice, there are situations where you may need to extract more than just one word at a time. This can be particularly useful when you’re interested in extracting phrases or multi-word expressions from your text data. To accomplish this, you simply need to specify that the pattern you are searching for is a phrase. This allows you to extract contiguous sequences of words that form meaningful units of text.\nFor example, if you’re analyzing a text and want to extract phrases like “poor alice”, “mad hatter”, or “cheshire cat”, you can easily do so by specifying these phrases as your search patterns.\n\n# extract concordances for the phrase \"poor alice\" using the kwic function from the quanteda package\nkwic_pooralice &lt;- quanteda::kwic(\n  # tokenizing the input text\n  quanteda::tokens(text),\n  # specifying the search pattern as the phrase \"poor alice\"\n  pattern = phrase(\"poor alice\")\n) %&gt;%\n  # converting the result to a data frame for easier manipulation\n  as.data.frame()\n\n\n\ndocnamefromtoprekeywordpostpatterntext11,5411,542go through , ” thoughtpoor Alice, “ it would bepoor alicetext12,1302,131; but , alas forpoor Alice! when she got topoor alicetext12,3322,333use now , ” thoughtpoor Alice, “ to pretend topoor alicetext12,8872,888to the garden door .Poor Alice! It was as muchpoor alicetext13,6043,605right words , ” saidpoor Alice, and her eyes filledpoor alicetext16,8766,877mean it ! ” pleadedpoor Alice. “ But you’re sopoor alicetext17,2907,291more ! ” And herepoor Alicebegan to cry again ,poor alicetext18,2398,240at home , ” thoughtpoor Alice, “ when one wasn’tpoor alicetext111,78811,789to it ! ” pleadedpoor Alicein a piteous tone .poor alicetext119,14119,142” This answer so confusedpoor Alice, that she let thepoor alice\n\n\nIn addition to exact words or phrases, there are situations where you may need to extract more or less fixed patterns from your text data. These patterns might allow for variations in spelling, punctuation, or formatting. To search for such flexible patterns, you need to incorporate regular expressions into your search pattern.\nRegular expressions (regex) are powerful tools for pattern matching and text manipulation. They allow you to define flexible search patterns that can match a wide range of text variations. For example, you can use regex to find all instances of a word regardless of whether it’s in lowercase or uppercase, or to identify patterns like dates, email addresses, or URLs.\nTo incorporate regular expressions into your search pattern, you can use functions like grepl() or grep() in base R, or str_detect() and str_extract() in the stringr package. These functions allow you to specify regex patterns to search for within your text data.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nExtract the first 10 concordances for the phrase the hatter.\n\n\n\nAnswer\n\n\nkwic_thehatter &lt;- quanteda::kwic(x = quanteda::tokens(text), pattern = phrase(\"the hatter\"))\n# inspect\nkwic_thehatter %&gt;%\n  as.data.frame() %&gt;%\n  head(10)\n\n   docname  from    to                    pre    keyword\n1    text1 16576 16577   wish I’d gone to see the Hatter\n2    text1 16607 16608 and the March Hare and the Hatter\n3    text1 16859 16860 wants cutting , ” said the Hatter\n4    text1 16905 16906     it’s very rude . ” The Hatter\n5    text1 17049 17050         a bit ! ” said the Hatter\n6    text1 17174 17175      with you , ” said the Hatter\n7    text1 17209 17210  , which wasn’t much . The Hatter\n8    text1 17287 17288  days wrong ! ” sighed the Hatter\n9    text1 17338 17339         in as well , ” the Hatter\n10   text1 17453 17454 should it ? ” muttered the Hatter\n                          post    pattern\n1      instead ! ” CHAPTER VII the hatter\n2        were having tea at it the hatter\n3        . He had been looking the hatter\n4    opened his eyes very wide the hatter\n5           . “ You might just the hatter\n6  , and here the conversation the hatter\n7       was the first to break the hatter\n8               . “ I told you the hatter\n9   grumbled : “ you shouldn’t the hatter\n10       . “ Does _your_ watch the hatter\n\n\n\n\nHow many instances are there of the phrase the hatter?\n\n\n\nAnswer\n\n\nkwic_thehatter %&gt;%\n  as.data.frame() %&gt;%\n  nrow()\n\n[1] 51\n\n\n\n\nExtract concordances for the phrase the cat and show the first 5 concordance lines.\n\n\n\nAnswer\n\n\nkwic_thecat &lt;- quanteda::kwic(x = quanteda::tokens(text), pattern = phrase(\"the cat\"))\n# inspect\nkwic_thecat %&gt;%\n  as.data.frame() %&gt;%\n  head(5)\n\n  docname  from    to               pre keyword                     post\n1   text1   932   933   ! ” ( Dinah was the cat             . ) “ I hope\n2   text1 15624 15625 a few yards off . The Cat only grinned when it saw\n3   text1 15749 15750   get to , ” said the Cat         . “ I don’t much\n4   text1 15775 15776   you go , ” said the Cat            . “ — so long\n5   text1 15805 15806  do that , ” said the Cat          , “ if you only\n  pattern\n1 the cat\n2 the cat\n3 the cat\n4 the cat\n5 the cat"
  },
  {
    "objectID": "tutorials/kwics/kwics.html#concordancing-using-regular-expressions",
    "href": "tutorials/kwics/kwics.html#concordancing-using-regular-expressions",
    "title": "Concordancing with R",
    "section": "Concordancing Using Regular Expressions",
    "text": "Concordancing Using Regular Expressions\nRegular expressions provide a powerful means of searching for abstract patterns within text data, offering unparalleled flexibility beyond concrete words or phrases. Often abbreviated as regex or regexp, a regular expression is a special sequence of characters that describe a pattern to be matched in a text.\nYou can conceptualize regular expressions as highly potent combinations of wildcards, offering an extensive range of pattern-matching capabilities. For instance, the sequence [a-z]{1,3} is a regular expression that signifies one to three lowercase characters. Searching for this regular expression would yield results such as “is”, “a”, “an”, “of”, “the”, “my”, “our”, and other short words.\nThere are three fundamental types of regular expressions:\n\nRegular expressions for individual symbols and frequencies: These regular expressions represent single characters and determine their frequencies within the text. For example, [a-z] matches any lowercase letter, [0-9] matches any digit, and {1,3} specifies a range of occurrences (one to three in this case).\nRegular expressions for classes of symbols: These regular expressions represent classes or groups of symbols with shared characteristics. For instance, \\d matches any digit, \\w matches any word character (alphanumeric characters and underscores), and \\s matches any whitespace character.\nRegular expressions for structural properties: These regular expressions represent structural properties or patterns within the text. For example, ^ matches the start of a line, $ matches the end of a line, and \\b matches a word boundary.\n\nThe regular expressions below show the first type of regular expressions, i.e. regular expressions that stand for individual symbols and determine frequencies.\n\n\nRegEx Symbol/SequenceExplanationExample?The preceding item is optional and will be matched at most oncewalk[a-z]? = walk, walks*The preceding item will be matched zero or more timeswalk[a-z]* = walk, walks, walked, walking+The preceding item will be matched one or more timeswalk[a-z]+ = walks, walked, walking{n}The preceding item is matched exactly n timeswalk[a-z]{2} = walked{n,}The preceding item is matched n or more timeswalk[a-z]{2,} = walked, walking{n,m}The preceding item is matched at least n times, but not more than m timeswalk[a-z]{2,3} = walked, walking\n\n\nThe regular expressions below show the second type of regular expressions, i.e. regular expressions that stand for classes of symbols.\n\n\nRegEx Symbol/SequenceExplanation[ab]lower case a and b[AB]upper case a and b[12]digits 1 and 2[:digit:]digits: 0 1 2 3 4 5 6 7 8 9[:lower:]lower case characters: a–z[:upper:]upper case characters: A–Z[:alpha:]alphabetic characters: a–z and A–Z[:alnum:]digits and alphabetic characters[:punct:]punctuation characters: . , ; etc.[:graph:]graphical characters: [:alnum:] and [:punct:][:blank:]blank characters: Space and tab[:space:]space characters: Space, tab, newline, and other space characters[:print:]printable characters: [:alnum:], [:punct:] and [:space:]\n\n\nThe regular expressions that denote classes of symbols are enclosed in [] and :. The last type of regular expressions, i.e. regular expressions that stand for structural properties are shown below.\n\n\nRegEx Symbol/SequenceExplanation\\\\wWord characters: [[:alnum:]_]\\\\WNo word characters: [^[:alnum:]_]\\\\sSpace characters: [[:blank:]]\\\\SNo space characters: [^[:blank:]]\\\\dDigits: [[:digit:]]\\\\DNo digits: [^[:digit:]]\\\\bWord edge\\\\BNo word edge&lt;Word beginning&gt;Word end^Beginning of a string$End of a string\n\n\nTo incorporate regular expressions into your KWIC searches, you include them in your search pattern and set the valuetype argument to \"regex\". This allows you to specify complex search patterns that go beyond exact word matches.\nFor example, consider the search pattern \"\\\\balic.*|\\\\bhatt.*\". In this pattern:\n\n\\\\b indicates a word boundary, ensuring that the subsequent characters are at the beginning of a word.\nalic.* matches any sequence of characters (.*) that begins with alic.\nhatt.* matches any sequence of characters that begins with hatt.\nThe | operator functions as an OR operator, allowing the pattern to match either alic.* or hatt.*.\n\nAs a result, this search pattern retrieves elements that contain alic or hatt followed by any characters, but only where alic and hatt are the first letters of a word. Consequently, words like “malice” or “shatter” would not be retrieved.\nBy uaing regular expressions in your KWIC searches, you can conduct more nuanced and precise searches, capturing specific patterns or variations within your text data.\n\n# define search patterns\npatterns &lt;- c(\"\\\\balic.*|\\\\bhatt.*\")\nkwic_regex &lt;- quanteda::kwic(\n  # define text\n  quanteda::tokens(text),\n  # define search pattern\n  patterns,\n  # define valuetype\n  valuetype = \"regex\"\n) %&gt;%\n  # make it a data frame\n  as.data.frame()\n\n\n\ndocnamefromtoprekeywordpostpatterntext144Down the Rabbit-HoleAlicewas beginning to get very\\balic.*|\\bhatt.*text16363a book , ” thoughtAlice“ without pictures or conversations\\balic.*|\\bhatt.*text1143143in that ; nor didAlicethink it so _very_ much\\balic.*|\\bhatt.*text1229229and then hurried on ,Alicestarted to her feet ,\\balic.*|\\bhatt.*text1299299In another moment down wentAliceafter it , never once\\balic.*|\\bhatt.*text1338338down , so suddenly thatAlicehad not a moment to\\balic.*|\\bhatt.*text1521521“ Well ! ” thoughtAliceto herself , “ after\\balic.*|\\bhatt.*text1647647for , you see ,Alicehad learnt several things of\\balic.*|\\bhatt.*text1719719got to ? ” (Alicehad no idea what Latitude\\balic.*|\\bhatt.*text1910910else to do , soAlicesoon began talking again .\\balic.*|\\bhatt.*\n\n\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nExtract the first 10 concordances for words containing exu.\n\n\n\nAnswer\n\n\nkwic_exu &lt;- quanteda::kwic(x = quanteda::tokens(text), pattern = \".*exu.*\", valuetype = \"regex\")\n# inspect\nkwic_exu %&gt;%\n  as.data.frame() %&gt;%\n  head(10)\n\n[1] docname from    to      pre     keyword post    pattern\n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\nHow many instances are there of words beginning with pit?\n\n\n\nAnswer\n\n\nquanteda::kwic(x = quanteda::tokens(text), pattern = \"\\\\bpit.*\", valuetype = \"regex\") %&gt;%\n  as.data.frame() %&gt;%\n  nrow()\n\n[1] 5\n\n\n\n\nExtract concordances for words ending with ption and show the first 5 concordance lines.\n\n\n\nAnswer\n\n\nquanteda::kwic(x = quanteda::tokens(text), pattern = \"ption\\\\b\", valuetype = \"regex\") %&gt;%\n  as.data.frame() %&gt;%\n  head(5)\n\n  docname from   to                         pre  keyword\n1   text1 5775 5775 adjourn , for the immediate adoption\n                          post  pattern\n1 of more energetic remedies — ption\\\\b"
  },
  {
    "objectID": "tutorials/kwics/kwics.html#concordancing-and-piping",
    "href": "tutorials/kwics/kwics.html#concordancing-and-piping",
    "title": "Concordancing with R",
    "section": "Concordancing and Piping",
    "text": "Concordancing and Piping\nQuite often, we want to retrieve patterns only if they occur in a specific context. For instance, we might be interested in instances of “alice”, but only if the preceding word is “poor” or “little”. While such conditional concordances could be extracted using regular expressions, they are more easily retrieved by piping.\nPiping is achieved using the %&gt;% function from the dplyr package, and the piping sequence can be interpreted as “and then”. We can then filter those concordances that contain “alice” using the filter function from the dplyr package. Note that the $ symbol stands for the end of a string, so “poor$” signifies that “poor” is the last element in the string that precedes the nodeword.\n\n# extract KWIC concordances\nquanteda::kwic(\n  # input  tokenized text\n  x = quanteda::tokens(text),\n  # define search pattern (\"alice\")\n  pattern = \"alice\"\n  # pipe (and then)\n) %&gt;%\n  # convert result to data frame\n  as.data.frame() %&gt;%\n  # filter concordances with \"poor\" or \"little\" preceding \"alice\"\n  # save result in object called \"kwic_pipe\"\n  dplyr::filter(stringr::str_detect(pre, \"poor$|little$\")) -&gt; kwic_pipe\n\n\n\ndocnamefromtoprekeywordpostpatterntext11,5421,542through , ” thought poorAlice, “ it would bealicetext11,7251,725” but the wise littleAlicewas not going to doalicetext12,1312,131but , alas for poorAlice! when she got toalicetext12,3332,333now , ” thought poorAlice, “ to pretend toalicetext13,6053,605words , ” said poorAlice, and her eyes filledalicetext16,8776,877it ! ” pleaded poorAlice. “ But you’re soalicetext17,2917,291! ” And here poorAlicebegan to cry again ,alicetext18,2408,240home , ” thought poorAlice, “ when one wasn’talicetext111,78911,789it ! ” pleaded poorAlicein a piteous tone .alicetext119,14219,142This answer so confused poorAlice, that she let thealice\n\n\nIn this code:\n\nquanteda::kwic: This function extracts KWIC concordances from the input text.\nquanteda::tokens(text): The input text is tokenized using the tokens function from the quanteda package.\npattern = \"alice\": Specifies the search pattern as “alice”.\n%&gt;%: The pipe operator (%&gt;%) chains together multiple operations, passing the result of one operation as the input to the next.\nas.data.frame(): Converts the resulting concordance object into a data frame.\ndplyr::filter(...): Filters the concordances based on the specified condition, which is whether “poor” or “little” precedes “alice”.\n\nPiping is an indispensable tool in R, commonly used across various data science domains, including text processing. This powerful function, denoted by %&gt;%, allows for a more streamlined and readable workflow by chaining together multiple operations in a sequential manner.\nInstead of nesting functions or creating intermediate variables, piping allows to take an easy-to-understand and more intuitive approach to data manipulation and analysis. With piping, each operation is performed “and then” the next, leading to code that is easier to understand and maintain.\nWhile piping is frequently used in the context of text processing, its versatility extends far beyond. In data wrangling, modeling, visualization, and beyond, piping offers a concise and elegant solution for composing complex workflows.\nBy leveraging piping, R users can enhance their productivity and efficiency, making their code more expressive and succinct while maintaining clarity and readability. It’s a fundamental tool in the toolkit of every R programmer, empowering them to tackle data science challenges with confidence and ease."
  },
  {
    "objectID": "tutorials/kwics/kwics.html#ordering-and-arranging-kwics",
    "href": "tutorials/kwics/kwics.html#ordering-and-arranging-kwics",
    "title": "Concordancing with R",
    "section": "Ordering and Arranging KWICs",
    "text": "Ordering and Arranging KWICs\nWhen examining concordances, it’s often beneficial to reorder them based on their context rather than the order in which they appeared in the text or texts. This allows for a more organized and structured analysis of the data. To reorder concordances, we can utilize the arrange function from the dplyr package, which takes the column according to which we want to rearrange the data as its main argument.\n\nOrdering Alphabetically\nIn the example below, we extract all instances of “alice” and then arrange the instances according to the content of the post column in alphabetical order.\n\n# extract KWIC concordances\nquanteda::kwic(\n  # input  tokenized text\n  x = quanteda::tokens(text),\n  # define search pattern (\"alice\")\n  pattern = \"alice\"\n  # end function and pipe (and then)\n) %&gt;%\n  # convert result to data frame\n  as.data.frame() %&gt;%\n  # arrange concordances based on the content of the \"post\" column\n  # save result in object called \"kwic_ordered\"\n  dplyr::arrange(post) -&gt; kwic_ordered\n\n\n\ndocnamefromtoprekeywordpostpatterntext17,7547,754happen : “ ‘ MissAlice! Come here directly ,alicetext12,8882,888the garden door . PoorAlice! It was as muchalicetext12,1312,131but , alas for poorAlice! when she got toalicetext130,89130,891voice , the name “Alice! ” CHAPTER XII .alicetext18,4238,423“ Oh , you foolishAlice! ” she answered herselfalicetext12,6062,606and curiouser ! ” criedAlice( she was so muchalicetext125,86125,861I haven’t , ” saidAlice) — “ and perhapsalicetext132,27532,275explain it , ” saidAlice, ( she had grownalicetext132,84332,843for you ? ” saidAlice, ( she had grownalicetext11,6781,678here before , ” saidAlice, ) and round thealice\n\n\n\n\nOrdering by Co-Occurrence Frequency\nArranging concordances based on alphabetical properties may not always be the most informative approach. A more insightful option is to arrange concordances according to the frequency of co-occurring terms or collocates. This allows us to identify the most common words that appear alongside our search term, providing valuable insights into its usage patterns.\nTo accomplish this, we need to follow these steps:\n\nCreate a new variable or column that represents the word that co-occurs with the search term. In the example below, we use the mutate function from the dplyr package to create a new column called post_word. We then use the str_remove_all function from the stringr package to extract the word that immediately follows the search term. This is achieved by removing everything except for the word following the search term (including any white space).\nGroup the data by the word that immediately follows the search term.\nCreate a new column called post_word_freq that represents the frequencies of all the words that immediately follow the search term.\nArrange the concordances by the frequency of the collocates in descending order. This is accomplished by using the arrange function and specifying the column post_word_freq in descending order (indicated by the - symbol).\n\n\nquanteda::kwic(\n  # define text\n  x = quanteda::tokens(text),\n  # define search pattern\n  pattern = \"alice\"\n) %&gt;%\n  # make it a data frame\n  as.data.frame() %&gt;%\n  # extract word following the nodeword\n  dplyr::mutate(post1 = str_remove_all(post, \" .*\")) %&gt;%\n  # group following words\n  dplyr::group_by(post1) %&gt;%\n  # extract frequencies of the following words\n  dplyr::mutate(post1_freq = n()) %&gt;%\n  # arrange/order by the frequency of the following word\n  dplyr::arrange(-post1_freq) -&gt; kwic_ordered_coll\n\n\n\ndocnamefromtoprekeywordpostpatternpost1post1_freqtext11,5421,542through , ” thought poorAlice, “ it would bealice,78text11,6781,678here before , ” saidAlice, ) and round thealice,78text12,3332,333now , ” thought poorAlice, “ to pretend toalice,78text12,4102,410eat it , ” saidAlice, “ and if italice,78text12,7392,739to them , ” thoughtAlice, “ or perhaps theyalice,78text12,9452,945of yourself , ” saidAlice, “ a great girlalice,78text13,6053,605words , ” said poorAlice, and her eyes filledalice,78text13,7513,751oh dear ! ” criedAlice, with a sudden burstalice,78text13,9183,918narrow escape ! ” saidAlice, a good deal frightenedalice,78text14,1814,181so much ! ” saidAlice, as she swam aboutalice,78\n\n\nIn this code:\n\nmutate: This function from the dplyr package creates a new column in the data frame.\nstr_remove_all: This function from the stringr package removes all occurrences of a specified pattern from a character string.\ngroup_by: This function from the dplyr package groups the data by a specified variable.\nn(): This function from the dplyr package calculates the number of observations in each group.\narrange: This function from the dplyr package arranges the rows of a data frame based on the values of one or more columns.\n\nWe add more columns according to which we could arrange the concordance following the same schema. For example, we could add another column that represented the frequency of words that immediately preceded the search term and then arrange according to this column."
  },
  {
    "objectID": "tutorials/kwics/kwics.html#ordering-by-multiple-co-occurrence-frequencies",
    "href": "tutorials/kwics/kwics.html#ordering-by-multiple-co-occurrence-frequencies",
    "title": "Concordancing with R",
    "section": "Ordering by Multiple Co-Occurrence Frequencies",
    "text": "Ordering by Multiple Co-Occurrence Frequencies\nIn this section, we extract the three words preceding and following the nodeword “alice” from the concordance data and organize the results by the frequencies of the following words (you can also order by the preceding words which is why we also extract them).\nWe begin by iterating through each row of the concordance data using rowwise(). Then, we extract the three words following the nodeword (“alice”) and the three words preceding it from the post and pre columns, respectively. These words are split using the strsplit function and stored in separate columns (post1, post2, post3, pre1, pre2, pre3).\nNext, we group the data by each of the following words (post1, post2, post3, pre1, pre2, pre3) and calculate the frequency of each word using the n() function within each group. This allows us to determine how often each word occurs in relation to the nodeword “alice”.\nFinally, we arrange the concordances based on the frequencies of the following words (post1, post2, post3) in descending order using the arrange() function, storing the result in the mykwic_following data frame.\n\nmykwic %&gt;%\n  dplyr::rowwise() %&gt;% # Row-wise operation for each entry\n  # Extract words preceding and following the node word\n  # Extracting the first word following the node word\n  dplyr::mutate(\n    post1 = unlist(strsplit(post, \" \"))[1],\n    # Extracting the second word following the node word\n    post2 = unlist(strsplit(post, \" \"))[2],\n    # Extracting the third word following the node word\n    post3 = unlist(strsplit(post, \" \"))[3],\n    # Extracting the last word preceding the node word\n    pre1 = unlist(strsplit(pre, \" \"))[length(unlist(strsplit(pre, \" \")))],\n    # Extracting the second-to-last word preceding the node word\n    pre2 = unlist(strsplit(pre, \" \"))[length(unlist(strsplit(pre, \" \"))) - 1],\n    # Extracting the third-to-last word preceding the node word\n    pre3 = unlist(strsplit(pre, \" \"))[length(unlist(strsplit(pre, \" \"))) - 2]\n  ) %&gt;%\n  # Extract frequencies of the words around the node word\n  # Grouping by the first word following the node word and counting its frequency\n  dplyr::group_by(post1) %&gt;%\n  dplyr::mutate(npost1 = n()) %&gt;%\n  # Grouping by the second word following the node word and counting its frequency\n  dplyr::group_by(post2) %&gt;%\n  dplyr::mutate(npost2 = n()) %&gt;%\n  # Grouping by the third word following the node word and counting its frequency\n  dplyr::group_by(post3) %&gt;%\n  dplyr::mutate(npost3 = n()) %&gt;%\n  # Grouping by the last word preceding the node word and counting its frequency\n  dplyr::group_by(pre1) %&gt;%\n  dplyr::mutate(npre1 = n()) %&gt;%\n  # Grouping by the second-to-last word preceding the node word and counting its frequency\n  dplyr::group_by(pre2) %&gt;%\n  dplyr::mutate(npre2 = n()) %&gt;%\n  # Grouping by the third-to-last word preceding the node word and counting its frequency\n  dplyr::group_by(pre3) %&gt;%\n  dplyr::mutate(npre3 = n()) %&gt;%\n  # Arranging the results\n  # Arranging in descending order of frequencies of words following the node word\n  dplyr::arrange(-npost1, -npost2, -npost3) -&gt; mykwic_following\n\n\n\ndocnamefromtoprekeywordpostpatternpost1post2post3pre1pre2pre3npost1npost2npost3npre1npre2npre3text12,9452,945of yourself , ” saidAlice, “ a great girlAlice,“asaid”,788913115164106text12,4102,410eat it , ” saidAlice, “ and if itAlice,“andsaid”,78899115164106text16,5256,525you know , ” saidAlice, “ and why itAlice,“andsaid”,78899115164106text128,75128,751the jury-box , ” thoughtAlice, “ and those twelveAlice,“andthought”,7889926164106text12,3332,333now , ” thought poorAlice, “ to pretend toAlice,“topoorthought”788971039text14,3124,312, now , ” thoughtAlice, “ to speak toAlice,“tothought”,7889726164106text11,5421,542through , ” thought poorAlice, “ it would beAlice,“itpoorthought”788951039text116,11316,113very much , ” saidAlice, “ but I haven’tAlice,“butsaid”,78894115164106text126,69326,693“ Yes , ” saidAlice, “ I’ve often seenAlice,“I’vesaid”,78894115164106text122,35422,354matter much , ” thoughtAlice, “ as all theAlice,“asthought”,7889226164106\n\n\nThe updated concordance now presents the arrangement based on the frequency of words following the node word. This means that the words occurring most frequently immediately after the keyword “alice” are listed first, followed by less frequent ones.\nIt’s essential to note that the arrangement can be customized by modifying the arguments within the dplyr::arrange function. By altering the order and content of these arguments, you can adjust the sorting criteria. For instance, if you want to prioritize the frequency of words preceding the node word instead, you can simply rearrange the arguments accordingly. This flexibility empowers users to tailor the arrangement to suit their specific analysis goals and preferences."
  },
  {
    "objectID": "tutorials/kwics/kwics.html#concordances-from-transcriptions",
    "href": "tutorials/kwics/kwics.html#concordances-from-transcriptions",
    "title": "Concordancing with R",
    "section": "Concordances from Transcriptions",
    "text": "Concordances from Transcriptions\nSince many analyses rely on transcripts as their main data source, and transcripts often require additional processing due to their specific features, we will now demonstrate concordancing using transcripts. To begin, we will load five example transcripts representing the first five files from the Irish component of the International Corpus of English2. These transcripts will serve as our dataset for conducting concordance analysis.\nWe first load these files so that we can process them and extract KWICs. To load the files, the code below dynamically generates URLs for a series of text files, then reads the content of each file into R, storing the text data in the transcripts object. This is a common procedure when working with multiple text files or when the filenames follow a consistent pattern.\n\n# define corpus files\nfiles &lt;- paste(\"tutorials/kwics/data/ICEIrelandSample/S1A-00\", 1:5, \".txt\", sep = \"\")\n# load corpus files\ntranscripts &lt;- sapply(files, function(x) {\n  x &lt;- readLines(x)\n})\n\n\n\n.&lt;S1A-001 Riding&gt;&lt;I&gt;&lt;S1A-001$A&gt; &lt;#&gt; Well how did the riding go tonight&lt;S1A-001$B&gt; &lt;#&gt; It was good so it was &lt;#&gt; Just I I couldn't believe that she was going to let me jump &lt;,&gt; that was only the fourth time you know &lt;#&gt; It was great &lt;&&gt; laughter &lt;/&&gt;&lt;S1A-001$A&gt; &lt;#&gt; What did you call your horse&lt;S1A-001$B&gt; &lt;#&gt; I can't remember &lt;#&gt; Oh Mary 's Town &lt;,&gt; oh&lt;S1A-001$A&gt; &lt;#&gt; And how did Mabel do&lt;S1A-001$B&gt; &lt;#&gt; Did you not see her whenever she was going over the jumps &lt;#&gt; There was one time her horse refused and it refused three times &lt;#&gt; And then &lt;,&gt; she got it round and she just lined it up straight and she just kicked it and she hit it with the whip &lt;,&gt; and over it went the last time you know &lt;#&gt; And Stephanie told her she was very determined and very well-ridden &lt;&&gt; laughter &lt;/&&gt; because it had refused the other times you know &lt;#&gt; But Stephanie wouldn't let her give up on it &lt;#&gt; She made her keep coming back and keep coming back &lt;,&gt; until &lt;,&gt; it jumped it you know &lt;#&gt; It was good&lt;S1A-001$A&gt; &lt;#&gt; Yeah I 'm not so sure her jumping 's improving that much &lt;#&gt; She uh &lt;,&gt; seemed to be holding the reins very tight\n\n\nThe first ten lines shown above let us know that, after the header (&lt;S1A-001 Riding&gt;) and the symbol which indicates the start of the transcript (&lt;I&gt;), each utterance is preceded by a sequence which indicates the section, file, and speaker (e.g. &lt;S1A-001$A&gt;). The first utterance is thus uttered by speaker A in file 001 of section S1A. In addition, there are several sequences that provide meta-linguistic information which indicate the beginning of a speech unit (&lt;#&gt;), pauses (&lt;,&gt;), and laughter (&lt;&&gt; laughter &lt;/&&gt;).\nTo perform the concordancing, we need to change the format of the transcripts because the kwic function only works on character, corpus, tokens object- in their present form, the transcripts represent a list which contains vectors of strings. To change the format, we collapse the individual utterances into a single character vector for each transcript.\n\ntranscripts_collapsed &lt;- sapply(files, function(x) {\n  # read-in text\n  x &lt;- readLines(x)\n  # paste all lines together\n  x &lt;- paste0(x, collapse = \" \")\n  # remove superfluous white spaces\n  x &lt;- str_squish(x)\n})\n\n\n\n.&lt;S1A-001 Riding&gt; &lt;I&gt; &lt;S1A-001$A&gt; &lt;#&gt; Well how did the riding go tonight &lt;S1A-001$B&gt; &lt;#&gt; It was good so it was &lt;#&gt; Just I I couldn't believe that she was going to let me jump &lt;,&gt; that was only the fourth time you know &lt;#&gt; It was great &lt;&&gt; laughter &lt;/&&gt; &lt;S1A-001$A&gt; &lt;#&gt; What did you call your horse &lt;S1A-001$B&gt; &lt;#&gt; I can't remember &lt;#&gt; Oh Mary 's Town &lt;,&gt; oh &lt;S1A-001$A&gt; &lt;#&gt; And how did Mabel do &lt;S1A-001$B&gt; &lt;#&gt; Did you not see her whenever she was going over the jumps &lt;#&gt; There was one time her horse&lt;S1A-002 Dinner chat 1&gt; &lt;I&gt; &lt;S1A-002$A&gt; &lt;#&gt; He 's been married for three years and is now &lt;{&gt; &lt;[&gt; getting divorced &lt;/[&gt; &lt;S1A-002$B&gt; &lt;#&gt; &lt;[&gt; No no &lt;/[&gt; &lt;/{&gt; he 's got married last year and he 's getting &lt;{&gt; &lt;[&gt; divorced &lt;/[&gt; &lt;S1A-002$A&gt; &lt;#&gt; &lt;[&gt; He 's now &lt;/[&gt; &lt;/{&gt; getting divorced &lt;S1A-002$C&gt; &lt;#&gt; Just right &lt;S1A-002$D&gt; &lt;#&gt; A wee girl of her age like &lt;S1A-002$E&gt; &lt;#&gt; Well there was a guy &lt;S1A-002$C&gt; &lt;#&gt; How long did she try it for &lt;#&gt; An hour a a year &lt;S1A-002$B&gt; &lt;#&gt; Mhm &lt;{&gt; &lt;[&gt; mhm &lt;/[&gt; &lt;S1A-002$E&lt;S1A-003 Dinner chat 2&gt; &lt;I&gt; &lt;S1A-003$A&gt; &lt;#&gt; I &lt;.&gt; wa &lt;/.&gt; I want to go to Peru but uh &lt;S1A-003$B&gt; &lt;#&gt; Do you &lt;S1A-003$A&gt; &lt;#&gt; Oh aye &lt;S1A-003$B&gt; &lt;#&gt; I 'd love to go to Peru &lt;S1A-003$A&gt; &lt;#&gt; I want I want to go up the Machu Picchu before it falls off the edge of the mountain &lt;S1A-003$B&gt; &lt;#&gt; Lima 's supposed to be a bit dodgy &lt;S1A-003$A&gt; &lt;#&gt; Mm &lt;S1A-003$B&gt; &lt;#&gt; Bet it would be &lt;S1A-003$B&gt; &lt;#&gt; Mm &lt;S1A-003$A&gt; &lt;#&gt; But I I just I I would like &lt;,&gt; Machu Picchu is collapsing &lt;S1A-003$B&gt; &lt;#&gt; I don't know wh&lt;S1A-004 Nursing home 1&gt; &lt;I&gt; &lt;S1A-004$A&gt; &lt;#&gt; Honest to God &lt;,&gt; I think the young ones &lt;#&gt; Sure they 're flying on Monday in I think it 's Shannon &lt;#&gt; This is from Texas &lt;S1A-004$B&gt; &lt;#&gt; This English girl &lt;S1A-004$A&gt; &lt;#&gt; The youngest one &lt;,&gt; the dentist &lt;,&gt; she 's married to the dentist &lt;#&gt; Herself and her husband &lt;,&gt; three children and she 's six months pregnant &lt;S1A-004$C&gt; &lt;#&gt; Oh God &lt;S1A-004$B&gt; &lt;#&gt; And where are they going &lt;S1A-004$A&gt; &lt;#&gt; Coming to Dublin to the mother &lt;{&gt; &lt;[&gt; or &lt;unclear&gt; 3 sy&lt;S1A-005 Masons&gt; &lt;I&gt; &lt;S1A-005$A&gt; &lt;#&gt; Right shall we risk another beer or shall we try and &lt;,&gt; &lt;{&gt; &lt;[&gt; ride the bikes down there or do something like that &lt;/[&gt; &lt;S1A-005$B&gt; &lt;#&gt; &lt;[&gt; Well &lt;,&gt; what about the &lt;/[&gt; &lt;/{&gt; provisions &lt;#&gt; What time &lt;{&gt; &lt;[&gt; &lt;unclear&gt; 4 sylls &lt;/unclear&gt; &lt;/[&gt; &lt;S1A-005$C&gt; &lt;#&gt; &lt;[&gt; Is is your &lt;/[&gt; &lt;/{&gt; man coming here &lt;S1A-005$B&gt; &lt;#&gt; &lt;{&gt; &lt;[&gt; Yeah &lt;/[&gt; &lt;S1A-005$A&gt; &lt;#&gt; &lt;[&gt; He said &lt;/[&gt; &lt;/{&gt; he would meet us here &lt;S1A-005$B&gt; &lt;#&gt; Just the boat 's arriving you know a few minutes ' wa\n\n\nWe now move on to extracting concordances. We begin by splitting the text simply by white space. This ensures that tags and markup remain intact, preventing accidental splitting. Additionally, we extend the context surrounding our target word or phrase. While the default is five tokens before and after the keyword, we opt to widen this context to 10 tokens. Furthermore, for improved organization and readability, we refine the file names. Instead of using the full path, we extract only the name of the text. This simplifies the presentation of results and enhances clarity when navigating through the corpus.\n\nkwic_trans &lt;- quanteda::kwic(\n  # tokenize transcripts\n  quanteda::tokens(transcripts_collapsed, what = \"fasterword\"),\n  # define search pattern\n  pattern = phrase(\"you know\"),\n  # extend context\n  window = 10\n) %&gt;%\n  # make it a data frame\n  as.data.frame() %&gt;%\n  # clean docnames / filenames / text names\n  dplyr::mutate(docname = str_replace_all(docname, \".*/(.*?).txt\", \"\\\\1\"))\n\n\n\ndocnamefromtoprekeywordpostpatternS1A-0014243let me jump &lt;,&gt; that was only the fourth timeyou know&lt;#&gt; It was great &lt;&&gt; laughter &lt;/&&gt; &lt;S1A-001$A&gt; &lt;#&gt; Whatyou knowS1A-001140141the whip &lt;,&gt; and over it went the last timeyou know&lt;#&gt; And Stephanie told her she was very determined andyou knowS1A-001164165&lt;&&gt; laughter &lt;/&&gt; because it had refused the other timesyou know&lt;#&gt; But Stephanie wouldn't let her give up on ityou knowS1A-001193194and keep coming back &lt;,&gt; until &lt;,&gt; it jumped ityou know&lt;#&gt; It was good &lt;S1A-001$A&gt; &lt;#&gt; Yeah I 'm notyou knowS1A-001402403'd be far better waiting &lt;,&gt; for that one &lt;,&gt;you knowand starting anew fresh &lt;S1A-001$A&gt; &lt;#&gt; Yeah but I meanyou knowS1A-001443444the best goes top of the league &lt;,&gt; &lt;{&gt; &lt;[&gt;you know&lt;/[&gt; &lt;S1A-001$A&gt; &lt;#&gt; &lt;[&gt; So &lt;/[&gt; &lt;/{&gt; it 's likeyou knowS1A-001484485I 'm not sure now &lt;#&gt; We didn't discuss ityou know&lt;S1A-001$A&gt; &lt;#&gt; Well it sounds like more money &lt;S1A-001$B&gt; &lt;#&gt;you knowS1A-001598599on Monday and do without her lesson on Tuesday &lt;,&gt;you know&lt;#&gt; But I was keeping her going cos I saysyou knowS1A-001727728to take it tomorrow &lt;,&gt; that she could take heryou knowthe wee shoulder bag she has &lt;S1A-001$A&gt; &lt;#&gt; Mhm &lt;S1A-001$B&gt;you knowS1A-001808809&lt;,&gt; and &lt;,&gt; sort of show them around &lt;,&gt; uhmyou knowtheir timetable and &lt;,&gt; give them their timetable and showyou know"
  },
  {
    "objectID": "tutorials/kwics/kwics.html#custom-concordances",
    "href": "tutorials/kwics/kwics.html#custom-concordances",
    "title": "Concordancing with R",
    "section": "Custom Concordances",
    "text": "Custom Concordances\nAs R represents a fully-fledged programming environment, we can, of course, also write our own, customized concordance function. The code below shows how you could go about doing so. Note, however, that this function only works if you enter more than a single file.\n\nmykwic &lt;- function(txts, pattern, context) {\n  # activate packages\n  require(stringr)\n  # list files\n  txts &lt;- txts[stringr::str_detect(txts, pattern)]\n  conc &lt;- sapply(txts, function(x) {\n    # determine length of text\n    lngth &lt;- as.vector(unlist(nchar(x)))\n    # determine position of hits\n    idx &lt;- str_locate_all(x, pattern)\n    idx &lt;- idx[[1]]\n    ifelse(nrow(idx) &gt;= 1, idx &lt;- idx, return(NA))\n    # define start position of hit\n    token.start &lt;- idx[, 1]\n    # define end position of hit\n    token.end &lt;- idx[, 2]\n    # define start position of preceding context\n    pre.start &lt;- ifelse(token.start - context &lt; 1, 1, token.start - context)\n    # define end position of preceding context\n    pre.end &lt;- token.start - 1\n    # define start position of subsequent context\n    post.start &lt;- token.end + 1\n    # define end position of subsequent context\n    post.end &lt;- ifelse(token.end + context &gt; lngth, lngth, token.end + context)\n    # extract the texts defined by the positions\n    PreceedingContext &lt;- substring(x, pre.start, pre.end)\n    Token &lt;- substring(x, token.start, token.end)\n    SubsequentContext &lt;- substring(x, post.start, post.end)\n    Id &lt;- 1:length(Token)\n    conc &lt;- cbind(Id, PreceedingContext, Token, SubsequentContext)\n    # return concordance\n    return(conc)\n  })\n  concdf &lt;- do.call(rbind, conc) %&gt;%\n    as.data.frame()\n  return(concdf)\n}\n\nWe can now try if this function works by searching for the sequence you know in the transcripts that we have loaded earlier. One difference between the kwic function provided by the quanteda package and the customized concordance function used here is that the kwic function uses the number of words to define the context window, while the mykwic function uses the number of characters or symbols instead (which is why we use a notably higher number to define the context window).\n\nkwic_youknow &lt;- mykwic(transcripts_collapsed, \"you know\", 50)\n\n\n\nIdPreceedingContextTokenSubsequentContext1 to let me jump &lt;,&gt; that was only the fourth time you know &lt;#&gt; It was great &lt;&&gt; laughter &lt;/&&gt; &lt;S1A-001$A&gt; &lt;#2 with the whip &lt;,&gt; and over it went the last time you know &lt;#&gt; And Stephanie told her she was very determine3ghter &lt;/&&gt; because it had refused the other times you know &lt;#&gt; But Stephanie wouldn't let her give up on it 4k and keep coming back &lt;,&gt; until &lt;,&gt; it jumped it you know &lt;#&gt; It was good &lt;S1A-001$A&gt; &lt;#&gt; Yeah I 'm not so 5she 'd be far better waiting &lt;,&gt; for that one &lt;,&gt; you know and starting anew fresh &lt;S1A-001$A&gt; &lt;#&gt; Yeah but 6er 's the best goes top of the league &lt;,&gt; &lt;{&gt; &lt;[&gt; you know &lt;/[&gt; &lt;S1A-001$A&gt; &lt;#&gt; &lt;[&gt; So &lt;/[&gt; &lt;/{&gt; it 's like \n\n\nAs this concordance function only works for more than one text, we split the text into chapters and assign each section a name.\n\n# read in text\ntext_split &lt;- text %&gt;%\n  stringr::str_squish() %&gt;%\n  stringr::str_split(\"[CHAPTER]{7,7} [XVI]{1,7}\\\\. \") %&gt;%\n  unlist()\ntext_split &lt;- text_split[which(nchar(text_split) &gt; 2000)]\n# add names\nnames(text_split) &lt;- paste0(\"text\", 1:length(text_split))\n# inspect data\nnchar(text_split)\n\n text1  text2  text3  text4  text5  text6  text7  text8  text9 text10 text11 \n 11331  10888   9137  13830  11767  13730  12563  13585  12527  11287  10292 \ntext12 \n 11564 \n\n\nNow that we have named elements, we can search for the pattern poor alice. We also need to clean the concordance as some sections do not contain any instances of the search pattern. To clean the data, we select only the columns File, PreceedingContext, Token, and SubsequentContext and then remove all rows where information is missing.\n\nmykwic_pooralice &lt;- mykwic(text_split, \"poor Alice\", 50)\n\n\n\nIdPreceedingContextTokenSubsequentContext1; “and even if my head would go through,” thought poor Alice, “it would be of very little use without my shoul2d on going into the garden at once; but, alas for poor Alice! when she got to the door, she found she had forg3 to be two people. “But it’s no use now,” thought poor Alice, “to pretend to be two people! Why, there’s hardl1!” “I’m sure those are not the right words,” said poor Alice, and her eyes filled with tears again as she went1lking such nonsense!” “I didn’t mean it!” pleaded poor Alice. “But you’re so easily offended, you know!” The M2onder if I shall ever see you any more!” And here poor Alice began to cry again, for she felt very lonely and \n\n\nYou can go ahead and modify the customized concordance function to suit your needs."
  },
  {
    "objectID": "tutorials/kwics/kwics.html#footnotes",
    "href": "tutorials/kwics/kwics.html#footnotes",
    "title": "Concordancing with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎\nThis data is freely available after registration. To get access to the data represented in the Irish Component of the International Corpus of Eng;lish (or any other component), you or your institution will need a valid licence. You need to send your request from an academic edu e-mail to proof your educational status. To get an academic licence with download access please fill in the licence form (PDF, 82 KB) and send it to ice@es.uzh.ch. You should get the credentials for downloading here and unpacking the corpora within about 10 working days.↩︎"
  },
  {
    "objectID": "tutorials/string/string.html#footnotes",
    "href": "tutorials/string/string.html#footnotes",
    "title": "String Processing in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/regression/regression.html#simple-linear-regression",
    "href": "tutorials/regression/regression.html#simple-linear-regression",
    "title": "Introduction",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nThis section focuses on a very widely used statistical method which is called regression. Regressions are used when we try to understand how independent variables correlate with a dependent or outcome variable. So, if you want to investigate how a certain factor affects an outcome, then a regression is the way to go. We will have a look at two simple examples to understand what the concepts underlying a regression mean and how a regression works. The R code, that we will use, is adapted from many highly recommendable introductions which also focus on regression (among other types of analyses), for example, Gries, Winter, Levshina, Winter or Wilcox. Baayen is also very good but probably not the first book one should read about statistics but it is highly recommendable for advanced learners.\nAlthough the basic logic underlying regressions is identical to the conceptual underpinnings of analysis of variance (ANOVA), a related method, sociolinguists have traditionally favored regression analysis in their studies while ANOVAs have been the method of choice in psycholinguistics. The preference for either method is grounded in historical happenstances and the culture of these subdisciplines rather than in methodological reasoning. However, ANOVA are more restricted in that they can only take numeric dependent variables and they have stricter model assumptions that are violated more readily. In addition, a minor difference between regressions and ANOVA lies in the fact that regressions are based on the \\(t\\)-distribution while ANOVAs use the F-distribution (however, the F-value is simply the value of t squared or t2). Both t- and F-values report on the ratio between explained and unexplained variance.\nThe idea behind regression analysis is expressed formally in the equation below where\\(f_{(x)}\\) is the y-value we want to predict, \\(\\alpha\\) is the intercept (the point where the regression line crosses the y-axis at x = 0), \\(\\beta\\) is the coefficient (the slope of the regression line).\n\\[\\begin{equation}\nf_{(x)} = \\alpha + \\beta_{i}x + \\epsilon\n\\end{equation}\\]\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (x) and add the value of the intercept (point where line crosses the y-axis at x = 0).\nHowever, the idea behind regressions can best be described graphically: imagine a cloud of points (like the points in the scatterplot in the upper left panel below). Regressions aim to find that line which has the minimal summed distance between points and the line (like the line in the lower panels). Technically speaking, the aim of a regression is to find the line with the minimal deviance (or the line with the minimal sum of residuals). Residuals are the distance between the line and the points (the red lines) and it is also called variance.\nThus, regression lines are those lines where the sum of the red lines should be minimal. The slope of the regression line is called coefficient and the point where the regression line crosses the y-axis at x = 0 is called the intercept.\n\n\n\n\n\n\n\n\n\nA word about standard errors (SE) is in order here because most commonly used statistics programs will provide SE values when reporting regression models. The SE is a measure that tells us how much the coefficients were to vary if the same regression were applied to many samples from the same population. A relatively small SE value therefore indicates that the coefficients will remain very stable if the same regression model is fitted to many different samples with identical parameters. In contrast, a large SE tells you that the model is volatile and not very stable or reliable as the coefficients vary substantially if the model is applied to many samples.\nMathematically, the SE is the standard deviation (SD) divided by the square root of the sample size (N) (see below).The SD is the square root of the deviance (that is, the SD is the square root of the sum of the mean \\(\\bar{x}\\) minus each data point (xi) squared divided by the sample size (N) minus 1).\n\\[\\begin{equation}\nStandard Error (SE) = \\frac{\\sum (\\bar{x}-x_{i})^2/N-1}{\\sqrt{N}} = \\frac{SD}{\\sqrt{N}}\n\\end{equation}\\]\n\nExample 1: Preposition Use across Real-Time\nWe will now turn to our first example. In this example, we will investigate whether the frequency of prepositions has changed from Middle English to Late Modern English. The reasoning behind this example is that Old English was highly synthetic compared with Present-Day English which comparatively analytic. In other words, while Old English speakers used case to indicate syntactic relations, speakers of Present-Day English use word order and prepositions to indicate syntactic relationships. This means that the loss of case had to be compensated by different strategies and maybe these strategies continued to develop and increase in frequency even after the change from synthetic to analytic had been mostly accomplished. And this prolonged change in compensatory strategies is what this example will focus on.\nThe analysis is based on data extracted from the Penn Corpora of Historical English (see http://www.ling.upenn.edu/hist-corpora/), that consists of 603 texts written between 1125 and 1900. In preparation of this example, all elements that were part-of-speech tagged as prepositions were extracted from the PennCorpora.\nThen, the relative frequencies (per 1,000 words) of prepositions per text were calculated. This frequency of prepositions per 1,000 words represents our dependent variable. In a next step, the date when each letter had been written was extracted. The resulting two vectors were combined into a table which thus contained for each text, when it was written (independent variable) and its relative frequency of prepositions (dependent or outcome variable).\nA regression analysis will follow the steps described below:\n\nExtraction and processing of the data\nData visualization\nApplying the regression analysis to the data\nDiagnosing the regression model and checking whether or not basic model assumptions have been violated.\n\nIn a first step, we load functions that we may need (which in this case is a function that we will use to summarize the results of the analysis).\n\n# load functions\nsource(\"rscripts/slrsummary.r\")\n\nAfter preparing our session, we can now load and inspect the data to get a first impression of its properties.\n\n# load data\nslrdata  &lt;- base::readRDS(\"tutorials/regression/data/sld.rda\", \"rb\")\n\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nInspecting the data is very important because it can happen that a data set may not load completely or that variables which should be numeric have been converted to character variables. If unchecked, then such issues could go unnoticed and cause trouble.\nWe will now plot the data to get a better understanding of what the data looks like.\n\np1 &lt;- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth()\np2 &lt;- ggplot(slrdata, aes(Date, Prepositions)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\") # with linear model smoothing!\n# display plots\nggpubr::ggarrange(p1, p2, ncol = 2, nrow = 1)\n\n\n\n\n\n\n\n\nBefore beginning with the regression analysis, we will center the year. We center the values of year by subtracting each value from the mean of year. This can be useful when dealing with numeric variables because if we did not center year, we would get estimated values for year 0 (a year when English did not even exist yet). If a variable is centered, the regression provides estimates of the model refer to the mean of that numeric variable. In other words, centering can be very helpful, especially with respect to the interpretation of the results that regression models report.\n\n# center date\nslrdata$Date &lt;- slrdata$Date - mean(slrdata$Date) \n\nWe will now begin the regression analysis by generating a first regression model and inspect its results.\n\n# create initial model\nm1.lm &lt;- lm(Prepositions ~ Date, data = slrdata)\n# inspect results\nsummary(m1.lm)\n\n\nCall:\nlm(formula = Prepositions ~ Date, data = slrdata)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-69.1012471 -13.8549421   0.5779091  13.3208913  62.8580401 \n\nCoefficients:\n                   Estimate      Std. Error   t value             Pr(&gt;|t|)    \n(Intercept) 132.19009310987   0.83863748040 157.62483 &lt; 0.0000000000000002 ***\nDate          0.01732180307   0.00726746646   2.38347             0.017498 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.4339648 on 535 degrees of freedom\nMultiple R-squared:  0.010507008,   Adjusted R-squared:  0.00865748837 \nF-statistic: 5.68093894 on 1 and 535 DF,  p-value: 0.017498081\n\n\nThe summary output starts by repeating the regression equation. Then, the model provides the distribution of the residuals. The residuals should be distributed normally with the absolute values of the Min and Max as well as the 1Q (first quartile) and 3Q (third quartile) being similar or ideally identical. In our case, the values are very similar which suggests that the residuals are distributed evenly and follow a normal distribution. The next part of the report is the coefficients table. The estimate for the intercept is the value of y at x = 0. The estimate for Date represents the slope of the regression line and tells us that with each year, the predicted frequency of prepositions increase by .01732 prepositions. The t-value is the Estimate divided by the standard error (Std. Error). Based on the t-value, the p-value can be calculated manually as shown below.\n\n# use pt function (which uses t-values and the degrees of freedom)\n2*pt(-2.383, nrow(slrdata)-1)\n\n[1] 0.0175196401501\n\n\nThe R2-values tell us how much variance is explained by our model. The baseline value represents a model that uses merely the mean. 0.0105 means that our model explains only 1.05 percent of the variance (0.010 x 100) - which is a tiny amount. The problem of the multiple R2 is that it will increase even if we add variables that explain almost no variance. Hence, multiple R2 encourages the inclusion of junk variables.\n\\[\\begin{equation}\nR^2 = R^2_{multiple} = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar y)^2}\n\\end{equation}\\]\nThe adjusted R2-value takes the number of predictors into account and, thus, the adjusted R2 will always be lower than the multiple R2. This is so because the adjusted R2 penalizes models for having predictors. The equation for the adjusted R2 below shows that the amount of variance that is explained by all the variables in the model (the top part of the fraction) must outweigh the inclusion of the number of variables (k) (lower part of the fraction). Thus, the adjusted R2 will decrease when variables are added that explain little or even no variance while it will increase if variables are added that explain a lot of variance.\n\\[\\begin{equation}\nR^2_{adjusted} = 1 - (\\frac{(1 - R^2)(n - 1)}{n - k - 1})\n\\end{equation}\\]\nIf there is a big difference between the two R2-values, then the model contains (many) predictors that do not explain much variance which is not good. The F-statistic and the associated p-value tell us that the model, despite explaining almost no variance, is still significantly better than an intercept-only base-line model (or using the overall mean to predict the frequency of prepositions per text).\nWe can test this and also see where the F-values comes from by comparing the\n\n# create intercept-only base-line model\nm0.lm &lt;- lm(Prepositions ~ 1, data = slrdata)\n# compare the base-line and the more saturated model\nanova(m1.lm, m0.lm, test = \"F\")\n\nAnalysis of Variance Table\n\nModel 1: Prepositions ~ Date\nModel 2: Prepositions ~ 1\n  Res.Df         RSS Df   Sum of Sq       F   Pr(&gt;F)  \n1    535 202058.2576                                  \n2    536 204203.8289 -1 -2145.57126 5.68094 0.017498 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F- and p-values are exactly those reported by the summary which shows where the F-values comes from and what it means; namely it denote the difference between the base-line and the more saturated model.\nThe degrees of freedom associated with the residual standard error are the number of cases in the model minus the number of predictors (including the intercept). The residual standard error is square root of the sum of the squared residuals of the model divided by the degrees of freedom. Have a look at he following to clear this up:\n\n# DF = N - number of predictors (including intercept)\nDegreesOfFreedom &lt;- nrow(slrdata)-length(coef(m1.lm))\n# sum of the squared residuals\nSumSquaredResiduals &lt;- sum(resid(m1.lm)^2)\n# Residual Standard Error\nsqrt(SumSquaredResiduals/DegreesOfFreedom); DegreesOfFreedom\n\n[1] 19.4339647585\n\n\n[1] 535\n\n\nWe will now check if mathematical assumptions have been violated (homogeneity of variance) or whether the data contains outliers. We check this using diagnostic plots.\n\n# generate data\ndf2 &lt;- data.frame(id = 1:length(resid(m1.lm)),\n                 residuals = resid(m1.lm),\n                 standard = rstandard(m1.lm),\n                 studend = rstudent(m1.lm))\n# generate plots\np1 &lt;- ggplot(df2, aes(x = id, y = residuals)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Residuals\", x = \"Index\")\np2 &lt;- ggplot(df2, aes(x = id, y = standard)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Standardized Residuals\", x = \"Index\")\np3 &lt;- ggplot(df2, aes(x = id, y = studend)) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  geom_point() +\n  labs(y = \"Studentized Residuals\", x = \"Index\")\n# display plots\nggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)\n\n\n\n\n\n\n\n\nThe left graph shows the residuals of the model (i.e., the differences between the observed and the values predicted by the regression model). The problem with this plot is that the residuals are not standardized and so they cannot be compared to the residuals of other models. To remedy this deficiency, residuals are normalized by dividing the residuals by their standard deviation. Then, the normalized residuals can be plotted against the observed values (center panel). In this way, not only are standardized residuals obtained, but the values of the residuals are transformed into z-values, and one can use the z-distribution to find problematic data points. There are three rules of thumb regarding finding problematic data points through standardized residuals (Field, Miles, and Field, 268–69):\n\nPoints with values higher than 3.29 should be removed from the data.\nIf more than 1% of the data points have values higher than 2.58, then the error rate of our model is too high.\nIf more than 5% of the data points have values greater than 1.96, then the error rate of our model is too high.\n\nThe right panel shows the * studentized residuals* (adjusted predicted values: each data point is divided by the standard error of the residuals). In this way, it is possible to use Student’s t-distribution to diagnose our model.\nAdjusted predicted values are residuals of a special kind: the model is calculated without a data point and then used to predict this data point. The difference between the observed data point and its predicted value is then called the adjusted predicted value. In summary, studentized residuals are very useful because they allow us to identify influential data points.\nThe plots show that there are two potentially problematic data points (the top-most and bottom-most point). These two points are clearly different from the other data points and may therefore be outliers. We will test later if these points need to be removed.\nWe will now generate more diagnostic plots.\n\n# generate plots\nautoplot(m1.lm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\n\n\n\n\nThe diagnostic plots are very positive and we will go through why this is so for each panel. The graph in the upper left panel is useful for finding outliers or for determining the correlation between residuals and predicted values: when a trend becomes visible in the line or points (e.g., a rising trend or a zigzag line), then this would indicate that the model would be problematic (in such cases, it can help to remove data points that are too influential (outliers)).\nThe graphic in the upper right panel indicates whether the residuals are normally distributed (which is desirable) or whether the residuals do not follow a normal distribution. If the points lie on the line, the residuals follow a normal distribution. For example, if the points are not on the line at the top and bottom, it shows that the model does not predict small and large values well and that it therefore does not have a good fit.\nThe graphic in the lower left panel provides information about homoscedasticity. Homoscedasticity means that the variance of the residuals remains constant and does not correlate with any independent variable. In unproblematic cases, the graphic shows a flat line. If there is a trend in the line, we are dealing with heteroscedasticity, that is, a correlation between independent variables and the residuals, which is very problematic for regressions.\nThe graph in the lower right panel shows problematic influential data points that disproportionately affect the regression (this would be problematic). If such influential data points are present, they should be either weighted (one could generate a robust rather than a simple linear regression) or they must be removed. The graph displays Cook’s distance, which shows how the regression changes when a model without this data point is calculated. The cook distance thus shows the influence a data point has on the regression as a whole. Data points that have a Cook’s distance value greater than 1 are problematic (Field, Miles, and Field, 269).\nThe so-called leverage is also a measure that indicates how strongly a data point affects the accuracy of the regression. Leverage values range between 0 (no influence) and 1 (strong influence: suboptimal!). To test whether a specific data point has a high leverage value, we calculate a cut-off point that indicates whether the leverage is too strong or still acceptable. The following two formulas are used for this:\n\\[\\begin{equation}\nLeverage = \\frac{3(k + 1)}{n} |  \\frac{2(k + 1)}{n}\n\\end{equation}\\]\nWe will look more closely at leverage in the context of multiple linear regression and will therefore end the current analysis by summarizing the results of the regression analysis in a table.\n\n# create summary table\nslrsummary(m1.lm)  \n\n\n\nParametersEstimatePearson's rStd. Errort valuePr(&gt;|t|)P-value sig.(Intercept)132.190.84157.620p &lt; .001***Date0.020.10.012.380.0175p &lt; .05*Model statisticsValueNumber of cases in model537Residual standard error on 535 DF19.43Multiple R-squared0.0105Adjusted R-squared0.0087F-statistic (1, 535)5.68Model p-value0.0175\n\n\nAn alternative but less informative summary table of the results of a regression analysis can be generated using the tab_model function from the sjPlot package (Lüdecke) (as is shown below).\n\n# generate summary table\nsjPlot::tab_model(m1.lm) \n\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n132.19\n130.54 – 133.84\n&lt;0.001\n\n\nDate\n0.02\n0.00 – 0.03\n0.017\n\n\nObservations\n537\n\n\nR2 / R2 adjusted\n0.011 / 0.009\n\n\n\n\n\n\n\n\nTypically, the results of regression analyses are presented in such tables as they include all important measures of model quality and significance, as well as the magnitude of the effects.\nIn addition, the results of simple linear regressions should be summarized in writing.\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(m1.lm)\n\nWe fitted a linear model (estimated using OLS) to predict Prepositions with\nDate (formula: Prepositions ~ Date). The model explains a statistically\nsignificant and very weak proportion of variance (R2 = 0.01, F(1, 535) = 5.68,\np = 0.017, adj. R2 = 8.66e-03). The model's intercept, corresponding to Date =\n0, is at 132.19 (95% CI [130.54, 133.84], t(535) = 157.62, p &lt; .001). Within\nthis model:\n\n  - The effect of Date is statistically significant and positive (beta = 0.02,\n95% CI [3.05e-03, 0.03], t(535) = 2.38, p = 0.017; Std. beta = 0.10, 95% CI\n[0.02, 0.19])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nWe can use this output to write up a final report:\nA simple linear regression has been fitted to the data. A visual assessment of the model diagnostic graphics did not indicate any problematic or disproportionately influential data points (outliers) and performed significantly better compared to an intercept-only base line model but only explained .87 percent of the variance (adjusted R2: .0087, F-statistic (1, 535): 5,68, p-value: 0.0175*). The final minimal adequate linear regression model is based on 537 data points and confirms a significant and positive correlation between the year in which the text was written and the relative frequency of prepositions (coefficient estimate: .02 (standardized : 0.10, 95% CI [0.02, 0.19]), SE: 0.01, t-value535: 2.38, p-value: .0175*). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation.\n\n\nExample 2: Teaching Styles\nIn the previous example, we dealt with two numeric variables, while the following example deals with a categorical independent variable and a numeric dependent variable. The ability for regressions to handle very different types of variables makes regressions a widely used and robust method of analysis.\nIn this example, we are dealing with two groups of students that have been randomly assigned to be exposed to different teaching methods. Both groups undergo a language learning test after the lesson with a maximum score of 20 points.\nThe question that we will try to answer is whether the students in group A have performed significantly better than those in group B which would indicate that the teaching method to which group A was exposed works better than the teaching method to which group B was exposed.\nLet’s move on to implementing the regression in R. In a first step, we load the data set and inspect its structure.\n\n# load data\nslrdata2  &lt;- base::readRDS(\"tutorials/regression/data/sgd.rda\", \"rb\")\n\n\n\nGroupScoreA15A12A11A18A15A15A9A19A14A13A11A12A18A15A16\n\n\nNow, we graphically display the data. In this case, a boxplot represents a good way to visualize the data.\n\n# extract means\nslrdata2 %&gt;%\n  dplyr::group_by(Group) %&gt;%\n  dplyr::mutate(Mean = round(mean(Score), 1), SD = round(sd(Score), 1)) %&gt;%\n  ggplot(aes(Group, Score)) + \n  geom_boxplot(fill=c(\"orange\", \"darkgray\")) +\n  geom_text(aes(label = paste(\"M = \", Mean, sep = \"\"), y = 1)) +\n  geom_text(aes(label = paste(\"SD = \", SD, sep = \"\"), y = 0)) +\n  theme_bw(base_size = 15) +\n  labs(x = \"Group\") +                      \n  labs(y = \"Test score (Points)\", cex = .75) +   \n  coord_cartesian(ylim = c(0, 20)) +  \n  guides(fill = FALSE)                \n\n\n\n\n\n\n\n\nThe data indicate that group A did significantly better than group B. We will test this impression by generating the regression model and creating the model and extracting the model summary.\n\n# generate regression model\nm2.lm &lt;- lm(Score ~ Group, data = slrdata2) \n# inspect results\nsummary(m2.lm)                             \n\n\nCall:\nlm(formula = Score ~ Group, data = slrdata2)\n\nResiduals:\n        Min          1Q      Median          3Q         Max \n-6.76666667 -1.93333333  0.15000000  2.06666667  6.23333333 \n\nCoefficients:\n                Estimate   Std. Error  t value               Pr(&gt;|t|)    \n(Intercept) 14.933333333  0.534571121 27.93517 &lt; 0.000000000000000222 ***\nGroupB      -3.166666667  0.755997730 -4.18873            0.000096692 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.92796662 on 58 degrees of freedom\nMultiple R-squared:  0.232249929,   Adjusted R-squared:  0.219012859 \nF-statistic:  17.545418 on 1 and 58 DF,  p-value: 0.0000966923559\n\n\nThe model summary reports that Group A performed significantly better compared with Group B. This is shown by the fact that the p-value (the value in the column with the header (Pr(&gt;|t|)) is smaller than .001 as indicated by the three * after the p-values). Also, the negative Estimate for Group B indicates that Group B has lower scores than Group A. We will now generate the diagnostic graphics.3\n\npar(mfrow = c(1, 3))        # plot window: 1 plot/row, 3 plots/column\nplot(resid(m2.lm))     # generate diagnostic plot\nplot(rstandard(m2.lm)) # generate diagnostic plot\nplot(rstudent(m2.lm)); par(mfrow = c(1, 1))  # restore normal plot window\n\n\n\n\n\n\n\n\nThe graphics do not indicate outliers or other issues, so we can continue with more diagnostic graphics.\n\npar(mfrow = c(2, 2)) # generate a plot window with 2x2 panels\nplot(m2.lm); par(mfrow = c(1, 1)) # restore normal plot window\n\n\n\n\n\n\n\n\nThese graphics also show no problems. In this case, the data can be summarized in the next step.\n\n# tabulate results\nslrsummary(m2.lm)\n\n\n\nParametersEstimatePearson's rStd. Errort valuePr(&gt;|t|)P-value sig.(Intercept)14.930.5327.940p &lt; .001***GroupB-3.170.480.76-4.190.0001p &lt; .001***Model statisticsValueNumber of cases in model60Residual standard error on 58 DF2.93Multiple R-squared0.2322Adjusted R-squared0.219F-statistic (1, 58)17.55Model p-value0.0001\n\n\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(m2.lm)\n\nWe fitted a linear model (estimated using OLS) to predict Score with Group\n(formula: Score ~ Group). The model explains a statistically significant and\nmoderate proportion of variance (R2 = 0.23, F(1, 58) = 17.55, p &lt; .001, adj. R2\n= 0.22). The model's intercept, corresponding to Group = A, is at 14.93 (95% CI\n[13.86, 16.00], t(58) = 27.94, p &lt; .001). Within this model:\n\n  - The effect of Group [B] is statistically significant and negative (beta =\n-3.17, 95% CI [-4.68, -1.65], t(58) = -4.19, p &lt; .001; Std. beta = -0.96, 95%\nCI [-1.41, -0.50])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nWe can use this output to write up a final report:\nA simple linear regression was fitted to the data. A visual assessment of the model diagnostics did not indicate any problematic or disproportionately influential data points (outliers). The final linear regression model is based on 60 data points, performed significantly better than an intercept-only base line model (F (1, 58): 17.55, p-value &lt;. 001\\(***\\)), and reported that the model explained 21.9 percent of variance which confirmed a good model fit. According to this final model, group A scored significantly better on the language learning test than group B (coefficient: -3.17, 95% CI [-4.68, -1.65], Std. : -0.96, 95% CI [-1.41, -0.50], SE: 0.48, t-value58: -4.19, p-value &lt;. 001\\(***\\)). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."
  },
  {
    "objectID": "tutorials/regression/regression.html#multiple-linear-regression",
    "href": "tutorials/regression/regression.html#multiple-linear-regression",
    "title": "Introduction",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nIn contrast to simple linear regression, which estimates the effect of a single predictor, multiple linear regression estimates the effect of various predictor (see the equation below). A multiple linear regression can thus test the effects of various predictors simultaneously.\n\\[\\begin{equation}\n\nf_{(x)} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i+1} + \\dots + \\beta_{n}x_{i+n} + \\epsilon\n\n\\end{equation}\\]\nThere exists a wealth of literature focusing on multiple linear regressions and the concepts it is based on. For instance, there are Achen, Bortz, Crawley, Faraway, Field, Miles, and Field, Gries, Levshina, Winter and Wilcox to name just a few. Introductions to regression modeling in R are Baayen, Crawley, Gries, or Levshina.\nThe model diagnostics we are dealing with here are partly identical to the diagnostic methods discussed in the section on simple linear regression. Because of this overlap, diagnostics will only be described in more detail if they have not been described in the section on simple linear regression.\n\n\n\n\nEXCURSION\n\n\n\n\n\n\n`\n\n\nA note on sample size and power\n\nA brief note on minimum necessary sample or data set size appears necessary here. Although there appears to be a general assumption that 25 data points per group are sufficient, this is not necessarily correct (it is merely a general rule of thumb that is actually often incorrect). Such rules of thumb are inadequate because the required sample size depends on the number of variables in a given model, the size of the effect and the variance of the effect - in other words, the minimum necessary sample size relates to statistical power (see here for a tutorial on power). If a model contains many variables, then this requires a larger sample size than a model which only uses very few predictors.\nAlso, to detect an effect with a very minor effect size, one needs a substantially larger sample compared to cases where the effect is very strong. In fact, when dealing with small effects, model require a minimum of 600 cases to reliably detect these effects. Finally, effects that are very robust and do not vary much require a much smaller sample size compared with effects that are spurious and vary substantially. Since the sample size depends on the effect size and variance as well as the number of variables, there is no final one-size-fits-all answer to what the best sample size is.\nAnother, slightly better but still incorrect, rule of thumb is that the more data, the better. This is not correct because models based on too many cases are prone for overfitting and thus report correlations as being significant that are not. However, given that there are procedures that can correct for overfitting, larger data sets are still preferable to data sets that are simply too small to warrant reliable results. In conclusion, it remains true that the sample size depends on the effect under investigation.\n\n\n`\n\nDespite there being no ultimate rule of thumb, Field, Miles, and Field (273–75), based on Green, provide data-driven suggestions for the minimal size of data required for regression models that aim to find medium sized effects (k = number of predictors; categorical variables with more than two levels should be transformed into dummy variables):\n\nIf one is merely interested in the overall model fit (something I have not encountered), then the sample size should be at least 50 + k (k = number of predictors in model).\nIf one is only interested in the effect of specific variables, then the sample size should be at least 104 + k (k = number of predictors in model).\nIf one is only interested in both model fit and the effect of specific variables, then the sample size should be at least the higher value of 50 + k or 104 + k (k = number of predictors in model).\n\nYou will see in the R code below that there is already a function that tests whether the sample size is sufficient.\n\nExample: Gifts and Availability\nThe example we will go through here is taken from Field, Miles, and Field. In this example, the research question is if the money that men spend on presents for women depends on the women’s attractiveness and their relationship status. To answer this research question, we will implement a multiple linear regression and start by loading the data and inspect its structure and properties.\n\n# load data\nmlrdata  &lt;- base::readRDS(\"tutorials/regression/data/mld.rda\", \"rb\")\n\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nThe data set consist of three variables stored in three columns. The first column contains the relationship status of the present giver (in this study this were men), the second whether the man is interested in the woman (the present receiver in this study), and the third column represents the money spend on the present. The data set represents 100 cases and the mean amount of money spend on a present is 88.38 dollars. In a next step, we visualize the data to get a more detailed impression of the relationships between variables.\n\n# create plots\np1 &lt;- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot() + \n  theme_bw()\n# plot 2\np2 &lt;- ggplot(mlrdata, aes(attraction, money)) +\n  geom_boxplot() +\n  theme_bw()\n# plot 3\np3 &lt;- ggplot(mlrdata, aes(x = money)) +\n  geom_histogram(aes(y=..density..)) +            \n  theme_bw() +         \n  geom_density(alpha=.2, fill = \"gray50\") \n# plot 4\np4 &lt;- ggplot(mlrdata, aes(status, money)) +\n  geom_boxplot(aes(fill = factor(status))) + \n  scale_fill_manual(values = c(\"grey30\", \"grey70\")) + \n  facet_wrap(~ attraction) + \n  guides(fill = \"none\") +\n  theme_bw()\n# show plots\ngridExtra::grid.arrange(grobs = list(p1, p2, p3, p4), widths = c(1, 1), layout_matrix = rbind(c(1, 2), c(3, 4)))\n\n\n\n\n\n\n\n\nThe upper left figure consists of a boxplot which shows how much money was spent by relationship status. The figure suggests that men spend more on women if they are not in a relationship. The next figure shows the relationship between the money spend on presents and whether or not the men were interested in the women.\nThe boxplot in the upper right panel suggests that men spend substantially more on women if the men are interested in them. The next figure depicts the distribution of the amounts of money spend on the presents for the women. In addition, the figure indicates the existence of two outliers (dots in the boxplot)\nThe histogram in the lower left panel shows that, although the mean amount of money spent on presents is 88.38 dollars, the distribution peaks around 50 dollars indicating that on average, men spend about 50 dollars on presents. Finally, we will plot the amount of money spend on presents against relationship status by attraction in order to check whether the money spent on presents is affected by an interaction between attraction and relationship status.\nThe boxplot in the lower right panel confirms the existence of an interaction (a non-additive term) as men only spend more money on women if the men single and they are interested in the women. If men are not interested in the women, then the relationship has no effect as they spend an equal amount of money on the women regardless of whether they are in a relationship or not.\nWe will now start to implement the regression model. In a first step, we create two saturated models that contain all possible predictors (main effects and interactions). The two models are identical but one is generated with the lm and the other with the glm function as these functions offer different model parameters in their output.\n\nm1.mlr = lm(                      # generate lm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  data = mlrdata)                 # def. data\nm1.glm = glm(                     # generate glm regression object\n  money ~ 1 + attraction*status,  # def. regression formula (1 = intercept)\n  family = gaussian,              # def. linkage function\n  data = mlrdata)                 # def. data\n\nAfter generating the saturated models we can now start with the model fitting. Model fitting refers to a process that aims at find the model that explains a maximum of variance with a minimum of predictors (see Field, Miles, and Field, 318). Model fitting is therefore based on the principle of parsimony which is related to Occam’s razor according to which explanations that require fewer assumptions are more likely to be true.\n\n\nAutomatic Model Fitting and Why You Should Not Use It\nIn this section, we will use a step-wise step-down procedure that uses decreases in AIC (Akaike Information Criterion) as the criterion to minimize the model in a step-wise manner. This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value.\nWe use this method here just so that you know it exists and how to implement it but you should rather avoid using automated model fitting. The reason for avoiding automated model fitting is that the algorithm only checks if the AIC has decreased but not if the model is stable or reliable. Thus, automated model fitting has the problem that you can never be sure that the way that lead you to the final model is reliable and that all models were indeed stable. Imagine you want to climb down from a roof top and you have a ladder. The problem is that you do not know if and how many steps are broken. This is similar to using automated model fitting. In other sections, we will explore better methods to fit models (manual step-wise step-up and step-down procedures, for example).\nThe AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same data set with the same (number of) cases (LL stands for logged likelihood or LogLikelihood and k represents the number of predictors in the model (including the intercept); the LL represents a measure of how good the model fits the data).\n\\[\\begin{equation}\nAkaike Information Criterion (AIC) = -2LL + 2k\n\\end{equation}\\]\nAn alternative to the AIC is the BIC (Bayesian Information Criterion). Both AIC and BIC penalize models for including variables in a model. The penalty of the BIC is bigger than the penalty of the AIC and it includes the number of cases in the model (LL stands for logged likelihood or LogLikelihood, k represents the number of predictors in the model (including the intercept), and N represents the number of cases in the model).\n\\[\\begin{equation}\nBayesian Information Criterion (BIC) = -2LL + 2k * log(N)\n\\end{equation}\\]\nInteractions are evaluated first and only if all insignificant interactions have been removed would the procedure start removing insignificant main effects (that are not part of significant interactions). Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model.\n\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n\nStart:  AIC=592.52\nmoney ~ 1 + attraction * status\n\n                    Df   Sum of Sq         RSS         AIC\n&lt;none&gt;                             34557.56428 592.5211556\n- attraction:status  1 24947.25481 59504.81909 644.8642395\n\n\n\nCall:\nlm(formula = money ~ 1 + attraction * status, data = mlrdata)\n\nCoefficients:\n                         (Intercept)               attractionNotInterested  \n                             99.1548                              -47.6628  \n                        statusSingle  attractionNotInterested:statusSingle  \n                             57.6928                              -63.1788  \n\n\nThe automated model fitting procedure informs us that removing predictors has not caused a decrease in the AIC. The saturated model is thus also the final minimal adequate model. We will now inspect the final minimal model and go over the model report.\n\nm2.mlr = lm(                       # generate lm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  data = mlrdata)                  # def. data\nm2.glm = glm(                      # generate glm regression object\n  money ~ (status + attraction)^2, # def. regression formula\n  family = gaussian,               # def. linkage function\n  data = mlrdata)                  # def. data\n# inspect final minimal model\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-45.0760 -14.2580   0.4596  11.9315  44.1424 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.79459947 26.13050\nstatusSingle                          57.69280000   5.36637403 10.75080\nattractionNotInterested              -47.66280000   5.36637403 -8.88175\nstatusSingle:attractionNotInterested -63.17880000   7.58919893 -8.32483\n                                                   Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.000000000000000222 ***\nstatusSingle                         &lt; 0.000000000000000222 ***\nattractionNotInterested                 0.00000000000003751 ***\nstatusSingle:attractionNotInterested    0.00000000000058085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.9729973 on 96 degrees of freedom\nMultiple R-squared:  0.852041334,   Adjusted R-squared:  0.847417626 \nF-statistic: 184.276619 on 3 and 96 DF,  p-value: &lt; 0.0000000000000002220446\n\n\nThe first element of the report is called Call and it reports the regression formula of the model. Then, the report provides the residual distribution (the range, median and quartiles of the residuals) which allows drawing inferences about the distribution of differences between observed and expected values. If the residuals are distributed non-normally, then this is a strong indicator that the model is unstable and unreliable because mathematical assumptions on which the model is based are violated.\nNext, the model summary reports the most important part: a table with model statistics of the fixed-effects structure of the model. The table contains the estimates (coefficients of the predictors), standard errors, t-values, and the p-values which show whether a predictor significantly correlates with the dependent variable that the model investigates.\nAll main effects (status and attraction) as well as the interaction between status and attraction is reported as being significantly correlated with the dependent variable (money). An interaction occurs if a correlation between the dependent variable and a predictor is affected by another predictor.\nThe top most term is called intercept and has a value of 99.15 which represents the base estimate to which all other estimates refer. To exemplify what this means, let us consider what the model would predict that a man would spend on a present if he interested in the woman but he is also in a relationship. The amount he would spend (based on the model would be 99.15 dollars (which is the intercept). This means that the intercept represents the predicted value if all predictors take the base or reference level. And since being in relationship but being interested are the case, and because the interaction does not apply, the predicted value in our example is exactly the intercept (see below).\n\n#intercept  Single  NotInterested  Single:NotInterested\n99.15     + 57.69  + 0           + 0     # 156.8 single + interested\n\n[1] 156.84\n\n99.15     + 57.69  - 47.66       - 63.18 # 46.00 single + not interested\n\n[1] 46\n\n99.15     - 0      + 0           - 0     # 99.15 relationship + interested\n\n[1] 99.15\n\n99.15     - 0      - 47.66       - 0     # 51.49 relationship + not interested\n\n[1] 51.49\n\n\nNow, let us consider what a man would spend if he is in a relationship and he is not attracted to the women. In that case, the model predicts that the man would spend only 51.49 dollars on a present: the intercept (99.15) minus 47.66 because the man is not interested (and no additional subtraction because the interaction does not apply).\nWe can derive the same results easier using the predict function.\n\n# make prediction based on the model for original data\nprediction &lt;- predict(m2.mlr, newdata = mlrdata)\n# inspect predictions\ntable(round(prediction,2))\n\n\n 46.01  51.49  99.15 156.85 \n    25     25     25     25 \n\n\nBelow the table of coefficients, the regression summary reports model statistics that provide information about how well the model performs. The difference between the values and the values in the coefficients table is that the model statistics refer to the model as a whole rather than focusing on individual predictors.\nThe multiple R2-value is a measure of how much variance the model explains. A multiple R2-value of 0 would inform us that the model does not explain any variance while a value of .852 mean that the model explains 85.2 percent of the variance. A value of 1 would inform us that the model explains 100 percent of the variance and that the predictions of the model match the observed values perfectly. Multiplying the multiple R2-value thus provides the percentage of explained variance. Models that have a multiple R2-value equal or higher than .05 are deemed substantially significant (see Szmrecsanyi, 55). It has been claimed that models should explain a minimum of 5 percent of variance but this is problematic as it is not uncommon for models to have very low explanatory power while still performing significantly and systematically better than chance. In addition, the total amount of variance is negligible in cases where one is interested in very weak but significant effects. It is much more important for model to perform significantly better than minimal base-line models because if this is not the case, then the model does not have any predictive and therefore no explanatory power.\nThe adjusted R2-value considers the amount of explained variance in light of the number of predictors in the model (it is thus somewhat similar to the AIC and BIC) and informs about how well the model would perform if it were applied to the population that the sample is drawn from. Ideally, the difference between multiple and adjusted R2-value should be very small as this means that the model is not overfitted. If, however, the difference between multiple and adjusted R2-value is substantial, then this would strongly suggest that the model is unstable and overfitted to the data while being inadequate for drawing inferences about the population. Differences between multiple and adjusted R2-values indicate that the data contains outliers that cause the distribution of the data on which the model is based to differ from the distributions that the model mathematically requires to provide reliable estimates. The difference between multiple and adjusted R2-value in our model is very small (85.2-84.7=.05) and should not cause concern.\nBefore continuing, we will calculate the confidence intervals of the coefficients.\n\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n\n                                              2.5 %         97.5 %\n(Intercept)                           91.6225795890 106.6870204110\nstatusSingle                          47.0406317400  68.3449682600\nattractionNotInterested              -58.3149682600 -37.0106317400\nstatusSingle:attractionNotInterested -78.2432408219 -48.1143591781\n\n# create and compare baseline- and minimal adequate model\nm0.mlr &lt;- lm(money ~1, data = mlrdata)\nanova(m0.mlr, m2.mlr)\n\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(&gt;F)    \n1     99 233562.28650                                                    \n2     96  34557.56428  3 199004.7222 184.27662 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we compare the final minimal adequate model to the base-line model to test whether then final model significantly outperforms the baseline model.\n\n# compare baseline- and minimal adequate model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(&gt;F)    \n(Intercept) 781015.8300  1 2169.64133 &lt; 0.000000000000000222 ***\nResiduals    34557.5643 96                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe comparison between the two model confirms that the minimal adequate model performs significantly better (makes significantly more accurate estimates of the outcome variable) compared with the baseline model.\n\n\nModel diagnostics\nWe now check the model performance using the model_performance() function and to diagnose the model, we are using the check_model() function from the performance package (ludeke2021performance?).\nWe start by checking the performance of the null-model against the performance values of the final minimal adequate model. Ideally, the final minimal adequate model would show higher explanatory power (higher R2 values) and better parsimony (lower AIC and AICc as well as BIC values).\n\nperformance::model_performance(m0.mlr)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n--------------------------------------------------------------------\n1063.391 | 1063.515 | 1068.601 | 0.000 |     0.000 | 48.328 | 48.572\n\nperformance::model_performance(m2.mlr)\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n-----------------------------------------------------------------\n878.309 | 878.947 | 891.335 | 0.852 |     0.847 | 18.590 | 18.973\n\n\nThe output confirms that the final minimal adequate model performs substantively better than the null model (higher explanatory power and better parsimony). We now diagnose the model using the check_model() function.\n\nperformance::check_model(m2.mlr)\n\n\n\n\n\n\n\n\n\n\nOutlier Detection\nIn a next step, we now need to look for outliers check whether removing data points disproportionately decreases model fit. To begin with, we generate diagnostic plots.\n\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe plots do not show severe problems such as funnel shaped patterns or drastic deviations from the diagonal line in Normal Q-Q plot (have a look at the explanation of what to look for and how to interpret these diagnostic plots in the section on simple linear regression) but data points 52, 64, and 83 are repeatedly indicated as potential outliers.\n\n# determine a cutoff for data points that have D-values higher than 4/(n-k-1)\ncutoff &lt;- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 3 rows/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n\n[1] 52 83\n\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nThe graphs indicate that data points 52, 64, and 83 may be problematic. We will therefore statistically evaluate whether these data points need to be removed. In order to find out which data points require removal, we extract the influence measure statistics and add them to out data set.\n\n# extract influence statistics\ninfl &lt;- influence.measures(m2.mlr)\n# add infl. statistics to data\nmlrdata &lt;- data.frame(mlrdata, infl[[1]], infl[[2]])\n# annotate too influential data points\nremove &lt;- apply(infl$is.inf, 1, function(x) {\n  ifelse(x == TRUE, return(\"remove\"), return(\"keep\")) } )\n# add annotation to data\nmlrdata &lt;- data.frame(mlrdata, remove)\n# number of rows before removing outliers\nnrow(mlrdata)\n\n[1] 100\n\n# remove outliers\nmlrdata &lt;- mlrdata[mlrdata$remove == \"keep\", ]\n# number of rows after removing outliers\nnrow(mlrdata)\n\n[1] 98\n\n\nThe difference in row in the data set before and after removing data points indicate that two data points which represented outliers have been removed.\n\n\n\nNOTEIn general, outliers should not simply be removed unless there are good reasons for it (this could be that the outliers represent measurement errors). If a data set contains outliers, one should rather switch to methods that are better at handling outliers, e.g. by using weights to account for data points with high leverage. One alternative would be to switch to a robust regression (see here). However, here we show how to proceed by removing outliers as this is a common, though potentially problematic, method of dealing with outliers.\n\n\n\n\n\n\n\n\nRerun Regression\nAs we have decided to remove the outliers which means that we are now dealing with a different data set, we need to rerun the regression analysis. As the steps are identical to the regression analysis performed above, the steps will not be described in greater detail.\n\n# recreate regression models on new data\nm0.mlr = lm(money ~ 1, data = mlrdata)\nm0.glm = glm(money ~ 1, family = gaussian, data = mlrdata)\nm1.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm1.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# automated AIC based model fitting\nstep(m1.mlr, direction = \"both\")\n\nStart:  AIC=570.29\nmoney ~ (status + attraction)^2\n\n                    Df Sum of Sq         RSS         AIC\n&lt;none&gt;                           30411.31714 570.2850562\n- status:attraction  1 21646.862 52058.17914 620.9646729\n\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nCoefficients:\n                         (Intercept)                          statusSingle  \n                          99.1548000                            55.8535333  \n             attractionNotInterested  statusSingle:attractionNotInterested  \n                         -47.6628000                           -59.4613667  \n\n\n\n# create new final models\nm2.mlr = lm(money ~ (status + attraction)^2, data = mlrdata)\nm2.glm = glm(money ~ status * attraction, family = gaussian,\n             data = mlrdata)\n# inspect final minimal model\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.000000000000000222 ***\nstatusSingle                         &lt; 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,   Adjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: &lt; 0.0000000000000002220446\n\n\n\n# extract confidence intervals of the coefficients\nconfint(m2.mlr)\n\n                                              2.5 %         97.5 %\n(Intercept)                           92.0121609656 106.2974390344\nstatusSingle                          45.6476377202  66.0594289465\nattractionNotInterested              -57.7640169936 -37.5615830064\nstatusSingle:attractionNotInterested -73.8946826590 -45.0280506744\n\n\n\n# compare baseline with final model\nanova(m0.mlr, m2.mlr)\n\nAnalysis of Variance Table\n\nModel 1: money ~ 1\nModel 2: money ~ (status + attraction)^2\n  Res.Df          RSS Df   Sum of Sq         F                 Pr(&gt;F)    \n1     97 213227.06081                                                    \n2     94  30411.31714  3 182815.7437 188.35839 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# compare baseline with final model\nAnova(m0.mlr, m2.mlr, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: money\n                 Sum Sq Df    F value                 Pr(&gt;F)    \n(Intercept) 760953.2107  1 2352.07181 &lt; 0.000000000000000222 ***\nResiduals    30411.3171 94                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nAdditional Model Diagnostics\nAfter rerunning the regression analysis on the updated data set, we again create diagnostic plots in order to check whether there are potentially problematic data points.\n\n# generate plots\nautoplot(m2.mlr) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n# determine a cutoff for data points that have\n# D-values higher than 4/(n-k-1)\ncutoff &lt;- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))\n# start plotting\npar(mfrow = c(1, 2))           # display plots in 1 row/2 columns\nqqPlot(m2.mlr, main=\"QQ Plot\") # create qq-plot\n\n84 88 \n82 86 \n\nplot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nAlthough the diagnostic plots indicate that additional points may be problematic, but these data points deviate substantially less from the trend than was the case with the data points that have already been removed. To make sure that retaining the data points that are deemed potentially problematic by the diagnostic plots, is acceptable, we extract diagnostic statistics and add them to the data.\n\n# add model diagnostics to the data\nmlrdata &lt;- mlrdata %&gt;%\n  dplyr::mutate(residuals = resid(m2.mlr),\n                standardized.residuals = rstandard(m2.mlr),\n                studentized.residuals = rstudent(m2.mlr),\n                cooks.distance = cooks.distance(m2.mlr),\n                dffit = dffits(m2.mlr),\n                leverage = hatvalues(m2.mlr),\n                covariance.ratios = covratio(m2.mlr),\n                fitted = m2.mlr$fitted.values)\n\nWe can now use these diagnostic statistics to create more precise diagnostic plots.\n\n# plot 5\np5 &lt;- ggplot(mlrdata,\n             aes(studentized.residuals)) +\n  theme(legend.position = \"none\")+\n  geom_histogram(aes(y=..density..),\n                 binwidth = .2,\n                 colour=\"black\",\n                 fill=\"gray90\") +\n  labs(x = \"Studentized Residual\", y = \"Density\") +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE),\n                            sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)),\n                colour = \"red\", size = 1) +\n  theme_bw(base_size = 8)\n# plot 6\np6 &lt;- ggplot(mlrdata, aes(fitted, studentized.residuals)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", colour = \"Red\")+\n  theme_bw(base_size = 8)+\n  labs(x = \"Fitted Values\",\n       y = \"Studentized Residual\")\n# plot 7\np7 &lt;- qplot(sample = mlrdata$studentized.residuals) +\n  theme_bw(base_size = 8) +\n  labs(x = \"Theoretical Values\",\n       y = \"Observed Values\")\ngridExtra::grid.arrange(p5, p6, p7, nrow = 1)\n\n\n\n\n\n\n\n\nThe new diagnostic plots do not indicate outliers that require removal. With respect to such data points the following parameters should be considered:\n\nData points with standardized residuals &gt; 3.29 should be removed (Field, Miles, and Field, 269)\nIf more than 1 percent of data points have standardized residuals exceeding values &gt; 2.58, then the error rate of the model is unacceptable (Field, Miles, and Field, 269).\nIf more than 5 percent of data points have standardized residuals exceeding values &gt; 1.96, then the error rate of the model is unacceptable (Field, Miles, and Field, 269)\nIn addition, data points with Cook’s D-values &gt; 1 should be removed (Field, Miles, and Field, 269)\nAlso, data points with leverage values higher than \\(3(k + 1)/N\\) or \\(2(k + 1)/N\\) (k = Number of predictors, N = Number of cases in model) should be removed (Field, Miles, and Field, 270)\nThere should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead.\nPredictors cannot substantially correlate with each other (multicollinearity) (see the subsection on (multi-)collinearity in the section of multiple binomial logistic regression for more details about (multi-)collinearity). If a model contains predictors that have variance inflation factors (VIF) &gt; 10 the model is unreliable (Myers) and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic (Szmrecsanyi, 215) Indeed, Zuur, Ieno, and Elphick propose that variables with VIFs exceeding 3 should be removed!\n\n\n\n\nNOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See Gries for a more elaborate explanation.\n\n\n\n\n\n\n\nThe mean value of VIFs should be ~ 1 (Bowerman and O’Connell).\n\nThe following code chunk evaluates these criteria.\n\n# 1: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$standardized.residuals &gt; 3.29)\n\nnamed integer(0)\n\n# 2: optimal = 1\n# (listed data points should be removed)\nstdres_258 &lt;- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) &gt; 2.58, 1, 0) } ))\n(sum(stdres_258) / length(stdres_258)) * 100\n\n[1] 0\n\n# 3: optimal = 5\n# (listed data points should be removed)\nstdres_196 &lt;- as.vector(sapply(mlrdata$standardized.residuals, function(x) {\nifelse(sqrt((x^2)) &gt; 1.96, 1, 0) } ))\n(sum(stdres_196) / length(stdres_196)) * 100\n\n[1] 6.12244897959\n\n# 4: optimal = 0\n# (listed data points should be removed)\nwhich(mlrdata$cooks.distance &gt; 1)\n\nnamed integer(0)\n\n# 5: optimal = 0\n# (data points should be removed if cooks distance is close to 1)\nwhich(mlrdata$leverage &gt;= (3*mean(mlrdata$leverage)))\n\nnamed integer(0)\n\n# 6: checking autocorrelation:\n# Durbin-Watson test (optimal: high p-value)\ndwt(m2.mlr)\n\n lag  Autocorrelation D-W Statistic p-value\n   1 -0.0143324675649  1.9680423527   0.672\n Alternative hypothesis: rho != 0\n\n# 7: test multicollinearity 1\nvif(m2.mlr)\n\n                        statusSingle              attractionNotInterested \n                                2.00                                 1.96 \nstatusSingle:attractionNotInterested \n                                2.96 \n\n# 8: test multicollinearity 2\n1/vif(m2.mlr)\n\n                        statusSingle              attractionNotInterested \n                      0.500000000000                       0.510204081633 \nstatusSingle:attractionNotInterested \n                      0.337837837838 \n\n# 9: mean vif should not exceed 1\nmean(vif(m2.mlr))\n\n[1] 2.30666666667\n\n\nExcept for the mean VIF value (2.307) which should not exceed 1, all diagnostics are acceptable. We will now test whether the sample size is sufficient for our model. With respect to the minimal sample size and based on Green, Field, Miles, and Field (273–74) offer the following rules of thumb for an adequate sample size (k = number of predictors; categorical predictors with more than two levels should be recoded as dummy variables):\n\nif you are interested in the overall model: 50 + 8k (k = number of predictors)\nif you are interested in individual predictors: 104 + k\nif you are interested in both: take the higher value!\n\n\n\nEvaluation of Sample Size\nAfter performing the diagnostics, we will now test whether the sample size is adequate and what the values of R would be based on a random distribution in order to be able to estimate how likely a \\(\\beta\\)-error is given the present sample size (see Field, Miles, and Field, 274). Beta errors (or \\(\\beta\\)-errors) refer to the erroneous assumption that a predictor is not significant (based on the analysis and given the sample) although it does have an effect in the population. In other words, \\(\\beta\\)-error means to overlook a significant effect because of weaknesses of the analysis. The test statistics ranges between 0 and 1 where lower values are better. If the values approximate 1, then there is serious concern as the model is not reliable given the sample size. In such cases, unfortunately, the best option is to increase the sample size.\n\n# load functions\nsource(\"rscripts/SampleSizeMLR.r\")\nsource(\"rscripts/ExpR.r\")\n# check if sample size is sufficient\nsmplesz(m2.mlr)\n\n[1] \"Sample too small: please increase your sample by  9  data points\"\n\n# check beta-error likelihood\nexpR(m2.mlr)\n\n[1] \"Based on the sample size expect a false positive correlation of 0.0309 between the predictors and the predicted\"\n\n\nThe function smplesz reports that the sample size is insufficient by 9 data points according to Green. The likelihood of \\(\\beta\\)-errors, however, is very small (0.0309). As a last step, we summarize the results of the regression analysis.\n\n# tabulate model results\nsjPlot::tab_model(m0.glm, m2.glm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nmoney\nmoney\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n88.12\n78.72 – 97.52\n&lt;0.001\n99.15\n92.10 – 106.21\n&lt;0.001\n\n\nstatus [Single]\n\n\n\n55.85\n45.78 – 65.93\n&lt;0.001\n\n\nattraction\n[NotInterested]\n\n\n\n-47.66\n-57.63 – -37.69\n&lt;0.001\n\n\nstatus [Single] ×\nattraction\n[NotInterested]\n\n\n\n-59.46\n-73.71 – -45.21\n&lt;0.001\n\n\nObservations\n98\n98\n\n\nR2\n-0.000\n0.857\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values in this report is incorrect! As we have seen above, and is also shown in the table below, the correct R2 values are: multiple R2 0.8574, adjusted R2 0.8528.\n\n\n\n\n\n\nAdditionally, we can inspect the summary of the regression model as shown below to extract additional information.\n\nsummary(m2.mlr)\n\n\nCall:\nlm(formula = money ~ (status + attraction)^2, data = mlrdata)\n\nResiduals:\n         Min           1Q       Median           3Q          Max \n-35.76416667 -13.50520000  -0.98948333  10.59887500  38.77166667 \n\nCoefficients:\n                                         Estimate   Std. Error  t value\n(Intercept)                           99.15480000   3.59735820 27.56323\nstatusSingle                          55.85353333   5.14015367 10.86612\nattractionNotInterested              -47.66280000   5.08743275 -9.36873\nstatusSingle:attractionNotInterested -59.46136667   7.26927504 -8.17982\n                                                   Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.000000000000000222 ***\nstatusSingle                         &lt; 0.000000000000000222 ***\nattractionNotInterested               0.0000000000000040429 ***\nstatusSingle:attractionNotInterested  0.0000000000013375166 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.986791 on 94 degrees of freedom\nMultiple R-squared:  0.857375902,   Adjusted R-squared:  0.852824069 \nF-statistic: 188.358387 on 3 and 94 DF,  p-value: &lt; 0.0000000000000002220446\n\n\nAlthough Field, Miles, and Field suggest that the main effects of the predictors involved in the interaction should not be interpreted, they are interpreted here to illustrate how the results of a multiple linear regression can be reported.\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(m2.mlr)\n\nWe fitted a linear model (estimated using OLS) to predict money with status and\nattraction (formula: money ~ (status + attraction)^2). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.86,\nF(3, 94) = 188.36, p &lt; .001, adj. R2 = 0.85). The model's intercept,\ncorresponding to status = Relationship and attraction = Interested, is at 99.15\n(95% CI [92.01, 106.30], t(94) = 27.56, p &lt; .001). Within this model:\n\n  - The effect of status [Single] is statistically significant and positive (beta\n= 55.85, 95% CI [45.65, 66.06], t(94) = 10.87, p &lt; .001; Std. beta = 1.19, 95%\nCI [0.97, 1.41])\n  - The effect of attraction [NotInterested] is statistically significant and\nnegative (beta = -47.66, 95% CI [-57.76, -37.56], t(94) = -9.37, p &lt; .001; Std.\nbeta = -1.02, 95% CI [-1.23, -0.80])\n  - The effect of status [Single] × attraction [NotInterested] is statistically\nsignificant and negative (beta = -59.46, 95% CI [-73.89, -45.03], t(94) =\n-8.18, p &lt; .001; Std. beta = -1.27, 95% CI [-1.58, -0.96])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nWe can use this output to write up a final report:\nA multiple linear regression was fitted to the data using an automated, step-wise, AIC-based (Akaike’s Information Criterion) procedure. The model fitting arrived at a final minimal model. During the model diagnostics, two outliers were detected and removed. Further diagnostics did not find other issues after the removal.\nThe final minimal adequate regression model is based on 98 data points and performs highly significantly better than a minimal baseline model (multiple R2: .857, adjusted R2: .853, F-statistic (3, 94): 154.4, AIC: 850.4, BIC: 863.32, p&lt;.001\\(***\\)). The final minimal adequate regression model reports attraction and status as significant main effects. The relationship status of men correlates highly significantly and positively with the amount of money spend on the women’s presents (SE: 5.14, t-value: 10.87, p&lt;.001\\(***\\)). This shows that men spend 156.8 dollars on presents if they are single while they spend 99,15 dollars if they are in a relationship. Whether men are attracted to women also correlates highly significantly and positively with the money they spend on women (SE: 5.09, t-values: -9.37, p&lt;.001\\(***\\)). If men are not interested in women, they spend 47.66 dollar less on a present for women compared with women the men are interested in.\nFurthermore, the final minimal adequate regression model reports a highly significant interaction between relationship status and attraction (SE: 7.27, t-value: -8.18, p&lt;.001\\(***\\)): If men are single but they are not interested in a women, a man would spend only 59.46 dollars on a present compared to all other constellations."
  },
  {
    "objectID": "tutorials/regression/regression.html#multiple-binomial-logistic-regression",
    "href": "tutorials/regression/regression.html#multiple-binomial-logistic-regression",
    "title": "Introduction",
    "section": "Multiple Binomial Logistic Regression",
    "text": "Multiple Binomial Logistic Regression\nLogistic regression is a multivariate analysis technique that builds on and is very similar in terms of its implementation to linear regression but logistic regressions take dependent variables that represent nominal rather than numeric scaling (Harrell Jr). The difference requires that the linear regression must be modified in certain ways to avoid producing non-sensical outcomes. The most fundamental difference between logistic and linear regressions is that logistic regression work on the probabilities of an outcome (the likelihood), rather than the outcome itself. In addition, the likelihoods on which the logistic regression works must be logged (logarithmized) in order to avoid produce predictions that produce values greater than 1 (instance occurs) and 0 (instance does not occur). You can check this by logging the values from -10 to 10 using the plogis function as shown below.\n\nround(plogis(-10:10), 5)\n\n [1] 0.00005 0.00012 0.00034 0.00091 0.00247 0.00669 0.01799 0.04743 0.11920\n[10] 0.26894 0.50000 0.73106 0.88080 0.95257 0.98201 0.99331 0.99753 0.99909\n[19] 0.99966 0.99988 0.99995\n\n\nIf we visualize these logged values, we get an S-shaped curve which reflects the logistic function.\n\n\n\n\n\n\n\n\n\nTo understand what this mean, we will use a very simple example. In this example, we want to see whether the height of men affect their likelihood of being in a relationship. The data we use represents a data set consisting of two variables: height and relationship.\n\n\n\n\n\n\n\n\n\nThe left panel of the Figure above shows that a linear model would predict values for the relationship status, which represents a factor (0 = Single and 1 = In a Relationship), that are nonsensical because values above 1 or below 0 do not make sense. In contrast to a linear regression, which predicts actual values, such as the frequencies of prepositions in a certain text, a logistic regression predicts probabilities of events (for example, being in a relationship) rather than actual values. The center panel shows the predictions of a logistic regression and we see that a logistic regression also has an intercept and a (very steep) slope but that the regression line also predicts values that are above 1 and below 0. However, when we log the predicted values we these predicted values are transformed into probabilities with values between 0 and 1. And the logged regression line has a S-shape which reflects the logistic function. Furthermore, we can then find the optimal line (the line with the lowest residual deviance) by comparing the sum of residuals - just as we did for a simple linear model and that way, we find the regression line for a logistic regression.\n\nExample 1: EH in Kiwi English\nTo exemplify how to implement a logistic regression in R (see Agresti; Agresti and Kateri) for very good and thorough introductions to this topic], we will analyze the use of the discourse particle eh in New Zealand English and test which factors correlate with its occurrence. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender, ethnicity, and age of that speaker and whether or not the speech unit contained an eh. To begin with, we clean the current work space, set option, install and activate relevant packages, load customized functions, and load the example data set.\n\n# load data\nblrdata  &lt;- base::readRDS(\"tutorials/regression/data/bld.rda\", \"rb\")\n\n\n\nIDGenderAgeEthnicityEH&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha0&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha1&lt;S1A-001#M&gt;MenYoungPakeha0\n\n\nThe summary of the data show that the data set contains 25,821 observations of five variables. The variable ID contains strings that represent a combination file and speaker of a speech unit. The second variable represents the gender, the third the age, and the fourth the ethnicity of speakers. The fifth variable represents whether or not a speech unit contained the discourse particle eh.\nNext, we factorize the variables in our data set. In other words, we specify that the strings represent variable levels and define new reference levels because as a default R will use the variable level which first occurs in alphabet ordering as the reference level for each variable, we redefine the variable levels for Age and Ethnicity.\n\nblrdata &lt;- blrdata %&gt;%\n  # factorize variables\n  dplyr::mutate(Age = factor(Age),\n                Gender = factor(Gender),\n                Ethnicity = factor(Ethnicity),\n                ID = factor(ID),\n                EH = factor(EH)) %&gt;%\n  # relevel Age (Reference = Young) and Ethnicity (Reference= Pakeha))\n  dplyr::mutate(Age = relevel(Age, \"Young\"),\n                Ethnicity = relevel(Ethnicity, \"Pakeha\"))\n\nAfter preparing the data, we will now plot the data to get an overview of potential relationships between variables.\n\nblrdata %&gt;%\n  dplyr::mutate(EH = ifelse(EH == \"0\", 0, 1)) %&gt;%\n  ggplot(aes(Age, EH, color = Gender)) +\n  facet_wrap(~Ethnicity) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of eh\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nWith respect to main effects, the Figure above indicates that men use eh more frequently than women, that young speakers use it more frequently compared with old speakers, and that speakers that are descendants of European settlers (Pakeha) use eh more frequently compared with Maori (the native inhabitants of New Zealand).\nThe plots in the lower panels do not indicate significant interactions between use of eh and the Age, Gender, and Ethnicity of speakers. In a next step, we will start building the logistic regression model.\n\n\nModel Building\nAs a first step, we need to define contrasts and use the datadist function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. Field, Miles, and Field (414–27) and Gries provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more.\n\n# set contrasts\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\n# extract distribution summaries for all potential variables\nblrdata.dist &lt;- datadist(blrdata)\n# store distribution summaries for all potential variables\noptions(datadist = \"blrdata.dist\")\n\nNext, we generate a minimal model that predicts the use of eh solely based on the intercept.\n\n# baseline glm model\nm0.blr = glm(EH ~ 1, family = binomial, data = blrdata)\n\n\n\nModel fitting\nWe will now start with the model fitting procedure. In the present case, we will use a manual step-wise step-up procedure during which predictors are added to the model if they significantly improve the model fit. In addition, we will perform diagnostics as we fit the model at each step of the model fitting process rather than after the fitting.\nWe will test two things in particular: whether the data has incomplete information or complete separation and if the model suffers from (multi-)collinearity.\nIncomplete information or complete separation means that the data does not contain all combinations of the predictor or the dependent variable. This is important because if the data does not contain cases of all combinations, the model will assume that it has found a perfect predictor. In such cases the model overestimates the effect of that that predictor and the results of that model are no longer reliable. For example, if eh was only used by young speakers in the data, the model would jump on that fact and say Ha! If there is an old speaker, that means that that speaker will never ever and under no circumstances say eh* - I can therefore ignore all other factors!*\nMulticollinearity means that predictors correlate and have shared variance. This means that whichever predictor is included first will take all the variance that it can explain and the remaining part of the variable that is shared will not be attributed to the other predictor. This may lead to reporting that a factor is not significant because all of the variance it can explain is already accounted for. However, if the other predictor were included first, then the original predictor would be returned as insignificant. This means that- depending on the order in which predictors are added - the results of the regression can differ dramatically and the model is therefore not reliable. Multicollinearity is actually a very common problem and there are various ways to deal with it but it cannot be ignored (at least in regression analyses).\nWe will start by adding Age to the minimal adequate model.\n\n# check incomplete information\nifelse(min(ftable(blrdata$Age, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\n# add age to the model\nm1.blr = glm(EH ~ Age, family = binomial, data = blrdata)\n# check multicollinearity (vifs should have values of 3 or lower for main effects)\nifelse(max(vif(m1.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\n# check if adding Age significantly improves model fit\nanova(m1.blr, m0.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age\nModel 2: EH ~ 1\n  Resid. Df  Resid. Dev Df     Deviance               Pr(&gt;Chi)    \n1     25819 32376.86081                                           \n2     25820 33007.75469 -1 -630.8938871 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs the data does not contain incomplete information, the vif values are below 3, and adding Age has significantly improved the model fit (the p-value of the ANOVA is lower than .05). We therefore proceed with Age included.\nWe continue by adding Gender. We add a second ANOVA test to see if including Gender affects the significance of other predictors in the model. If this were the case - if adding Gender would cause Age to become insignificant - then we could change the ordering in which we include predictors into our model.\n\nifelse(min(ftable(blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm2.blr &lt;- update(m1.blr, . ~ . +Gender)\nifelse(max(vif(m2.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m2.blr, m1.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender\nModel 2: EH ~ Age\n  Resid. Df  Resid. Dev Df    Deviance               Pr(&gt;Chi)    \n1     25818 32139.54089                                          \n2     25819 32376.86081 -1 -237.319914 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m2.blr, test = \"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: EH\n          LR Chisq Df             Pr(&gt;Chisq)    \nAge    668.6350712  1 &lt; 0.000000000000000222 ***\nGender 237.3199140  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, including Gender significantly improves model fit and the data does not contain incomplete information or complete separation. Also, including Gender does not affect the significance of Age. Now, we include Ethnicity.\n\nifelse(min(ftable(blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm3.blr &lt;- update(m2.blr, . ~ . +Ethnicity)\nifelse(max(vif(m3.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m3.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(&gt;Chi)\n1     25817 32139.27988                          \n2     25818 32139.54089 -1 -0.2610145356  0.60942\n\n\nSince adding Ethnicity does not significantly improve the model fit, we do not need to test if its inclusion affects the significance of other predictors. We continue without Ethnicity and include the interaction between Age and Gender.\n\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm4.blr &lt;- update(m2.blr, . ~ . +Age*Gender)\nifelse(max(vif(m4.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m4.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Age:Gender\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(&gt;Chi)\n1     25817 32139.41665                          \n2     25818 32139.54089 -1 -0.1242399135  0.72448\n\n\nThe interaction between Age and Gender is not significant which means that men and women do not behave differently with respect to their use of EH as they age. Also, the data does not contain incomplete information and the model does not suffer from multicollinearity - the predictors are not collinear. We can now include if there is a significant interaction between Age and Ethnicity.\n\nifelse(min(ftable(blrdata$Age, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm5.blr &lt;- update(m2.blr, . ~ . +Age*Ethnicity)\nifelse(max(vif(m5.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m5.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Age:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df    Deviance Pr(&gt;Chi)\n1     25816 32136.47224                        \n2     25818 32139.54089 -2 -3.06865451   0.2156\n\n\nAgain, no incomplete information or multicollinearity and no significant interaction. Now, we test if there exists a significant interaction between Gender and Ethnicity.\n\nifelse(min(ftable(blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm6.blr &lt;- update(m2.blr, . ~ . +Gender*Ethnicity)\nifelse(max(vif(m6.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m6.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Gender:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(&gt;Chi)\n1     25816 32139.26864                          \n2     25818 32139.54089 -2 -0.2722521853  0.87273\n\n\nAs the interaction between Gender and Ethnicity is not significant, we continue without it. In a final step, we include the three-way interaction between Age, Gender, and Ethnicity.\n\nifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, \"not possible\", \"possible\")\n\n[1] \"possible\"\n\nm7.blr &lt;- update(m2.blr, . ~ . +Gender*Ethnicity)\nifelse(max(vif(m7.blr)) &lt;= 3,  \"vifs ok\", \"WARNING: high vifs!\") # VIFs ok\n\n[1] \"vifs ok\"\n\nanova(m7.blr, m2.blr, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: EH ~ Age + Gender + Ethnicity + Gender:Ethnicity\nModel 2: EH ~ Age + Gender\n  Resid. Df  Resid. Dev Df      Deviance Pr(&gt;Chi)\n1     25816 32139.26864                          \n2     25818 32139.54089 -2 -0.2722521853  0.87273\n\n\nWe have found our final minimal adequate model because the 3-way interaction is also insignificant. As we have now arrived at the final minimal adequate model (m2.blr), we generate a final minimal model using the lrm function.\n\nm2.lrm &lt;- lrm(EH ~ Age+Gender, data = blrdata, x = T, y = T, linear.predictors = T)\nm2.lrm\n\nLogistic Regression Model\n\nlrm(formula = EH ~ Age + Gender, data = blrdata, x = T, y = T, \n    linear.predictors = T)\n\n                       Model Likelihood        Discrimination    Rank Discrim.    \n                             Ratio Test               Indexes          Indexes    \nObs         25821    LR chi2     868.21        R2       0.046    C       0.602    \n 0          17114    d.f.             2      R2(2,25821)0.033    Dxy     0.203    \n 1           8707    Pr(&gt; chi2) &lt;0.0001    R2(2,17312.8)0.049    gamma   0.302    \nmax |deriv| 3e-10                              Brier    0.216    tau-a   0.091    \n\n             Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept    -0.2324 0.0223 -10.44 &lt;0.0001 \nAge=Old      -0.8305 0.0335 -24.78 &lt;0.0001 \nGender=Women -0.4201 0.0273 -15.42 &lt;0.0001 \n\n\n\nanova(m2.lrm)\n\n                Wald Statistics          Response: EH \n\n Factor     Chi-Square d.f. P     \n Age        614.04     1    &lt;.0001\n Gender     237.65     1    &lt;.0001\n TOTAL      802.65     2    &lt;.0001\n\n\nAfter fitting the model, we validate the model to avoid arriving at a final minimal model that is overfitted to the data at hand.\n\n\nModel Validation\nTo validate a model, you can apply the validate function and apply it to a saturated model. The output of the validate function shows how often predictors are retained if the sample is re-selected with the same size but with placing back drawn data points. The execution of the function requires some patience as it is rather computationally expensive and it is, therefore, commented out below.\n\n# model validation (remove # to activate: output too long for website)\nm7.lrm &lt;- lrm(EH ~ (Age+Gender+Ethnicity)^3, data = blrdata, x = T, y = T, linear.predictors = T)\n#validate(m7.lrm, bw = T, B = 200)\n\nThe validate function shows that retaining two predictors (Age and Gender) is the best option and thereby confirms our final minimal adequate model as the best minimal model. In addition, we check whether we need to include a penalty for data points because they have too strong of an impact of the model fit. To see whether a penalty is warranted, we apply the pentrace function to the final minimal adequate model.\n\npentrace(m2.lrm, seq(0, 0.8, by = 0.05)) # determine penalty\n\n\nBest penalty:\n\n penalty            df\n     0.8 1.99925395138\n\n penalty            df           aic           bic         aic.c\n    0.00 2.00000000000 864.213801108 847.895914321 864.213336316\n    0.05 1.99995335085 864.213893816 847.896387637 864.213429042\n    0.10 1.99990670452 864.213985335 847.896859740 864.213520579\n    0.15 1.99986006100 864.214075641 847.897330609 864.213610904\n    0.20 1.99981342030 864.214164764 847.897800270 864.213700044\n    0.25 1.99976678241 864.214252710 847.898268733 864.213788009\n    0.30 1.99972014734 864.214339446 847.898735961 864.213874762\n    0.35 1.99967351509 864.214424993 847.899201978 864.213960327\n    0.40 1.99962688564 864.214509360 847.899666792 864.214044712\n    0.45 1.99958025902 864.214592526 847.900130382 864.214127896\n    0.50 1.99953363520 864.214674504 847.900592761 864.214209892\n    0.55 1.99948701420 864.214755279 847.901053914 864.214290685\n    0.60 1.99944039601 864.214834874 847.901513865 864.214370299\n    0.65 1.99939378063 864.214913276 847.901972599 864.214448719\n    0.70 1.99934716807 864.214990480 847.902430112 864.214525941\n    0.75 1.99930055832 864.215066506 847.902886425 864.214601985\n    0.80 1.99925395138 864.215141352 847.903341534 864.214676849\n\n\nThe values are so similar that a penalty is unnecessary. In a next step, we rename the final models.\n\nlr.glm &lt;- m2.blr  # rename final minimal adequate glm model\nlr.lrm &lt;- m2.lrm  # rename final minimal adequate lrm model\n\nNow, we calculate a Model Likelihood Ratio Test to check if the final model performs significantly better than the initial minimal base-line model. The result of this test is provided as a default if we call a summary of the lrm object.\n\nmodelChi &lt;- lr.glm$null.deviance - lr.glm$deviance\nchidf &lt;- lr.glm$df.null - lr.glm$df.residual\nchisq.prob &lt;- 1 - pchisq(modelChi, chidf)\nmodelChi; chidf; chisq.prob\n\n[1] 868.213801105\n\n\n[1] 2\n\n\n[1] 0\n\n\nThe code above provides three values: a \\(\\chi\\)2, the degrees of freedom, and a p-value. The p-value is lower than .05 and the results of the Model Likelihood Ratio Test therefore confirm that the final minimal adequate model performs significantly better than the initial minimal base-line model. Another way to extract the model likelihood test statistics is to use an ANOVA to compare the final minimal adequate model to the minimal base-line model.\nA handier way to get these statistics is by performing an ANOVA on the final minimal model which, if used this way, is identical to a Model Likelihood Ratio test.\n\nanova(m0.glm, lr.glm, test = \"Chi\") # Model Likelihood Ratio Test\n\nWarning in anova.glmlist(c(list(object), dotargs), dispersion = dispersion, :\nmodels with response '\"EH\"' removed because response differs from model 1\n\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: money\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df  Resid. Dev Pr(&gt;Chi)\nNULL                    97 213227.0608         \n\n\nIn a next step, we calculate pseudo-R2 values which represent the amount of residual variance that is explained by the final minimal adequate model. We cannot use the ordinary R2 because the model works on the logged probabilities rather than the values of the dependent variable.\n\n# calculate pseudo R^2\n# number of cases\nncases &lt;- length(fitted(lr.glm))\nR2.hl &lt;- modelChi/lr.glm$null.deviance\nR.cs &lt;- 1 - exp ((lr.glm$deviance - lr.glm$null.deviance)/ncases)\nR.n &lt;- R.cs /( 1- ( exp (-(lr.glm$null.deviance/ ncases))))\n# function for extracting pseudo-R^2\nlogisticPseudoR2s &lt;- function(LogModel) {\n  dev &lt;- LogModel$deviance\n    nullDev &lt;- LogModel$null.deviance\n    modelN &lt;-  length(LogModel$fitted.values)\n    R.l &lt;-  1 -  dev / nullDev\n    R.cs &lt;- 1- exp ( -(nullDev - dev) / modelN)\n    R.n &lt;- R.cs / ( 1 - ( exp (-(nullDev / modelN))))\n    cat(\"Pseudo R^2 for logistic regression\\n\")\n    cat(\"Hosmer and Lemeshow R^2  \", round(R.l, 3), \"\\n\")\n    cat(\"Cox and Snell R^2        \", round(R.cs, 3), \"\\n\")\n    cat(\"Nagelkerke R^2           \", round(R.n, 3),    \"\\n\") }\nlogisticPseudoR2s(lr.glm)\n\nPseudo R^2 for logistic regression\nHosmer and Lemeshow R^2   0.026 \nCox and Snell R^2         0.033 \nNagelkerke R^2            0.046 \n\n\nThe low pseudo-R2 values show that our model has very low explanatory power. For instance, the value of Hosmer and Lemeshow R2 (0.026) “is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables” (Field, Miles, and Field, 317). In essence, all the pseudo-R2 values are measures of how substantive the model is (how much better it is compared to a baseline model). Next, we extract the confidence intervals for the coefficients of the model.\n\n# extract the confidence intervals for the coefficients\nconfint(lr.glm)\n\n                      2.5 %          97.5 %\n(Intercept) -0.276050866644 -0.188778707888\nAgeOld      -0.896486392283 -0.765095825337\nGenderWomen -0.473530977599 -0.366703827312\n\n\nDespite having low explanatory and predictive power, the age of speakers and their gender are significant as the confidence intervals of the coefficients do not overlap with 0.\n\n\nEffect Size\nIn a next step, we compute odds ratios and their confidence intervals. Odds Ratios represent a common measure of effect size and can be used to compare effect sizes across models. Odds ratios rang between 0 and infinity. Values of 1 indicate that there is no effect. The further away the values are from 1, the stronger the effect. If the values are lower than 1, then the variable level correlates negatively with the occurrence of the outcome (the probability decreases) while values above 1 indicate a positive correlation and show that the variable level causes an increase in the probability of the outcome (the occurrence of EH).\n\nexp(lr.glm$coefficients) # odds ratios\n\n   (Intercept)         AgeOld    GenderWomen \n0.792642499264 0.435815384592 0.656972294902 \n\nexp(confint(lr.glm))     # confidence intervals of the odds ratios\n\nWaiting for profiling to be done...\n\n\n                     2.5 %         97.5 %\n(Intercept) 0.758774333475 0.827969709589\nAgeOld      0.408000698617 0.465289342330\nGenderWomen 0.622799290895 0.693014866728\n\n\nThe odds ratios confirm that older speakers use eh significantly less often compared with younger speakers and that women use eh less frequently than men as the confidence intervals of the odds rations do not overlap with 1. In a next step, we calculate the prediction accuracy of the model.\n\n\nPrediction Accuracy\nIn order to calculate the prediction accuracy of the model, we generate a variable called Prediction that contains the predictions of pour model and which we add to the data. Then, we use the confusionMatrix function from the caret package (Kuhn 2021) to extract the prediction accuracy.\n\n# create variable with contains the prediction of the model\nblrdata &lt;- blrdata %&gt;%\n  dplyr::mutate(Prediction = predict(lr.glm, type = \"response\"),\n                Prediction = ifelse(Prediction &gt; .5, 1, 0),\n                Prediction = factor(Prediction, levels = c(\"0\", \"1\")),\n                EH = factor(EH, levels = c(\"0\", \"1\")))\n# create a confusion matrix with compares observed against predicted values\ncaret::confusionMatrix(blrdata$Prediction, blrdata$EH)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 17114  8707\n         1     0     0\n                                                    \n               Accuracy : 0.66279385                \n                 95% CI : (0.656990096, 0.668560948)\n    No Information Rate : 0.66279385                \n    P-Value [Acc &gt; NIR] : 0.5029107                 \n                                                    \n                  Kappa : 0                         \n                                                    \n Mcnemar's Test P-Value : &lt; 0.00000000000000022     \n                                                    \n            Sensitivity : 1.00000000                \n            Specificity : 0.00000000                \n         Pos Pred Value : 0.66279385                \n         Neg Pred Value :        NaN                \n             Prevalence : 0.66279385                \n         Detection Rate : 0.66279385                \n   Detection Prevalence : 1.00000000                \n      Balanced Accuracy : 0.50000000                \n                                                    \n       'Positive' Class : 0                         \n                                                    \n\n\nWe can see that out model has never predicted the use of eh which is common when dealing with rare phenomena. This is expected as the event s so rare that the probability of it not occurring substantively outweighs the probability of it occurring. As such, the prediction accuracy of our model is not significantly better compared to the prediction accuracy of the baseline model which is the no-information rate (NIR)) (p = 0.5029).\nWe can use the plot_model function from the sjPlot package (Lüdecke) to visualize the effects.\n\n# predicted probability\nefp1 &lt;- plot_model(lr.glm, type = \"pred\", terms = c(\"Age\"), axis.lim = c(0, 1)) \n# predicted percentage\nefp2 &lt;- plot_model(lr.glm, type = \"pred\", terms = c(\"Gender\"), axis.lim = c(0, 1)) \ngridExtra::grid.arrange(efp1, efp2, nrow = 1)\n\n\n\n\n\n\n\n\nAnd we can also combine the visualization of the effects in a single plot as shown below.\n\nsjPlot::plot_model(lr.glm, type = \"pred\", terms = c(\"Age\", \"Gender\"), axis.lim = c(0, 1)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Predicted Probabilty of eh\", title = \"\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\n\n\nModel Diagnostics\nWe are now in a position to perform model diagnostics and test if the model violates distributional requirements. In a first step, we test for the existence of multicollinearity.\n\n\nMulticollinearity\nMulticollinearity means that predictors in a model can be predicted by other predictors in the model (this means that they share variance with other predictors). If this is the case, the model results are unreliable because the presence of absence of one predictor has substantive effects on at least one other predictor.\nTo check whether the final minimal model contains predictors that correlate with each other, we extract variance inflation factors (VIF). If a model contains predictors that have variance inflation factors (VIF) &gt; 10 the model is unreliable (Myers). Gries shows that a VIF of 10 means that that predictor is explainable (predictable) from the other predictors in the model with an R2 of .9 (a VIF of 5 means that predictor is explainable (predictable) from the other predictors in the model with an R2 of .8).Indeed, predictors with VIF values greater than 4 are usually already problematic but, for large data sets, even VIFs greater than 2 can lead inflated standard errors (Jaeger 2013). Also, VIFs of 2.5 can be problematic (Szmrecsanyi, 215) and (Zuur, Ieno, and Elphick) proposes that variables with VIFs exceeding 3 should be removed.\n\n\n\nNOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See Gries or the excursion below for a more elaborate explanation.\n\n\n\n\n\n\n\n\n\n\nEXCURSION\n\n\n\n\n\n\n`\nWhat is multicollinearity?\n\n\nAnswer\n\n\nDuring the workshop on mixed-effects modeling, we talked about (multi-)collinearity and someone asked if collinearity reflected shared variance (what I thought) or predictability of variables (what the other person thought). Both answers are correct! We will see below why…\n\n\n\n\n(Multi-)collinearity reflects the predictability of predictors based on the values of other predictors!\n\n\n\n\nTo test this, I generate a data set with 4 independent variables a, b, c, and d as well as two potential response variables r1 (which is random) and r2 (where the first 50 data points are the same as in r1 but for the second 50 data points I have added a value of 50 to the data points 51 to 100 from r1). This means that the predictors a and d should both strongly correlate with r2.\n\n# load packages\nlibrary(dplyr)\nlibrary(rms)\n# create data set\n# responses\n# 100 random numbers\nr1 &lt;- rnorm(100, 50, 10)\n# 50 smaller + 50 larger numbers\nr2 &lt;- c(r1[1:50], r1[51:100] + 50)\n# predictors\na &lt;- c(rep(\"1\", 50), rep (\"0\", 50))\nb &lt;- rep(c(rep(\"1\", 25), rep (\"0\", 25)), 2)\nc &lt;- rep(c(rep(\"1\", 10), rep(\"0\", 10)), 5)\nd &lt;- c(rep(\"1\", 47), rep (\"0\", 3), rep (\"0\", 47), rep (\"1\", 3))\n# create data set\ndf &lt;- data.frame(r1, r2, a, b, c, d)\n:::\n::: {.cell} ::: {.cell-output-display}\n\n\nFirst 10 rows of df data.\n\n\nr1\nr2\na\nb\nc\nd\n\n\n\n\n55.3805120133\n55.3805120133\n1\n1\n1\n1\n\n\n50.5169933942\n50.5169933942\n1\n1\n1\n1\n\n\n67.5135734932\n67.5135734932\n1\n1\n1\n1\n\n\n60.1733973728\n60.1733973728\n1\n1\n1\n1\n\n\n72.3678161278\n72.3678161278\n1\n1\n1\n1\n\n\n52.4309002788\n52.4309002788\n1\n1\n1\n1\n\n\n55.6904236277\n55.6904236277\n1\n1\n1\n1\n\n\n44.2366484882\n44.2366484882\n1\n1\n1\n1\n\n\n55.5987335816\n55.5987335816\n1\n1\n1\n1\n\n\n58.3120761737\n58.3120761737\n1\n1\n1\n1\n\n\n\n\n\n\n::: :::\nHere are the visualizations of r1 and r2\n::: {.cell} ::: {.cell-output-display}  :::\n::: {.cell-output-display}  ::: :::\nFit first model\nNow, I fit a first model. As the response is random, we do not expect any of the predictors to have a significant effect and we expect the R2 to be rather low.\n::: {.cell}\nm1 &lt;- lm(r1 ~ a + b + c + d, data = df)\n# inspect model\nsummary(m1)\n::: {.cell-output .cell-output-stdout}\n\nCall:\nlm(formula = r1 ~ a + b + c + d, data = df)\n\nResiduals:\n          Min            1Q        Median            3Q           Max \n-25.076373713  -7.774694324   0.933388851   6.630369791  22.021652947 \n\nCoefficients:\n                Estimate   Std. Error  t value             Pr(&gt;|t|)    \n(Intercept) 46.077236782  2.059313248 22.37505 &lt; 0.0000000000000002 ***\na1           5.957479586  4.593681927  1.29689             0.197812    \nb1           5.148440925  2.098541787  2.45334             0.015977 *  \nc1          -0.213502165  2.188893729 -0.09754             0.922504    \nd1          -5.232737413  4.515342995 -1.15888             0.249410    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.4927089 on 95 degrees of freedom\nMultiple R-squared:  0.075627922, Adjusted R-squared:  0.0367069924 \nF-statistic: 1.94311705 on 4 and 95 DF,  p-value: 0.109586389\n::: :::\nWe now check for (multi-)collinearity using the vif function from the rms package (Harrell Jr 2021). Variables a and d should have high variance inflation factor values (vif-values) because they overlap very much!\n::: {.cell}\n# extract vifs\nrms::vif(m1)\n::: {.cell-output .cell-output-stdout}\n           a1            b1            c1            d1 \n4.79166666667 1.00000000000 1.08796296296 4.62962962963 \n::: :::\nVariables a and d do indeed have high vif-values.\nFit second model\nWe now fit a second model to the response which has higher values for the latter part of the response. Both a and d strongly correlate with the response. But because a and d are collinear, d should not be reported as being significant by the model. The R2 of the model should be rather high (given the correlation between the response r2 and a and d).\n::: {.cell}\nm2 &lt;- lm(r2 ~ a + b + c + d, data = df)\n# inspect model\nsummary(m2)\n::: {.cell-output .cell-output-stdout}\n\nCall:\nlm(formula = r2 ~ a + b + c + d, data = df)\n\nResiduals:\n          Min            1Q        Median            3Q           Max \n-25.076373713  -7.774694324   0.933388851   6.630369791  22.021652947 \n\nCoefficients:\n                 Estimate    Std. Error  t value               Pr(&gt;|t|)    \n(Intercept)  96.077236782   2.059313248 46.65499 &lt; 0.000000000000000222 ***\na1          -44.042520414   4.593681927 -9.58763  0.0000000000000012574 ***\nb1            5.148440925   2.098541787  2.45334               0.015977 *  \nc1           -0.213502165   2.188893729 -0.09754               0.922504    \nd1           -5.232737413   4.515342995 -1.15888               0.249410    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.4927089 on 95 degrees of freedom\nMultiple R-squared:  0.851726565, Adjusted R-squared:  0.845483473 \nF-statistic: 136.427041 on 4 and 95 DF,  p-value: &lt; 0.0000000000000002220446\n::: :::\nAgain, we extract the vif-values.\n::: {.cell}\n# extract vifs\nrms::vif(m2)\n::: {.cell-output .cell-output-stdout}\n           a1            b1            c1            d1 \n4.79166666667 1.00000000000 1.08796296296 4.62962962963 \n::: :::\nThe vif-values are identical which shows that what matters is if the variables are predictable. To understand how we arrive at vif-values, we inspect the model matrix.\n::: {.cell}\n# inspect model matrix\nmm &lt;- model.matrix(m2)\n:::\n::: {.cell} ::: {.cell-output-display}\n\n\nFirst 15 rows of the model matrix.\n\n\n(Intercept)\na1\nb1\nc1\nd1\n\n\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n1\n\n\n1\n1\n1\n0\n1\n\n\n\n\n\n\n::: :::\nWe now fit a linear model in which we predict d from the other predictors in the model matrix.\n::: {.cell}\nmt &lt;- lm(mm[,5] ~ mm[,1:4])\nsummary(mt)$r.squared\n::: {.cell-output .cell-output-stdout}\n[1] 0.784\n::: :::\nThe R2 shows that the values of d are explained to 78.4 percent by the values of the other predictors in the model.\nNow, we can write a function (taken from Gries) that converts this R2 value\n::: {.cell}\nR2.to.VIF &lt;- function(some.modelmatrix.r2) {\nreturn(1/(1-some.modelmatrix.r2)) } \nR2.to.VIF(0.784)\n::: {.cell-output .cell-output-stdout}\n[1] 4.62962962963\n::: :::\nThe function outputs the vif-value of d. This shows that the vif-value of d represents its predictability from the other predictors in the model matrix which represents the amount of shared variance between d and the other predictors in the model.\n\n\n`\n\n\nWe now extract and check the VIFs of the model.\n\nvif(lr.glm)\n\n       AgeOld   GenderWomen \n1.00481494539 1.00481494539 \n\n\nIn addition, predictors with 1/VIF values \\(&lt;\\) .1 must be removed (data points with values above .2 are considered problematic) (Menard) and the mean value of VIFs should be \\(~\\) 1 (Bowerman and O’Connell).\n\nmean(vif(lr.glm))\n\n[1] 1.00481494539\n\n\n\nOutlier detection\nIn order to detect potential outliers, we will calculate diagnostic parameters and add these to our data set.\n\ninfl &lt;- influence.measures(lr.glm) # calculate influence statistics\nblrdata &lt;- data.frame(blrdata, infl[[1]], infl[[2]]) # add influence statistics\n\nIn a next step, we use these diagnostic parameters to check if there are data points which should be removed as they unduly affect the model fit.\n\n\nSample Size\nWe now check whether the sample size is sufficient for our analysis (Green).\n\nif you are interested in the overall model: 50 + 8k (k = number of predictors)\nif you are interested in individual predictors: 104 + k\nif you are interested in both: take the higher value!\n\n\n# function to evaluate sample size\nsmplesz &lt;- function(x) {\n  ifelse((length(x$fitted) &lt; (104 + ncol(summary(x)$coefficients)-1)) == TRUE,\n    return(\n      paste(\"Sample too small: please increase your sample by \",\n      104 + ncol(summary(x)$coefficients)-1 - length(x$fitted),\n      \" data points\", collapse = \"\")),\n    return(\"Sample size sufficient\")) }\n# apply unction to model\nsmplesz(lr.glm)\n\n[1] \"Sample size sufficient\"\n\n\nAccording to rule of thumb provided in Green, the sample size is sufficient for our analysis.\n\n\nSummarizing Results\nAs a final step, we summarize our findings in tabulated form.\n\nsjPlot::tab_model(lr.glm)\n\n\n\n\n \nEH\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.79\n0.76 – 0.83\n&lt;0.001\n\n\nAge [Old]\n0.44\n0.41 – 0.47\n&lt;0.001\n\n\nGender [Women]\n0.66\n0.62 – 0.69\n&lt;0.001\n\n\nObservations\n25821\n\n\nR2 Tjur\n0.032\n\n\n\n\n\n\n\n\nA more detailed summary table can be retrieved as follows:\n\n# load function\nsource(\"rscripts/blrsummary.r\")\n# calculate accuracy \npredict.acc &lt;- caret::confusionMatrix(blrdata$Prediction, blrdata$EH)\npredict.acc &lt;- predict.acc[3]$overall[[1]]\n# create summary table\nblrsummarytb &lt;- blrsummary(lr.glm, lr.lrm, predict.acc) \n\n\n\nStatisticsEstimateVIFOddsRatioCI(2.5%)CI(97.5%)Std. Errorz valuePr(&gt;|z|)Significance(Intercept)-0.230.790.760.830.02-10.440p &lt; .001***AgeOld-0.8310.440.410.470.03-24.780p &lt; .001***GenderWomen-0.4210.660.620.690.03-15.420p &lt; .001***Model statisticsValueNumber of cases in model25821Observed misses0 :17114Observed successes1 :8707Null deviance33007.75Residual deviance32139.54R2 (Nagelkerke)0.046R2 (Hosmer & Lemeshow)0.026R2 (Cox & Snell)0.033C0.602Somers' Dxy0.203AIC32145.54Prediction accuracy0.66%Model Likelihood Ratio TestModel L.R.: 868.21df: 2p-value: 0sig: p &lt; .001***\n\n\nR2 (Hosmer & Lemeshow)\nHosmer and Lemeshow’s R2 “is the proportional reduction in the absolute value of the log-likelihood measure and as such it is a measure of how much the badness of fit improves as a result of the inclusion of the predictor variables. It can vary between 0 (indicating that the predictors are useless at predicting the outcome variable) and 1 (indicating that the model predicts the outcome variable perfectly)” (Field, Miles, and Field, 317).\nR2 (Cox & Snell)\n“Cox and Snell’s R2 (1989) is based on the deviance of the model (-2LL(new») and the deviance of the baseline model (-2LL(baseline), and the sample size, n […]. However, this statistic never reaches its theoretical maximum of 1.\nR2 (Nagelkerke)\nSince R2 (Cox & Snell) never reaches its theoretical maximum of 1, Nagelkerke (1991) suggested Nagelkerke’s R2 (Field, Miles, and Field, 317–18).\nSomers’ Dxy\nSomers’ Dxy is a rank correlation between predicted probabilities and observed responses ranges between 0 (randomness) and 1 (perfect prediction). Somers’ Dxy should have a value higher than .5 for the model to be meaningful (Baayen, 204).\nC\nC is an index of concordance between the predicted probability and the observed response. When C takes the value 0.5, the predictions are random, when it is 1, prediction is perfect. A value above 0.8 indicates that the model may have some real predictive capacity (Baayen, 204).\nAkaike information criteria (AIC)\nAkaike information criteria (AlC = -2LL + 2k) provide a value that reflects a ratio between the number of predictors in the model and the variance that is explained by these predictors. Changes in AIC can serve as a measure of whether the inclusion of a variable leads to a significant increase in the amount of variance that is explained by the model. “You can think of this as the price you pay for something: you get a better value of R2, but you pay a higher price, and was that higher price worth it? These information criteria help you to decide. The BIC is the same as the AIC but adjusts the penalty included in the AlC (i.e., 2k) by the number of cases: BlC = -2LL + 2k x log(n) in which n is the number of cases in the model” (Field, Miles, and Field, 318).\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(lr.glm)\n\nWe fitted a logistic model (estimated using ML) to predict EH with Age and\nGender (formula: EH ~ Age + Gender). The model's explanatory power is weak\n(Tjur's R2 = 0.03). The model's intercept, corresponding to Age = Young and\nGender = Men, is at -0.23 (95% CI [-0.28, -0.19], p &lt; .001). Within this model:\n\n  - The effect of Age [Old] is statistically significant and negative (beta =\n-0.83, 95% CI [-0.90, -0.77], p &lt; .001; Std. beta = -0.83, 95% CI [-0.90,\n-0.77])\n  - The effect of Gender [Women] is statistically significant and negative (beta\n= -0.42, 95% CI [-0.47, -0.37], p &lt; .001; Std. beta = -0.42, 95% CI [-0.47,\n-0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n\nWe can use this output to write up a final report:\nWe fitted a logistic model (estimated using ML) to predict the use of the utterance-final discourse particle eh with Age and Gender (formula: EH ~ Age + Gender). The model’s explanatory power is weak (Tjur’s R2 = 0.03). The model’s intercept, corresponding to Age = Young and Gender = Men, is at -0.23 (95% CI [-0.28, -0.19], p &lt; .001). Within this model:\n\nThe effect of Age [Old] is statistically significant and negative (beta = -0.83, 95% CI [-0.90, -0.77], p &lt; .001; Std. beta = -0.83, 95% CI [-0.90, -0.77])\nThe effect of Gender [Women] is statistically significant and negative (beta = -0.42, 95% CI [-0.47, -0.37], p &lt; .001; Std. beta = -0.42, 95% CI [-0.47, -0.37])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using\n\n\nOrdinal Regression\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable (Agresti). For this reason, ordinal regression is one of the key methods in analysing Likert data.\nTo see how an ordinal regression is implemented in R, we load and inspect the ´ordinaldata´ data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students’ final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (very likely, somewhat likely, and unlikely) and reflects the committees’ assessment of whether the student is likely to succeed in the program.\n\n# load data\nordata  &lt;- base::readRDS(\"tutorials/regression/data/ord.rda\", \"rb\") %&gt;%\n  dplyr::rename(Recommend = 1, \n              Internal = 2, \n              Exchange = 3, \n              FinalScore = 4) %&gt;%\n  dplyr::mutate(FinalScore = round(FinalScore, 2))\n\n\n\nRecommendInternalExchangeFinalScorevery likely003.26somewhat likely103.21unlikely113.94somewhat likely002.81somewhat likely002.53unlikely012.59somewhat likely002.56somewhat likely002.73unlikely003.00somewhat likely103.50unlikely113.65somewhat likely002.84very likely013.90somewhat likely002.68unlikely103.57\n\n\nIn a first step, we need to relevel the ordinal variable to represent an ordinal factor (or a progression from “unlikely” over “somewhat likely” to “very likely”. And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n# relevel data\nordata &lt;- ordata %&gt;%\n  dplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %&gt;%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %&gt;%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n\nNow that the dependent variable is releveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n1.900000 2.720000 2.990000 2.998925 3.270000 4.000000 \n\n\n[1] 0.397940933861\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n# visualize data\nggplot(ordata, aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\n\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modeling by using the polr function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course.\n\n# fit ordered logit model and store results 'm'\nm.or &lt;- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n# summarize model\nsummary(m.or)\n\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                          Value  Std. Error     t value\nInternalInternal   1.0476639736 0.265789132 3.941711114\nExchangeNoExchange 0.0586810805 0.297858820 0.197009713\nFinalScore         0.6157435960 0.260631273 2.362508493\n\nIntercepts:\n                            Value       Std. Error  t value    \nunlikely|somewhat likely    2.261997617 0.882173598 2.564118471\nsomewhat likely|very likely 4.357441779 0.904467831 4.817685749\n\nResidual Deviance: 717.024871356 \nAIC: 727.024871356 \n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n## store table\n(ctable &lt;- coef(summary(m.or)))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639736459 0.265789131533 3.941711113632\nExchangeNoExchange          0.0586810804767 0.297858819642 0.197009712679\nFinalScore                  0.6157435959832 0.260631273036 2.362508492595\nunlikely|somewhat likely    2.2619976171265 0.882173597890 2.564118471169\nsomewhat likely|very likely 4.3574417786829 0.904467830674 4.817685749458\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n## calculate and store p values\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable &lt;- cbind(ctable, \"p value\" = p))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639736459 0.265789131533 3.941711113632\nExchangeNoExchange          0.0586810804767 0.297858819642 0.197009712679\nFinalScore                  0.6157435959832 0.260631273036 2.362508492595\nunlikely|somewhat likely    2.2619976171265 0.882173597890 2.564118471169\nsomewhat likely|very likely 4.3574417786829 0.904467830674 4.817685749458\n                                        p value\nInternalInternal            0.00008090238230957\nExchangeNoExchange          0.84381993712009706\nFinalScore                  0.01815172553987886\nunlikely|somewhat likely    0.01034382311355037\nsomewhat likely|very likely 0.00000145232867123\n\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n# extract profiled confidence intervals\nci &lt;- confint(m.or)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m.or), ci))\n\n                              OR          2.5 %        97.5 %\nInternalInternal   2.85098336080 1.695837799502 4.81711408248\nExchangeNoExchange 1.06043699275 0.595033205643 1.91977108407\nFinalScore         1.85103250827 1.113625249814 3.09849059341\n\n\nThe odds ratios show that internal students are 2.85 times or 285 percent as likely as non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 times or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant.\n\n\nPoisson Regression\nThis section is based on this tutorials on how to perform a Poisson regression in R.\n\n\n\nNOTEPoisson regressions are used to analyze data where the dependent variable represents counts.\n\n\n\n\n\n\nThis applied particularly to counts that are based on observations of something that is measured in set intervals. For instances the number of pauses in two-minute-long conversations. Poisson regressions are particularly appealing when dealing with rare events, i.e. when something only occurs very infrequently. In such cases, normal linear regressions do not work because the instances that do occur are automatically considered outliers. Therefore, it is useful to check if the data conform to a Poisson distribution.\nHowever, the tricky thing about Poisson regressions is that the data has to conform to the Poisson distribution which is, according to my experience, rarely the case, unfortunately. The Gaussian Normal Distribution is very flexible because it is defined by two parameters, the mean (mu, i.e. \\(\\mu\\)) and the standard deviation (sigma, i.e. \\(\\sigma\\)). This allows the normal distribution to take very different shapes (for example, very high and slim (compressed) or very wide and flat). In contrast, the Poisson is defined by only one parameter (lambda, i.e. \\(\\lambda\\)) which mean that if we have a mean of 2, then the standard deviation is also 2 (actually we would have to say that the mean is \\(\\lambda\\) and the standard deviation is also \\(\\lambda\\) or \\(\\lambda\\) = \\(\\mu\\) = \\(\\sigma\\)). This is much trickier for natural data as this means that the Poisson distribution is very rigid.\n\n\n\n\n\n\n\n\n\nAs we can see, as \\(\\lambda\\) takes on higher values, the distribution becomes wider and flatter - a compressed distribution with a high mean can therefore not be Poisson-distributed. We will now start by loading the data.\n\n# load data\npoissondata  &lt;- base::readRDS(\"tutorials/regression/data/prd.rda\", \"rb\")\n\n\n\nIdPausesLanguageAlcohol450German411080Russian41150German44670German421530German40510Russian421640German461330German4020German33530German4610German401280English38160German441060German37890German40\n\n\nWe will clean the data by factorizing Id which is currently considered a numeric variable rather than a factor.\n\n# process data\npoissondata &lt;- poissondata %&gt;%\n  mutate(Id = factor(Id, levels = 1:200, labels = 1:200))\n# inspect data\nstr(poissondata)\n\n'data.frame':   200 obs. of  4 variables:\n $ Id      : Factor w/ 200 levels \"1\",\"2\",\"3\",\"4\",..: 45 108 15 67 153 51 164 133 2 53 ...\n $ Pauses  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Language: chr  \"German\" \"Russian\" \"German\" \"German\" ...\n $ Alcohol : int  41 41 44 42 40 42 46 40 33 46 ...\n\n\nFirst, we check if the conditions for a Poisson regression are met.\n\n# output the results\ngf = vcd::goodfit(poissondata$Pauses, \n                  type= \"poisson\", \n                  method= \"ML\")\n# inspect results\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df            P(&gt; X^2)\nLikelihood Ratio 33.0122916717  5 0.00000374234139957\n\n\nIf the p-values is smaller than .05, then data is not Poisson distributed which means that it differs significantly from a Poisson distribution and is very likely over-dispersed. We will check the divergence from a Poisson distribution visually by plotting the observed counts against the expected counts if the data were Poisson distributed.\n\nplot(gf,main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\nAlthough the goodfit function reported that the data differs significantly from the Poisson distribution, the fit is rather good. We can use an additional Levene’s test to check if variance homogeneity is given.\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\n\n# check homogeneity\nleveneTest(poissondata$Pauses, poissondata$Language, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df  F value        Pr(&gt;F)    \ngroup   2 17.15274 0.00000013571 ***\n      197                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test indicates that variance homogeneity is also violated. Since both the approximation to a Poisson distribution and variance homogeneity are violated, we should switch either to a quasi-Poisson model or a negative binomial model. However, as we are only interested in how to implement a Poisson model here, we continue despite the fact that this could not be recommended if we were actually interested in accurate results based on a reliable model.\nIn a next step, we summarize Progression by inspecting the means and standard deviations of the individual variable levels.\n\n# extract mean and standard deviation\nwith(poissondata, tapply(Pauses, Language, function(x) {\n  sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n\n               English                 German                Russian \n\"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\" \"M (SD) = 0.20 (0.40)\" \n\n\nNow, we visualize the data.\n\n# plot data\nggplot(poissondata, aes(Pauses, fill = Language)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  scale_fill_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\n\n\n\n\n\n# calculate Poisson regression\nm1.poisson &lt;- glm(Pauses ~ Language + Alcohol, family=\"poisson\", data=poissondata)\n# inspect model\nsummary(m1.poisson)\n\n\nCall:\nglm(formula = Pauses ~ Language + Alcohol, family = \"poisson\", \n    data = poissondata)\n\nCoefficients:\n                     Estimate    Std. Error  z value         Pr(&gt;|z|)    \n(Intercept)     -4.1632652529  0.6628774832 -6.28060 0.00000000033728 ***\nLanguageGerman  -0.7140499158  0.3200148750 -2.23130         0.025661 *  \nLanguageRussian -1.0838591456  0.3582529824 -3.02540         0.002483 ** \nAlcohol          0.0701523975  0.0105992050  6.61865 0.00000000003625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.6722345  on 199  degrees of freedom\nResidual deviance: 189.4496199  on 196  degrees of freedom\nAIC: 373.5045031\n\nNumber of Fisher Scoring iterations: 6\n\n\nIn addition to the Estimates for the coefficients, we could also calculate the confidence intervals for the coefficients (LL stands for lower limit and UL for upper limit in the table below).\n\n# calculate model\ncov.m1 &lt;- sandwich::vcovHC(m1.poisson, type=\"HC0\")\n# extract standard error\nstd.err &lt;- sqrt(diag(cov.m1))\n# extract robust estimates\nr.est &lt;- cbind(Estimate= coef(m1.poisson), \n               \"Robust SE\" = std.err,\n               \"Pr(&gt;|z|)\" = 2 * pnorm(abs(coef(m1.poisson)/std.err),\n                                      lower.tail=FALSE),\nLL = coef(m1.poisson) - 1.96 * std.err,\nUL = coef(m1.poisson) + 1.96 * std.err)\n# inspect data\nr.est\n\n                        Estimate       Robust SE                 Pr(&gt;|z|)\n(Intercept)     -4.1632652529178 0.6480942940026 0.0000000001328637743949\nLanguageGerman  -0.7140499157783 0.2986422497410 0.0168031204011183689373\nLanguageRussian -1.0838591456208 0.3210481575684 0.0007354744824167337974\nAlcohol          0.0701523974937 0.0104351647012 0.0000000000178397516955\n                              LL               UL\n(Intercept)     -5.4335300691629 -2.8930004366726\nLanguageGerman  -1.2993887252708 -0.1287111062859\nLanguageRussian -1.7131135344549 -0.4546047567867\nAlcohol          0.0496994746793  0.0906053203082\n\n\nWe can now calculate the p-value of the model.\n\nwith(m1.poisson, cbind(res.deviance = deviance, df = df.residual,\n  p = pchisq(deviance, df.residual, lower.tail=FALSE)))\n\n     res.deviance  df              p\n[1,] 189.44961991 196 0.618227445717\n\n\nNow, we check, if removing Language leads to a significant decrease in model fit.\n\n# remove Language from the model\nm2.poisson &lt;- update(m1.poisson, . ~ . -Language)\n# check if dropping Language causes a significant decrease in model fit\nanova(m2.poisson, m1.poisson, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Pauses ~ Alcohol\nModel 2: Pauses ~ Language + Alcohol\n  Resid. Df  Resid. Dev Df   Deviance   Pr(&gt;Chi)    \n1       198 204.0213018                             \n2       196 189.4496199  2 14.5716819 0.00068517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now calculate robust coefficients using the msm package (Jackson 2011).\n\n# get estimates\ns &lt;- msm::deltamethod(list(~ exp(x1), ~ exp(x2), ~ exp(x3), ~ exp(x4)), \n                                                coef(m1.poisson), cov.m1)\n# exponentiate old estimates dropping the p values\nrexp.est &lt;- exp(r.est[, -3])\n# replace SEs with estimates for exponentiated coefficients\nrexp.est[, \"Robust SE\"] &lt;- s\n# display results\nrexp.est\n\n                       Estimate       Robust SE               LL\n(Intercept)     0.0155566784084 0.0100821945101 0.00436765044896\nLanguageGerman  0.4896571063601 0.1462322998451 0.27269843575906\nLanguageRussian 0.3382875026084 0.1086065794408 0.18030353649622\nAlcohol         1.0726716412682 0.0111935052470 1.05095521026088\n                             UL\n(Intercept)     0.0554097096208\nLanguageGerman  0.8792279322820\nLanguageRussian 0.6346987787641\nAlcohol         1.0948368101199\n\n\n\n# extract predicted values\n(s1 &lt;- data.frame(Alcohol = mean(poissondata$Alcohol),\n  Language = factor(1:3, levels = 1:3, labels = names(table(poissondata$Language)))))\n\n  Alcohol Language\n1  52.645  English\n2  52.645   German\n3  52.645  Russian\n\n# show results\npredict(m1.poisson, s1, type=\"response\", se.fit=TRUE)\n\n$fit\n             1              2              3 \n0.624944591447 0.306008560283 0.211410945109 \n\n$se.fit\n              1               2               3 \n0.0862811728183 0.0883370633684 0.0705010813453 \n\n$residual.scale\n[1] 1\n\n\n\n## calculate and store predicted values\npoissondata$Predicted &lt;- predict(m1.poisson, type=\"response\")\n## order by program and then by math\npoissondata &lt;- poissondata[with(poissondata, order(Language, Alcohol)), ]\n\n\n## create the plot\nggplot(poissondata, aes(x = Alcohol, y = Predicted, colour = Language)) +\n  geom_point(aes(y = Pauses), alpha=.5, \n             position=position_jitter(h=.2)) +\n  geom_line(size = 1) +\n  labs(x = \"Alcohol (ml)\", y = \"Expected number of pauses\") +\n  scale_color_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\n\n\n\n\n\n\nRobust Regression\nRobust regression represent an alternative to simple linear models which can handle overly influential data points (outliers). Robust regressions allow us to retain outliers in the data rather than having to remove them from the data by adding weights (Rousseeuw and Leroy). Thus, robust regressions are used when there are outliers present in the data and we can thus not use traditional models but we have no good argument to remove these data points.\n\n\n\nNOTERobust regressions allow us to handle overly influential data points (outliers) by using weights. Thus, robust regressions enable us to retain all data points.\n\n\n\n\n\n\nWe begin by loading a data set (the mlrdata set which have used for multiple linear regression).\n\n# load data\nrobustdata  &lt;- base::readRDS(\"tutorials/regression/data/mld.rda\", \"rb\")\n\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nWe first fit an ordinary linear model (and although we know from the section on multiple regression that the interaction between status and attraction is significant, we will disregard this for now as this will help to explain the weighing procedure which is the focus of this section).\n\n# create model\nslm &lt;- lm(money ~ status+attraction, data = robustdata)\n# inspect model\nsummary(slm)\n\n\nCall:\nlm(formula = money ~ status + attraction, data = robustdata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-60.87070 -15.78645  -2.61010  13.88770  59.93710 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             114.94950000   4.28993616  26.79515\nstatusSingle             26.10340000   4.95359160   5.26959\nattractionNotInterested -79.25220000   4.95359160 -15.99894\n                                      Pr(&gt;|t|)    \n(Intercept)             &lt; 0.000000000000000222 ***\nstatusSingle                     0.00000082576 ***\nattractionNotInterested &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.767958 on 97 degrees of freedom\nMultiple R-squared:  0.745229335,   Adjusted R-squared:  0.739976331 \nF-statistic: 141.867286 on 2 and 97 DF,  p-value: &lt; 0.0000000000000002220446\n\n\nWe now check whether the model is well fitted using diagnostic plots.\n\n# generate plots\nautoplot(slm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\n\n\n\n\nThe diagnostic plots indicate that there are three outliers in the data (data points 52, 83 and possibly 64). Therefore, we need to evaluate if the outliers severely affect the fit of the model.\n\nrobustdata[c(52, 64, 83),]\n\n   status    attraction  money\n52 Single NotInterested   0.93\n64 Single NotInterested  84.28\n83 Single    Interested 200.99\n\n\nWe can now calculate Cook’s distance and standardized residuals check if the values of the potentially problematic points have unacceptably high values (-2 &lt; ok &lt; 2).\n\nCooksDistance &lt;- cooks.distance(slm)\nStandardizedResiduals &lt;- stdres(slm)\na &lt;- cbind(robustdata, CooksDistance, StandardizedResiduals)\na[CooksDistance &gt; 4/100, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n\n\nWe will calculate the absolute value and reorder the table so that it is easier to check the values.\n\nAbsoluteStandardizedResiduals &lt;- abs(StandardizedResiduals)\na &lt;- cbind(robustdata, CooksDistance, StandardizedResiduals, AbsoluteStandardizedResiduals)\nasorted &lt;- a[order(-AbsoluteStandardizedResiduals), ]\nasorted[1:10, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n78       Single    Interested 188.76 0.0394313968241         1.95572122040\n21 Relationship NotInterested  81.90 0.0369837409025         1.89404933081\n24 Relationship NotInterested  81.56 0.0364414260634         1.88011125419\n   AbsoluteStandardizedResiduals\n52                 2.49535435377\n83                 2.45708203516\n88                 2.16151282221\n84                 2.15782333134\n1                  2.07565427025\n65                 2.03662765573\n67                 1.98907421786\n78                 1.95572122040\n21                 1.89404933081\n24                 1.88011125419\n\n\nAs Cook’s distance and the standardized residuals do have unacceptable values, we re-calculate the linear model as a robust regression and inspect the results\n\n# create robust regression model\nrmodel &lt;- robustbase::lmrob(money ~ status + attraction, data = robustdata)\n# inspect model\nsummary(rmodel)\n\n\nCall:\nrobustbase::lmrob(formula = money ~ status + attraction, data = robustdata)\n \\--&gt; method = \"MM\"\nResiduals:\n         Min           1Q       Median           3Q          Max \n-61.14269796 -15.20405781  -1.48712081  14.43502508  62.42342804 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             113.18405781   3.89777692  29.03811\nstatusSingle             25.38251415   5.08841106   4.98830\nattractionNotInterested -76.49387400   5.06626449 -15.09867\n                                      Pr(&gt;|t|)    \n(Intercept)             &lt; 0.000000000000000222 ***\nstatusSingle                      0.0000026725 ***\nattractionNotInterested &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 22.3497532 \nMultiple R-squared:  0.740716949,   Adjusted R-squared:  0.735370907 \nConvergence in 11 IRWLS iterations\n\nRobustness weights: \n 10 weights are ~= 1. The remaining 90 ones are summarized as\n       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. \n0.415507215 0.856134403 0.947485769 0.889078657 0.986192099 0.998890516 \nAlgorithmic parameters: \n           tuning.chi                    bb            tuning.psi \n1.5476399999999999046 0.5000000000000000000 4.6850610000000001421 \n           refine.tol               rel.tol             scale.tol \n0.0000001000000000000 0.0000001000000000000 0.0000000001000000000 \n            solve.tol              zero.tol           eps.outlier \n0.0000001000000000000 0.0000000001000000000 0.0010000000000000000 \n                eps.x     warn.limit.reject     warn.limit.meanrw \n0.0000000000018189894 0.5000000000000000000 0.5000000000000000000 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nThe output shows that both status and attraction are significant but, as we have seen above, the effect that really matters is the interaction between status and attraction.\nWe will briefly check the weights to understand the process of weighing better. The idea of weighing is to downgrade data points that are too influential while not punishing data points that have a good fit and are thus less influential. This means that the problematic data points should have lower weights than other data points (the maximum is 1 - so points can only be made “lighter”).\n\nhweights &lt;- data.frame(status = robustdata$status, resid = rmodel$resid, weight = rmodel$rweights)\nhweights2 &lt;- hweights[order(rmodel$rweights), ]\nhweights2[1:15, ]\n\n         status          resid         weight\n83       Single  62.4234280426 0.415507214894\n52       Single -61.1426979566 0.434323578289\n88       Single  55.2134280426 0.521220526760\n84       Single  55.1234280426 0.522529106622\n78       Single  50.1934280426 0.593234343069\n65       Single -49.9526979566 0.596626306571\n1  Relationship  49.6398161925 0.601024865582\n67       Single -48.7926979566 0.612874578464\n21 Relationship  45.2098161925 0.661914499988\n24 Relationship  44.8698161925 0.666467581490\n39 Relationship -43.8940578083 0.679427975993\n79       Single  40.8234280426 0.719104361372\n58       Single -40.5226979566 0.722893449575\n89       Single  39.9734280426 0.729766992775\n95       Single  39.8234280426 0.731633375015\n\n\nThe values of the weights support our assumption that those data points that were deemed too influential are made lighter as they now only have weights of 0.415507214894 and 0.434323578289 respectively. This was, however, not the focus of this sections as this section merely served to introduce the concept of weights and how they can be used in the context of a robust linear regression.\n\n\nMixed-Effects Regression\nIn contrast to fixed-effects regression models, mixed-effects models assume a hierarchical data structure in which data points are grouped or nested in higher order categories (e.g. students within classes). Mixed-effects models are rapidly increasing in use in data analysis because they allow us to incorporate hierarchical or nested data structures. Mixed-effects models are, of course, an extension of fixed-effects regression models and also multivariate and come in different types.\nIn the following, we will go over the most relevant and frequently used types of mixed-effect regression models, mixed-effects linear regression models and mixed-effects binomial logistic regression models.\nThe major difference between these types of models is that they take different types of dependent variables. While linear models take numeric dependent variables, logistic models take nominal variables.\n\nLinear Mixed-Effects Regression\nThe following focuses on an extension of ordinary multiple linear regressions: mixed-effects regression linear regression. Mixed-effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. (multi-)collinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of mixed-effects regression modeling are that they are prone to producing high \\(\\beta\\)-errors (see Johnson) and that they require rather large data sets.\n\nIntroduction\nSo far, the regression models that we have used only had fixed-effects. Having only fixed-effects means that all data points are treated as if they are completely independent and thus on the same hierarchical level. However, it is very common that the data is nested in the sense that data points are not independent because they are, for instance produced by the same speaker or are grouped by some other characteristic. In such cases, the data is considered hierarchical and statistical models should incorporate such structural features of the data they work upon. Fortunately, modeling hierarchical or nested data structures is very easy thanks to the lme4 package (Bates et al. 2015).\nWith respect to regression modeling, hierarchical structures are incorporated by what is called random effects. When models only have a fixed-effects structure, then they make use of only a single intercept and/or slope (as in the left panel in the figure below), while mixed effects models have intercepts for each level of a random effect. If the random effect structure represents speakers then this would mean that a mixed-model would have a separate intercept and/or slope for each speaker (in addition to the overall intercept that is shown as an orange line in the figure below).\n\n\n\n\n\n\n\n\n\nThe idea behind regression analysis is expressed formally in the equation below where\\(f_{(x)}\\) is the y-value we want to predict, \\(\\alpha\\) is the intercept (the point where the regression line crosses the y-axis at x = 0), \\(\\beta\\) is the coefficient (the slope of the regression line), and x is the value of a predictor (e.g. 180cm - if we would like to predict the weight of a person based on their height). The \\(\\epsilon\\) is an error term that reflects the difference between the predicted value and the (actually) observed value (\\(\\epsilon\\) is thus a residual that is important as regressions assume that residuals are, e.g., normally distributed).\n\\[\\begin{equation}\nf_{(x)} = \\alpha + \\beta x + \\epsilon\n\\end{equation}\\]\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (\\(x\\)) and add the value of the intercept (point where line crosses the y-axis at x = 0).\nThe equation below represents a formal representation of a mixed-effects regression with varying intercepts (see Winter, 235).\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta x + \\epsilon\n\\end{equation}\\]\nIn this random intercept model, each level of a random variable has a different intercept. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the product of the predictor coefficient and the value of x.\nFinally, the equation below represents a formal representation of a mixed-effects regression with varying intercepts and varying slopes (see Winter, 235).\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta_{i}x + \\epsilon\n\\end{equation}\\]\nIn this last model, each level of a random variable has a different intercept and a different slope. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the coefficient of that random effect level multiplied by the value of x.\n\n\nRandom Effects\nRandom Effects can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to fixed-effects models, that have only 1 intercept and one slope (left panel in the figure above), mixed-effects models can therefore have various random intercepts (center panel) or various random slopes, or both, various random intercepts and various random slopes (right panel).\nWhat features do distinguish random and fixed effects?\n\nRandom effects represent a higher level variable under which data points are grouped. This implies that random effects must be categorical (or nominal but they a´cannot be continuous!) (see Winter, 236).\nRandom effects represent a sample of an infinite number of possible levels. For instance, speakers, trials, items, subjects, or words represent a potentially infinite pool of elements from which many different samples can be drawn. Thus, random effects represent a random sample sample. Fixed effects, on the other hand, typically do not represent a random sample but a fixed set of variable levels (e.g. Age groups, or parts-of-speech).\nRandom effects typically represent many different levels while fixed effects typically have only a few. Zuur, Hilbe, and Ieno propose that a variable may be used as a fixed effect if it has less than 5 levels while it should be treated as a random effect if it has more than 10 levels. Variables with 5 to 10 levels can be used as both. However, this is a rule of thumb and ignores the theoretical reasons (random sample and nestedness) for considering something as a random effect and it also is at odds with the way that repeated measures are models (namely as mixed effects) although they typically only have very few levels.\nFixed effects represent an effect that if we draw many samples, the effect would be consistent across samples (Winter) while random effects should vary for each new sample that is drawn.\n\nIn the following, we will only focus on models with random intercepts because this is the more common method and because including both random intercepts and random slopes requires larger data sets (but have a better fit because intercepts are not forced to be parallel and the lines therefore have a better fit). You should, however, always think about what random effects structure is appropriate for your model - a very recommendable explanation of how to chose which random effects structure is best (and about what the determining factors for this decision are) is give in Winter (241–44). Also, consider the center and the right plots above to understand what is meant by random intercepts and random slopes.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted (Field, Miles, and Field). We test whether including random effects is warranted by comparing a model, that bases its estimates of the depended variable solely on the base intercept (the mean), with a model, that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the random-effect model explains significantly more variance than the simple model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified if they reduce residual deviance.\n\n\nExample: Preposition Use across Time by Genre\nTo explore how to implement a mixed-effects model in R we revisit the preposition data that contains relative frequencies of prepositions in English texts written between 1150 and 1913. As a first step, and to prepare our analysis, we load necessary R packages, specify options, and load as well as provide an overview of the data.\n\n# load data\nlmmdata  &lt;- base::readRDS(\"tutorials/regression/data/lmd.rda\", \"rb\") %&gt;%\n  # convert date into a numeric variable\n  dplyr::mutate(Date = as.numeric(Date))\n\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nThe data set contains the date when the text was written (Date), the genre of the text (Genre), the name of the text (Text), the relative frequency of prepositions in the text (Prepositions), and the region in which the text was written (Region). We now plot the data to get a first impression of its structure.\n\np1 &lt;- ggplot(lmmdata, aes(x = Date, y = Prepositions)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, color = \"red\", linetype = \"dashed\") +\n  theme_bw() +\n  labs(y = \"Frequency\\n(Prepositions)\")\np2 &lt;- ggplot(lmmdata, aes(x = reorder(Genre, -Prepositions), y = Prepositions)) +\n  geom_boxplot() +\n  theme_bw() + \n  theme(axis.text.x = element_text(angle=90)) +\n  labs(x = \"Genre\", y = \"Frequency\\n(Prepositions)\")\np3 &lt;- ggplot(lmmdata, aes(Prepositions)) +\n  geom_histogram() +\n  theme_bw() + \n  labs(y = \"Count\", x = \"Frequency (Prepositions)\")\ngridExtra::grid.arrange(grobs = list(p1, p2, p3), widths = c(1, 1), layout_matrix = rbind(c(1, 1), c(2, 3)))\n\n\n\n\n\n\n\n\nThe scatter plot in the upper panel indicates that the use of prepositions has moderately increased over time while the boxplots in the lower left panel show that the genres differ quite substantially with respect to their median frequencies of prepositions per text. Finally, the histogram in the lower right panel show that preposition use is distributed normally with a mean of 132.2 prepositions per text.\n\np4 &lt;- ggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  labs(x = \"Year\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\np5 &lt;- ggplot(lmmdata, aes(Region, Prepositions)) +\n  geom_boxplot() +\n  labs(x = \"Region\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\ngridExtra::grid.arrange(p4, p5, nrow = 1)\n\n\n\n\n\n\n\n\n\nggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  facet_wrap(~ Genre, nrow = 4) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs(x = \"Date of composition\", y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\n\n\n\n\nCentering or even scaling numeric variables is useful for later interpretation of regression models: if the date variable were not centered, the regression would show the effects of variables at year 0(!). If numeric variables are centered, other variables are variables are considered relative not to 0 but to the mean of that variable (in this case the mean of years in our data). Centering simply means that the mean of the numeric variable is subtracted from each value.\n\nlmmdata$DateUnscaled &lt;- lmmdata$Date\nlmmdata$Date &lt;- scale(lmmdata$Date, scale = F)\n\n\n\nDateGenreTextPrepositionsRegionDateUnscaled109.8696461825Sciencealbin166.01North1,73684.8696461825Educationanon139.86North1,711181.8696461825PrivateLetterausten130.78North1,808251.8696461825Educationbain151.29North1,878116.8696461825Educationbarclay145.72North1,743281.8696461825Educationbenson120.77North1,908279.8696461825Diarybenson119.17North1,906270.8696461825Philosophyboethja132.96North1,897158.8696461825Philosophyboethri130.49North1,785149.8696461825Diaryboswell135.94North1,776278.8696461825Travelbradley154.20North1,90584.8696461825Educationbrightland149.14North1,711135.8696461825Sermonburton159.71North1,76299.8696461825Sermonbutler157.49North1,726208.8696461825PrivateLettercarlyle124.16North1,835\n\n\nWe now set up a fixed-effects model with the glm function and a mixed-effects model using the glmer function from the lme4 package (Bates et al. 2015) with Genre as a random effect.\n\n# generate models\nm0.glm &lt;- glm(Prepositions ~ 1, family = gaussian, data = lmmdata)\nm0.lmer = lmer(Prepositions ~ 1 + (1|Genre), REML = T, data = lmmdata)\n\nNow that we have created the base-line models, we will test whether including a random effect structure is mathematically justified. It is important to note here that we are not going to test if including a random effect structure is theoretically motivated but simply if it causes a decrease in variance.\n\n\nTesting Random Effects\nAs a first step in the modeling process, we now need to determine whether or not including a random effect structure is justified. We do so by comparing the AIC of the base-line model without random intercepts to the AIC of the model with random intercepts.\n\nAIC(logLik(m0.glm))\n\n[1] 4718.19031114\n\nAIC(logLik(m0.lmer))\n\n[1] 4497.77554693\n\n\nThe inclusion of a random effect structure with random intercepts is justified as the AIC of the model with random intercepts is substantially lower than the AIC of the model without random intercepts.\nWhile I do not how how to test if including a random effect is justified, there are often situations, which require to test exactly which random effect structure is best. When doing this, it is important to use restricted maximum likelihood (REML = TRUE or method = REML) rather than maximum likelihood (see Pinheiro and Bates; Winter, 226).\n\n# generate models with 2 different random effect structures\nma.lmer = lmer(Prepositions ~ Date + (1|Genre), REML = T, data = lmmdata)\nmb.lmer = lmer(Prepositions ~ Date + (1 + Date | Genre), REML = T, data = lmmdata)\n# compare models\nanova(ma.lmer, mb.lmer, test = \"Chisq\", refit = F)\n\nData: lmmdata\nModels:\nma.lmer: Prepositions ~ Date + (1 | Genre)\nmb.lmer: Prepositions ~ Date + (1 + Date | Genre)\n        npar         AIC         BIC       logLik    deviance    Chisq Df\nma.lmer    4 4499.148092 4516.292084 -2245.574046 4491.148092            \nmb.lmer    6 4486.699509 4512.415498 -2237.349755 4474.699509 16.44858  2\n        Pr(&gt;Chisq)    \nma.lmer               \nmb.lmer 0.00026806 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model comparison shows that the model with the more complex random effect structure has a significantly better fit to the data compared with the model with the simpler random effect structure. However, we will continue with the model with the simpler structure because this is just an example.\n\n\n\nNOTEIn a real analysis, we would switch to a model with random intercepts and random slopes for Genre because it has a significantly better fit to the data.\n\n\n\n\n\n\n\n\nModel Fitting\nAfter having determined that including a random effect structure is justified, we can continue by fitting the model and including diagnostics as we go. Including diagnostics in the model fitting process can save time and prevent relying on models which only turn out to be unstable if we would perform the diagnostics after the fact.\nWe begin fitting our model by adding Date as a fixed effect and compare this model to our mixed-effects base-line model to see if Date improved the model fit by explaining variance and if Date significantly correlates with our dependent variable (this means that the difference between the models is the effect (size) of Date!)\n\nm1.lmer &lt;- lmer(Prepositions ~ (1|Genre) + Date, REML = T, data = lmmdata)\nanova(m1.lmer, m0.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm0.lmer: Prepositions ~ 1 + (1 | Genre)\nm1.lmer: Prepositions ~ (1 | Genre) + Date\n        npar         AIC         BIC       logLik    deviance  Chisq Df\nm0.lmer    3 4501.947337 4514.805331 -2247.973668 4495.947337          \nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736 8.9296  1\n        Pr(&gt;Chisq)   \nm0.lmer              \nm1.lmer  0.0028059 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nThe model with Date is the better model (significant p-value and lower AIC). The significant p-value shows that Date correlates significantly with Prepositions (\\(\\chi\\)2(1): 8.929600937904, p = 0.00281) . The \\(\\chi\\)2 value here is labeled Chisq and the degrees of freedom are calculated by subtracting the smaller number of DFs from the larger number of DFs.\nWe now test whether Region should also be part of the final minimal adequate model. The easiest way to add predictors is by using the update function (it saves time and typing).\n\n# generate model\nm2.lmer &lt;- update(m1.lmer, .~.+ Region)\n# test vifs\ncar::vif(m2.lmer)\n\n         Date        Region \n1.20287668037 1.20287668037 \n\n# compare models                \nanova(m2.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm2.lmer: Prepositions ~ (1 | Genre) + Date + Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm2.lmer    5 4494.624343 4516.054333 -2242.312171 4484.624343 2.39339  1\n        Pr(&gt;Chisq)\nm1.lmer           \nm2.lmer    0.12185\n\n\nThree things tell us that Region should not be included:\n\nthe AIC does not decrease,\nthe BIC increases(!), and\nthe p-value is higher than .05.\n\nThis means, that we will continue fitting the model without having Region included. Well… not quite - just as a note on including variables: while Region is not significant as a main effect, it must still be included in a model if it were part of a significant interaction. To test if this is indeed the case, we fit another model with the interaction between Date and Region as predictor.\n\n# generate model\nm3.lmer &lt;- update(m1.lmer, .~.+ Region*Date)\n# extract vifs\ncar::vif(m3.lmer)\n\n         Date        Region   Date:Region \n1.96923042279 1.20324697637 1.78000887980 \n\n# compare models                \nanova(m3.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm3.lmer: Prepositions ~ (1 | Genre) + Date + Region + Date:Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm3.lmer    6 4496.124872 4521.840861 -2242.062436 4484.124872 2.89286  2\n        Pr(&gt;Chisq)\nm1.lmer           \nm3.lmer    0.23541\n\n\nAgain, the high p-value and the increase in AIC and BIC show that we have found our minimal adequate model with only contains Date as a main effect. In a next step, we can inspect the final minimal adequate model, i.e. the most parsimonious (the model that explains a maximum of variance with a minimum of predictors).\n\n# inspect results\nsummary(m1.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Prepositions ~ (1 | Genre) + Date\n   Data: lmmdata\n\nREML criterion at convergence: 4491.1\n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-3.734915441 -0.657038004  0.005865025  0.661298615  3.596659863 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev.  \n Genre    (Intercept) 159.021120 12.6103576\n Residual             228.764179 15.1249522\nNumber of obs: 537, groups:  Genre, 16\n\nFixed effects:\n                   Estimate      Std. Error  t value\n(Intercept) 133.88516211469   3.24749296223 41.22724\nDate          0.01894493515   0.00632363682  2.99589\n\nCorrelation of Fixed Effects:\n     (Intr)\nDate 0.005 \n\n\n\n\nModel Diagnostics\nWe can now evaluate the goodness of fit of the model and check if mathematical requirements and assumptions have been violated. In a first step, we generate diagnostic plots that focus on the random effect structure.\n\nplot(m1.lmer, Genre ~ resid(.), abline = 0 ) # generate diagnostic plots\n\n\n\n\n\n\n\n\nThe plot shows that there are some outliers (points outside the boxes) and that the variability within letters is greater than in other genres we therefore examine the genres in isolation standardized residuals versus fitted values (Pinheiro and Bates, 175).\n\nplot(m1.lmer, resid(., type = \"pearson\") ~ fitted(.) | Genre, id = 0.05, \n     adj = -0.3, pch = 20, col = \"gray40\")\n\n\n\n\n\n\n\n\nThe plot shows the standardized residuals (or Pearson’s residuals) versus fitted values and suggests that there are outliers in the data (the names elements in the plots). To check if these outliers are a cause for concern, we will now use a Levene’s test to check if the variance is distributed homogeneously (homoscedasticity) or whether the assumption of variance homogeneity is violated (due to the outliers).\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\nWe use Levene’s test here merely to check if it substantiates the impressions we got from the visual inspection.\n\n# check homogeneity\nleveneTest(lmmdata$Prepositions, lmmdata$Genre, center = mean)\n\nWarning in leveneTest.default(lmmdata$Prepositions, lmmdata$Genre, center =\nmean): lmmdata$Genre coerced to factor.\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value   Pr(&gt;F)  \ngroup  15 1.74289 0.039906 *\n      521                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test shows that the variance is distributed unevenly across genres which means that we do not simply continue but should either remove problematic data points (outliers) or use a weighing method.\nIn this case, we create a new model which uses weights to compensate for heterogeneity of variance and thus the influence of outliers - which is an alternative to removing the data points and rerunning the analysis (Pinheiro and Bates, 177). However, to do so, we need to use a different function (the lme function) which means that we have to create two models: the old minimal adequate model and the new minimal adequate model with added weights. After we have created these models, we will compare them to see if including weights has improved the fit.\n\n# generate models\nm4.lme = lme(Prepositions ~ Date, random = ~1|Genre, data = lmmdata, method = \"ML\")\nm5.lme &lt;- update(m4.lme, weights = varIdent(form = ~ 1 | Genre))\n# compare models\nanova(m5.lme, m4.lme)\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm5.lme     1 19 4485.84955680 4567.28352060 -2223.92477840                     \nm4.lme     2  4 4495.01773596 4512.16172834 -2243.50886798 1 vs 2 39.1681791556\n       p-value\nm5.lme        \nm4.lme  0.0006\n\n\nThe weight model (m5.lme) that uses weights to account for unequal variance is performing significantly better than the model without weights (m4.lme) and we therefore switch to the weight model and inspect its parameters.\n\n# inspect results\nsummary(m5.lme)        \n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n           AIC          BIC        logLik\n  4485.8495568 4567.2835206 -2223.9247784\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2632323711 14.3422048362\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340715615490  0.869521038482  0.788861254265  0.911719712514 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096572672893  0.978728647599  0.784966066137  0.736986014758  1.190619042239 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929751328  0.974641038177  0.848591142831  0.970869209380  1.086232439777 \nTrialProceeding \n 1.260188475499 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640155217 3.144361414827 520 42.6045221424  0.0000\nDate          0.0217416268 0.005454723299 520  3.9858349603  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-3.319117397700 -0.679728744836  0.014685431575  0.698705968152  3.103872340648 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\nWe can also use an ANOVA display which is more to the point.\n\nanova(m5.lme)          \n\n            numDF denDF        F-value p-value\n(Intercept)     1   520 1813.892139648  &lt;.0001\nDate            1   520   15.886880331  0.0001\n\n\nAs we did before, we now check, whether the final minimal model (with weights) outperforms an intercept-only base-line model.\n\n# generate base-line model\nm0.lme = lme(Prepositions ~ 1, random = ~1|Genre, data = lmmdata, method = \"ML\", weights = varIdent(form = ~ 1 | Genre))\nanova(m0.lme, m5.lme)  # test if date is significant\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm0.lme     1 18 4496.28563021 4573.43359592 -2230.14281511                     \nm5.lme     2 19 4485.84955680 4567.28352060 -2223.92477840 1 vs 2 12.4360734115\n       p-value\nm0.lme        \nm5.lme  0.0004\n\n\nOur final minimal adequate model with weights performs significantly better than an intercept only base-line model. Before doing the final diagnostics, we well inspect the estimates for the random effect structure to check if there are values which require further inspection (e.g. because they are drastically different from all other values).\n\n# extract estimates and sd for fixed and random effects\nintervals(m5.lme, which=\"fixed\")      \n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                        lower              est.             upper\n(Intercept) 127.7983166699800 133.9640155217270 140.1297143734741\nDate          0.0110455977276   0.0217416268231   0.0324376559185\n\n\nThe random effect estimates do not show any outliers or drastically increased or decreased values which means that the random effect structure is fine.\n\n\nEffect Sizes\nWe will now extract effect sizes (in the example: the effect size of Date) and calculate normalized effect size measures (this effect size measure works for all fixed effects). When you have factorial design, you can take the square root of the squared t-value divided by the t-value squared plus the degrees of freedom to calculate the effect size:\n\\[\\begin{equation}\n\nr = \\sqrt{ \\frac{ t^2}{(t^2 + df) } } = \\sqrt{ \\frac{ 3.99^2}{(3.99^2 + 520) } } = 0.172\n\n\\end{equation}\\]\n\n\n\n\nNOTETwo words of warning though: br&gt;1. In our case, the effect we are interested in is not factorial but continuous which means that we should not use this effect size measure. We only show this here as an example for how you can calculate the effect size measure r.2. Only apply this function to main effects that are not involved in interactions as they are meaningless because the amount of variance explained by main effects involved in interactions is unclear (Field, Miles, and Field, 641).\n\n\n\n\n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n&lt;0.001\n\n\nDate\n0.02\n0.01 – 0.03\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.70\n\n\n\nτ00 Genre\n150.39\n\n\nICC\n0.42\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.432\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\nThe marginal R2 (marginal coefficient of determination) represents the variance explained by the fixed effects while the conditional R2 is interpreted as a variance explained by the entire model, including both fixed and random effects (Bartoń).\nThe respective call for the model is:\n\n# extract R2s\nr.squaredGLMM(m1.lmer)\n\n                 R2m            R2c\n[1,] 0.0121971160219 0.417270545268\n\n\nThe effects can be visualized using the plot_model function from the sjPlot package (Lüdecke).\n\nsjPlot::plot_model(m5.lme, type = \"pred\", terms = c(\"Date\")) +\n  # show uncentered date rather than centered date\n  scale_x_continuous(name = \"Date\", \n                     breaks = seq(-500, 300, 100), \n                     labels = seq(1150, 1950, 100))\n\n\n\n\n\n\n\n\nWhile we have already shown that the effect of Date is significant, it is small which means that the number of prepositions per text does not correlate very strongly with time. This suggests that other factors that are not included in the model also impact the frequency of prepositions (and probably more meaningfully, too).\nBefore turning to the diagnostics, we will use the fitted (or predicted) and the observed values with a regression line for the predicted values. This will not only show how good the model fit the data but also the direction and magnitude of the effect.\n\n# extract predicted values\nlmmdata$Predicted &lt;- predict(m5.lme, lmmdata)\n# plot predicted values\nggplot(lmmdata, aes(DateUnscaled, Predicted)) +\n  facet_wrap(~Genre) +\n  geom_point(aes(x = DateUnscaled, y = Prepositions), color = \"gray80\", size = .5) +\n  geom_smooth(aes(y = Predicted), color = \"gray20\", linetype = \"solid\", \n              se = T, method = \"lm\") +\n  guides(color=guide_legend(override.aes=list(fill=NA))) +  \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"top\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  xlab(\"Date of composition\")\n\n\n\n\n\n\n\n\n\n\nModel Diagnostics\nIn addition, we generate diagnostic plots. What we wish to see in the diagnostic plots is a cloud of dots in the middle of the window without any structure. What we do not want to see is a funnel-shaped cloud because this indicates an increase of the errors/residuals with an increase of the predictor(s) (because this would indicate heteroscedasticity) (Pinheiro and Bates, 182).\n\n# start plotting\npar(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns\nplot(m5.lme, pch = 20, col = \"black\", lty = \"dotted\"); par(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nWhat a wonderful unstructured cloud - the lack of structure tells us that the model is “healthy” and does not suffer from heteroscedasticity. We will now create more diagnostic plots to find potential problems (Pinheiro and Bates, 21).\n\n# fitted values by Genre\nplot(m5.lme, form = resid(., type = \"p\") ~ fitted(.) | Genre, abline = 0, \n     cex = .5, pch = 20, col = \"black\")\n\n\n\n\n\n\n\n\nIn contrast to the unweight model, no data points are named which indicates that the outliers do no longer have unwarranted influence on the model. Now, we check the residuals of fitted values against observed values (Pinheiro and Bates, 179). What we would like to see is a straight, upwards going line.\n\n# residuals of fitted values against observed\nqqnorm(m5.lme, pch = 20, col = \"black\")\n\n\n\n\n\n\n\n\nA beautiful, straight line! The qqplot does not indicate any problems. It is, unfortunately, rather common that the dots deviate from the straight line at the very bottom or the very top which means that the model is good at estimating values around the middle of the dependent variable but rather bad at estimating lower or higher values. Next, we check the residuals by “Genre” (Pinheiro and Bates, 179).\n\n# residuals by genre\nqqnorm(m5.lme, ~resid(.) | Genre, pch = 20, col = \"black\" )\n\n\n\n\n\n\n\n\nBeautiful straight lines - perfection! Now, we inspect the observed responses versus the within-group fitted values (Pinheiro and Bates, 178).\n\n# observed responses versus the within-group fitted values\nplot(m5.lme, Prepositions ~ fitted(.), id = 0.05, adj = -0.3, \n     xlim = c(80, 220), cex = .8, pch = 20, col = \"blue\")\n\n\n\n\n\n\n\n\nAlthough some data points are named, the plot does not show any structure, like a funnel, which would have been problematic.\n\n\nReporting Results\nBefore we do the write-up, we have a look at the model summary as this will provide us with at least some of the parameters that we want to report.\n\nsummary(m5.lme)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n           AIC          BIC        logLik\n  4485.8495568 4567.2835206 -2223.9247784\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2632323711 14.3422048362\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340715615490  0.869521038482  0.788861254265  0.911719712514 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096572672893  0.978728647599  0.784966066137  0.736986014758  1.190619042239 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929751328  0.974641038177  0.848591142831  0.970869209380  1.086232439777 \nTrialProceeding \n 1.260188475499 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640155217 3.144361414827 520 42.6045221424  0.0000\nDate          0.0217416268 0.005454723299 520  3.9858349603  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-3.319117397700 -0.679728744836  0.014685431575  0.698705968152  3.103872340648 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n&lt;0.001\n\n\nDate\n0.02\n0.01 – 0.03\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.70\n\n\n\nτ00 Genre\n150.39\n\n\nICC\n0.42\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.432\n\n\n\n\n\n\nWe can now use the information extracted above to write up a final report:\nA mixed-effect linear regression model which contained the genre of texts as random effect was fit to the data in a step-wise-step up procedure. Due to the presence of outliers in the data, weights were included into the model which led to a significantly improved model fit compared to an un-weight model (\\(\\chi\\)2(2): 39.17, p: 0.0006). The final minimal adequate model performed significantly better than an intercept-only base-line model (\\(\\chi\\)2(1): 12.44, p =.0004) and showed that the frequency of prepositions increases significantly but only marginally with the date of composition (Estimate: 0.02, CI: 0.01-0.03, p &lt; .001, marginal R2 = 0.0174, conditional R2 = 0.4324). Neither the region where the text was composed nor a higher order interaction between genre and region significantly correlated with the use of prepositions in the data.\n\n\nRemarks on Prediction\nWhile the number of intercepts, the model reports, and the way how mixed- and fixed-effects arrive at predictions differ, their predictions are extremely similar and almost identical (at least when dealing with a simple random effect structure). Consider the following example where we create analogous fixed and mixed effect models and plot their predicted frequencies of prepositions per genre across the un-centered date of composition. The predictions of the mixed-effects model are plotted as a solid red line, while the predictions of the fixed-effects model are plotted as dashed blue lines.\n\n# create lm model\nm5.lmeunweight &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmePredictions &lt;- fitted(m5.lmeunweight, lmmdata)\nm5.lm &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmPredictions &lt;- fitted(m5.lm, lmmdata)\n# plot predictions\nggplot(lmmdata, aes(x = DateUnscaled, y = lmePredictions, group = Genre)) +\n  geom_line(aes(y = lmmdata$lmePredictions), linetype = \"solid\", color = \"red\") +\n  geom_line(aes(y = lmmdata$lmPredictions), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ Genre, nrow = 4) +\n  theme_bw() +\n  labs(x = \"Date of composition\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\n\n\n\n\nThe predictions overlap almost perfectly which means that the predictions of both are almost identical - irrespective of whether genre is part of the mixed or the fixed effects structure.\n\n\n\nMixed-Effects Binomial Logistic Regression\nWe now turn to an extension of binomial logistic regression: mixed-effects binomial logistic regression. As is the case with linear mixed-effects models logistic mixed effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. multicollinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of regression modeling are that they are prone to producing high \\(\\beta\\)-errors (see Johnson) and that they require rather large data sets.\n\nIntroduction\nAs is the case with linear mixed-effects models, binomial logistic mixed-effect models are multivariate analyses that treat data points as hierarchical or grouped in some way. In other words, they take into account that the data is nested in the sense that data points are produced by the same speaker or are grouped by some other characteristics. In mixed-models, hierarchical structures are modelled as random effects. If the random effect structure represents speakers then this means that a mixed-model would have a separate intercept and/or slope for each speaker.\nRandom Effects in linear models can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to linear mixed-effects models, random effects differ in the position and the slope of the logistic function that is applied to the likelihood of the dependent variable. Random intercepts (upper right panel) or various random slopes (lower left panel), or both, various random intercepts and various random slopes (lower right panel). In the following, we will only focus on models with random intercepts because this is the by far more common method and because including both random intercepts and random slopes requires huge amounts of data. Consider the Figure below to understand what is meant by random intercepts.\n\n\n\n\n\n\n\n\n\nThe upper left panel merely shows the logistic curve representing the predictions of a fixed-effects logistic regression with a single intercept and slope. The upper right panel shows the logistic curves representing the predictions of a of a mixed-effects logistic regression with random intercepts for each level of a grouping variable. The lower left panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with one intercept but random slopes for each level of a grouping variable. The lower right panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with random intercepts and random slopes for each level of a grouping variable.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted (Field, Miles, and Field). We test whether including random effects is warranted by comparing a model, that bases its estimates of the dependent variable solely on the base intercept, with a model that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the mixed-effects model explains significantly more variance than the fixed-effects model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified.\n\n\nExample: Discourse LIKE in Irish English\nIn this example we will investigate which factors correlate with the use of final discourse like (e.g. “The weather is shite, like!”) in Irish English. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender (Gender: Men versus Women) and age of that speaker (Age: Old versus Young), whether the interlocutors were of the same or a different gender (ConversationType: SameGender versus MixedGender), and whether another final discourse like had been used up to three speech units before (Priming: NoPrime versus Prime), whether or not the speech unit contained an final discourse like (SUFLike: 1 = yes, 0 = no. To begin with, we load the data and inspect the structure of the data set,\n\n# load data\nmblrdata  &lt;- base::readRDS(\"tutorials/regression/data/mbd.rda\", \"rb\")\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-061$CWomenYoungMixedGenderNoPrime0S1A-023$BWomenYoungMixedGenderNoPrime0S1A-054$AWomenYoungSameGenderNoPrime0S1A-090$BWomenYoungMixedGenderNoPrime0S1A-009$BWomenOldSameGenderPrime0S1A-085$EMenYoungMixedGenderPrime1S1A-003$CWomenYoungMixedGenderNoPrime1S1A-084$CWomenYoungSameGenderNoPrime0S1A-076$AWomenYoungSameGenderNoPrime0S1A-083$DMenOldMixedGenderNoPrime1S1A-068$AWomenYoungSameGenderNoPrime0S1A-066$BWomenYoungSameGenderNoPrime0S1A-061$AMenOldMixedGenderNoPrime1S1A-049$AWomenYoungSameGenderNoPrime0S1A-022$BWomenYoungMixedGenderNoPrime0\n\n\nAs all variables except for the dependent variable (SUFlike) are character strings, we factorize the independent variables.\n\n# def. variables to be factorized\nvrs &lt;- c(\"ID\", \"Age\", \"Gender\", \"ConversationType\", \"Priming\")\n# def. vector with variables\nfctr &lt;- which(colnames(mblrdata) %in% vrs)     \n# factorize variables\nmblrdata[,fctr] &lt;- lapply(mblrdata[,fctr], factor)\n# relevel Age (Young = Reference)\nmblrdata$Age &lt;- relevel(mblrdata$Age, \"Young\")\n# order data by ID\nmblrdata &lt;- mblrdata %&gt;%\n  dplyr::arrange(ID)\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderPrime0S1A-001$BWomenOldMixedGenderNoPrime1S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0\n\n\nBefore continuing, a few words about the minimum number of random effect levels and the minimum number of observations per random effect level are in order.\nWhile many data points per random variable level increases statistical power and thus to more robust estimates of the random effects (Austin and Leckie), it has been shown that small numbers of observations per random effect variable level do not cause serious bias and it does not negatively affect the estimates of the fixed-effects coefficients (Bell, Ferron, and Kromrey; Clarke; Clarke and Wheaton; Maas and Hox). The minimum number of observations per random effect variable level is therefore 1.\nIn simulation study, (Bell, Ferron, and Kromrey) tested the impact of random variable levels with only a single observation ranging from 0 to 70 percent. As long as there was a relatively high number of random effect variable levels (500 or more), small numbers of observations had almost no impact on bias and Type 1 error control.\nWe now plot the data to inspect the relationships within the data set.\n\nggplot(mblrdata, aes(Gender, SUFlike, color = Priming)) +\n  facet_wrap(Age~ConversationType) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of discourse like\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nThe upper left panel in the Figure above indicates that men use discourse like more frequently than women. The center right panel suggests that priming significantly increases the likelihood of discourse like being used. The center left panel suggests that speakers use discourse like more frequently in mixed-gender conversations. However, the lower right panel indicates an interaction between gender and conversation type as women appear to use discourse like less frequently in same gender conversations while the conversation type does not seem to have an effect on men. After visualizing the data, we will now turn to the model building process.\n\n\nModel Building\nAs a first step, we need to define contrasts and use the datadist function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. Field, Miles, and Field (414–27) and Gries provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more.\n\n# set options\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\nmblrdata.dist &lt;- datadist(mblrdata)\noptions(datadist = \"mblrdata.dist\")\n\nIn a next step, we generate fixed-effects minimal base-line models and a base-line mixed-model using the glmer function with a random intercept for ID (a lmer-object of the final minimal adequate model will be created later).\n\n# baseline model glm\nm0.glm = glm(SUFlike ~ 1, family = binomial, data = mblrdata) \n# base-line mixed-model\nm0.glmer = glmer(SUFlike ~ (1|ID), data = mblrdata, family = binomial) \n\n\n\nTesting the Random Effect\nNow, we check if including the random effect is permitted by comparing the AICs from the glm to AIC from the glmer model. If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that including random intercepts is justified.\n\naic.glmer &lt;- AIC(logLik(m0.glmer))\naic.glm &lt;- AIC(logLik(m0.glm))\naic.glmer; aic.glm\n\n[1] 1828.49227107\n\n\n[1] 1838.17334856\n\n\nThe AIC of the glmer object is smaller which shows that including the random intercepts is justified. To confirm whether the AIC reduction is sufficient for justifying the inclusion of a random-effect structure, we also test whether the mixed-effects minimal base-line model explains significantly more variance by applying a Model Likelihood Ratio Test to the fixed- and the mixed effects minimal base-line models.\n\n# test random effects\nnull.id = -2 * logLik(m0.glm) + 2 * logLik(m0.glmer)\npchisq(as.numeric(null.id), df=1, lower.tail=F) \n\n[1] 0.000631389572435\n\n# sig m0.glmer better than m0.glm\n\nThe p-value of the Model Likelihood Ratio Test is lower than .05 which shows that the inclusion of the random-effects structure is warranted. We can now continue with the model fitting process.\n\n\nModel Fitting\nThe next step is to fit the model which means that we aim to find the best model, i.e. the minimal adequate model. In this case, we will use the glmulti package to find the model with the lowest Bayesian Information Criterion (BIC) of all possible models. We add ´control = glmerControl(optimizer = “bobyqa”)´ to avoid unnecessary failures to converge.\n\n# wrapper function for linear mixed-models\nglmer.glmulti &lt;- function(formula,data, random=\"\",...){\n  glmer(paste(deparse(formula),random), \n        family = binomial, \n        data=data, \n        control=glmerControl(optimizer=\"bobyqa\"), ...)\n}\n# define formula\nform_glmulti = as.formula(paste(\"SUFlike ~ Gender + Age + ConversationType + Priming\"))\n# multi selection for glmer\nmfit &lt;- glmulti(form_glmulti,random=\"+(1 | ID)\", \n                data = mblrdata, method = \"h\", fitfunc = glmer.glmulti,\n                crit = \"bic\", intercept = TRUE, marginality = FALSE, level = 2)\n\nInitialization...\nTASK: Exhaustive screening of candidate set.\nFitting...\n\n\n\nAfter 50 models:\nBest model: SUFlike~1+Gender+ConversationType+Priming\nCrit= 1696.5877339969\nMean crit= 1753.96253323424\n\n\n\n\n\n\n\n\n\n\nAfter 100 models:\nBest model: SUFlike~1+Gender+ConversationType+Priming\nCrit= 1696.5877339969\nMean crit= 1731.89001011587\n\n\n\n\n\n\n\n\n\nCompleted.\n\n\nWe extract the best 5 models (best is here defined as the models with the lowest BIC).\n\n# extract best models\ntop &lt;- weightable(mfit)\ntop &lt;- top[1:5,]\n# inspect top 5 models\ntop\n\n                                                                                          model\n1                                             SUFlike ~ 1 + Gender + ConversationType + Priming\n2                            SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:Gender\n3 SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:Gender + Priming:ConversationType\n4                                               SUFlike ~ 1 + Gender + Priming + Priming:Gender\n5                  SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:ConversationType\n            bic         weights\n1 1696.58773400 0.2567900971793\n2 1696.76989551 0.2344349735686\n3 1696.76989551 0.2344349735686\n4 1699.62465884 0.0562494681405\n5 1699.83927868 0.0505259299850\n\n\nThe best model is has the formula SUFlike ~ 1 + Gender + ConversationType + Priming and we take this to be our final minimal adequate model, i.e. the most parsimonious model (the model which explains the relatively most variance with lowest number of predictors). Hence, we define our final minimal model and check its output.\n\nmlr.glmer &lt;- glmer(SUFlike ~ (1 | ID) + Gender + ConversationType + Priming, \n                   family = binomial,\n                   control=glmerControl(optimizer=\"bobyqa\"),\n                   data = mblrdata)\n# inspect final minimal adequate model\nsummary(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n   Data: mblrdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1668.6   1696.6   -829.3   1658.6     1995 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.579360650 -0.415523754 -0.330409042 -0.312054134  3.247885856 \n\nRandom effects:\n Groups Name        Variance     Std.Dev.   \n ID     (Intercept) 0.0837517494 0.289398945\nNumber of obs: 2000, groups:  ID, 208\n\nFixed effects:\n                               Estimate   Std. Error  z value\n(Intercept)                -1.067430840  0.149184383 -7.15511\nGenderWomen                -0.642887590  0.175327287 -3.66679\nConversationTypeSameGender -0.536428857  0.148819363 -3.60456\nPrimingPrime                1.866249307  0.163249208 11.43190\n                                         Pr(&gt;|z|)    \n(Intercept)                   0.00000000000083605 ***\nGenderWomen                            0.00024562 ***\nConversationTypeSameGender             0.00031268 ***\nPrimingPrime               &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now test whether the final minimal model performs significantly better than the minimal base-line model, and print the regression summary.\n\n# final model better than base-line model\nsigfit &lt;- anova(mlr.glmer, m0.glmer, test = \"Chi\") \n# inspect\nsigfit\n\nData: mblrdata\nModels:\nm0.glmer: SUFlike ~ (1 | ID)\nmlr.glmer: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n          npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.glmer     2 1828.492271 1839.694076 -912.2461355 1824.492271             \nmlr.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222 165.90905  3\n                      Pr(&gt;Chisq)    \nm0.glmer                            \nmlr.glmer &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# inspect final minimal adequate model\nprint(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n   Data: mblrdata\n      AIC       BIC    logLik  deviance  df.resid \n1668.5832 1696.5877 -829.2916 1658.5832      1995 \nRandom effects:\n Groups Name        Std.Dev.   \n ID     (Intercept) 0.289398945\nNumber of obs: 2000, groups:  ID, 208\nFixed Effects:\n               (Intercept)                 GenderWomen  \n              -1.067430840                -0.642887590  \nConversationTypeSameGender                PrimingPrime  \n              -0.536428857                 1.866249307  \n\n\n\n\nVisualizing Effects\nWe visualize the effects here by showing the probability of discourse like based on the predicted values.\n\nplot_model(mlr.glmer, type = \"pred\", terms = c(\"Gender\", \"Priming\", \"ConversationType\"))\n\n\n\n\n\n\n\n\nWe can see that discourse like is more likely to surface in primed contexts and among males. In conversations with both men and women, speakers use discourse like slightly less than in mixed conversations.\n\n\nExtracting Model Fit Parameters\nWe now extract model fit parameters (Baayen, 281).\n\nlibrary(Hmisc) # Ensure the package is loaded\nprobs = 1/(1+exp(-fitted(mlr.glmer)))\nprobs = binomial()$linkinv(fitted(mlr.glmer))\nsomers2(probs, as.numeric(mblrdata$SUFlike))\n\n                C               Dxy                 n           Missing \n   0.760226203516    0.520452407033 2000.000000000000    0.000000000000 \n\n\nThe two lines that start with probs are simply two different ways to do the same thing (you only need one of these).\nThe model fit parameters indicate a suboptimal fit. Both the C-value and Somers’s Dxy show poor fit between predicted and observed occurrences of discourse like. If the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity (Baayen, 204). Somers’ Dxy is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ Dxy values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction (Baayen, 204). The C.value of 0.760226203516 suggests that the model has some predictive and explanatory power, but not at an optimal level. We will now perform the model diagnostics.\n\n\nModel Diagnostics\nWe begin the model diagnostics by generating a diagnostic that plots the fitted or predicted values against the residuals.\n\nplot(mlr.glmer, pch = 20, col = \"black\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nAs a final step, we summarize our findings in tabulated form.\n\n# summarize final model\nsjPlot::tab_model(mlr.glmer)\n\n\n\n \nSUFlike\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.34\n0.26 – 0.46\n&lt;0.001\n\n\nGender [Women]\n0.53\n0.37 – 0.74\n&lt;0.001\n\n\nConversationType[SameGender]\n0.58\n0.44 – 0.78\n&lt;0.001\n\n\nPriming [Prime]\n6.46\n4.69 – 8.90\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 ID\n0.08\n\n\nICC\n0.02\n\n\nN ID\n208\n\nObservations\n2000\n\n\nMarginal R2 / Conditional R2\n0.131 / 0.152\n\n\n\n\n\n\n\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(mlr.glmer)\n\nWe fitted a logistic mixed model (estimated using ML and BOBYQA optimizer) to\npredict SUFlike with Gender, ConversationType and Priming (formula: SUFlike ~\nGender + ConversationType + Priming). The model included ID as random effect\n(formula: ~1 | ID). The model's total explanatory power is moderate\n(conditional R2 = 0.15) and the part related to the fixed effects alone\n(marginal R2) is of 0.13. The model's intercept, corresponding to Gender = Men,\nConversationType = MixedGender and Priming = NoPrime, is at -1.07 (95% CI\n[-1.36, -0.78], p &lt; .001). Within this model:\n\n  - The effect of Gender [Women] is statistically significant and negative (beta\n= -0.64, 95% CI [-0.99, -0.30], p &lt; .001; Std. beta = -0.64, 95% CI [-0.99,\n-0.30])\n  - The effect of ConversationType [SameGender] is statistically significant and\nnegative (beta = -0.54, 95% CI [-0.83, -0.24], p &lt; .001; Std. beta = -0.54, 95%\nCI [-0.83, -0.24])\n  - The effect of Priming [Prime] is statistically significant and positive (beta\n= 1.87, 95% CI [1.55, 2.19], p &lt; .001; Std. beta = 1.87, 95% CI [1.55, 2.19])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n\nWe can use this output to write up a final report:\nWe can use this output to write up a final report:\nWe fitted a logistic mixed model to predict the use of discourse like. The model included speakers as random effect (formula: ~1 | ID). The model’s total explanatory power is moderate (conditional R2 = 0.15) and the part related to the fixed effects alone (marginal R2) is of 0.13.\nRegarding fixed effects, the model reported that * women use discourse like statistically less compared to men (beta = -0.64 [-0.99, -0.30], p &lt; .001; Std. beta = -0.64 [-0.99, -0.30]) * speakers in conversations with other speakers of the same gender use discourse like significantly less compared to thier use in mixed-gender conversations (beta = -0.54 [-0.83, -0.24], p &lt; .001; Std. beta = -0.54 [-0.83, -0.24]) * Priming is significantly positively correlated with the use of discourse like (beta = 1.87 [1.55, 2.19], p &lt; .001; Std. beta = 1.87 [1.55, 2.19])\n\n\n\nMixed-Effects (Quasi-)Poisson and Negative-Binomial Regression\nLike fixed-effects Poisson models, mixed-effects Poisson models take counts as dependent variables. The data for this analysis was collected on three separate evenings (Trial). The number of the filler uhm (UHM) was counted in two-minute conversations that were either in English, German, Russian, or Mandarin (Language). In addition, the number of shots that speakers drank before they talked was recorded (Shots).\n\n# load data\ncountdata  &lt;- base::readRDS(\"tutorials/regression/data/cld.rda\", \"rb\")\n\n\n# inspect data\ncountdata %&gt;%\n  as.data.frame() %&gt;%\n  head(15) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %&gt;%\n  flextable::border_outer()\n\nIDTrialLanguageGenderUHMShots13RussianMan0023RussianMan0033GermanMan0541GermanMan0351GermanWoman2663GermanMan1571MandarinMan1183GermanWoman0493RussianWoman00102GermanMan02113RussianMan01122GermanMan01133RussianWoman01142RussianWoman44152EnglishMan04\n\n\nSince the data contains character variables, we need to factorize the data before we can analyse it further and we also remove the ID column.\n\n# factorize variables\ncountdata &lt;- countdata %&gt;%\n  dplyr::select(-ID) %&gt;%\n  dplyr::mutate_if(is.character, factor)\n\n\n# inspect data\ncountdata %&gt;%\n  as.data.frame() %&gt;%\n  head(15) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %&gt;%\n  flextable::border_outer()\n\nTrialLanguageGenderUHMShots3RussianMan003RussianMan003GermanMan051GermanMan031GermanWoman263GermanMan151MandarinMan113GermanWoman043RussianWoman002GermanMan023RussianMan012GermanMan013RussianWoman012RussianWoman442EnglishMan04\n\n\nAfter the data is factorized, we can visualize the data.\n\ncountdata %&gt;%\n  # prepare data\n  dplyr::select(Language, Shots) %&gt;%\n  dplyr::group_by(Language) %&gt;%\n  dplyr::mutate(Mean = round(mean(Shots), 1)) %&gt;%\n  dplyr::mutate(SD = round(sd(Shots), 1)) %&gt;%\n  # start plot\n  ggplot(aes(Language, Shots, color = Language, fill = Language)) +\n  geom_violin(trim=FALSE, color = \"gray20\")+ \n  geom_boxplot(width=0.1, fill=\"white\", color = \"gray20\") +\n  geom_text(aes(y=-4,label=paste(\"mean: \", Mean, sep = \"\")), size = 3, color = \"black\") +\n  geom_text(aes(y=-5,label=paste(\"SD: \", SD, sep = \"\")), size = 3, color = \"black\") +\n  scale_fill_manual(values=rep(\"grey90\",4)) + \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"none\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  ylim(-5, 15) +\n  labs(x = \"Language\", y = \"Shots\")\n\n\n\n\n\n\n\n\nThe violin plots show that the English speakers drank more shots than speakers of other languages with Mandarin speakers drinking the fewest shots.\nIn the present case, we will a Boruta variable selection procedure to streamline the model fitting process. Thus, before fitting the model, we will test which variables have any kind of relationship with the dependent variable and therefore deserve to be evaluated in the regression modeling. As this is just an example, we will only consider variables which are deemed important and disregard both unimportant and tentative variables. We start the Boruta analysis by setting a seed and running an initial Boruta analysis.\n\n# perform variable selection\nset.seed(20191220)\nboruta &lt;- Boruta(UHM ~.,data=countdata)\nprint(boruta)\n\nBoruta performed 99 iterations in 2.85972809792 secs.\n 2 attributes confirmed important: Language, Shots;\n 1 attributes confirmed unimportant: Gender;\n 1 tentative attributes left: Trial;\n\n\nAs only Shots is confirmed as important, we will only check for the effect of Shots and include Language as a random effect in the regression modeling. Including Language as a random effect is probably not justified statistically (given that the Boruta analysis showed that it only has a tentative effect) but for theoretical reasons as the speakers are nested into Languages. Before we start with the modeling, however, we proceed by checking if the data does indeed approximate a Poisson distribution.\n\n# output the results\ngf = goodfit(countdata$UHM,type= \"poisson\", method= \"ML\")\nplot(gf, main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\nThe data does not perfectly match a distribution that would be expected if the data approximated a Poisson distribution. We will use a goodness-of-fit test to check if the data does indeed diverge significantly from being Poisson distributed. If the p-values of the goodness-of-fit test is smaller than .05, then the distribution of the data differs significantly from a Poisson distribution and, given the visualization is likely over-dispersed.\nIn case of overdispersion, we may have to use a quasi-Poisson or, even better, a negative binomial model but we will, for now continue with the Poisson model and perform diagnostics later to check if we have to switch to a more robust method. One effect of overdispersion is that the standard errors of a model are biased and quasi-Poisson models scale the standard errors to compensate bias. However, Zuur, Hilbe, and Ieno suggest to use negative-binomial model instead. This is so because the scaling of the standard errors performed by quasi-Poisson models only affects the significance of coefficients (the p-values) but it does not affect the coefficients which, however, may be affected themselves by overdispersion. Thus, the coefficients of Poisson as well as quasi-Poisson models (which are identical) may be unreliable when dealing with overdispersion. Negative binomial models, in contrast, include an additional dispersion or heterogeneity parameter which accommodates overdispersion better than merely scaling the standard errors (see Zuur, Hilbe, and Ieno, 21).\n\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df                                     P(&gt; X^2)\nLikelihood Ratio 153.422771085  5 0.000000000000000000000000000000249336691328\n\n\nThe p-value is indeed smaller than .05 which means that we should indeed use a negative-binomial model rather than a Poisson model. We will ignore this, for now, and proceed to fit a Poisson mixed-effects model and check what happens if a Poisson model is fit to over-dispersed data.\n\nMixed-Effects Poisson Regression\nIn a first step, we create mixed-effect intercept-only baseline models and then test if including “Shots” significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.glmer = glmer(UHM ~ 1 + (1 | Language), data = countdata, family = poisson,\n                 control=glmerControl(optimizer=\"bobyqa\"))\n# add Shots\nm1.glmer &lt;- update(m0.glmer, .~.+ Shots)\nAnova(m1.glmer, test = \"Chi\")           \n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: UHM\n       Chisq Df             Pr(&gt;Chisq)    \nShots 321.25  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. However, we get the warning that the fitted mixed model is (almost / near) singular. In such cases, the model should not be reported. As this is only an example, we will continue by having a look at the model summary.\n\nsummary(m1.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: UHM ~ (1 | Language) + Shots\n   Data: countdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1041.8   1054.5   -517.9   1035.8      497 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.509633852 -0.666423093 -0.592950422  0.586114082  4.338639382 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Language (Intercept) 0        0       \nNumber of obs: 500, groups:  Language, 4\n\nFixed effects:\n                 Estimate    Std. Error   z value               Pr(&gt;|z|)    \n(Intercept) -1.2789168850  0.0893313003 -14.31656 &lt; 0.000000000000000222 ***\nShots        0.2336279071  0.0130347632  17.92345 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nShots -0.806\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe model summary confirms that the number of shots does have a significantly positive effect on the number of occurrences of uhm. Furthermore, the scaled residuals are distributed very unevenly which suggests overdispersion. Including Language as a random effect is not justified given that they have 0 variance and a standard deviation of 0 (which means that Language does not account for or explain any additional variance).\nWe now check if the model suffers from overdispersion following Zuur, Hilbe, and Ieno (138).\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.glmer, type = \"pearson\")\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors &lt;- length(fixef(m1.glmer)) +1\n# calculate overdispersion\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.16901460967\n\n\nThe data is slightly over-dispersed. It would also be advisable to plot the Cook’s distance (which should not show data points with values &gt; 1). If there are data points with high Cook’s D values, we could exclude them which would, very likely reduce the overdispersion (see Zuur, Hilbe, and Ieno, 22). We ignore this, for now, and use diagnostic plots to check if the plots indicate problems.\n\ndiag_data &lt;- data.frame(PearsonResiduals, fitted(m1.glmer)) %&gt;%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 &lt;- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 &lt;- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 &lt;- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngridExtra::grid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\n\n\n\n\nThe diagnostic plots show problems as the dots in the first two plots are not random but show a pattern in the lower left corner. In addition, the variance of English (left boxplot) is notable larger than the variance of Russian (right boxplot). As a final step, we plot the predicted vales of the model to check if the predictions make sense.\n\nplot_model(m1.glmer, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe model predicts that the instances of uhm increase with the number of shots. Note that the increase is not homogeneous as the y-axis labels indicate! We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.glmer, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations (overdispersion measure &gt; 1 and excessive multicollinearity (singular fit)) had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nThe summary of the model can be extracted using the tab_model function from the sjPlot package (Lüdecke).\n\nsjPlot::tab_model(m1.glmer)\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.33\n&lt;0.001\n\n\nShots\n1.26\n1.23 – 1.30\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\n\nr.squaredGLMM(m1.glmer)\n\n                     R2m            R2c\ndelta     0.208910675833 0.208910675833\nlognormal 0.294971461467 0.294971461467\ntrigamma  0.120419615931 0.120419615931\n\n\nAlso note that our model suffers from a serious problem (near singular fit). If this were not just an example, you should not(!) report this model!\n\n\nMixed-Effects Quasi-Possion Regression\nThe Quasi-Poisson Regression is a generalization of the Poisson regression and is used when modeling an overdispersed count variable. Poisson models are based on the Poisson distribution which is defined as a distribution where the variance is equal to the mean (which is very restrictive and not often the case). Quasi-Poisson models scale the standard errors which has a positive effect when dealing with overdispersed data.\nTherefore, when the variance is greater than the mean, a Quasi-Poisson model, which assumes that the variance is a linear function of the mean, is more appropriate as it handles over-dispersed data better than normal Poisson-models.\nWe begin the model fitting process by creating a mixed- and a fixed-effects intercept-only base-line model. Unfortunately, there is not yet a procedure in place for quasi-Poisson models to test if the inclusion of random effects is justified. However, here the Boruta also provides valuable information: Language was only considered tentative but not important which suggests that it will not explain variance which means that including Language as a random effect may not be justified. This would require further inspection. Because we are only dealing with an example here, we ignore this fact (which you should not do in proper analyses) and continue right away with adding shots.\n\n# base-line mixed-model\nm0.qp = glmmPQL(UHM ~ 1, random = ~ 1 | Language, data = countdata, \n                   family = quasipoisson(link='log'))\n\niteration 1\n\n\niteration 2\n\n\niteration 3\n\n\niteration 4\n\n# add Shots\nm1.qp &lt;- update(m0.qp, .~.+ Shots)\n\niteration 1\n\nAnova(m1.qp, test = \"Chi\")           # SIG! (p&lt;0.0000000000000002 ***)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: zz\n         Chisq Df             Pr(&gt;Chisq)    \nShots 276.4523  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. We will now have a look at the model summary.\n\nsummary(m1.qp)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: countdata \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 | Language\n               (Intercept)      Residual\nStdDev: 0.0000407429998994 1.07801257915\n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  UHM ~ Shots \n                      Value       Std.Error  DF        t-value p-value\n(Intercept) -1.278929721022 0.0964886326622 495 -13.2547190870       0\nShots        0.233630231741 0.0140795660798 495  16.5935676154       0\n Correlation: \n      (Intr)\nShots -0.806\n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-1.400417640410 -0.618228066947 -0.550071160975  0.543731905020  4.024828853743 \n\nNumber of Observations: 500\nNumber of Groups: 4 \n\n\nThe model summary does not provide much information such as,e.g. AIC or BIC values. The coefficient for Shots is highly significant (p &lt;.001) and the data is notably over-dispersed (the Standardized Within-Group Residuals deviate substantially from a normal distribution with higher values having a thick tail). Also, in contrast to the Poisson model, Language does explain at least a minimal share of the variance now as the mean and standard deviation are no longer 0. Note also, that the coefficients are identical to the Poisson coefficients but the standard errors and p-values differ (the model provides t- rather than z-values).\nIn a next step, we will calculate the odds ratios of the coefficient (as we only have one). We will use the coefficients from the fixed-effects model as the coefficients for mixed- and fixed-effects models are identical (the random effect structure only affects the standard error and p-values but not the coefficients; you can check by uncommenting the summary command).\n\nm1.glm = glm(UHM ~ Shots, data = countdata, family = quasipoisson(link='log'))\nexp(coef(m1.glm))\n\n   (Intercept)          Shots \n0.278338612235 1.263174385573 \n\n\nThe standardized or \\(\\beta\\)-coefficient tells us that the likelihood of uhm increases by 1.26 (or 26.32 percent) with each additional shot.\nBefore inspecting the relationship between Shots and uhm, we will check if the overdispersion was reduced.\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.qp, type = \"pearson\")\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors &lt;- length(fixef(m1.qp)) +1\n# calculate overdispersion\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.00603621722\n\n\nThe overdispersion has indeed decreased and is not so close to 1 that overdispersion is no longer an issue.\nWe continue to diagnose the model by plotting the Pearson’s residuals against fitted values. This diagnostic plot should not show a funnel-like structure or patterning as we observed in the case of the Poisson model.\n\n# diagnostic plot\nplot(m1.qp, pch = 20, col = \"black\", lty= \"dotted\", ylab = \"Pearson's residuals\")\n\n\n\n\n\n\n\n\nIndeed, the plot exhibits a (slight) funnel shape (but not drastically so) and thus indicates heteroscedasticity. However, the patterning that we observed with the Poisson model has disappeared. We continue by plotting the random effect adjustments.\n\n# generate diagnostic plots\nplot(m1.qp, Language ~ resid(.), abline = 0, fill = \"gray70\") \n\n\n\n\n\n\n\n\nThe adjustments by “Language” are marginal (which was somewhat expected given that Language was only deemed tentative), which shows that there is very little variation between the languages and that we have no statistical reason to include Language as a random effect.\nIn a final step, we plot the fixed-effect of Shots using the predictorEffects function from the effects package (Fox and Weisberg 2019).\n\nplot_model(m1.qp, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe effects plot shows that the number of uhms increases exponentially with the number of shots a speaker has had. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.qp, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nGiven that the overdispersion measure of this Quasi-Poisson model is close to 1, that the model did not suffer from excessive multicollinearity (singular fit), and because this model shows improvements compared to the Poisson model with respect to the model diagnostics (some adjustments by Language and less patterning in the diagnostic plots), we would choose this quasi-Poisson model over the Poisson model.\nFinally, we extract the summary table of this model.\n\nsjPlot::tab_model(m1.qp)\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.34\n&lt;0.001\n\n\nShots\n1.26\n1.23 – 1.30\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.00\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\nMarginal R2 / Conditional R2\n1.000 / NA\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\n\nr.squaredGLMM(m1.qp)\n\n                      R2m             R2c\ndelta     0.1856941875702 0.1856941884278\nlognormal 0.2752763976645 0.2752763989357\ntrigamma  0.0977040660495 0.0977040665007\n\n\n\n\nMixed-Effects Negative Binomial Regression\nNegative binomial regression models are a generalization of Poisson regression which loosens the restrictive assumption that the variance is equal to the mean made by the Poisson model. This is a major advantage as the most common issue that one faces with Poisson regressions is that the data deviate too substantially from the assumed Poisson distribution.\nTo implement a Negative-Binomial Mixed-Effects Regression, we first create a mixed-effects intercept-only baseline model and then test if including Shots significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.nb = glmer.nb(UHM ~ 1 + (1 | Language), data = countdata)\n# add Shots\nm1.nb &lt;- update(m0.nb, .~.+ Shots)\n\nboundary (singular) fit: see help('isSingular')\n\nanova(m1.nb, m0.nb)           \n\nData: countdata\nModels:\nm0.nb: UHM ~ 1 + (1 | Language)\nm1.nb: UHM ~ (1 | Language) + Shots\n      npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.nb    3 1159.000894 1171.644718 -576.5004470 1153.000894             \nm1.nb    4 1051.593288 1068.451721 -521.7966442 1043.593288 109.40761  1\n                  Pr(&gt;Chisq)    \nm0.nb                           \nm1.nb &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe negative-binomial model also reports a significant impact of shots on the number of uhms. In a next step, we calculate the overdispersion.\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.nb, type = \"pearson\")\n# extract number of betas + predictors + sigma\nNumberOfPredictors &lt;- 2+1+1\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# calculate overdispersion parameter\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases / NumberOfPredictors)# show overdispersion parameter\nOverdispersion\n\n[1] 2.46949245769\n\n\nThe overdispersion has increased which is rather suboptimal. In this case, we would report the Quasi-Poisson Regression rather than the Negative Binomial Model (which is rather rare as Negative Binomial Models typically perform better than (Quasi-)Poisson models. However, this tutorial focuses merely on how to implement a Negative Binomial Mixed-Effects Regression and we thus continue with generating diagnostic plots to check for problems.\n\ndiag_data &lt;- data.frame(PearsonResiduals, fitted(m1.nb)) %&gt;%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 &lt;- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 &lt;- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 &lt;- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngridExtra::grid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\n\n\n\n\nThe diagnostics show patterning similar to the one we saw with the Poisson model which suggest that the negative binomial model is also not an optimal model for our data. We continue by plotting the predicted values and, subsequently, summarize the analysis.\n\nplot_model(m1.nb, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe effect plot shows that the predicted number of shots increases exponentially with each shot. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.nb, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nA mixed-effect negative binomial regression model which contained the language in which the conversation took place as random effect was fit to the data. Prior to the regression modeling, a Boruta analysis was applied to determine whether any of the predictors had a meaningful relationship with the dependent variable (instances of uhm). Since the Boruta analysis indicated that only the number of shots a speaker had was important, only “Shots” was tested during model fitting. The final minimal adequate model showed that the number of uhm as fillers increases significantly, and near-linearly with the number of shots speakers had (\\(\\chi\\)2(1):83.0, p &lt;.0001, \\(\\beta\\): 0.2782). An inspection of the random effect structure conveyed that there was almost no variability between languages and language did not contribute meaningfully to the model fit.\n\n\n\nMixed-Effects Multinomial Regression\nIn this section, we will focus on how to implement a mixed-effects multinomial regression model using the mblogit function from the mclogit package (see Elff 2021). As we have already gone though model fitting and model validation procedures above, we will strictly see how to implement this type of model here - we will not go through all the other steps that a proper regression analysis would require.\nWe begin the analysis by loading the example data set. The data represents observations gathered during an experiment where speakers had to report what they saw. The responses are categorized into four groups:\n\n# description data\npict  &lt;- base::readRDS(\"tutorials/regression/data/pict.rda\", \"rb\")\n# inspect\nhead(pict)\n\n  Id Participant       Group Item     Response Gender Age\n1  1        G001 German_Mono    1  NumeralNoun   Male  18\n2  2        G002 German_Mono    3  NumeralNoun   Male  18\n3  3        G003 German_Mono    4  NumeralNoun   Male  18\n4  4        G004 German_Mono    6 QuantAdjNoun   Male  18\n5  5        G005 German_Mono    8  NumeralNoun   Male  18\n6  6        G006 German_Mono    9 QuantAdjNoun   Male  18\n\n\nIn a first step, we generate a baseline model that we call m0. This model only contains the random effect structure and the intercept as the sole predictor.\n\nm0.mn &lt;- mblogit(formula = Response ~ 1, \n              random = ~ 1 | Participant, \n              data = pict)\n\n\nIteration 1 - deviance = 2452.16571809 - criterion = 0.840346812509\n\n\n\nIteration 2 - deviance = 2412.75547122 - criterion = 0.0646013613319\n\n\n\nIteration 3 - deviance = 2411.26894466 - criterion = 0.00534731178597\n\n\n\nIteration 4 - deviance = 2411.2582473 - criterion = 0.0000446387422471\n\n\n\nIteration 5 - deviance = 2411.25823534 - criterion = 0.0000000031405953873\nconverged\n\n\nIn this case, the algorithm did not converge properly - if this were a real analysis, we could not simply continue but would have to inspect possible causes for this. However, as this is just a showcase, we will ignore this and move on. Next, we add the fixed effects (Gender and Group).\n\nm1.mn &lt;- mblogit(formula = Response ~ Gender + Group, \n              random = ~ 1 | Item, \n              data = pict)\n\n\nIteration 1 - deviance = 1652.54499424 - criterion = 0.800708039017\n\n\n\nIteration 2 - deviance = 1571.22116522 - criterion = 0.0814899820197\n\n\n\nIteration 3 - deviance = 1551.91023391 - criterion = 0.0274469746195\n\n\n\nIteration 4 - deviance = 1547.29823345 - criterion = 0.00512667078353\n\n\n\nIteration 5 - deviance = 1546.02170442 - criterion = 0.000173543402508\n\n\n\nIteration 6 - deviance = 1545.8333116 - criterion = 0.000000244926398008\n\n\n\nIteration 7 - deviance = 1545.82769259 - criterion = 0.000000000000785569845713\nconverged\n\n\nNow, we can compare the models to see if including the fixed-effects into the model has significantly improved the model fit.\n\nanova(m0.mn, m1.mn)\n\nAnalysis of Deviance Table\n\nModel 1: Response ~ 1\nModel 2: Response ~ Gender + Group\n  Resid. Df  Resid. Dev Df    Deviance\n1      3261 2411.258235               \n2      3249 1545.827693 12 865.4305427\n\n\nAs the second model is significantly better, we are justified to believe that our fixed effects have explanatory power. We can now use the getSummary.mmblogit function to get a summary of the model with the fixed effects.\n\n# inspect\nmclogit::getSummary.mmblogit(m1.mn)\n\n$coef\n, , NumeralNoun/BareNoun\n\n                                  est             se             stat\n(Intercept)           1.0940536961694 0.795348462841   1.375565236225\nGenderMale            0.0691676134585 0.196851703842   0.351369137826\nGroupGerman_Mono     -3.2909055852599 0.304213383695 -10.817754121410\nGroupL2_Advanced     -0.4575232522124 0.307505930002  -1.487851802435\nGroupL2_Intermediate -1.1689603301872 0.320486374900  -3.647457183015\n                                                            p             lwr\n(Intercept)          0.16895627569603502426964780624984996393 -0.464800646159\nGenderMale           0.72531143261552322165641726314788684249 -0.316654636367\nGroupGerman_Mono     0.00000000000000000000000000283642762786 -3.887152860918\nGroupL2_Advanced     0.13678998075957157776194605958153260872 -1.060223800048\nGroupL2_Intermediate 0.00026484842867640642034496312184899125 -1.797102082527\n                                 upr\n(Intercept)           2.652908038498\nGenderMale            0.454989863284\nGroupGerman_Mono     -2.694658309602\nGroupL2_Advanced      0.145177295623\nGroupL2_Intermediate -0.540818577848\n\n, , QuantAdjNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)           0.595495714810 0.498285332593   1.19508979265\nGenderMale            0.299894756058 0.248324126895   1.20767466218\nGroupGerman_Mono     -3.856907426426 0.376376472714 -10.24747216162\nGroupL2_Advanced     -2.194953767755 0.347242020175  -6.32110643363\nGroupL2_Intermediate -3.016728204963 0.398140344234  -7.57704726148\n                                                         p             lwr\n(Intercept)          0.23205194960832503658920700218004640 -0.381125591097\nGenderMale           0.22717242789340932884734058916365029 -0.186811589147\nGroupGerman_Mono     0.00000000000000000000000121478955842 -4.594591757573\nGroupL2_Advanced     0.00000000025969706979146851136980930 -2.875535621216\nGroupL2_Intermediate 0.00000000000003535080320338597665829 -3.797068940455\n                                 upr\n(Intercept)           1.572117020716\nGenderMale            0.786601101264\nGroupGerman_Mono     -3.119223095278\nGroupL2_Advanced     -1.514371914293\nGroupL2_Intermediate -2.236387469472\n\n, , QuantNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)          -2.163121210173 0.753539267072 -2.870615115491\nGenderMale           -0.371289871064 0.430045391060 -0.863373678181\nGroupGerman_Mono     -2.420925019720 0.813762214781 -2.974978409844\nGroupL2_Advanced      0.789609605279 0.683753160717  1.154816753535\nGroupL2_Intermediate -0.148789198300 0.739325816561 -0.201249834602\n                                    p             lwr             upr\n(Intercept)          0.00409674000335 -3.640031034571 -0.686211385775\nGenderMale           0.38793204670759 -1.214163349258  0.471583607130\nGroupGerman_Mono     0.00293009169381 -4.015869652670 -0.825980386770\nGroupL2_Advanced     0.24816547492527 -0.550521964042  2.129741174601\nGroupL2_Intermediate 0.84050322615521 -1.597841171600  1.300262775001\n\n\n$Item\n, , 1\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)   5.46019439094 26.1766792826   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) -1.63804668966           NaN   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     1.63405993830 11.5842291808   NA NA  NA  NA\n\n, , 2\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  -1.63804668966           NaN   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1)  1.60628957168           NaN   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     0.49903254341 2.36453656619   NA NA  NA  NA\n\n, , 3\n\n                                             est             se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  1.63405993830 11.58422918076   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) 0.49903254341  2.36453656619   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)    1.51237743196            NaN   NA NA  NA  NA\n\n\n$Groups\nGroups by Item \n            10 \n\n$sumstat\n               LR                df          deviance          McFadden \n1476.294014648252   21.000000000000 1545.827692593037    0.488495883905 \n        Cox.Snell        Nagelkerke               AIC               BIC \n   0.741897420618    0.791357248659 1587.827692593037 1692.700285072724 \n                N \n1090.000000000000 \n\n$call\nmblogit(formula = Response ~ Gender + Group, data = pict, random = ~1 | \n    Item)\n\n$contrasts\n$contrasts$Gender\n[1] \"contr.treatment\"\n\n$contrasts$Group\n[1] \"contr.treatment\"\n\n\n$xlevels\n$xlevels$Gender\n[1] \"Female\" \"Male\"  \n\n$xlevels$Group\n[1] \"English_Mono\"    \"German_Mono\"     \"L2_Advanced\"     \"L2_Intermediate\"\n\n\nThe NAs (not available information) is a result of the model having a bad fit to the data and, optimally, we would need to inspect why the model has a bad fit. Again, we ignore this and move on. Next, we check the VIFs to see if the model does not violate multicollinearity assumptions.\n\ncar::vif(m1.mn) # maybe use cut-off of 5 (maybe 10)\n\n                GVIF Df GVIF^(1/(2*Df))\nGender 3.59671433201  1   1.89650054891\nGroup  9.94593851425  3   1.46647375519\n\n\nThe VIFs are a bit high - especially the GVIF for Group would be a cause for concern if this was not just a demo analysis! However, as we only want to implement a multinomial mixed-effects model here and not provide a proper, clean analysis, we will ignore this issue here.\nIn a next step, we visualize effects to get a better understanding of how the predictors that are part of the fixed-effect structure of the mode affect the outcome (the response variable).\n\nsjPlot::plot_model(m1.mn)\n\n\n\n\n\n\n\n\nFinally, we can extract an alternative summary table produced by the tab_model function from the sjPlot package (see Lüdecke).\n\nsjPlot::tab_model(m1.mn)\n\n\n\n \nResponse: NumeralNoun\nResponse: QuantAdjNoun\nResponse: QuantNoun\n\n\nPredictors\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\n\n\n(Intercept)\n2.99\n0.63 – 14.20\n0.169\n1.81\n0.68 – 4.82\n0.232\n0.11\n0.03 – 0.50\n0.004\n\n\nGenderMale\n1.07\n0.73 – 1.58\n0.725\n1.35\n0.83 – 2.20\n0.227\n0.69\n0.30 – 1.60\n0.388\n\n\nGroupGerman_Mono\n0.04\n0.02 – 0.07\n&lt;0.001\n0.02\n0.01 – 0.04\n&lt;0.001\n0.09\n0.02 – 0.44\n0.003\n\n\nGroupL2_Advanced\n0.63\n0.35 – 1.16\n0.137\n0.11\n0.06 – 0.22\n&lt;0.001\n2.20\n0.58 – 8.42\n0.248\n\n\nGroupL2_Intermediate\n0.31\n0.17 – 0.58\n&lt;0.001\n0.05\n0.02 – 0.11\n&lt;0.001\n0.86\n0.20 – 3.67\n0.841\n\n\n\nN Item\n10\n\nObservations\n1090\n\n\n\n\n\n\n\nThis is the final step in implementing a a mixed-effects multinomial regression model using the mblogit function from the mclogit package (see Elff 2021). We are aware that the analysis shown here is supervifial(!) - but please keep in mind that we just wanted to showcase the implementation here rather than providing a properly and carefully done analysis.\n\n\nMixed-Effects Ordinal Regression\nIn this section, we will strictly focus on how to implement a mixed-effects ordinal regression model using the clmm function from the ordinal package (see Christensen 2019). This type of regression model is extremely useful when dealing with Likert data or other types of questionnaire and survey data where the responses have some kind of hierarchical structure (i.e. responses are not truly independent because they come from different points in time or from different regions). load data\n\n# rating experiment data\nratex  &lt;- base::readRDS(\"tutorials/regression/data/ratex.rda\", \"rb\")\n# inspect data\nhead(ratex)\n\n  Rater Child Group       Accent AccentNumeric       Family\n1    R1  C001 Child StrongAccent             2 DomBilingual\n2    R2  C001 Child StrongAccent             2 DomBilingual\n3    R3  C001 Child StrongAccent             2 DomBilingual\n4    R4  C001 Child StrongAccent             2 DomBilingual\n5    R5  C001 Child StrongAccent             2 DomBilingual\n6    R6  C001 Child StrongAccent             2 DomBilingual\n\n\nWe now tabulate the data to get a better understanding of the data structure.\n\nratex %&gt;%\n  dplyr::group_by(Family, Accent) %&gt;%\n  dplyr::summarise(Frequency = n()) %&gt;%\n  tidyr::spread(Accent, Frequency)\n\n`summarise()` has grouped output by 'Family'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 4\n# Groups:   Family [3]\n  Family         NoAccent StrongAccent WeakAccent\n  &lt;fct&gt;             &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n1 DomBilingual         80          145        174\n2 EqualBilingual       20           22         63\n3 Monolingual         209            1         41\n\n\nNext, we visualize the data to inspect its properties.\n\nratex %&gt;%\n  ggplot(aes(Family, AccentNumeric, color = Group)) + \n  stat_summary(fun = mean, geom = \"point\") +          \n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nAn alternative plot shows other properties of the data.\n\nratex %&gt;%\n  dplyr::group_by(Family, Rater, Group) %&gt;%\n  dplyr::summarise(AccentMean = mean(AccentNumeric)) %&gt;%\n  ggplot(aes(Family, AccentMean, fill = Group)) + \n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"gray50\", \"gray85\"))\n\n`summarise()` has grouped output by 'Family', 'Rater'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nWe now start the modeling by generating a model with Family as the sole predictor.\n\n# fit baseline model\nm1.or &lt;- clmm(Accent ~ (1|Rater) + Family, link=\"logit\", data = ratex)\n# test for incomplete information\nifelse(min(ftable(ratex$Accent, ratex$Family)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\n# extract aic\naic.glmer &lt;- AIC(logLik(m1.or))\n# inspect aic\naic.glmer\n\n[1] 1380.25888675\n\n# summarize model\nsummary(m1.or)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: Accent ~ (1 | Rater) + Family\ndata:    ratex\n\n link  threshold nobs logLik  AIC     niter    max.grad cond.H \n logit flexible  755  -685.13 1380.26 371(435) 4.36e-07 5.9e+05\n\nRandom effects:\n Groups Name        Variance            Std.Dev.       \n Rater  (Intercept) 0.00000000353193271 0.0000594300657\nNumber of groups:  Rater 21 \n\nCoefficients:\n                         Estimate   Std. Error   z value             Pr(&gt;|z|)\nFamilyEqualBilingual  0.477744666  0.214313910   2.22918             0.025802\nFamilyMonolingual    -2.550224083  0.198849328 -12.82491 &lt; 0.0000000000000002\n                        \nFamilyEqualBilingual *  \nFamilyMonolingual    ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                             Estimate    Std. Error   z value\nNoAccent|StrongAccent   -1.0798112479  0.1058004233 -10.20611\nStrongAccent|WeakAccent  0.1060945018  0.0951352125   1.11520\n\n\nWe can now perform Post-Hoc tests to see which comparisons are significant.\n\nlsmeans(m1.or, pairwise~Family, adjust=\"tukey\")\n\n$lsmeans\n Family         lsmean     SE  df asymp.LCL asymp.UCL\n DomBilingual    0.487 0.0914 Inf     0.308     0.666\n EqualBilingual  0.965 0.1950 Inf     0.582     1.347\n Monolingual    -2.063 0.1740 Inf    -2.404    -1.722\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df z.ratio p.value\n DomBilingual - EqualBilingual   -0.478 0.214 Inf  -2.229  0.0664\n DomBilingual - Monolingual       2.550 0.199 Inf  12.825  &lt;.0001\n EqualBilingual - Monolingual     3.028 0.265 Inf  11.438  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nFinally, we can summarize the model.\n\nsjPlot::tab_model(m1.or)\n\n\n\n \nAccent\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\nNoAccent|StrongAccent\n0.34\n0.28 – 0.42\n&lt;0.001\n\n\nStrongAccent|WeakAccent\n1.11\n0.92 – 1.34\n0.265\n\n\nFamily [EqualBilingual]\n1.61\n1.06 – 2.45\n0.026\n\n\nFamily [Monolingual]\n0.08\n0.05 – 0.12\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Rater\n0.00\n\n\nN Rater\n21\n\nObservations\n755\n\n\nMarginal R2 / Conditional R2\n0.325 / NA\n\n\n\n\n\n\n\nAnd we can visualize the effects.\n\nplot_model(m1.or, type = \"pred\", terms = c(\"Family\"))\n\n\n\n\n\n\n\n\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results.\n\n\n\nCitation & Session Info\nSchweinberger, Martin. 2024. Fixed- and Mixed-Effects Regression Models in R. Brisbane: The University of Queensland. url: https://ladal.edu.au/tutorials/regression.html (Version 2024.12.23).\n@manual{schweinberger2024regression,\n  author = {Schweinberger, Martin},\n  title = {Fixed- and Mixed-Effects Regression Models in R},\n  note = {tutorials/regression/regression.html},\n  year = {2024},\n  organization = \"The University of Queensland, Australia. School of Languages and Cultures},\n  address = {Brisbane},\n  edition = {2024.12.23}\n}\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] see_0.9.0           performance_0.12.4  gridExtra_2.3      \n [4] vcd_1.4-13          tibble_3.2.1        stringr_1.5.1      \n [7] sjPlot_2.8.16       robustbase_0.99-4-1 rms_6.8-2          \n[10] ordinal_2023.12-4.1 nlme_3.1-166        MuMIn_1.48.4       \n[13] mclogit_0.9.6       MASS_7.3-61         lme4_1.1-35.5      \n[16] Matrix_1.7-1        knitr_1.48          Hmisc_5.2-1        \n[19] ggfortify_0.4.17    glmulti_1.0.8       leaps_3.2          \n[22] rJava_1.0-11        emmeans_1.10.5      effects_4.2-2      \n[25] car_3.1-3           carData_3.0-5       Boruta_8.0.0       \n[28] vip_0.4.1           ggpubr_0.6.0        ggplot2_3.5.1      \n[31] flextable_0.9.7     dplyr_1.1.4        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1           klippy_0.0.0.9500       polspline_1.1.25       \n  [4] datawizard_0.13.0       hardhat_1.4.0           pROC_1.18.5            \n  [7] rpart_4.1.23            lifecycle_1.0.4         rstatix_0.7.2          \n [10] globals_0.16.3          lattice_0.22-6          insight_0.20.5         \n [13] backports_1.5.0         survey_4.4-2            magrittr_2.0.3         \n [16] rmarkdown_2.28          yaml_2.3.10             zip_2.3.1              \n [19] askpass_1.2.1           RColorBrewer_1.1-3      cowplot_1.1.3          \n [22] DBI_1.2.3               minqa_1.2.8             lubridate_1.9.3        \n [25] multcomp_1.4-26         abind_1.4-8             expm_1.0-0             \n [28] purrr_1.0.2             nnet_7.3-19             TH.data_1.1-2          \n [31] sandwich_3.1-1          ipred_0.9-15            lava_1.8.0             \n [34] gdtools_0.4.0           ggrepel_0.9.6           memisc_0.99.31.8.1     \n [37] listenv_0.9.1           MatrixModels_0.5-3      parallelly_1.38.0      \n [40] svglite_2.1.3           codetools_0.2-20        xml2_1.3.6             \n [43] tidyselect_1.2.1        ggeffects_1.7.2         farver_2.1.2           \n [46] effectsize_0.8.9        stats4_4.4.1            base64enc_0.1-3        \n [49] jsonlite_1.8.9          caret_6.0-94            e1071_1.7-16           \n [52] Formula_1.2-5           survival_3.7-0          iterators_1.0.14       \n [55] systemfonts_1.1.0       foreach_1.5.2           tools_4.4.1            \n [58] ragg_1.3.3              Rcpp_1.0.13             glue_1.8.0             \n [61] prodlim_2024.06.25      ranger_0.16.0           xfun_0.49              \n [64] mgcv_1.9-1              msm_1.8.1               withr_3.0.2            \n [67] numDeriv_2016.8-1.1     fastmap_1.2.0           mitools_2.4            \n [70] boot_1.3-31             fansi_1.0.6             SparseM_1.84-2         \n [73] openssl_2.2.2           digest_0.6.37           timechange_0.3.0       \n [76] R6_2.5.1                estimability_1.5.1      textshaping_0.4.0      \n [79] colorspace_2.1-1        utf8_1.2.4              tidyr_1.3.1            \n [82] generics_0.1.3          fontLiberation_0.1.0    data.table_1.16.2      \n [85] recipes_1.1.0           class_7.3-22            report_0.5.9           \n [88] htmlwidgets_1.6.4       parameters_0.23.0       ModelMetrics_1.2.2.2   \n [91] pkgconfig_2.0.3         gtable_0.3.6            timeDate_4041.110      \n [94] lmtest_0.9-40           htmltools_0.5.8.1       fontBitstreamVera_0.1.1\n [97] kableExtra_1.4.0        scales_1.3.0            gower_1.0.1            \n[100] rstudioapi_0.17.1       reshape2_1.4.4          uuid_1.2-1             \n[103] coda_0.19-4.1           checkmate_2.3.2         nloptr_2.1.1           \n[106] proxy_0.4-27            zoo_1.8-12              sjlabelled_1.2.0       \n[109] parallel_4.4.1          foreign_0.8-87          pillar_1.9.0           \n[112] vctrs_0.6.5             ucminf_1.2.2            xtable_1.8-4           \n[115] cluster_2.1.6           htmlTable_2.4.3         evaluate_1.0.1         \n[118] mvtnorm_1.3-1           cli_3.6.3               compiler_4.4.1         \n[121] rlang_1.1.4             future.apply_1.11.3     ggsignif_0.6.4         \n[124] labeling_0.4.3          forcats_1.0.0           plyr_1.8.9             \n[127] sjmisc_2.8.10           stringi_1.8.4           viridisLite_0.4.2      \n[130] assertthat_0.2.1        munsell_0.5.1           bayestestR_0.15.0      \n[133] quantreg_5.99           fontquiver_0.2.1        sjstats_0.19.0         \n[136] hms_1.1.3               patchwork_1.3.0         future_1.34.0          \n[139] highr_0.11              haven_2.5.4             broom_1.0.7            \n[142] DEoptimR_1.1-3          officer_0.6.7          \n\n\n\nBack to top\nBack to HOME\n\n\n\nReferences"
  },
  {
    "objectID": "tutorials/regression/regression.html#ordinal-regression",
    "href": "tutorials/regression/regression.html#ordinal-regression",
    "title": "Introduction",
    "section": "Ordinal Regression",
    "text": "Ordinal Regression\nOrdinal regression is very similar to multiple linear regression but takes an ordinal dependent variable (Agresti). For this reason, ordinal regression is one of the key methods in analysing Likert data.\nTo see how an ordinal regression is implemented in R, we load and inspect the ´ordinaldata´ data set. The data set consists of 400 observations of students that were either educated at this school (Internal = 1) or not (Internal = 0). Some of the students have been abroad (Exchange = 1) while other have not (Exchange = 0). In addition, the data set contains the students’ final score of a language test (FinalScore) and the dependent variable which the recommendation of a committee for an additional, very prestigious program. The recommendation has three levels (very likely, somewhat likely, and unlikely) and reflects the committees’ assessment of whether the student is likely to succeed in the program.\n\n# load data\nordata  &lt;- base::readRDS(\"tutorials/regression/data/ord.rda\", \"rb\") %&gt;%\n  dplyr::rename(Recommend = 1, \n              Internal = 2, \n              Exchange = 3, \n              FinalScore = 4) %&gt;%\n  dplyr::mutate(FinalScore = round(FinalScore, 2))\n\n\n\nRecommendInternalExchangeFinalScorevery likely003.26somewhat likely103.21unlikely113.94somewhat likely002.81somewhat likely002.53unlikely012.59somewhat likely002.56somewhat likely002.73unlikely003.00somewhat likely103.50unlikely113.65somewhat likely002.84very likely013.90somewhat likely002.68unlikely103.57\n\n\nIn a first step, we need to relevel the ordinal variable to represent an ordinal factor (or a progression from “unlikely” over “somewhat likely” to “very likely”. And we will also factorize Internal and Exchange to make it easier to interpret the output later on.\n\n# relevel data\nordata &lt;- ordata %&gt;%\n  dplyr::mutate(Recommend = factor(Recommend, \n                           levels=c(\"unlikely\", \"somewhat likely\", \"very likely\"),\n                           labels=c(\"unlikely\",  \"somewhat likely\",  \"very likely\"))) %&gt;%\n  dplyr::mutate(Exchange = ifelse(Exchange == 1, \"Exchange\", \"NoExchange\")) %&gt;%\n  dplyr::mutate(Internal = ifelse(Internal == 1, \"Internal\", \"External\"))\n\nNow that the dependent variable is releveled, we check the distribution of the variable levels by tabulating the data. To get a better understanding of the data we create frequency tables across variables rather than viewing the variables in isolation.\n\n## three way cross tabs (xtabs) and flatten the table\nftable(xtabs(~ Exchange + Recommend + Internal, data = ordata))\n\n                           Internal External Internal\nExchange   Recommend                                 \nExchange   unlikely                       25        6\n           somewhat likely                12        4\n           very likely                     7        3\nNoExchange unlikely                      175       14\n           somewhat likely                98       26\n           very likely                    20       10\n\n\nWe also check the mean and standard deviation of the final score as final score is a numeric variable and cannot be tabulated (unless we convert it to a factor).\n\nsummary(ordata$FinalScore); sd(ordata$FinalScore)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n1.900000 2.720000 2.990000 2.998925 3.270000 4.000000 \n\n\n[1] 0.397940933861\n\n\nThe lowest score is 1.9 and the highest score is a 4.0 with a mean of approximately 3. Finally, we inspect the distributions graphically.\n\n# visualize data\nggplot(ordata, aes(x = Recommend, y = FinalScore)) +\n  geom_boxplot(size = .75) +\n  geom_jitter(alpha = .5) +\n  facet_grid(Exchange ~ Internal, margins = TRUE) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\n\n\n\nWe see that we have only few students that have taken part in an exchange program and there are also only few internal students overall. With respect to recommendations, only few students are considered to very likely succeed in the program. We can now start with the modeling by using the polr function. To make things easier for us, we will only consider the main effects here as this tutorial only aims to how to implement an ordinal regression but not how it should be done in a proper study - then, the model fitting and diagnostic procedures would have to be performed accurately, of course.\n\n# fit ordered logit model and store results 'm'\nm.or &lt;- polr(Recommend ~ Internal + Exchange + FinalScore, data = ordata, Hess=TRUE)\n# summarize model\nsummary(m.or)\n\nCall:\npolr(formula = Recommend ~ Internal + Exchange + FinalScore, \n    data = ordata, Hess = TRUE)\n\nCoefficients:\n                          Value  Std. Error     t value\nInternalInternal   1.0476639736 0.265789132 3.941711114\nExchangeNoExchange 0.0586810805 0.297858820 0.197009713\nFinalScore         0.6157435960 0.260631273 2.362508493\n\nIntercepts:\n                            Value       Std. Error  t value    \nunlikely|somewhat likely    2.261997617 0.882173598 2.564118471\nsomewhat likely|very likely 4.357441779 0.904467831 4.817685749\n\nResidual Deviance: 717.024871356 \nAIC: 727.024871356 \n\n\nThe results show that having studied here at this school increases the chances of receiving a positive recommendation but that having been on an exchange has a negative but insignificant effect on the recommendation. The final score also correlates positively with a positive recommendation but not as much as having studied here.\n\n## store table\n(ctable &lt;- coef(summary(m.or)))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639736459 0.265789131533 3.941711113632\nExchangeNoExchange          0.0586810804767 0.297858819642 0.197009712679\nFinalScore                  0.6157435959832 0.260631273036 2.362508492595\nunlikely|somewhat likely    2.2619976171265 0.882173597890 2.564118471169\nsomewhat likely|very likely 4.3574417786829 0.904467830674 4.817685749458\n\n\nAs the regression report does not provide p-values, we have to calculate them separately (after having calculated them, we add them to the coefficient table).\n\n## calculate and store p values\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable &lt;- cbind(ctable, \"p value\" = p))\n\n                                      Value     Std. Error        t value\nInternalInternal            1.0476639736459 0.265789131533 3.941711113632\nExchangeNoExchange          0.0586810804767 0.297858819642 0.197009712679\nFinalScore                  0.6157435959832 0.260631273036 2.362508492595\nunlikely|somewhat likely    2.2619976171265 0.882173597890 2.564118471169\nsomewhat likely|very likely 4.3574417786829 0.904467830674 4.817685749458\n                                        p value\nInternalInternal            0.00008090238230957\nExchangeNoExchange          0.84381993712009706\nFinalScore                  0.01815172553987886\nunlikely|somewhat likely    0.01034382311355037\nsomewhat likely|very likely 0.00000145232867123\n\n\nAs predicted, Exchange does not have a significant effect but FinalScore and Internal both correlate significantly with the likelihood of receiving a positive recommendation.\n\n# extract profiled confidence intervals\nci &lt;- confint(m.or)\n# calculate odds ratios and combine them with profiled CIs\nexp(cbind(OR = coef(m.or), ci))\n\n                              OR          2.5 %        97.5 %\nInternalInternal   2.85098336080 1.695837799502 4.81711408248\nExchangeNoExchange 1.06043699275 0.595033205643 1.91977108407\nFinalScore         1.85103250827 1.113625249814 3.09849059341\n\n\nThe odds ratios show that internal students are 2.85 times or 285 percent as likely as non-internal students to receive positive evaluations and that a 1-point increase in the test score lead to a 1.85 times or 185 percent increase in the chances of receiving a positive recommendation. The effect of an exchange is slightly negative but, as we have seen above, not significant."
  },
  {
    "objectID": "tutorials/regression/regression.html#poisson-regression",
    "href": "tutorials/regression/regression.html#poisson-regression",
    "title": "Introduction",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nThis section is based on this tutorials on how to perform a Poisson regression in R.\n\n\n\nNOTEPoisson regressions are used to analyze data where the dependent variable represents counts.\n\n\n\n\n\n\nThis applied particularly to counts that are based on observations of something that is measured in set intervals. For instances the number of pauses in two-minute-long conversations. Poisson regressions are particularly appealing when dealing with rare events, i.e. when something only occurs very infrequently. In such cases, normal linear regressions do not work because the instances that do occur are automatically considered outliers. Therefore, it is useful to check if the data conform to a Poisson distribution.\nHowever, the tricky thing about Poisson regressions is that the data has to conform to the Poisson distribution which is, according to my experience, rarely the case, unfortunately. The Gaussian Normal Distribution is very flexible because it is defined by two parameters, the mean (mu, i.e. \\(\\mu\\)) and the standard deviation (sigma, i.e. \\(\\sigma\\)). This allows the normal distribution to take very different shapes (for example, very high and slim (compressed) or very wide and flat). In contrast, the Poisson is defined by only one parameter (lambda, i.e. \\(\\lambda\\)) which mean that if we have a mean of 2, then the standard deviation is also 2 (actually we would have to say that the mean is \\(\\lambda\\) and the standard deviation is also \\(\\lambda\\) or \\(\\lambda\\) = \\(\\mu\\) = \\(\\sigma\\)). This is much trickier for natural data as this means that the Poisson distribution is very rigid.\n\n\n\n\n\n\n\n\n\nAs we can see, as \\(\\lambda\\) takes on higher values, the distribution becomes wider and flatter - a compressed distribution with a high mean can therefore not be Poisson-distributed. We will now start by loading the data.\n\n# load data\npoissondata  &lt;- base::readRDS(\"tutorials/regression/data/prd.rda\", \"rb\")\n\n\n\nIdPausesLanguageAlcohol450German411080Russian41150German44670German421530German40510Russian421640German461330German4020German33530German4610German401280English38160German441060German37890German40\n\n\nWe will clean the data by factorizing Id which is currently considered a numeric variable rather than a factor.\n\n# process data\npoissondata &lt;- poissondata %&gt;%\n  mutate(Id = factor(Id, levels = 1:200, labels = 1:200))\n# inspect data\nstr(poissondata)\n\n'data.frame':   200 obs. of  4 variables:\n $ Id      : Factor w/ 200 levels \"1\",\"2\",\"3\",\"4\",..: 45 108 15 67 153 51 164 133 2 53 ...\n $ Pauses  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Language: chr  \"German\" \"Russian\" \"German\" \"German\" ...\n $ Alcohol : int  41 41 44 42 40 42 46 40 33 46 ...\n\n\nFirst, we check if the conditions for a Poisson regression are met.\n\n# output the results\ngf = vcd::goodfit(poissondata$Pauses, \n                  type= \"poisson\", \n                  method= \"ML\")\n# inspect results\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df            P(&gt; X^2)\nLikelihood Ratio 33.0122916717  5 0.00000374234139957\n\n\nIf the p-values is smaller than .05, then data is not Poisson distributed which means that it differs significantly from a Poisson distribution and is very likely over-dispersed. We will check the divergence from a Poisson distribution visually by plotting the observed counts against the expected counts if the data were Poisson distributed.\n\nplot(gf,main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\nAlthough the goodfit function reported that the data differs significantly from the Poisson distribution, the fit is rather good. We can use an additional Levene’s test to check if variance homogeneity is given.\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\n\n# check homogeneity\nleveneTest(poissondata$Pauses, poissondata$Language, center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df  F value        Pr(&gt;F)    \ngroup   2 17.15274 0.00000013571 ***\n      197                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test indicates that variance homogeneity is also violated. Since both the approximation to a Poisson distribution and variance homogeneity are violated, we should switch either to a quasi-Poisson model or a negative binomial model. However, as we are only interested in how to implement a Poisson model here, we continue despite the fact that this could not be recommended if we were actually interested in accurate results based on a reliable model.\nIn a next step, we summarize Progression by inspecting the means and standard deviations of the individual variable levels.\n\n# extract mean and standard deviation\nwith(poissondata, tapply(Pauses, Language, function(x) {\n  sprintf(\"M (SD) = %1.2f (%1.2f)\", mean(x), sd(x))\n}))\n\n               English                 German                Russian \n\"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\" \"M (SD) = 0.20 (0.40)\" \n\n\nNow, we visualize the data.\n\n# plot data\nggplot(poissondata, aes(Pauses, fill = Language)) +\n  geom_histogram(binwidth=.5, position=\"dodge\") +\n  scale_fill_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))\n\n\n\n\n\n\n\n\n\n# calculate Poisson regression\nm1.poisson &lt;- glm(Pauses ~ Language + Alcohol, family=\"poisson\", data=poissondata)\n# inspect model\nsummary(m1.poisson)\n\n\nCall:\nglm(formula = Pauses ~ Language + Alcohol, family = \"poisson\", \n    data = poissondata)\n\nCoefficients:\n                     Estimate    Std. Error  z value         Pr(&gt;|z|)    \n(Intercept)     -4.1632652529  0.6628774832 -6.28060 0.00000000033728 ***\nLanguageGerman  -0.7140499158  0.3200148750 -2.23130         0.025661 *  \nLanguageRussian -1.0838591456  0.3582529824 -3.02540         0.002483 ** \nAlcohol          0.0701523975  0.0105992050  6.61865 0.00000000003625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.6722345  on 199  degrees of freedom\nResidual deviance: 189.4496199  on 196  degrees of freedom\nAIC: 373.5045031\n\nNumber of Fisher Scoring iterations: 6\n\n\nIn addition to the Estimates for the coefficients, we could also calculate the confidence intervals for the coefficients (LL stands for lower limit and UL for upper limit in the table below).\n\n# calculate model\ncov.m1 &lt;- sandwich::vcovHC(m1.poisson, type=\"HC0\")\n# extract standard error\nstd.err &lt;- sqrt(diag(cov.m1))\n# extract robust estimates\nr.est &lt;- cbind(Estimate= coef(m1.poisson), \n               \"Robust SE\" = std.err,\n               \"Pr(&gt;|z|)\" = 2 * pnorm(abs(coef(m1.poisson)/std.err),\n                                      lower.tail=FALSE),\nLL = coef(m1.poisson) - 1.96 * std.err,\nUL = coef(m1.poisson) + 1.96 * std.err)\n# inspect data\nr.est\n\n                        Estimate       Robust SE                 Pr(&gt;|z|)\n(Intercept)     -4.1632652529178 0.6480942940026 0.0000000001328637743949\nLanguageGerman  -0.7140499157783 0.2986422497410 0.0168031204011183689373\nLanguageRussian -1.0838591456208 0.3210481575684 0.0007354744824167337974\nAlcohol          0.0701523974937 0.0104351647012 0.0000000000178397516955\n                              LL               UL\n(Intercept)     -5.4335300691629 -2.8930004366726\nLanguageGerman  -1.2993887252708 -0.1287111062859\nLanguageRussian -1.7131135344549 -0.4546047567867\nAlcohol          0.0496994746793  0.0906053203082\n\n\nWe can now calculate the p-value of the model.\n\nwith(m1.poisson, cbind(res.deviance = deviance, df = df.residual,\n  p = pchisq(deviance, df.residual, lower.tail=FALSE)))\n\n     res.deviance  df              p\n[1,] 189.44961991 196 0.618227445717\n\n\nNow, we check, if removing Language leads to a significant decrease in model fit.\n\n# remove Language from the model\nm2.poisson &lt;- update(m1.poisson, . ~ . -Language)\n# check if dropping Language causes a significant decrease in model fit\nanova(m2.poisson, m1.poisson, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Pauses ~ Alcohol\nModel 2: Pauses ~ Language + Alcohol\n  Resid. Df  Resid. Dev Df   Deviance   Pr(&gt;Chi)    \n1       198 204.0213018                             \n2       196 189.4496199  2 14.5716819 0.00068517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now calculate robust coefficients using the msm package (Jackson 2011).\n\n# get estimates\ns &lt;- msm::deltamethod(list(~ exp(x1), ~ exp(x2), ~ exp(x3), ~ exp(x4)), \n                                                coef(m1.poisson), cov.m1)\n# exponentiate old estimates dropping the p values\nrexp.est &lt;- exp(r.est[, -3])\n# replace SEs with estimates for exponentiated coefficients\nrexp.est[, \"Robust SE\"] &lt;- s\n# display results\nrexp.est\n\n                       Estimate       Robust SE               LL\n(Intercept)     0.0155566784084 0.0100821945101 0.00436765044896\nLanguageGerman  0.4896571063601 0.1462322998451 0.27269843575906\nLanguageRussian 0.3382875026084 0.1086065794408 0.18030353649622\nAlcohol         1.0726716412682 0.0111935052470 1.05095521026088\n                             UL\n(Intercept)     0.0554097096208\nLanguageGerman  0.8792279322820\nLanguageRussian 0.6346987787641\nAlcohol         1.0948368101199\n\n\n\n# extract predicted values\n(s1 &lt;- data.frame(Alcohol = mean(poissondata$Alcohol),\n  Language = factor(1:3, levels = 1:3, labels = names(table(poissondata$Language)))))\n\n  Alcohol Language\n1  52.645  English\n2  52.645   German\n3  52.645  Russian\n\n# show results\npredict(m1.poisson, s1, type=\"response\", se.fit=TRUE)\n\n$fit\n             1              2              3 \n0.624944591447 0.306008560283 0.211410945109 \n\n$se.fit\n              1               2               3 \n0.0862811728183 0.0883370633684 0.0705010813453 \n\n$residual.scale\n[1] 1\n\n\n\n## calculate and store predicted values\npoissondata$Predicted &lt;- predict(m1.poisson, type=\"response\")\n## order by program and then by math\npoissondata &lt;- poissondata[with(poissondata, order(Language, Alcohol)), ]\n\n\n## create the plot\nggplot(poissondata, aes(x = Alcohol, y = Predicted, colour = Language)) +\n  geom_point(aes(y = Pauses), alpha=.5, \n             position=position_jitter(h=.2)) +\n  geom_line(size = 1) +\n  labs(x = \"Alcohol (ml)\", y = \"Expected number of pauses\") +\n  scale_color_manual(values=c(\"gray30\", \"gray50\", \"gray70\"))"
  },
  {
    "objectID": "tutorials/regression/regression.html#robust-regression",
    "href": "tutorials/regression/regression.html#robust-regression",
    "title": "Introduction",
    "section": "Robust Regression",
    "text": "Robust Regression\nRobust regression represent an alternative to simple linear models which can handle overly influential data points (outliers). Robust regressions allow us to retain outliers in the data rather than having to remove them from the data by adding weights (Rousseeuw and Leroy). Thus, robust regressions are used when there are outliers present in the data and we can thus not use traditional models but we have no good argument to remove these data points.\n\n\n\nNOTERobust regressions allow us to handle overly influential data points (outliers) by using weights. Thus, robust regressions enable us to retain all data points.\n\n\n\n\n\n\nWe begin by loading a data set (the mlrdata set which have used for multiple linear regression).\n\n# load data\nrobustdata  &lt;- base::readRDS(\"tutorials/regression/data/mld.rda\", \"rb\")\n\n\n\nstatusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51\n\n\nWe first fit an ordinary linear model (and although we know from the section on multiple regression that the interaction between status and attraction is significant, we will disregard this for now as this will help to explain the weighing procedure which is the focus of this section).\n\n# create model\nslm &lt;- lm(money ~ status+attraction, data = robustdata)\n# inspect model\nsummary(slm)\n\n\nCall:\nlm(formula = money ~ status + attraction, data = robustdata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-60.87070 -15.78645  -2.61010  13.88770  59.93710 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             114.94950000   4.28993616  26.79515\nstatusSingle             26.10340000   4.95359160   5.26959\nattractionNotInterested -79.25220000   4.95359160 -15.99894\n                                      Pr(&gt;|t|)    \n(Intercept)             &lt; 0.000000000000000222 ***\nstatusSingle                     0.00000082576 ***\nattractionNotInterested &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.767958 on 97 degrees of freedom\nMultiple R-squared:  0.745229335,   Adjusted R-squared:  0.739976331 \nF-statistic: 141.867286 on 2 and 97 DF,  p-value: &lt; 0.0000000000000002220446\n\n\nWe now check whether the model is well fitted using diagnostic plots.\n\n# generate plots\nautoplot(slm) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\n\n\n\n\n\n\n\nThe diagnostic plots indicate that there are three outliers in the data (data points 52, 83 and possibly 64). Therefore, we need to evaluate if the outliers severely affect the fit of the model.\n\nrobustdata[c(52, 64, 83),]\n\n   status    attraction  money\n52 Single NotInterested   0.93\n64 Single NotInterested  84.28\n83 Single    Interested 200.99\n\n\nWe can now calculate Cook’s distance and standardized residuals check if the values of the potentially problematic points have unacceptably high values (-2 &lt; ok &lt; 2).\n\nCooksDistance &lt;- cooks.distance(slm)\nStandardizedResiduals &lt;- stdres(slm)\na &lt;- cbind(robustdata, CooksDistance, StandardizedResiduals)\na[CooksDistance &gt; 4/100, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n\n\nWe will calculate the absolute value and reorder the table so that it is easier to check the values.\n\nAbsoluteStandardizedResiduals &lt;- abs(StandardizedResiduals)\na &lt;- cbind(robustdata, CooksDistance, StandardizedResiduals, AbsoluteStandardizedResiduals)\nasorted &lt;- a[order(-AbsoluteStandardizedResiduals), ]\nasorted[1:10, ]\n\n         status    attraction  money   CooksDistance StandardizedResiduals\n52       Single NotInterested   0.93 0.0641937458854        -2.49535435377\n83       Single    Interested 200.99 0.0622397126545         2.45708203516\n88       Single    Interested 193.78 0.0481663678409         2.16151282221\n84       Single    Interested 193.69 0.0480020776213         2.15782333134\n1  Relationship NotInterested  86.33 0.0444158829857         2.07565427025\n65       Single NotInterested  12.12 0.0427613629698        -2.03662765573\n67       Single NotInterested  13.28 0.0407877963315        -1.98907421786\n78       Single    Interested 188.76 0.0394313968241         1.95572122040\n21 Relationship NotInterested  81.90 0.0369837409025         1.89404933081\n24 Relationship NotInterested  81.56 0.0364414260634         1.88011125419\n   AbsoluteStandardizedResiduals\n52                 2.49535435377\n83                 2.45708203516\n88                 2.16151282221\n84                 2.15782333134\n1                  2.07565427025\n65                 2.03662765573\n67                 1.98907421786\n78                 1.95572122040\n21                 1.89404933081\n24                 1.88011125419\n\n\nAs Cook’s distance and the standardized residuals do have unacceptable values, we re-calculate the linear model as a robust regression and inspect the results\n\n# create robust regression model\nrmodel &lt;- robustbase::lmrob(money ~ status + attraction, data = robustdata)\n# inspect model\nsummary(rmodel)\n\n\nCall:\nrobustbase::lmrob(formula = money ~ status + attraction, data = robustdata)\n \\--&gt; method = \"MM\"\nResiduals:\n         Min           1Q       Median           3Q          Max \n-61.14269796 -15.20405781  -1.48712081  14.43502508  62.42342804 \n\nCoefficients:\n                            Estimate   Std. Error   t value\n(Intercept)             113.18405781   3.89777692  29.03811\nstatusSingle             25.38251415   5.08841106   4.98830\nattractionNotInterested -76.49387400   5.06626449 -15.09867\n                                      Pr(&gt;|t|)    \n(Intercept)             &lt; 0.000000000000000222 ***\nstatusSingle                      0.0000026725 ***\nattractionNotInterested &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 22.3497532 \nMultiple R-squared:  0.740716949,   Adjusted R-squared:  0.735370907 \nConvergence in 11 IRWLS iterations\n\nRobustness weights: \n 10 weights are ~= 1. The remaining 90 ones are summarized as\n       Min.     1st Qu.      Median        Mean     3rd Qu.        Max. \n0.415507215 0.856134403 0.947485769 0.889078657 0.986192099 0.998890516 \nAlgorithmic parameters: \n           tuning.chi                    bb            tuning.psi \n1.5476399999999999046 0.5000000000000000000 4.6850610000000001421 \n           refine.tol               rel.tol             scale.tol \n0.0000001000000000000 0.0000001000000000000 0.0000000001000000000 \n            solve.tol              zero.tol           eps.outlier \n0.0000001000000000000 0.0000000001000000000 0.0010000000000000000 \n                eps.x     warn.limit.reject     warn.limit.meanrw \n0.0000000000018189894 0.5000000000000000000 0.5000000000000000000 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nThe output shows that both status and attraction are significant but, as we have seen above, the effect that really matters is the interaction between status and attraction.\nWe will briefly check the weights to understand the process of weighing better. The idea of weighing is to downgrade data points that are too influential while not punishing data points that have a good fit and are thus less influential. This means that the problematic data points should have lower weights than other data points (the maximum is 1 - so points can only be made “lighter”).\n\nhweights &lt;- data.frame(status = robustdata$status, resid = rmodel$resid, weight = rmodel$rweights)\nhweights2 &lt;- hweights[order(rmodel$rweights), ]\nhweights2[1:15, ]\n\n         status          resid         weight\n83       Single  62.4234280426 0.415507214894\n52       Single -61.1426979566 0.434323578289\n88       Single  55.2134280426 0.521220526760\n84       Single  55.1234280426 0.522529106622\n78       Single  50.1934280426 0.593234343069\n65       Single -49.9526979566 0.596626306571\n1  Relationship  49.6398161925 0.601024865582\n67       Single -48.7926979566 0.612874578464\n21 Relationship  45.2098161925 0.661914499988\n24 Relationship  44.8698161925 0.666467581490\n39 Relationship -43.8940578083 0.679427975993\n79       Single  40.8234280426 0.719104361372\n58       Single -40.5226979566 0.722893449575\n89       Single  39.9734280426 0.729766992775\n95       Single  39.8234280426 0.731633375015\n\n\nThe values of the weights support our assumption that those data points that were deemed too influential are made lighter as they now only have weights of 0.415507214894 and 0.434323578289 respectively. This was, however, not the focus of this sections as this section merely served to introduce the concept of weights and how they can be used in the context of a robust linear regression."
  },
  {
    "objectID": "tutorials/regression/regression.html#linear-mixed-effects-regression",
    "href": "tutorials/regression/regression.html#linear-mixed-effects-regression",
    "title": "Introduction",
    "section": "Linear Mixed-Effects Regression",
    "text": "Linear Mixed-Effects Regression\nThe following focuses on an extension of ordinary multiple linear regressions: mixed-effects regression linear regression. Mixed-effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. (multi-)collinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of mixed-effects regression modeling are that they are prone to producing high \\(\\beta\\)-errors (see Johnson) and that they require rather large data sets.\n\nIntroduction\nSo far, the regression models that we have used only had fixed-effects. Having only fixed-effects means that all data points are treated as if they are completely independent and thus on the same hierarchical level. However, it is very common that the data is nested in the sense that data points are not independent because they are, for instance produced by the same speaker or are grouped by some other characteristic. In such cases, the data is considered hierarchical and statistical models should incorporate such structural features of the data they work upon. Fortunately, modeling hierarchical or nested data structures is very easy thanks to the lme4 package (Bates et al. 2015).\nWith respect to regression modeling, hierarchical structures are incorporated by what is called random effects. When models only have a fixed-effects structure, then they make use of only a single intercept and/or slope (as in the left panel in the figure below), while mixed effects models have intercepts for each level of a random effect. If the random effect structure represents speakers then this would mean that a mixed-model would have a separate intercept and/or slope for each speaker (in addition to the overall intercept that is shown as an orange line in the figure below).\n\n\n\n\n\n\n\n\n\nThe idea behind regression analysis is expressed formally in the equation below where\\(f_{(x)}\\) is the y-value we want to predict, \\(\\alpha\\) is the intercept (the point where the regression line crosses the y-axis at x = 0), \\(\\beta\\) is the coefficient (the slope of the regression line), and x is the value of a predictor (e.g. 180cm - if we would like to predict the weight of a person based on their height). The \\(\\epsilon\\) is an error term that reflects the difference between the predicted value and the (actually) observed value (\\(\\epsilon\\) is thus a residual that is important as regressions assume that residuals are, e.g., normally distributed).\n\\[\\begin{equation}\nf_{(x)} = \\alpha + \\beta x + \\epsilon\n\\end{equation}\\]\nIn other words, to estimate how much some weights who is 180cm tall, we would multiply the coefficient (slope of the line) with 180 (\\(x\\)) and add the value of the intercept (point where line crosses the y-axis at x = 0).\nThe equation below represents a formal representation of a mixed-effects regression with varying intercepts (see Winter, 235).\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta x + \\epsilon\n\\end{equation}\\]\nIn this random intercept model, each level of a random variable has a different intercept. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the product of the predictor coefficient and the value of x.\nFinally, the equation below represents a formal representation of a mixed-effects regression with varying intercepts and varying slopes (see Winter, 235).\n\\[\\begin{equation}\nf_{(x)} = \\alpha_{i} + \\beta_{i}x + \\epsilon\n\\end{equation}\\]\nIn this last model, each level of a random variable has a different intercept and a different slope. To predict the value of a data point, we would thus take the appropriate intercept value (the model intercept + the intercept of the random effect) and add the coefficient of that random effect level multiplied by the value of x.\n\n\nRandom Effects\nRandom Effects can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to fixed-effects models, that have only 1 intercept and one slope (left panel in the figure above), mixed-effects models can therefore have various random intercepts (center panel) or various random slopes, or both, various random intercepts and various random slopes (right panel).\nWhat features do distinguish random and fixed effects?\n\nRandom effects represent a higher level variable under which data points are grouped. This implies that random effects must be categorical (or nominal but they a´cannot be continuous!) (see Winter, 236).\nRandom effects represent a sample of an infinite number of possible levels. For instance, speakers, trials, items, subjects, or words represent a potentially infinite pool of elements from which many different samples can be drawn. Thus, random effects represent a random sample sample. Fixed effects, on the other hand, typically do not represent a random sample but a fixed set of variable levels (e.g. Age groups, or parts-of-speech).\nRandom effects typically represent many different levels while fixed effects typically have only a few. Zuur, Hilbe, and Ieno propose that a variable may be used as a fixed effect if it has less than 5 levels while it should be treated as a random effect if it has more than 10 levels. Variables with 5 to 10 levels can be used as both. However, this is a rule of thumb and ignores the theoretical reasons (random sample and nestedness) for considering something as a random effect and it also is at odds with the way that repeated measures are models (namely as mixed effects) although they typically only have very few levels.\nFixed effects represent an effect that if we draw many samples, the effect would be consistent across samples (Winter) while random effects should vary for each new sample that is drawn.\n\nIn the following, we will only focus on models with random intercepts because this is the more common method and because including both random intercepts and random slopes requires larger data sets (but have a better fit because intercepts are not forced to be parallel and the lines therefore have a better fit). You should, however, always think about what random effects structure is appropriate for your model - a very recommendable explanation of how to chose which random effects structure is best (and about what the determining factors for this decision are) is give in Winter (241–44). Also, consider the center and the right plots above to understand what is meant by random intercepts and random slopes.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted (Field, Miles, and Field). We test whether including random effects is warranted by comparing a model, that bases its estimates of the depended variable solely on the base intercept (the mean), with a model, that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the random-effect model explains significantly more variance than the simple model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified if they reduce residual deviance.\n\n\nExample: Preposition Use across Time by Genre\nTo explore how to implement a mixed-effects model in R we revisit the preposition data that contains relative frequencies of prepositions in English texts written between 1150 and 1913. As a first step, and to prepare our analysis, we load necessary R packages, specify options, and load as well as provide an overview of the data.\n\n# load data\nlmmdata  &lt;- base::readRDS(\"tutorials/regression/data/lmd.rda\", \"rb\") %&gt;%\n  # convert date into a numeric variable\n  dplyr::mutate(Date = as.numeric(Date))\n\n\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North1,905Travelbradley154.20North1,711Educationbrightland149.14North1,762Sermonburton159.71North1,726Sermonbutler157.49North1,835PrivateLettercarlyle124.16North\n\n\nThe data set contains the date when the text was written (Date), the genre of the text (Genre), the name of the text (Text), the relative frequency of prepositions in the text (Prepositions), and the region in which the text was written (Region). We now plot the data to get a first impression of its structure.\n\np1 &lt;- ggplot(lmmdata, aes(x = Date, y = Prepositions)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, color = \"red\", linetype = \"dashed\") +\n  theme_bw() +\n  labs(y = \"Frequency\\n(Prepositions)\")\np2 &lt;- ggplot(lmmdata, aes(x = reorder(Genre, -Prepositions), y = Prepositions)) +\n  geom_boxplot() +\n  theme_bw() + \n  theme(axis.text.x = element_text(angle=90)) +\n  labs(x = \"Genre\", y = \"Frequency\\n(Prepositions)\")\np3 &lt;- ggplot(lmmdata, aes(Prepositions)) +\n  geom_histogram() +\n  theme_bw() + \n  labs(y = \"Count\", x = \"Frequency (Prepositions)\")\ngridExtra::grid.arrange(grobs = list(p1, p2, p3), widths = c(1, 1), layout_matrix = rbind(c(1, 1), c(2, 3)))\n\n\n\n\n\n\n\n\nThe scatter plot in the upper panel indicates that the use of prepositions has moderately increased over time while the boxplots in the lower left panel show that the genres differ quite substantially with respect to their median frequencies of prepositions per text. Finally, the histogram in the lower right panel show that preposition use is distributed normally with a mean of 132.2 prepositions per text.\n\np4 &lt;- ggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  labs(x = \"Year\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\np5 &lt;- ggplot(lmmdata, aes(Region, Prepositions)) +\n  geom_boxplot() +\n  labs(x = \"Region\", y = \"Prepositions per 1,000 words\") +\n  geom_smooth(method = \"lm\")  + \n  theme_bw()\ngridExtra::grid.arrange(p4, p5, nrow = 1)\n\n\n\n\n\n\n\n\n\nggplot(lmmdata, aes(Date, Prepositions)) +\n  geom_point() +\n  facet_wrap(~ Genre, nrow = 4) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs(x = \"Date of composition\", y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\n\n\n\n\nCentering or even scaling numeric variables is useful for later interpretation of regression models: if the date variable were not centered, the regression would show the effects of variables at year 0(!). If numeric variables are centered, other variables are variables are considered relative not to 0 but to the mean of that variable (in this case the mean of years in our data). Centering simply means that the mean of the numeric variable is subtracted from each value.\n\nlmmdata$DateUnscaled &lt;- lmmdata$Date\nlmmdata$Date &lt;- scale(lmmdata$Date, scale = F)\n\n\n\nDateGenreTextPrepositionsRegionDateUnscaled109.8696461825Sciencealbin166.01North1,73684.8696461825Educationanon139.86North1,711181.8696461825PrivateLetterausten130.78North1,808251.8696461825Educationbain151.29North1,878116.8696461825Educationbarclay145.72North1,743281.8696461825Educationbenson120.77North1,908279.8696461825Diarybenson119.17North1,906270.8696461825Philosophyboethja132.96North1,897158.8696461825Philosophyboethri130.49North1,785149.8696461825Diaryboswell135.94North1,776278.8696461825Travelbradley154.20North1,90584.8696461825Educationbrightland149.14North1,711135.8696461825Sermonburton159.71North1,76299.8696461825Sermonbutler157.49North1,726208.8696461825PrivateLettercarlyle124.16North1,835\n\n\nWe now set up a fixed-effects model with the glm function and a mixed-effects model using the glmer function from the lme4 package (Bates et al. 2015) with Genre as a random effect.\n\n# generate models\nm0.glm &lt;- glm(Prepositions ~ 1, family = gaussian, data = lmmdata)\nm0.lmer = lmer(Prepositions ~ 1 + (1|Genre), REML = T, data = lmmdata)\n\nNow that we have created the base-line models, we will test whether including a random effect structure is mathematically justified. It is important to note here that we are not going to test if including a random effect structure is theoretically motivated but simply if it causes a decrease in variance.\n\n\nTesting Random Effects\nAs a first step in the modeling process, we now need to determine whether or not including a random effect structure is justified. We do so by comparing the AIC of the base-line model without random intercepts to the AIC of the model with random intercepts.\n\nAIC(logLik(m0.glm))\n\n[1] 4718.19031114\n\nAIC(logLik(m0.lmer))\n\n[1] 4497.77554693\n\n\nThe inclusion of a random effect structure with random intercepts is justified as the AIC of the model with random intercepts is substantially lower than the AIC of the model without random intercepts.\nWhile I do not how how to test if including a random effect is justified, there are often situations, which require to test exactly which random effect structure is best. When doing this, it is important to use restricted maximum likelihood (REML = TRUE or method = REML) rather than maximum likelihood (see Pinheiro and Bates; Winter, 226).\n\n# generate models with 2 different random effect structures\nma.lmer = lmer(Prepositions ~ Date + (1|Genre), REML = T, data = lmmdata)\nmb.lmer = lmer(Prepositions ~ Date + (1 + Date | Genre), REML = T, data = lmmdata)\n# compare models\nanova(ma.lmer, mb.lmer, test = \"Chisq\", refit = F)\n\nData: lmmdata\nModels:\nma.lmer: Prepositions ~ Date + (1 | Genre)\nmb.lmer: Prepositions ~ Date + (1 + Date | Genre)\n        npar         AIC         BIC       logLik    deviance    Chisq Df\nma.lmer    4 4499.148092 4516.292084 -2245.574046 4491.148092            \nmb.lmer    6 4486.699509 4512.415498 -2237.349755 4474.699509 16.44858  2\n        Pr(&gt;Chisq)    \nma.lmer               \nmb.lmer 0.00026806 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model comparison shows that the model with the more complex random effect structure has a significantly better fit to the data compared with the model with the simpler random effect structure. However, we will continue with the model with the simpler structure because this is just an example.\n\n\n\nNOTEIn a real analysis, we would switch to a model with random intercepts and random slopes for Genre because it has a significantly better fit to the data.\n\n\n\n\n\n\n\n\nModel Fitting\nAfter having determined that including a random effect structure is justified, we can continue by fitting the model and including diagnostics as we go. Including diagnostics in the model fitting process can save time and prevent relying on models which only turn out to be unstable if we would perform the diagnostics after the fact.\nWe begin fitting our model by adding Date as a fixed effect and compare this model to our mixed-effects base-line model to see if Date improved the model fit by explaining variance and if Date significantly correlates with our dependent variable (this means that the difference between the models is the effect (size) of Date!)\n\nm1.lmer &lt;- lmer(Prepositions ~ (1|Genre) + Date, REML = T, data = lmmdata)\nanova(m1.lmer, m0.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm0.lmer: Prepositions ~ 1 + (1 | Genre)\nm1.lmer: Prepositions ~ (1 | Genre) + Date\n        npar         AIC         BIC       logLik    deviance  Chisq Df\nm0.lmer    3 4501.947337 4514.805331 -2247.973668 4495.947337          \nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736 8.9296  1\n        Pr(&gt;Chisq)   \nm0.lmer              \nm1.lmer  0.0028059 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nThe model with Date is the better model (significant p-value and lower AIC). The significant p-value shows that Date correlates significantly with Prepositions (\\(\\chi\\)2(1): 8.929600937904, p = 0.00281) . The \\(\\chi\\)2 value here is labeled Chisq and the degrees of freedom are calculated by subtracting the smaller number of DFs from the larger number of DFs.\nWe now test whether Region should also be part of the final minimal adequate model. The easiest way to add predictors is by using the update function (it saves time and typing).\n\n# generate model\nm2.lmer &lt;- update(m1.lmer, .~.+ Region)\n# test vifs\ncar::vif(m2.lmer)\n\n         Date        Region \n1.20287668037 1.20287668037 \n\n# compare models                \nanova(m2.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm2.lmer: Prepositions ~ (1 | Genre) + Date + Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm2.lmer    5 4494.624343 4516.054333 -2242.312171 4484.624343 2.39339  1\n        Pr(&gt;Chisq)\nm1.lmer           \nm2.lmer    0.12185\n\n\nThree things tell us that Region should not be included:\n\nthe AIC does not decrease,\nthe BIC increases(!), and\nthe p-value is higher than .05.\n\nThis means, that we will continue fitting the model without having Region included. Well… not quite - just as a note on including variables: while Region is not significant as a main effect, it must still be included in a model if it were part of a significant interaction. To test if this is indeed the case, we fit another model with the interaction between Date and Region as predictor.\n\n# generate model\nm3.lmer &lt;- update(m1.lmer, .~.+ Region*Date)\n# extract vifs\ncar::vif(m3.lmer)\n\n         Date        Region   Date:Region \n1.96923042279 1.20324697637 1.78000887980 \n\n# compare models                \nanova(m3.lmer, m1.lmer, test = \"Chi\")\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: lmmdata\nModels:\nm1.lmer: Prepositions ~ (1 | Genre) + Date\nm3.lmer: Prepositions ~ (1 | Genre) + Date + Region + Date:Region\n        npar         AIC         BIC       logLik    deviance   Chisq Df\nm1.lmer    4 4495.017736 4512.161728 -2243.508868 4487.017736           \nm3.lmer    6 4496.124872 4521.840861 -2242.062436 4484.124872 2.89286  2\n        Pr(&gt;Chisq)\nm1.lmer           \nm3.lmer    0.23541\n\n\nAgain, the high p-value and the increase in AIC and BIC show that we have found our minimal adequate model with only contains Date as a main effect. In a next step, we can inspect the final minimal adequate model, i.e. the most parsimonious (the model that explains a maximum of variance with a minimum of predictors).\n\n# inspect results\nsummary(m1.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Prepositions ~ (1 | Genre) + Date\n   Data: lmmdata\n\nREML criterion at convergence: 4491.1\n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-3.734915441 -0.657038004  0.005865025  0.661298615  3.596659863 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev.  \n Genre    (Intercept) 159.021120 12.6103576\n Residual             228.764179 15.1249522\nNumber of obs: 537, groups:  Genre, 16\n\nFixed effects:\n                   Estimate      Std. Error  t value\n(Intercept) 133.88516211469   3.24749296223 41.22724\nDate          0.01894493515   0.00632363682  2.99589\n\nCorrelation of Fixed Effects:\n     (Intr)\nDate 0.005 \n\n\n\n\nModel Diagnostics\nWe can now evaluate the goodness of fit of the model and check if mathematical requirements and assumptions have been violated. In a first step, we generate diagnostic plots that focus on the random effect structure.\n\nplot(m1.lmer, Genre ~ resid(.), abline = 0 ) # generate diagnostic plots\n\n\n\n\n\n\n\n\nThe plot shows that there are some outliers (points outside the boxes) and that the variability within letters is greater than in other genres we therefore examine the genres in isolation standardized residuals versus fitted values (Pinheiro and Bates, 175).\n\nplot(m1.lmer, resid(., type = \"pearson\") ~ fitted(.) | Genre, id = 0.05, \n     adj = -0.3, pch = 20, col = \"gray40\")\n\n\n\n\n\n\n\n\nThe plot shows the standardized residuals (or Pearson’s residuals) versus fitted values and suggests that there are outliers in the data (the names elements in the plots). To check if these outliers are a cause for concern, we will now use a Levene’s test to check if the variance is distributed homogeneously (homoscedasticity) or whether the assumption of variance homogeneity is violated (due to the outliers).\n\n\n\nNOTEThe use of Levene’s test to check if the model is heteroscedastic is generally not recommended as it is too lax when dealing with few observations (because in such cases it does not have the power to identify heteroscedasticity) while it is too harsh when dealing with many observations (when heteroscedasticity typically is not a severe problem).\n\n\n\n\n\n\nWe use Levene’s test here merely to check if it substantiates the impressions we got from the visual inspection.\n\n# check homogeneity\nleveneTest(lmmdata$Prepositions, lmmdata$Genre, center = mean)\n\nWarning in leveneTest.default(lmmdata$Prepositions, lmmdata$Genre, center =\nmean): lmmdata$Genre coerced to factor.\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value   Pr(&gt;F)  \ngroup  15 1.74289 0.039906 *\n      521                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Levene’s test shows that the variance is distributed unevenly across genres which means that we do not simply continue but should either remove problematic data points (outliers) or use a weighing method.\nIn this case, we create a new model which uses weights to compensate for heterogeneity of variance and thus the influence of outliers - which is an alternative to removing the data points and rerunning the analysis (Pinheiro and Bates, 177). However, to do so, we need to use a different function (the lme function) which means that we have to create two models: the old minimal adequate model and the new minimal adequate model with added weights. After we have created these models, we will compare them to see if including weights has improved the fit.\n\n# generate models\nm4.lme = lme(Prepositions ~ Date, random = ~1|Genre, data = lmmdata, method = \"ML\")\nm5.lme &lt;- update(m4.lme, weights = varIdent(form = ~ 1 | Genre))\n# compare models\nanova(m5.lme, m4.lme)\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm5.lme     1 19 4485.84955680 4567.28352060 -2223.92477840                     \nm4.lme     2  4 4495.01773596 4512.16172834 -2243.50886798 1 vs 2 39.1681791556\n       p-value\nm5.lme        \nm4.lme  0.0006\n\n\nThe weight model (m5.lme) that uses weights to account for unequal variance is performing significantly better than the model without weights (m4.lme) and we therefore switch to the weight model and inspect its parameters.\n\n# inspect results\nsummary(m5.lme)        \n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n           AIC          BIC        logLik\n  4485.8495568 4567.2835206 -2223.9247784\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2632323711 14.3422048362\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340715615490  0.869521038482  0.788861254265  0.911719712514 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096572672893  0.978728647599  0.784966066137  0.736986014758  1.190619042239 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929751328  0.974641038177  0.848591142831  0.970869209380  1.086232439777 \nTrialProceeding \n 1.260188475499 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640155217 3.144361414827 520 42.6045221424  0.0000\nDate          0.0217416268 0.005454723299 520  3.9858349603  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-3.319117397700 -0.679728744836  0.014685431575  0.698705968152  3.103872340648 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\nWe can also use an ANOVA display which is more to the point.\n\nanova(m5.lme)          \n\n            numDF denDF        F-value p-value\n(Intercept)     1   520 1813.892139648  &lt;.0001\nDate            1   520   15.886880331  0.0001\n\n\nAs we did before, we now check, whether the final minimal model (with weights) outperforms an intercept-only base-line model.\n\n# generate base-line model\nm0.lme = lme(Prepositions ~ 1, random = ~1|Genre, data = lmmdata, method = \"ML\", weights = varIdent(form = ~ 1 | Genre))\nanova(m0.lme, m5.lme)  # test if date is significant\n\n       Model df           AIC           BIC         logLik   Test       L.Ratio\nm0.lme     1 18 4496.28563021 4573.43359592 -2230.14281511                     \nm5.lme     2 19 4485.84955680 4567.28352060 -2223.92477840 1 vs 2 12.4360734115\n       p-value\nm0.lme        \nm5.lme  0.0004\n\n\nOur final minimal adequate model with weights performs significantly better than an intercept only base-line model. Before doing the final diagnostics, we well inspect the estimates for the random effect structure to check if there are values which require further inspection (e.g. because they are drastically different from all other values).\n\n# extract estimates and sd for fixed and random effects\nintervals(m5.lme, which=\"fixed\")      \n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                        lower              est.             upper\n(Intercept) 127.7983166699800 133.9640155217270 140.1297143734741\nDate          0.0110455977276   0.0217416268231   0.0324376559185\n\n\nThe random effect estimates do not show any outliers or drastically increased or decreased values which means that the random effect structure is fine.\n\n\nEffect Sizes\nWe will now extract effect sizes (in the example: the effect size of Date) and calculate normalized effect size measures (this effect size measure works for all fixed effects). When you have factorial design, you can take the square root of the squared t-value divided by the t-value squared plus the degrees of freedom to calculate the effect size:\n\\[\\begin{equation}\n\nr = \\sqrt{ \\frac{ t^2}{(t^2 + df) } } = \\sqrt{ \\frac{ 3.99^2}{(3.99^2 + 520) } } = 0.172\n\n\\end{equation}\\]\n\n\n\n\nNOTETwo words of warning though: br&gt;1. In our case, the effect we are interested in is not factorial but continuous which means that we should not use this effect size measure. We only show this here as an example for how you can calculate the effect size measure r.2. Only apply this function to main effects that are not involved in interactions as they are meaningless because the amount of variance explained by main effects involved in interactions is unclear (Field, Miles, and Field, 641).\n\n\n\n\n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n&lt;0.001\n\n\nDate\n0.02\n0.01 – 0.03\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.70\n\n\n\nτ00 Genre\n150.39\n\n\nICC\n0.42\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.432\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\nThe marginal R2 (marginal coefficient of determination) represents the variance explained by the fixed effects while the conditional R2 is interpreted as a variance explained by the entire model, including both fixed and random effects (Bartoń).\nThe respective call for the model is:\n\n# extract R2s\nr.squaredGLMM(m1.lmer)\n\n                 R2m            R2c\n[1,] 0.0121971160219 0.417270545268\n\n\nThe effects can be visualized using the plot_model function from the sjPlot package (Lüdecke).\n\nsjPlot::plot_model(m5.lme, type = \"pred\", terms = c(\"Date\")) +\n  # show uncentered date rather than centered date\n  scale_x_continuous(name = \"Date\", \n                     breaks = seq(-500, 300, 100), \n                     labels = seq(1150, 1950, 100))\n\n\n\n\n\n\n\n\nWhile we have already shown that the effect of Date is significant, it is small which means that the number of prepositions per text does not correlate very strongly with time. This suggests that other factors that are not included in the model also impact the frequency of prepositions (and probably more meaningfully, too).\nBefore turning to the diagnostics, we will use the fitted (or predicted) and the observed values with a regression line for the predicted values. This will not only show how good the model fit the data but also the direction and magnitude of the effect.\n\n# extract predicted values\nlmmdata$Predicted &lt;- predict(m5.lme, lmmdata)\n# plot predicted values\nggplot(lmmdata, aes(DateUnscaled, Predicted)) +\n  facet_wrap(~Genre) +\n  geom_point(aes(x = DateUnscaled, y = Prepositions), color = \"gray80\", size = .5) +\n  geom_smooth(aes(y = Predicted), color = \"gray20\", linetype = \"solid\", \n              se = T, method = \"lm\") +\n  guides(color=guide_legend(override.aes=list(fill=NA))) +  \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"top\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  xlab(\"Date of composition\")\n\n\n\n\n\n\n\n\n\n\nModel Diagnostics\nIn addition, we generate diagnostic plots. What we wish to see in the diagnostic plots is a cloud of dots in the middle of the window without any structure. What we do not want to see is a funnel-shaped cloud because this indicates an increase of the errors/residuals with an increase of the predictor(s) (because this would indicate heteroscedasticity) (Pinheiro and Bates, 182).\n\n# start plotting\npar(mfrow = c(2, 2))           # display plots in 2 rows and 2 columns\nplot(m5.lme, pch = 20, col = \"black\", lty = \"dotted\"); par(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nWhat a wonderful unstructured cloud - the lack of structure tells us that the model is “healthy” and does not suffer from heteroscedasticity. We will now create more diagnostic plots to find potential problems (Pinheiro and Bates, 21).\n\n# fitted values by Genre\nplot(m5.lme, form = resid(., type = \"p\") ~ fitted(.) | Genre, abline = 0, \n     cex = .5, pch = 20, col = \"black\")\n\n\n\n\n\n\n\n\nIn contrast to the unweight model, no data points are named which indicates that the outliers do no longer have unwarranted influence on the model. Now, we check the residuals of fitted values against observed values (Pinheiro and Bates, 179). What we would like to see is a straight, upwards going line.\n\n# residuals of fitted values against observed\nqqnorm(m5.lme, pch = 20, col = \"black\")\n\n\n\n\n\n\n\n\nA beautiful, straight line! The qqplot does not indicate any problems. It is, unfortunately, rather common that the dots deviate from the straight line at the very bottom or the very top which means that the model is good at estimating values around the middle of the dependent variable but rather bad at estimating lower or higher values. Next, we check the residuals by “Genre” (Pinheiro and Bates, 179).\n\n# residuals by genre\nqqnorm(m5.lme, ~resid(.) | Genre, pch = 20, col = \"black\" )\n\n\n\n\n\n\n\n\nBeautiful straight lines - perfection! Now, we inspect the observed responses versus the within-group fitted values (Pinheiro and Bates, 178).\n\n# observed responses versus the within-group fitted values\nplot(m5.lme, Prepositions ~ fitted(.), id = 0.05, adj = -0.3, \n     xlim = c(80, 220), cex = .8, pch = 20, col = \"blue\")\n\n\n\n\n\n\n\n\nAlthough some data points are named, the plot does not show any structure, like a funnel, which would have been problematic.\n\n\nReporting Results\nBefore we do the write-up, we have a look at the model summary as this will provide us with at least some of the parameters that we want to report.\n\nsummary(m5.lme)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: lmmdata \n           AIC          BIC        logLik\n  4485.8495568 4567.2835206 -2223.9247784\n\nRandom effects:\n Formula: ~1 | Genre\n          (Intercept)      Residual\nStdDev: 12.2632323711 14.3422048362\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Genre \n Parameter estimates:\n          Bible       Biography           Diary       Education         Fiction \n 1.000000000000  0.340715615490  0.869521038482  0.788861254265  0.911719712514 \n       Handbook         History             Law      Philosophy   PrivateLetter \n 1.096572672893  0.978728647599  0.784966066137  0.736986014758  1.190619042239 \n   PublicLetter        Religion         Science          Sermon          Travel \n 1.218929751328  0.974641038177  0.848591142831  0.970869209380  1.086232439777 \nTrialProceeding \n 1.260188475499 \nFixed effects:  Prepositions ~ Date \n                     Value      Std.Error  DF       t-value p-value\n(Intercept) 133.9640155217 3.144361414827 520 42.6045221424  0.0000\nDate          0.0217416268 0.005454723299 520  3.9858349603  0.0001\n Correlation: \n     (Intr)\nDate 0.004 \n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-3.319117397700 -0.679728744836  0.014685431575  0.698705968152  3.103872340648 \n\nNumber of Observations: 537\nNumber of Groups: 16 \n\n\n\nsjPlot::tab_model(m5.lme)\n\n\n\n \nPrepositions\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n133.96\n127.80 – 140.13\n&lt;0.001\n\n\nDate\n0.02\n0.01 – 0.03\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n205.70\n\n\n\nτ00 Genre\n150.39\n\n\nICC\n0.42\n\n\nN Genre\n16\n\nObservations\n537\n\n\nMarginal R2 / Conditional R2\n0.017 / 0.432\n\n\n\n\n\n\nWe can now use the information extracted above to write up a final report:\nA mixed-effect linear regression model which contained the genre of texts as random effect was fit to the data in a step-wise-step up procedure. Due to the presence of outliers in the data, weights were included into the model which led to a significantly improved model fit compared to an un-weight model (\\(\\chi\\)2(2): 39.17, p: 0.0006). The final minimal adequate model performed significantly better than an intercept-only base-line model (\\(\\chi\\)2(1): 12.44, p =.0004) and showed that the frequency of prepositions increases significantly but only marginally with the date of composition (Estimate: 0.02, CI: 0.01-0.03, p &lt; .001, marginal R2 = 0.0174, conditional R2 = 0.4324). Neither the region where the text was composed nor a higher order interaction between genre and region significantly correlated with the use of prepositions in the data.\n\n\nRemarks on Prediction\nWhile the number of intercepts, the model reports, and the way how mixed- and fixed-effects arrive at predictions differ, their predictions are extremely similar and almost identical (at least when dealing with a simple random effect structure). Consider the following example where we create analogous fixed and mixed effect models and plot their predicted frequencies of prepositions per genre across the un-centered date of composition. The predictions of the mixed-effects model are plotted as a solid red line, while the predictions of the fixed-effects model are plotted as dashed blue lines.\n\n# create lm model\nm5.lmeunweight &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmePredictions &lt;- fitted(m5.lmeunweight, lmmdata)\nm5.lm &lt;- lm(Prepositions ~ DateUnscaled + Genre, data = lmmdata)\nlmmdata$lmPredictions &lt;- fitted(m5.lm, lmmdata)\n# plot predictions\nggplot(lmmdata, aes(x = DateUnscaled, y = lmePredictions, group = Genre)) +\n  geom_line(aes(y = lmmdata$lmePredictions), linetype = \"solid\", color = \"red\") +\n  geom_line(aes(y = lmmdata$lmPredictions), linetype = \"dashed\", color = \"blue\") +\n  facet_wrap(~ Genre, nrow = 4) +\n  theme_bw() +\n  labs(x = \"Date of composition\") +\n  labs(y = \"Prepositions per 1,000 words\") +\n  coord_cartesian(ylim = c(0, 220))\n\n\n\n\n\n\n\n\nThe predictions overlap almost perfectly which means that the predictions of both are almost identical - irrespective of whether genre is part of the mixed or the fixed effects structure."
  },
  {
    "objectID": "tutorials/regression/regression.html#mixed-effects-binomial-logistic-regression",
    "href": "tutorials/regression/regression.html#mixed-effects-binomial-logistic-regression",
    "title": "Introduction",
    "section": "Mixed-Effects Binomial Logistic Regression",
    "text": "Mixed-Effects Binomial Logistic Regression\nWe now turn to an extension of binomial logistic regression: mixed-effects binomial logistic regression. As is the case with linear mixed-effects models logistic mixed effects models have the following advantages over simpler statistical tests:\n\nMixed-effects models are multivariate, i.e. they test the effect of several predictors simultaneously while controlling for the effect of all other predictors.\nMixed models allow to statistically incorporate within-speaker variability and are thus fit to model hierarchical or nested data structures. This applies if several observations are produced by an individual speaker, for instance.\nMixed-models provide a wealth of diagnostic statistics which enables us to control e.g. multicollinearity, i.e. correlations between predictors, and to test whether conditions or requirements are violated (e.g. homogeneity of variance, etc.).\n\nMajor disadvantages of regression modeling are that they are prone to producing high \\(\\beta\\)-errors (see Johnson) and that they require rather large data sets.\n\nIntroduction\nAs is the case with linear mixed-effects models, binomial logistic mixed-effect models are multivariate analyses that treat data points as hierarchical or grouped in some way. In other words, they take into account that the data is nested in the sense that data points are produced by the same speaker or are grouped by some other characteristics. In mixed-models, hierarchical structures are modelled as random effects. If the random effect structure represents speakers then this means that a mixed-model would have a separate intercept and/or slope for each speaker.\nRandom Effects in linear models can be visualized using two parameters: the intercept (the point where the regression line crosses the y-axis at x = 0) and the slope (the acclivity of the regression line). In contrast to linear mixed-effects models, random effects differ in the position and the slope of the logistic function that is applied to the likelihood of the dependent variable. Random intercepts (upper right panel) or various random slopes (lower left panel), or both, various random intercepts and various random slopes (lower right panel). In the following, we will only focus on models with random intercepts because this is the by far more common method and because including both random intercepts and random slopes requires huge amounts of data. Consider the Figure below to understand what is meant by random intercepts.\n\n\n\n\n\n\n\n\n\nThe upper left panel merely shows the logistic curve representing the predictions of a fixed-effects logistic regression with a single intercept and slope. The upper right panel shows the logistic curves representing the predictions of a of a mixed-effects logistic regression with random intercepts for each level of a grouping variable. The lower left panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with one intercept but random slopes for each level of a grouping variable. The lower right panel shows the logistic curves representing the predictions of a mixed-effects logistic regression with random intercepts and random slopes for each level of a grouping variable.\nAfter adding random intercepts, predictors (or fixed effects) are added to the model (just like with multiple regression). So mixed-effects are called mixed-effects because they contain both random and fixed effects.\nIn terms of general procedure, random effects are added first, and only after we have ascertained that including random effects is warranted, we test whether including fixed-effects is warranted (Field, Miles, and Field). We test whether including random effects is warranted by comparing a model, that bases its estimates of the dependent variable solely on the base intercept, with a model that bases its estimates of the dependent variable solely on the intercepts of the random effect. If the mixed-effects model explains significantly more variance than the fixed-effects model without random effect structure, then we continue with the mixed-effects model. In other words, including random effects is justified.\n\n\nExample: Discourse LIKE in Irish English\nIn this example we will investigate which factors correlate with the use of final discourse like (e.g. “The weather is shite, like!”) in Irish English. The data set represents speech units in a corpus that were coded for the speaker who uttered a given speech unit, the gender (Gender: Men versus Women) and age of that speaker (Age: Old versus Young), whether the interlocutors were of the same or a different gender (ConversationType: SameGender versus MixedGender), and whether another final discourse like had been used up to three speech units before (Priming: NoPrime versus Prime), whether or not the speech unit contained an final discourse like (SUFLike: 1 = yes, 0 = no. To begin with, we load the data and inspect the structure of the data set,\n\n# load data\nmblrdata  &lt;- base::readRDS(\"tutorials/regression/data/mbd.rda\", \"rb\")\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-061$CWomenYoungMixedGenderNoPrime0S1A-023$BWomenYoungMixedGenderNoPrime0S1A-054$AWomenYoungSameGenderNoPrime0S1A-090$BWomenYoungMixedGenderNoPrime0S1A-009$BWomenOldSameGenderPrime0S1A-085$EMenYoungMixedGenderPrime1S1A-003$CWomenYoungMixedGenderNoPrime1S1A-084$CWomenYoungSameGenderNoPrime0S1A-076$AWomenYoungSameGenderNoPrime0S1A-083$DMenOldMixedGenderNoPrime1S1A-068$AWomenYoungSameGenderNoPrime0S1A-066$BWomenYoungSameGenderNoPrime0S1A-061$AMenOldMixedGenderNoPrime1S1A-049$AWomenYoungSameGenderNoPrime0S1A-022$BWomenYoungMixedGenderNoPrime0\n\n\nAs all variables except for the dependent variable (SUFlike) are character strings, we factorize the independent variables.\n\n# def. variables to be factorized\nvrs &lt;- c(\"ID\", \"Age\", \"Gender\", \"ConversationType\", \"Priming\")\n# def. vector with variables\nfctr &lt;- which(colnames(mblrdata) %in% vrs)     \n# factorize variables\nmblrdata[,fctr] &lt;- lapply(mblrdata[,fctr], factor)\n# relevel Age (Young = Reference)\nmblrdata$Age &lt;- relevel(mblrdata$Age, \"Young\")\n# order data by ID\nmblrdata &lt;- mblrdata %&gt;%\n  dplyr::arrange(ID)\n\n\n\nIDGenderAgeConversationTypePrimingSUFlikeS1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$AMenOldSameGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderPrime0S1A-001$BWomenOldMixedGenderNoPrime1S1A-001$BWomenOldMixedGenderNoPrime0S1A-001$BWomenOldMixedGenderNoPrime0\n\n\nBefore continuing, a few words about the minimum number of random effect levels and the minimum number of observations per random effect level are in order.\nWhile many data points per random variable level increases statistical power and thus to more robust estimates of the random effects (Austin and Leckie), it has been shown that small numbers of observations per random effect variable level do not cause serious bias and it does not negatively affect the estimates of the fixed-effects coefficients (Bell, Ferron, and Kromrey; Clarke; Clarke and Wheaton; Maas and Hox). The minimum number of observations per random effect variable level is therefore 1.\nIn simulation study, (Bell, Ferron, and Kromrey) tested the impact of random variable levels with only a single observation ranging from 0 to 70 percent. As long as there was a relatively high number of random effect variable levels (500 or more), small numbers of observations had almost no impact on bias and Type 1 error control.\nWe now plot the data to inspect the relationships within the data set.\n\nggplot(mblrdata, aes(Gender, SUFlike, color = Priming)) +\n  facet_wrap(Age~ConversationType) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position = \"top\") +\n  labs(x = \"\", y = \"Observed Probabilty of discourse like\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nThe upper left panel in the Figure above indicates that men use discourse like more frequently than women. The center right panel suggests that priming significantly increases the likelihood of discourse like being used. The center left panel suggests that speakers use discourse like more frequently in mixed-gender conversations. However, the lower right panel indicates an interaction between gender and conversation type as women appear to use discourse like less frequently in same gender conversations while the conversation type does not seem to have an effect on men. After visualizing the data, we will now turn to the model building process.\n\n\nModel Building\nAs a first step, we need to define contrasts and use the datadist function to store aspects of our variables that can be accessed later when plotting and summarizing the model. Contrasts define what and how variable levels should be compared and therefore influences how the results of the regression analysis are presented. In this case, we use treatment contrasts which are in-built. Treatment contrasts mean that we assess the significance of levels of a predictor against a baseline which is the reference level of a predictor. Field, Miles, and Field (414–27) and Gries provide very good and accessible explanations of contrasts and how to manually define contrasts if you would like to know more.\n\n# set options\noptions(contrasts  =c(\"contr.treatment\", \"contr.poly\"))\nmblrdata.dist &lt;- datadist(mblrdata)\noptions(datadist = \"mblrdata.dist\")\n\nIn a next step, we generate fixed-effects minimal base-line models and a base-line mixed-model using the glmer function with a random intercept for ID (a lmer-object of the final minimal adequate model will be created later).\n\n# baseline model glm\nm0.glm = glm(SUFlike ~ 1, family = binomial, data = mblrdata) \n# base-line mixed-model\nm0.glmer = glmer(SUFlike ~ (1|ID), data = mblrdata, family = binomial) \n\n\n\nTesting the Random Effect\nNow, we check if including the random effect is permitted by comparing the AICs from the glm to AIC from the glmer model. If the AIC of the glmer object is smaller than the AIC of the glm object, then this indicates that including random intercepts is justified.\n\naic.glmer &lt;- AIC(logLik(m0.glmer))\naic.glm &lt;- AIC(logLik(m0.glm))\naic.glmer; aic.glm\n\n[1] 1828.49227107\n\n\n[1] 1838.17334856\n\n\nThe AIC of the glmer object is smaller which shows that including the random intercepts is justified. To confirm whether the AIC reduction is sufficient for justifying the inclusion of a random-effect structure, we also test whether the mixed-effects minimal base-line model explains significantly more variance by applying a Model Likelihood Ratio Test to the fixed- and the mixed effects minimal base-line models.\n\n# test random effects\nnull.id = -2 * logLik(m0.glm) + 2 * logLik(m0.glmer)\npchisq(as.numeric(null.id), df=1, lower.tail=F) \n\n[1] 0.000631389572435\n\n# sig m0.glmer better than m0.glm\n\nThe p-value of the Model Likelihood Ratio Test is lower than .05 which shows that the inclusion of the random-effects structure is warranted. We can now continue with the model fitting process.\n\n\nModel Fitting\nThe next step is to fit the model which means that we aim to find the best model, i.e. the minimal adequate model. In this case, we will use the glmulti package to find the model with the lowest Bayesian Information Criterion (BIC) of all possible models. We add ´control = glmerControl(optimizer = “bobyqa”)´ to avoid unnecessary failures to converge.\n\n# wrapper function for linear mixed-models\nglmer.glmulti &lt;- function(formula,data, random=\"\",...){\n  glmer(paste(deparse(formula),random), \n        family = binomial, \n        data=data, \n        control=glmerControl(optimizer=\"bobyqa\"), ...)\n}\n# define formula\nform_glmulti = as.formula(paste(\"SUFlike ~ Gender + Age + ConversationType + Priming\"))\n# multi selection for glmer\nmfit &lt;- glmulti(form_glmulti,random=\"+(1 | ID)\", \n                data = mblrdata, method = \"h\", fitfunc = glmer.glmulti,\n                crit = \"bic\", intercept = TRUE, marginality = FALSE, level = 2)\n\nInitialization...\nTASK: Exhaustive screening of candidate set.\nFitting...\n\n\n\nAfter 50 models:\nBest model: SUFlike~1+Gender+ConversationType+Priming\nCrit= 1696.5877339969\nMean crit= 1753.96253323424\n\n\n\n\n\n\n\n\n\n\nAfter 100 models:\nBest model: SUFlike~1+Gender+ConversationType+Priming\nCrit= 1696.5877339969\nMean crit= 1731.89001011587\n\n\n\n\n\n\n\n\n\nCompleted.\n\n\nWe extract the best 5 models (best is here defined as the models with the lowest BIC).\n\n# extract best models\ntop &lt;- weightable(mfit)\ntop &lt;- top[1:5,]\n# inspect top 5 models\ntop\n\n                                                                                          model\n1                                             SUFlike ~ 1 + Gender + ConversationType + Priming\n2                            SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:Gender\n3 SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:Gender + Priming:ConversationType\n4                                               SUFlike ~ 1 + Gender + Priming + Priming:Gender\n5                  SUFlike ~ 1 + Gender + ConversationType + Priming + Priming:ConversationType\n            bic         weights\n1 1696.58773400 0.2567900971793\n2 1696.76989551 0.2344349735686\n3 1696.76989551 0.2344349735686\n4 1699.62465884 0.0562494681405\n5 1699.83927868 0.0505259299850\n\n\nThe best model is has the formula SUFlike ~ 1 + Gender + ConversationType + Priming and we take this to be our final minimal adequate model, i.e. the most parsimonious model (the model which explains the relatively most variance with lowest number of predictors). Hence, we define our final minimal model and check its output.\n\nmlr.glmer &lt;- glmer(SUFlike ~ (1 | ID) + Gender + ConversationType + Priming, \n                   family = binomial,\n                   control=glmerControl(optimizer=\"bobyqa\"),\n                   data = mblrdata)\n# inspect final minimal adequate model\nsummary(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n   Data: mblrdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1668.6   1696.6   -829.3   1658.6     1995 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.579360650 -0.415523754 -0.330409042 -0.312054134  3.247885856 \n\nRandom effects:\n Groups Name        Variance     Std.Dev.   \n ID     (Intercept) 0.0837517494 0.289398945\nNumber of obs: 2000, groups:  ID, 208\n\nFixed effects:\n                               Estimate   Std. Error  z value\n(Intercept)                -1.067430840  0.149184383 -7.15511\nGenderWomen                -0.642887590  0.175327287 -3.66679\nConversationTypeSameGender -0.536428857  0.148819363 -3.60456\nPrimingPrime                1.866249307  0.163249208 11.43190\n                                         Pr(&gt;|z|)    \n(Intercept)                   0.00000000000083605 ***\nGenderWomen                            0.00024562 ***\nConversationTypeSameGender             0.00031268 ***\nPrimingPrime               &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe now test whether the final minimal model performs significantly better than the minimal base-line model, and print the regression summary.\n\n# final model better than base-line model\nsigfit &lt;- anova(mlr.glmer, m0.glmer, test = \"Chi\") \n# inspect\nsigfit\n\nData: mblrdata\nModels:\nm0.glmer: SUFlike ~ (1 | ID)\nmlr.glmer: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n          npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.glmer     2 1828.492271 1839.694076 -912.2461355 1824.492271             \nmlr.glmer    5 1668.583222 1696.587734 -829.2916108 1658.583222 165.90905  3\n                      Pr(&gt;Chisq)    \nm0.glmer                            \nmlr.glmer &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# inspect final minimal adequate model\nprint(mlr.glmer, corr = F)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SUFlike ~ (1 | ID) + Gender + ConversationType + Priming\n   Data: mblrdata\n      AIC       BIC    logLik  deviance  df.resid \n1668.5832 1696.5877 -829.2916 1658.5832      1995 \nRandom effects:\n Groups Name        Std.Dev.   \n ID     (Intercept) 0.289398945\nNumber of obs: 2000, groups:  ID, 208\nFixed Effects:\n               (Intercept)                 GenderWomen  \n              -1.067430840                -0.642887590  \nConversationTypeSameGender                PrimingPrime  \n              -0.536428857                 1.866249307  \n\n\n\n\nVisualizing Effects\nWe visualize the effects here by showing the probability of discourse like based on the predicted values.\n\nplot_model(mlr.glmer, type = \"pred\", terms = c(\"Gender\", \"Priming\", \"ConversationType\"))\n\n\n\n\n\n\n\n\nWe can see that discourse like is more likely to surface in primed contexts and among males. In conversations with both men and women, speakers use discourse like slightly less than in mixed conversations.\n\n\nExtracting Model Fit Parameters\nWe now extract model fit parameters (Baayen, 281).\n\nlibrary(Hmisc) # Ensure the package is loaded\nprobs = 1/(1+exp(-fitted(mlr.glmer)))\nprobs = binomial()$linkinv(fitted(mlr.glmer))\nsomers2(probs, as.numeric(mblrdata$SUFlike))\n\n                C               Dxy                 n           Missing \n   0.760226203516    0.520452407033 2000.000000000000    0.000000000000 \n\n\nThe two lines that start with probs are simply two different ways to do the same thing (you only need one of these).\nThe model fit parameters indicate a suboptimal fit. Both the C-value and Somers’s Dxy show poor fit between predicted and observed occurrences of discourse like. If the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity (Baayen, 204). Somers’ Dxy is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ Dxy values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction (Baayen, 204). The C.value of 0.760226203516 suggests that the model has some predictive and explanatory power, but not at an optimal level. We will now perform the model diagnostics.\n\n\nModel Diagnostics\nWe begin the model diagnostics by generating a diagnostic that plots the fitted or predicted values against the residuals.\n\nplot(mlr.glmer, pch = 20, col = \"black\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nAs a final step, we summarize our findings in tabulated form.\n\n# summarize final model\nsjPlot::tab_model(mlr.glmer)\n\n\n\n \nSUFlike\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.34\n0.26 – 0.46\n&lt;0.001\n\n\nGender [Women]\n0.53\n0.37 – 0.74\n&lt;0.001\n\n\nConversationType[SameGender]\n0.58\n0.44 – 0.78\n&lt;0.001\n\n\nPriming [Prime]\n6.46\n4.69 – 8.90\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 ID\n0.08\n\n\nICC\n0.02\n\n\nN ID\n208\n\nObservations\n2000\n\n\nMarginal R2 / Conditional R2\n0.131 / 0.152\n\n\n\n\n\n\n\nWe can use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(mlr.glmer)\n\nWe fitted a logistic mixed model (estimated using ML and BOBYQA optimizer) to\npredict SUFlike with Gender, ConversationType and Priming (formula: SUFlike ~\nGender + ConversationType + Priming). The model included ID as random effect\n(formula: ~1 | ID). The model's total explanatory power is moderate\n(conditional R2 = 0.15) and the part related to the fixed effects alone\n(marginal R2) is of 0.13. The model's intercept, corresponding to Gender = Men,\nConversationType = MixedGender and Priming = NoPrime, is at -1.07 (95% CI\n[-1.36, -0.78], p &lt; .001). Within this model:\n\n  - The effect of Gender [Women] is statistically significant and negative (beta\n= -0.64, 95% CI [-0.99, -0.30], p &lt; .001; Std. beta = -0.64, 95% CI [-0.99,\n-0.30])\n  - The effect of ConversationType [SameGender] is statistically significant and\nnegative (beta = -0.54, 95% CI [-0.83, -0.24], p &lt; .001; Std. beta = -0.54, 95%\nCI [-0.83, -0.24])\n  - The effect of Priming [Prime] is statistically significant and positive (beta\n= 1.87, 95% CI [1.55, 2.19], p &lt; .001; Std. beta = 1.87, 95% CI [1.55, 2.19])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n\nWe can use this output to write up a final report:\nWe can use this output to write up a final report:\nWe fitted a logistic mixed model to predict the use of discourse like. The model included speakers as random effect (formula: ~1 | ID). The model’s total explanatory power is moderate (conditional R2 = 0.15) and the part related to the fixed effects alone (marginal R2) is of 0.13.\nRegarding fixed effects, the model reported that * women use discourse like statistically less compared to men (beta = -0.64 [-0.99, -0.30], p &lt; .001; Std. beta = -0.64 [-0.99, -0.30]) * speakers in conversations with other speakers of the same gender use discourse like significantly less compared to thier use in mixed-gender conversations (beta = -0.54 [-0.83, -0.24], p &lt; .001; Std. beta = -0.54 [-0.83, -0.24]) * Priming is significantly positively correlated with the use of discourse like (beta = 1.87 [1.55, 2.19], p &lt; .001; Std. beta = 1.87 [1.55, 2.19])"
  },
  {
    "objectID": "tutorials/regression/regression.html#mixed-effects-quasi-poisson-and-negative-binomial-regression",
    "href": "tutorials/regression/regression.html#mixed-effects-quasi-poisson-and-negative-binomial-regression",
    "title": "Introduction",
    "section": "Mixed-Effects (Quasi-)Poisson and Negative-Binomial Regression",
    "text": "Mixed-Effects (Quasi-)Poisson and Negative-Binomial Regression\nLike fixed-effects Poisson models, mixed-effects Poisson models take counts as dependent variables. The data for this analysis was collected on three separate evenings (Trial). The number of the filler uhm (UHM) was counted in two-minute conversations that were either in English, German, Russian, or Mandarin (Language). In addition, the number of shots that speakers drank before they talked was recorded (Shots).\n\n# load data\ncountdata  &lt;- base::readRDS(\"tutorials/regression/data/cld.rda\", \"rb\")\n\n\n# inspect data\ncountdata %&gt;%\n  as.data.frame() %&gt;%\n  head(15) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %&gt;%\n  flextable::border_outer()\n\nIDTrialLanguageGenderUHMShots13RussianMan0023RussianMan0033GermanMan0541GermanMan0351GermanWoman2663GermanMan1571MandarinMan1183GermanWoman0493RussianWoman00102GermanMan02113RussianMan01122GermanMan01133RussianWoman01142RussianWoman44152EnglishMan04\n\n\nSince the data contains character variables, we need to factorize the data before we can analyse it further and we also remove the ID column.\n\n# factorize variables\ncountdata &lt;- countdata %&gt;%\n  dplyr::select(-ID) %&gt;%\n  dplyr::mutate_if(is.character, factor)\n\n\n# inspect data\ncountdata %&gt;%\n  as.data.frame() %&gt;%\n  head(15) %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::set_caption(caption = \"First 15 rows of the countdata data.\")  %&gt;%\n  flextable::border_outer()\n\nTrialLanguageGenderUHMShots3RussianMan003RussianMan003GermanMan051GermanMan031GermanWoman263GermanMan151MandarinMan113GermanWoman043RussianWoman002GermanMan023RussianMan012GermanMan013RussianWoman012RussianWoman442EnglishMan04\n\n\nAfter the data is factorized, we can visualize the data.\n\ncountdata %&gt;%\n  # prepare data\n  dplyr::select(Language, Shots) %&gt;%\n  dplyr::group_by(Language) %&gt;%\n  dplyr::mutate(Mean = round(mean(Shots), 1)) %&gt;%\n  dplyr::mutate(SD = round(sd(Shots), 1)) %&gt;%\n  # start plot\n  ggplot(aes(Language, Shots, color = Language, fill = Language)) +\n  geom_violin(trim=FALSE, color = \"gray20\")+ \n  geom_boxplot(width=0.1, fill=\"white\", color = \"gray20\") +\n  geom_text(aes(y=-4,label=paste(\"mean: \", Mean, sep = \"\")), size = 3, color = \"black\") +\n  geom_text(aes(y=-5,label=paste(\"SD: \", SD, sep = \"\")), size = 3, color = \"black\") +\n  scale_fill_manual(values=rep(\"grey90\",4)) + \n  theme_set(theme_bw(base_size = 10)) +\n  theme(legend.position=\"none\", legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) + \n  ylim(-5, 15) +\n  labs(x = \"Language\", y = \"Shots\")\n\n\n\n\n\n\n\n\nThe violin plots show that the English speakers drank more shots than speakers of other languages with Mandarin speakers drinking the fewest shots.\nIn the present case, we will a Boruta variable selection procedure to streamline the model fitting process. Thus, before fitting the model, we will test which variables have any kind of relationship with the dependent variable and therefore deserve to be evaluated in the regression modeling. As this is just an example, we will only consider variables which are deemed important and disregard both unimportant and tentative variables. We start the Boruta analysis by setting a seed and running an initial Boruta analysis.\n\n# perform variable selection\nset.seed(20191220)\nboruta &lt;- Boruta(UHM ~.,data=countdata)\nprint(boruta)\n\nBoruta performed 99 iterations in 2.85972809792 secs.\n 2 attributes confirmed important: Language, Shots;\n 1 attributes confirmed unimportant: Gender;\n 1 tentative attributes left: Trial;\n\n\nAs only Shots is confirmed as important, we will only check for the effect of Shots and include Language as a random effect in the regression modeling. Including Language as a random effect is probably not justified statistically (given that the Boruta analysis showed that it only has a tentative effect) but for theoretical reasons as the speakers are nested into Languages. Before we start with the modeling, however, we proceed by checking if the data does indeed approximate a Poisson distribution.\n\n# output the results\ngf = goodfit(countdata$UHM,type= \"poisson\", method= \"ML\")\nplot(gf, main=\"Count data vs Poisson distribution\")\n\n\n\n\n\n\n\n\nThe data does not perfectly match a distribution that would be expected if the data approximated a Poisson distribution. We will use a goodness-of-fit test to check if the data does indeed diverge significantly from being Poisson distributed. If the p-values of the goodness-of-fit test is smaller than .05, then the distribution of the data differs significantly from a Poisson distribution and, given the visualization is likely over-dispersed.\nIn case of overdispersion, we may have to use a quasi-Poisson or, even better, a negative binomial model but we will, for now continue with the Poisson model and perform diagnostics later to check if we have to switch to a more robust method. One effect of overdispersion is that the standard errors of a model are biased and quasi-Poisson models scale the standard errors to compensate bias. However, Zuur, Hilbe, and Ieno suggest to use negative-binomial model instead. This is so because the scaling of the standard errors performed by quasi-Poisson models only affects the significance of coefficients (the p-values) but it does not affect the coefficients which, however, may be affected themselves by overdispersion. Thus, the coefficients of Poisson as well as quasi-Poisson models (which are identical) may be unreliable when dealing with overdispersion. Negative binomial models, in contrast, include an additional dispersion or heterogeneity parameter which accommodates overdispersion better than merely scaling the standard errors (see Zuur, Hilbe, and Ieno, 21).\n\nsummary(gf)\n\n\n     Goodness-of-fit test for poisson distribution\n\n                           X^2 df                                     P(&gt; X^2)\nLikelihood Ratio 153.422771085  5 0.000000000000000000000000000000249336691328\n\n\nThe p-value is indeed smaller than .05 which means that we should indeed use a negative-binomial model rather than a Poisson model. We will ignore this, for now, and proceed to fit a Poisson mixed-effects model and check what happens if a Poisson model is fit to over-dispersed data.\n\nMixed-Effects Poisson Regression\nIn a first step, we create mixed-effect intercept-only baseline models and then test if including “Shots” significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.glmer = glmer(UHM ~ 1 + (1 | Language), data = countdata, family = poisson,\n                 control=glmerControl(optimizer=\"bobyqa\"))\n# add Shots\nm1.glmer &lt;- update(m0.glmer, .~.+ Shots)\nAnova(m1.glmer, test = \"Chi\")           \n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: UHM\n       Chisq Df             Pr(&gt;Chisq)    \nShots 321.25  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. However, we get the warning that the fitted mixed model is (almost / near) singular. In such cases, the model should not be reported. As this is only an example, we will continue by having a look at the model summary.\n\nsummary(m1.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: UHM ~ (1 | Language) + Shots\n   Data: countdata\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1041.8   1054.5   -517.9   1035.8      497 \n\nScaled residuals: \n         Min           1Q       Median           3Q          Max \n-1.509633852 -0.666423093 -0.592950422  0.586114082  4.338639382 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Language (Intercept) 0        0       \nNumber of obs: 500, groups:  Language, 4\n\nFixed effects:\n                 Estimate    Std. Error   z value               Pr(&gt;|z|)    \n(Intercept) -1.2789168850  0.0893313003 -14.31656 &lt; 0.000000000000000222 ***\nShots        0.2336279071  0.0130347632  17.92345 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nShots -0.806\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe model summary confirms that the number of shots does have a significantly positive effect on the number of occurrences of uhm. Furthermore, the scaled residuals are distributed very unevenly which suggests overdispersion. Including Language as a random effect is not justified given that they have 0 variance and a standard deviation of 0 (which means that Language does not account for or explain any additional variance).\nWe now check if the model suffers from overdispersion following Zuur, Hilbe, and Ieno (138).\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.glmer, type = \"pearson\")\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors &lt;- length(fixef(m1.glmer)) +1\n# calculate overdispersion\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.16901460967\n\n\nThe data is slightly over-dispersed. It would also be advisable to plot the Cook’s distance (which should not show data points with values &gt; 1). If there are data points with high Cook’s D values, we could exclude them which would, very likely reduce the overdispersion (see Zuur, Hilbe, and Ieno, 22). We ignore this, for now, and use diagnostic plots to check if the plots indicate problems.\n\ndiag_data &lt;- data.frame(PearsonResiduals, fitted(m1.glmer)) %&gt;%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 &lt;- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 &lt;- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 &lt;- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngridExtra::grid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\n\n\n\n\nThe diagnostic plots show problems as the dots in the first two plots are not random but show a pattern in the lower left corner. In addition, the variance of English (left boxplot) is notable larger than the variance of Russian (right boxplot). As a final step, we plot the predicted vales of the model to check if the predictions make sense.\n\nplot_model(m1.glmer, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe model predicts that the instances of uhm increase with the number of shots. Note that the increase is not homogeneous as the y-axis labels indicate! We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.glmer, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations (overdispersion measure &gt; 1 and excessive multicollinearity (singular fit)) had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nThe summary of the model can be extracted using the tab_model function from the sjPlot package (Lüdecke).\n\nsjPlot::tab_model(m1.glmer)\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.33\n&lt;0.001\n\n\nShots\n1.26\n1.23 – 1.30\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\n\nr.squaredGLMM(m1.glmer)\n\n                     R2m            R2c\ndelta     0.208910675833 0.208910675833\nlognormal 0.294971461467 0.294971461467\ntrigamma  0.120419615931 0.120419615931\n\n\nAlso note that our model suffers from a serious problem (near singular fit). If this were not just an example, you should not(!) report this model!\n\n\nMixed-Effects Quasi-Possion Regression\nThe Quasi-Poisson Regression is a generalization of the Poisson regression and is used when modeling an overdispersed count variable. Poisson models are based on the Poisson distribution which is defined as a distribution where the variance is equal to the mean (which is very restrictive and not often the case). Quasi-Poisson models scale the standard errors which has a positive effect when dealing with overdispersed data.\nTherefore, when the variance is greater than the mean, a Quasi-Poisson model, which assumes that the variance is a linear function of the mean, is more appropriate as it handles over-dispersed data better than normal Poisson-models.\nWe begin the model fitting process by creating a mixed- and a fixed-effects intercept-only base-line model. Unfortunately, there is not yet a procedure in place for quasi-Poisson models to test if the inclusion of random effects is justified. However, here the Boruta also provides valuable information: Language was only considered tentative but not important which suggests that it will not explain variance which means that including Language as a random effect may not be justified. This would require further inspection. Because we are only dealing with an example here, we ignore this fact (which you should not do in proper analyses) and continue right away with adding shots.\n\n# base-line mixed-model\nm0.qp = glmmPQL(UHM ~ 1, random = ~ 1 | Language, data = countdata, \n                   family = quasipoisson(link='log'))\n\niteration 1\n\n\niteration 2\n\n\niteration 3\n\n\niteration 4\n\n# add Shots\nm1.qp &lt;- update(m0.qp, .~.+ Shots)\n\niteration 1\n\nAnova(m1.qp, test = \"Chi\")           # SIG! (p&lt;0.0000000000000002 ***)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: zz\n         Chisq Df             Pr(&gt;Chisq)    \nShots 276.4523  1 &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA confirms that Shots have a significant impact on the number of instances of uhm. We will now have a look at the model summary.\n\nsummary(m1.qp)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: countdata \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 | Language\n               (Intercept)      Residual\nStdDev: 0.0000407429998994 1.07801257915\n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  UHM ~ Shots \n                      Value       Std.Error  DF        t-value p-value\n(Intercept) -1.278929721022 0.0964886326622 495 -13.2547190870       0\nShots        0.233630231741 0.0140795660798 495  16.5935676154       0\n Correlation: \n      (Intr)\nShots -0.806\n\nStandardized Within-Group Residuals:\n            Min              Q1             Med              Q3             Max \n-1.400417640410 -0.618228066947 -0.550071160975  0.543731905020  4.024828853743 \n\nNumber of Observations: 500\nNumber of Groups: 4 \n\n\nThe model summary does not provide much information such as,e.g. AIC or BIC values. The coefficient for Shots is highly significant (p &lt;.001) and the data is notably over-dispersed (the Standardized Within-Group Residuals deviate substantially from a normal distribution with higher values having a thick tail). Also, in contrast to the Poisson model, Language does explain at least a minimal share of the variance now as the mean and standard deviation are no longer 0. Note also, that the coefficients are identical to the Poisson coefficients but the standard errors and p-values differ (the model provides t- rather than z-values).\nIn a next step, we will calculate the odds ratios of the coefficient (as we only have one). We will use the coefficients from the fixed-effects model as the coefficients for mixed- and fixed-effects models are identical (the random effect structure only affects the standard error and p-values but not the coefficients; you can check by uncommenting the summary command).\n\nm1.glm = glm(UHM ~ Shots, data = countdata, family = quasipoisson(link='log'))\nexp(coef(m1.glm))\n\n   (Intercept)          Shots \n0.278338612235 1.263174385573 \n\n\nThe standardized or \\(\\beta\\)-coefficient tells us that the likelihood of uhm increases by 1.26 (or 26.32 percent) with each additional shot.\nBefore inspecting the relationship between Shots and uhm, we will check if the overdispersion was reduced.\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.qp, type = \"pearson\")\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# extract number of predictors (plus intercept)\nNumberOfPredictors &lt;- length(fixef(m1.qp)) +1\n# calculate overdispersion\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases-NumberOfPredictors)\n# inspect overdispersion\nOverdispersion\n\n[1] 1.00603621722\n\n\nThe overdispersion has indeed decreased and is not so close to 1 that overdispersion is no longer an issue.\nWe continue to diagnose the model by plotting the Pearson’s residuals against fitted values. This diagnostic plot should not show a funnel-like structure or patterning as we observed in the case of the Poisson model.\n\n# diagnostic plot\nplot(m1.qp, pch = 20, col = \"black\", lty= \"dotted\", ylab = \"Pearson's residuals\")\n\n\n\n\n\n\n\n\nIndeed, the plot exhibits a (slight) funnel shape (but not drastically so) and thus indicates heteroscedasticity. However, the patterning that we observed with the Poisson model has disappeared. We continue by plotting the random effect adjustments.\n\n# generate diagnostic plots\nplot(m1.qp, Language ~ resid(.), abline = 0, fill = \"gray70\") \n\n\n\n\n\n\n\n\nThe adjustments by “Language” are marginal (which was somewhat expected given that Language was only deemed tentative), which shows that there is very little variation between the languages and that we have no statistical reason to include Language as a random effect.\nIn a final step, we plot the fixed-effect of Shots using the predictorEffects function from the effects package (Fox and Weisberg 2019).\n\nplot_model(m1.qp, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe effects plot shows that the number of uhms increases exponentially with the number of shots a speaker has had. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.qp, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nGiven that the overdispersion measure of this Quasi-Poisson model is close to 1, that the model did not suffer from excessive multicollinearity (singular fit), and because this model shows improvements compared to the Poisson model with respect to the model diagnostics (some adjustments by Language and less patterning in the diagnostic plots), we would choose this quasi-Poisson model over the Poisson model.\nFinally, we extract the summary table of this model.\n\nsjPlot::tab_model(m1.qp)\n\n\n\n \nUHM\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.28\n0.23 – 0.34\n&lt;0.001\n\n\nShots\n1.26\n1.23 – 1.30\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.00\n\n\n\nτ00 Language\n0.00\n\n\nN Language\n4\n\nObservations\n500\n\n\nMarginal R2 / Conditional R2\n1.000 / NA\n\n\n\n\n\n\n\n\n\n\n\nNOTEThe R2 values of the summary table are incorrect (as indicated by the missing conditional R2 value). The more appropriate conditional and marginal coefficient of determination for generalized mixed-effect models can be extracted using the r.squaredGLMM function from the MuMIn package (Barton 2020).\n\n\n\n\n\n\n\nr.squaredGLMM(m1.qp)\n\n                      R2m             R2c\ndelta     0.1856941875702 0.1856941884278\nlognormal 0.2752763976645 0.2752763989357\ntrigamma  0.0977040660495 0.0977040665007\n\n\n\n\nMixed-Effects Negative Binomial Regression\nNegative binomial regression models are a generalization of Poisson regression which loosens the restrictive assumption that the variance is equal to the mean made by the Poisson model. This is a major advantage as the most common issue that one faces with Poisson regressions is that the data deviate too substantially from the assumed Poisson distribution.\nTo implement a Negative-Binomial Mixed-Effects Regression, we first create a mixed-effects intercept-only baseline model and then test if including Shots significantly improves model fit and, thus, has a significant impact on the number of uhms.\n\n# base-line mixed-model\nm0.nb = glmer.nb(UHM ~ 1 + (1 | Language), data = countdata)\n# add Shots\nm1.nb &lt;- update(m0.nb, .~.+ Shots)\n\nboundary (singular) fit: see help('isSingular')\n\nanova(m1.nb, m0.nb)           \n\nData: countdata\nModels:\nm0.nb: UHM ~ 1 + (1 | Language)\nm1.nb: UHM ~ (1 | Language) + Shots\n      npar         AIC         BIC       logLik    deviance     Chisq Df\nm0.nb    3 1159.000894 1171.644718 -576.5004470 1153.000894             \nm1.nb    4 1051.593288 1068.451721 -521.7966442 1043.593288 109.40761  1\n                  Pr(&gt;Chisq)    \nm0.nb                           \nm1.nb &lt; 0.000000000000000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe negative-binomial model also reports a significant impact of shots on the number of uhms. In a next step, we calculate the overdispersion.\n\n# extract pearson residuals\nPearsonResiduals &lt;- resid(m1.nb, type = \"pearson\")\n# extract number of betas + predictors + sigma\nNumberOfPredictors &lt;- 2+1+1\n# extract number of cases in model\nCases &lt;- nrow(countdata)\n# calculate overdispersion parameter\nOverdispersion &lt;- sum(PearsonResiduals^2) / (Cases / NumberOfPredictors)# show overdispersion parameter\nOverdispersion\n\n[1] 2.46949245769\n\n\nThe overdispersion has increased which is rather suboptimal. In this case, we would report the Quasi-Poisson Regression rather than the Negative Binomial Model (which is rather rare as Negative Binomial Models typically perform better than (Quasi-)Poisson models. However, this tutorial focuses merely on how to implement a Negative Binomial Mixed-Effects Regression and we thus continue with generating diagnostic plots to check for problems.\n\ndiag_data &lt;- data.frame(PearsonResiduals, fitted(m1.nb)) %&gt;%\n  dplyr::rename(Pearson = 1,\n                Fitted = 2)\np9 &lt;- ggplot(diag_data, aes(x = Fitted, y = Pearson)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\")\np10 &lt;- ggplot(countdata, aes(x = Shots, y = diag_data$Pearson)) +\n  geom_point()  +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  labs(y = \"Pearson\")\np11 &lt;- ggplot(countdata, aes(x = Language, y = diag_data$Pearson)) +\n  geom_boxplot() +\n  labs(y = \"Pearson\") + \n  theme(axis.text.x = element_text(angle=90))\ngridExtra::grid.arrange(p9, p10, p11, nrow = 1)\n\n\n\n\n\n\n\n\nThe diagnostics show patterning similar to the one we saw with the Poisson model which suggest that the negative binomial model is also not an optimal model for our data. We continue by plotting the predicted values and, subsequently, summarize the analysis.\n\nplot_model(m1.nb, type = \"pred\", terms = c(\"Shots\"))\n\n\n\n\n\n\n\n\nThe effect plot shows that the predicted number of shots increases exponentially with each shot. We now compare the predicted number of uhm with the actually observed instances of uhm to check if the results of the model make sense.\n\ncountdata %&gt;%\n  mutate(Predicted = predict(m1.nb, type = \"response\")) %&gt;%\n  dplyr::rename(Observed = UHM) %&gt;%\n  tidyr::gather(Type, Frequency, c(Observed, Predicted)) %&gt;%\n  dplyr::mutate(Shots = factor(Shots),\n                Type = factor(Type)) %&gt;%\n  dplyr::group_by(Shots, Type) %&gt;%\n  dplyr::summarize(Frequency = mean(Frequency)) %&gt;%\n  ggplot(aes(Shots, Frequency, group = Type, color = Type, linetype = Type)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"orange\", \"lightblue\")) \n\n\n\n\n\n\n\n\nThe comparison between the observed and the predicted uses of uhm becomes somewhat volatile and shows fluctuations after eight shots. We will now summarize the results as if the violations had NOT occurred(!) - again: this is only because we are practicing here - this would be absolutely unacceptable in a proper write-up of an analysis!\nA mixed-effect negative binomial regression model which contained the language in which the conversation took place as random effect was fit to the data. Prior to the regression modeling, a Boruta analysis was applied to determine whether any of the predictors had a meaningful relationship with the dependent variable (instances of uhm). Since the Boruta analysis indicated that only the number of shots a speaker had was important, only “Shots” was tested during model fitting. The final minimal adequate model showed that the number of uhm as fillers increases significantly, and near-linearly with the number of shots speakers had (\\(\\chi\\)2(1):83.0, p &lt;.0001, \\(\\beta\\): 0.2782). An inspection of the random effect structure conveyed that there was almost no variability between languages and language did not contribute meaningfully to the model fit."
  },
  {
    "objectID": "tutorials/regression/regression.html#mixed-effects-multinomial-regression",
    "href": "tutorials/regression/regression.html#mixed-effects-multinomial-regression",
    "title": "Introduction",
    "section": "Mixed-Effects Multinomial Regression",
    "text": "Mixed-Effects Multinomial Regression\nIn this section, we will focus on how to implement a mixed-effects multinomial regression model using the mblogit function from the mclogit package (see Elff 2021). As we have already gone though model fitting and model validation procedures above, we will strictly see how to implement this type of model here - we will not go through all the other steps that a proper regression analysis would require.\nWe begin the analysis by loading the example data set. The data represents observations gathered during an experiment where speakers had to report what they saw. The responses are categorized into four groups:\n\n# description data\npict  &lt;- base::readRDS(\"tutorials/regression/data/pict.rda\", \"rb\")\n# inspect\nhead(pict)\n\n  Id Participant       Group Item     Response Gender Age\n1  1        G001 German_Mono    1  NumeralNoun   Male  18\n2  2        G002 German_Mono    3  NumeralNoun   Male  18\n3  3        G003 German_Mono    4  NumeralNoun   Male  18\n4  4        G004 German_Mono    6 QuantAdjNoun   Male  18\n5  5        G005 German_Mono    8  NumeralNoun   Male  18\n6  6        G006 German_Mono    9 QuantAdjNoun   Male  18\n\n\nIn a first step, we generate a baseline model that we call m0. This model only contains the random effect structure and the intercept as the sole predictor.\n\nm0.mn &lt;- mblogit(formula = Response ~ 1, \n              random = ~ 1 | Participant, \n              data = pict)\n\n\nIteration 1 - deviance = 2452.16571809 - criterion = 0.840346812509\n\n\n\nIteration 2 - deviance = 2412.75547122 - criterion = 0.0646013613319\n\n\n\nIteration 3 - deviance = 2411.26894466 - criterion = 0.00534731178597\n\n\n\nIteration 4 - deviance = 2411.2582473 - criterion = 0.0000446387422471\n\n\n\nIteration 5 - deviance = 2411.25823534 - criterion = 0.0000000031405953873\nconverged\n\n\nIn this case, the algorithm did not converge properly - if this were a real analysis, we could not simply continue but would have to inspect possible causes for this. However, as this is just a showcase, we will ignore this and move on. Next, we add the fixed effects (Gender and Group).\n\nm1.mn &lt;- mblogit(formula = Response ~ Gender + Group, \n              random = ~ 1 | Item, \n              data = pict)\n\n\nIteration 1 - deviance = 1652.54499424 - criterion = 0.800708039017\n\n\n\nIteration 2 - deviance = 1571.22116522 - criterion = 0.0814899820197\n\n\n\nIteration 3 - deviance = 1551.91023391 - criterion = 0.0274469746195\n\n\n\nIteration 4 - deviance = 1547.29823345 - criterion = 0.00512667078353\n\n\n\nIteration 5 - deviance = 1546.02170442 - criterion = 0.000173543402508\n\n\n\nIteration 6 - deviance = 1545.8333116 - criterion = 0.000000244926398008\n\n\n\nIteration 7 - deviance = 1545.82769259 - criterion = 0.000000000000785569845713\nconverged\n\n\nNow, we can compare the models to see if including the fixed-effects into the model has significantly improved the model fit.\n\nanova(m0.mn, m1.mn)\n\nAnalysis of Deviance Table\n\nModel 1: Response ~ 1\nModel 2: Response ~ Gender + Group\n  Resid. Df  Resid. Dev Df    Deviance\n1      3261 2411.258235               \n2      3249 1545.827693 12 865.4305427\n\n\nAs the second model is significantly better, we are justified to believe that our fixed effects have explanatory power. We can now use the getSummary.mmblogit function to get a summary of the model with the fixed effects.\n\n# inspect\nmclogit::getSummary.mmblogit(m1.mn)\n\n$coef\n, , NumeralNoun/BareNoun\n\n                                  est             se             stat\n(Intercept)           1.0940536961694 0.795348462841   1.375565236225\nGenderMale            0.0691676134585 0.196851703842   0.351369137826\nGroupGerman_Mono     -3.2909055852599 0.304213383695 -10.817754121410\nGroupL2_Advanced     -0.4575232522124 0.307505930002  -1.487851802435\nGroupL2_Intermediate -1.1689603301872 0.320486374900  -3.647457183015\n                                                            p             lwr\n(Intercept)          0.16895627569603502426964780624984996393 -0.464800646159\nGenderMale           0.72531143261552322165641726314788684249 -0.316654636367\nGroupGerman_Mono     0.00000000000000000000000000283642762786 -3.887152860918\nGroupL2_Advanced     0.13678998075957157776194605958153260872 -1.060223800048\nGroupL2_Intermediate 0.00026484842867640642034496312184899125 -1.797102082527\n                                 upr\n(Intercept)           2.652908038498\nGenderMale            0.454989863284\nGroupGerman_Mono     -2.694658309602\nGroupL2_Advanced      0.145177295623\nGroupL2_Intermediate -0.540818577848\n\n, , QuantAdjNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)           0.595495714810 0.498285332593   1.19508979265\nGenderMale            0.299894756058 0.248324126895   1.20767466218\nGroupGerman_Mono     -3.856907426426 0.376376472714 -10.24747216162\nGroupL2_Advanced     -2.194953767755 0.347242020175  -6.32110643363\nGroupL2_Intermediate -3.016728204963 0.398140344234  -7.57704726148\n                                                         p             lwr\n(Intercept)          0.23205194960832503658920700218004640 -0.381125591097\nGenderMale           0.22717242789340932884734058916365029 -0.186811589147\nGroupGerman_Mono     0.00000000000000000000000121478955842 -4.594591757573\nGroupL2_Advanced     0.00000000025969706979146851136980930 -2.875535621216\nGroupL2_Intermediate 0.00000000000003535080320338597665829 -3.797068940455\n                                 upr\n(Intercept)           1.572117020716\nGenderMale            0.786601101264\nGroupGerman_Mono     -3.119223095278\nGroupL2_Advanced     -1.514371914293\nGroupL2_Intermediate -2.236387469472\n\n, , QuantNoun/BareNoun\n\n                                 est             se            stat\n(Intercept)          -2.163121210173 0.753539267072 -2.870615115491\nGenderMale           -0.371289871064 0.430045391060 -0.863373678181\nGroupGerman_Mono     -2.420925019720 0.813762214781 -2.974978409844\nGroupL2_Advanced      0.789609605279 0.683753160717  1.154816753535\nGroupL2_Intermediate -0.148789198300 0.739325816561 -0.201249834602\n                                    p             lwr             upr\n(Intercept)          0.00409674000335 -3.640031034571 -0.686211385775\nGenderMale           0.38793204670759 -1.214163349258  0.471583607130\nGroupGerman_Mono     0.00293009169381 -4.015869652670 -0.825980386770\nGroupL2_Advanced     0.24816547492527 -0.550521964042  2.129741174601\nGroupL2_Intermediate 0.84050322615521 -1.597841171600  1.300262775001\n\n\n$Item\n, , 1\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)   5.46019439094 26.1766792826   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) -1.63804668966           NaN   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     1.63405993830 11.5842291808   NA NA  NA  NA\n\n, , 2\n\n                                              est            se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  -1.63804668966           NaN   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1)  1.60628957168           NaN   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)     0.49903254341 2.36453656619   NA NA  NA  NA\n\n, , 3\n\n                                             est             se stat  p lwr upr\nNumeralNoun/BareNoun: VCov(~1,~1)  1.63405993830 11.58422918076   NA NA  NA  NA\nQuantAdjNoun/BareNoun: VCov(~1,~1) 0.49903254341  2.36453656619   NA NA  NA  NA\nQuantNoun/BareNoun: VCov(~1,~1)    1.51237743196            NaN   NA NA  NA  NA\n\n\n$Groups\nGroups by Item \n            10 \n\n$sumstat\n               LR                df          deviance          McFadden \n1476.294014648252   21.000000000000 1545.827692593037    0.488495883905 \n        Cox.Snell        Nagelkerke               AIC               BIC \n   0.741897420618    0.791357248659 1587.827692593037 1692.700285072724 \n                N \n1090.000000000000 \n\n$call\nmblogit(formula = Response ~ Gender + Group, data = pict, random = ~1 | \n    Item)\n\n$contrasts\n$contrasts$Gender\n[1] \"contr.treatment\"\n\n$contrasts$Group\n[1] \"contr.treatment\"\n\n\n$xlevels\n$xlevels$Gender\n[1] \"Female\" \"Male\"  \n\n$xlevels$Group\n[1] \"English_Mono\"    \"German_Mono\"     \"L2_Advanced\"     \"L2_Intermediate\"\n\n\nThe NAs (not available information) is a result of the model having a bad fit to the data and, optimally, we would need to inspect why the model has a bad fit. Again, we ignore this and move on. Next, we check the VIFs to see if the model does not violate multicollinearity assumptions.\n\ncar::vif(m1.mn) # maybe use cut-off of 5 (maybe 10)\n\n                GVIF Df GVIF^(1/(2*Df))\nGender 3.59671433201  1   1.89650054891\nGroup  9.94593851425  3   1.46647375519\n\n\nThe VIFs are a bit high - especially the GVIF for Group would be a cause for concern if this was not just a demo analysis! However, as we only want to implement a multinomial mixed-effects model here and not provide a proper, clean analysis, we will ignore this issue here.\nIn a next step, we visualize effects to get a better understanding of how the predictors that are part of the fixed-effect structure of the mode affect the outcome (the response variable).\n\nsjPlot::plot_model(m1.mn)\n\n\n\n\n\n\n\n\nFinally, we can extract an alternative summary table produced by the tab_model function from the sjPlot package (see Lüdecke).\n\nsjPlot::tab_model(m1.mn)\n\n\n\n \nResponse: NumeralNoun\nResponse: QuantAdjNoun\nResponse: QuantNoun\n\n\nPredictors\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\nOdds Ratios\nCI\np\n\n\n(Intercept)\n2.99\n0.63 – 14.20\n0.169\n1.81\n0.68 – 4.82\n0.232\n0.11\n0.03 – 0.50\n0.004\n\n\nGenderMale\n1.07\n0.73 – 1.58\n0.725\n1.35\n0.83 – 2.20\n0.227\n0.69\n0.30 – 1.60\n0.388\n\n\nGroupGerman_Mono\n0.04\n0.02 – 0.07\n&lt;0.001\n0.02\n0.01 – 0.04\n&lt;0.001\n0.09\n0.02 – 0.44\n0.003\n\n\nGroupL2_Advanced\n0.63\n0.35 – 1.16\n0.137\n0.11\n0.06 – 0.22\n&lt;0.001\n2.20\n0.58 – 8.42\n0.248\n\n\nGroupL2_Intermediate\n0.31\n0.17 – 0.58\n&lt;0.001\n0.05\n0.02 – 0.11\n&lt;0.001\n0.86\n0.20 – 3.67\n0.841\n\n\n\nN Item\n10\n\nObservations\n1090\n\n\n\n\n\n\n\nThis is the final step in implementing a a mixed-effects multinomial regression model using the mblogit function from the mclogit package (see Elff 2021). We are aware that the analysis shown here is supervifial(!) - but please keep in mind that we just wanted to showcase the implementation here rather than providing a properly and carefully done analysis."
  },
  {
    "objectID": "tutorials/regression/regression.html#mixed-effects-ordinal-regression",
    "href": "tutorials/regression/regression.html#mixed-effects-ordinal-regression",
    "title": "Introduction",
    "section": "Mixed-Effects Ordinal Regression",
    "text": "Mixed-Effects Ordinal Regression\nIn this section, we will strictly focus on how to implement a mixed-effects ordinal regression model using the clmm function from the ordinal package (see Christensen 2019). This type of regression model is extremely useful when dealing with Likert data or other types of questionnaire and survey data where the responses have some kind of hierarchical structure (i.e. responses are not truly independent because they come from different points in time or from different regions). load data\n\n# rating experiment data\nratex  &lt;- base::readRDS(\"tutorials/regression/data/ratex.rda\", \"rb\")\n# inspect data\nhead(ratex)\n\n  Rater Child Group       Accent AccentNumeric       Family\n1    R1  C001 Child StrongAccent             2 DomBilingual\n2    R2  C001 Child StrongAccent             2 DomBilingual\n3    R3  C001 Child StrongAccent             2 DomBilingual\n4    R4  C001 Child StrongAccent             2 DomBilingual\n5    R5  C001 Child StrongAccent             2 DomBilingual\n6    R6  C001 Child StrongAccent             2 DomBilingual\n\n\nWe now tabulate the data to get a better understanding of the data structure.\n\nratex %&gt;%\n  dplyr::group_by(Family, Accent) %&gt;%\n  dplyr::summarise(Frequency = n()) %&gt;%\n  tidyr::spread(Accent, Frequency)\n\n`summarise()` has grouped output by 'Family'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 4\n# Groups:   Family [3]\n  Family         NoAccent StrongAccent WeakAccent\n  &lt;fct&gt;             &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n1 DomBilingual         80          145        174\n2 EqualBilingual       20           22         63\n3 Monolingual         209            1         41\n\n\nNext, we visualize the data to inspect its properties.\n\nratex %&gt;%\n  ggplot(aes(Family, AccentNumeric, color = Group)) + \n  stat_summary(fun = mean, geom = \"point\") +          \n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.2) +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = c(\"gray20\", \"gray70\"))\n\n\n\n\n\n\n\n\nAn alternative plot shows other properties of the data.\n\nratex %&gt;%\n  dplyr::group_by(Family, Rater, Group) %&gt;%\n  dplyr::summarise(AccentMean = mean(AccentNumeric)) %&gt;%\n  ggplot(aes(Family, AccentMean, fill = Group)) + \n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"gray50\", \"gray85\"))\n\n`summarise()` has grouped output by 'Family', 'Rater'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nWe now start the modeling by generating a model with Family as the sole predictor.\n\n# fit baseline model\nm1.or &lt;- clmm(Accent ~ (1|Rater) + Family, link=\"logit\", data = ratex)\n# test for incomplete information\nifelse(min(ftable(ratex$Accent, ratex$Family)) == 0, \"incomplete information\", \"okay\")\n\n[1] \"okay\"\n\n# extract aic\naic.glmer &lt;- AIC(logLik(m1.or))\n# inspect aic\naic.glmer\n\n[1] 1380.25888675\n\n# summarize model\nsummary(m1.or)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: Accent ~ (1 | Rater) + Family\ndata:    ratex\n\n link  threshold nobs logLik  AIC     niter    max.grad cond.H \n logit flexible  755  -685.13 1380.26 371(435) 4.36e-07 5.9e+05\n\nRandom effects:\n Groups Name        Variance            Std.Dev.       \n Rater  (Intercept) 0.00000000353193271 0.0000594300657\nNumber of groups:  Rater 21 \n\nCoefficients:\n                         Estimate   Std. Error   z value             Pr(&gt;|z|)\nFamilyEqualBilingual  0.477744666  0.214313910   2.22918             0.025802\nFamilyMonolingual    -2.550224083  0.198849328 -12.82491 &lt; 0.0000000000000002\n                        \nFamilyEqualBilingual *  \nFamilyMonolingual    ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                             Estimate    Std. Error   z value\nNoAccent|StrongAccent   -1.0798112479  0.1058004233 -10.20611\nStrongAccent|WeakAccent  0.1060945018  0.0951352125   1.11520\n\n\nWe can now perform Post-Hoc tests to see which comparisons are significant.\n\nlsmeans(m1.or, pairwise~Family, adjust=\"tukey\")\n\n$lsmeans\n Family         lsmean     SE  df asymp.LCL asymp.UCL\n DomBilingual    0.487 0.0914 Inf     0.308     0.666\n EqualBilingual  0.965 0.1950 Inf     0.582     1.347\n Monolingual    -2.063 0.1740 Inf    -2.404    -1.722\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE  df z.ratio p.value\n DomBilingual - EqualBilingual   -0.478 0.214 Inf  -2.229  0.0664\n DomBilingual - Monolingual       2.550 0.199 Inf  12.825  &lt;.0001\n EqualBilingual - Monolingual     3.028 0.265 Inf  11.438  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nFinally, we can summarize the model.\n\nsjPlot::tab_model(m1.or)\n\n\n\n \nAccent\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\nNoAccent|StrongAccent\n0.34\n0.28 – 0.42\n&lt;0.001\n\n\nStrongAccent|WeakAccent\n1.11\n0.92 – 1.34\n0.265\n\n\nFamily [EqualBilingual]\n1.61\n1.06 – 2.45\n0.026\n\n\nFamily [Monolingual]\n0.08\n0.05 – 0.12\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Rater\n0.00\n\n\nN Rater\n21\n\nObservations\n755\n\n\nMarginal R2 / Conditional R2\n0.325 / NA\n\n\n\n\n\n\n\nAnd we can visualize the effects.\n\nplot_model(m1.or, type = \"pred\", terms = c(\"Family\"))\n\n\n\n\n\n\n\n\nThat’s it for this tutorial. We hope that you have enjoyed this tutorial and learned how to perform regression analysis including model fitting and model diagnostics as well as reporting regression results."
  },
  {
    "objectID": "tutorials/regression/regression.html#footnotes",
    "href": "tutorials/regression/regression.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m extremely grateful to Stefan Thomas Gries who provided very helpful feedback and pointed out many errors in previous versions of this tutorial. All remaining errors are, of course, my own.↩︎\nI’m very grateful to Antonio Dello Iacono who pointed out that the plots require additional discussion.↩︎\nI’m very grateful to Antonio Dello Iacono who pointed out that a previous version of this tutorial misinterpreted the results as I erroneously reversed the group results.↩︎"
  },
  {
    "objectID": "tutorials/sentiment/sentiment.html#preparation-and-session-set-up",
    "href": "tutorials/sentiment/sentiment.html#preparation-and-session-set-up",
    "title": "Introduction",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the packages so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"tibble\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"textdata\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"sentimentr\")\ninstall.packages(\"zoo\")\ninstall.packages(\"flextable\")\ninstall.packages(\"syuzhet\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# activate packages\nlibrary(dplyr)\nlibrary(flextable)\nlibrary(ggplot2)\nlibrary(Hmisc)\nlibrary(sentimentr)\nlibrary(stringr)\nlibrary(textdata)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(tidytext)\nlibrary(zoo)\nlibrary(syuzhet)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/sentiment/sentiment.html#getting-started",
    "href": "tutorials/sentiment/sentiment.html#getting-started",
    "title": "Introduction",
    "section": "Getting started",
    "text": "Getting started\nIn the following, we will perform a SA to investigate the emotionality of five different novels. We will start with the first example and load five pieces of literature.\n\ndarwin &lt;- base::readRDS(\"tutorials/sentiment/data/origindarwin.rda\", \"rb\")\ntwain &lt;- base::readRDS(\"tutorials/sentiment/data/twainhuckfinn.rda\", \"rb\")\norwell &lt;- base::readRDS(\"tutorials/sentiment/data/orwell.rda\", \"rb\")\nlovecraft &lt;- base::readRDS(\"tutorials/sentiment/data/lovecraftcolor.rda\", \"rb\")\n\n\n\n.THE ORIGIN OF SPECIES BY CHARLES DARWIN AN HISTORICAL SKETCH OF THE PROGRESS OF OPINION ON THE ORIGIN OF SPECIES INTRODUCTION When on board H.M.S. 'Beagle,' as naturalist, I was much struck with certain facts in the distribution of the organic beings in- habiting South America, and in the geological relations of the \n\n\nWrite function to clean data\n\ntxtclean &lt;- function(x, title) {\n    require(dplyr)\n    require(stringr)\n    require(tibble)\n    x &lt;- x %&gt;%\n        iconv(to = \"UTF-8\") %&gt;%\n        base::tolower() %&gt;%\n        paste0(collapse = \" \") %&gt;%\n        stringr::str_squish() %&gt;%\n        stringr::str_split(\" \") %&gt;%\n        unlist() %&gt;%\n        tibble::tibble() %&gt;%\n        dplyr::select(word = 1, everything()) %&gt;%\n        dplyr::mutate(novel = title) %&gt;%\n        dplyr::anti_join(stop_words) %&gt;%\n        dplyr::mutate(word = str_remove_all(word, \"\\\\W\")) %&gt;%\n        dplyr::filter(word != \"\")\n}\n\nProcess and clean texts.\n\n# process text data\ndarwin_clean &lt;- txtclean(darwin, \"darwin\")\nlovecraft_clean &lt;- txtclean(lovecraft, \"lovecraft\")\norwell_clean &lt;- txtclean(orwell, \"orwell\")\ntwain_clean &lt;- txtclean(twain, \"twain\")\n\n\n\nwordnovelorigindarwinspeciesdarwincharlesdarwindarwindarwinhistoricaldarwinsketchdarwinprogressdarwinopiniondarwinorigindarwinspeciesdarwin"
  },
  {
    "objectID": "tutorials/sentiment/sentiment.html#binning",
    "href": "tutorials/sentiment/sentiment.html#binning",
    "title": "Introduction",
    "section": "Binning",
    "text": "Binning\nThe following code chunk uses binning to determine the polarity and subsequently displaying changes in polarity across the development of the novels’ plots.\n\nnovels_bin &lt;- novels_anno %&gt;%\n    dplyr::group_by(novel) %&gt;%\n    dplyr::filter(is.na(sentiment) | sentiment == \"negative\" | sentiment == \"positive\") %&gt;%\n    dplyr::mutate(\n        sentiment = as.character(sentiment),\n        sentiment = case_when(\n            is.na(sentiment) ~ \"0\",\n            TRUE ~ sentiment\n        ),\n        sentiment = case_when(\n            sentiment == \"0\" ~ 0,\n            sentiment == \"positive\" ~ 1,\n            TRUE ~ -1\n        ),\n        id = 1:n(),\n        index = as.numeric(cut2(id, m = 100))\n    ) %&gt;%\n    dplyr::group_by(novel, index) %&gt;%\n    dplyr::summarize(\n        index = unique(index),\n        polarity = mean(sentiment)\n    )\n\n\n\nnovelindexpolaritydarwin10.03960396darwin20.12000000darwin30.11000000darwin40.09000000darwin50.01000000darwin60.11000000darwin70.03000000darwin80.08000000darwin90.04000000darwin100.01000000\n\n\nWe now have an average polarity for each bin and can plot this polarity over the development of the story.\n\nggplot(novels_bin, aes(index, polarity)) +\n    facet_wrap(vars(novel), scales = \"free_x\") +\n    geom_smooth(se = F, col = \"black\") +\n    theme_bw() +\n    labs(\n        y = \"polarity ratio (mean by bin)\",\n        x = \"index (bin)\"\n    )"
  },
  {
    "objectID": "tutorials/sentiment/sentiment.html#moving-average",
    "href": "tutorials/sentiment/sentiment.html#moving-average",
    "title": "Introduction",
    "section": "Moving average",
    "text": "Moving average\nAnother method for tracking changes in polarity over time is to calculate rolling or moving means. It should be noted thought that rolling means are not an optimal method for tracking changes over time and rather represent a method for smoothing chaotic time-series data. However, they can be used to complement the analysis of changes that are detected by binning.\nTo calculate moving averages, we will assign words with positive polarity a value +1 and words with negative polarity a value of -1 (neutral words are coded as 0). A rolling mean calculates the mean over a fixed window span. Once the initial mean is calculated, the window is shifted to the next position and the mean is calculated for that window of values, and so on. We set the window size to 100 words which represents an arbitrary value.\n\nnovels_change &lt;- novels_anno %&gt;%\n    dplyr::filter(is.na(sentiment) | sentiment == \"negative\" | sentiment == \"positive\") %&gt;%\n    dplyr::group_by(novel) %&gt;%\n    dplyr::mutate(\n        sentiment = as.character(sentiment),\n        sentiment = case_when(\n            is.na(sentiment) ~ \"0\",\n            TRUE ~ sentiment\n        ),\n        sentiment = case_when(\n            sentiment == \"0\" ~ 0,\n            sentiment == \"positive\" ~ 1,\n            TRUE ~ -1\n        ),\n        id = 1:n()\n    ) %&gt;%\n    dplyr::summarise(\n        id = id,\n        rmean = rollapply(sentiment, 100, mean, align = \"right\", fill = NA)\n    ) %&gt;%\n    na.omit()\n\n\n\nnovelidrmeandarwin1000.04darwin1010.04darwin1020.04darwin1030.04darwin1040.04darwin1050.04darwin1060.04darwin1070.04darwin1080.04darwin1090.04\n\n\nWe will now display the values of the rolling mean to check if three are notable trends in how the polarity shifts over the course of the unfolding of the story within George Orwell’s Nineteen Eighty-Four.\n\nggplot(novels_change, aes(id, rmean)) +\n    facet_wrap(vars(novel), scales = \"free_x\") +\n    geom_smooth(se = F, col = \"black\") +\n    theme_bw() +\n    labs(\n        y = \"polarity ratio (rolling mean, k = 100)\",\n        x = \"index (word in monograph)\"\n    )\n\n\n\n\n\n\n\n\nThe difference between the rolling mean and the binning is quite notable and results from the fact, that rolling means represent a smoothing method rather than a method to track changes over time."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html",
    "href": "tutorials/atap_docclass/atap_docclass.html",
    "title": "Classifying American Speeches",
    "section": "",
    "text": "This tutorial shows how to perform document classification using R. The entire R markdown document for the tutorial can be downloaded here.\nThe tutorial requires you to install and load several packages to analyze linguistic data. To help you, we directly include the commands to do so in the script and walk you through it step by step. Let’s get on it!"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#preparation-and-session-setup",
    "href": "tutorials/atap_docclass/atap_docclass.html#preparation-and-session-setup",
    "title": "Classifying American Speeches",
    "section": "Preparation and Session Setup",
    "text": "Preparation and Session Setup\nAs mentioned at the outset, this is an introduction to document classification based on R. A rudimentary familiarity with R and RStudio are helpful for getting the most out of this. If you have yet to install R or are new to it, we can recommend this tutorial to R, which walks you through the installation and shows a range of its functionalities. In the following, we assume that you have downloaded and installed both R and RStudio and take it from there.\nAlthough R in its basic version already contains a lot of functionalities, we do need to install some packages for the code below to run without exploding into a bouquet of error messages. If you already have the packages quanteda, quanteda.textmodels, stopwords and ggplot2 installed, just skip to the next heading. If you don’t have them yet, the first thing you want to do is run these first lines of code:\n\n# install.packages(\"quanteda\")\n# install.packages(\"quanteda.textmodels\")\n# install.packages(\"stopwords\")\n# install.packages(\"ggplot2\")\n\nThis may take a minute or three. In most cases, this will work without any hiccups. If you should get an error message, we recommend taking a moment to read what it says, and, if it does not make any sense to you, to google it. If an issue comes up for you, chances are that this has already happened to someone else - and, fortunately, the R community has a pretty good track record of responding to questions about technical issues. Generally, it is also a good idea to use a relatively new version of R. If you have last used R two years ago, do update it.\nOnce you have installed the packages, you’ll need to load them in the current session. This is done with the following lines of code:\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(stopwords)\nlibrary(ggplot2)\n\nWith the packages loaded, the stage is set. Now we need to dress up the actors."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#from-data-to-useable-objects",
    "href": "tutorials/atap_docclass/atap_docclass.html#from-data-to-useable-objects",
    "title": "Classifying American Speeches",
    "section": "From Data to Useable Objects",
    "text": "From Data to Useable Objects\nBefore we can do anything further, we need to load the data into R. As mentioned earlier, we are working with the CORPS II corpus compiled by Guerini et al. (2010). There are many ways to get the corpus into R, but we are going to download the data from the data directory of the LADAL GitHub repository. By executing the following line of code, you tell R to download the file containing the corpus from github and store it in the session under the variable name rt:\n\nrt &lt;- base::readRDS(\"tutorials/atap_docclass/data/SEL_perparty_v2.rda\", \"rb\")\n\nIn the meantime, we want to draw your attention to the second parameter of the readtext command, namely the specification defining the text field. These specifications are called flags. These are often useful and sometimes necessary. In this case, the flag does exactly what it says: it tells R in which column the text is stored.\nLet’s take a look at rt, the variable in which we stored the corpus. We can do this simply by str which returns the structure of the object:\n\nstr(rt)\n\nClasses 'readtext' and 'data.frame':    3247 obs. of  5 variables:\n $ doc_id: chr  \"SEL_perparty_v2.csv.1\" \"SEL_perparty_v2.csv.2\" \"SEL_perparty_v2.csv.3\" \"SEL_perparty_v2.csv.4\" ...\n $ text  : chr  \"We who are Christians usually think about Christ in terms of obligations of charity , and faith in God , and so\"| __truncated__ \"It is a very great honor and pleasure for me to be here and participate in a dinner this evening that aims at s\"| __truncated__ \"Thank you . Praise God . Thank you very much . Tonight , I would like you first of all to join me in rememberin\"| __truncated__ \"Thank you . Thank you . Praise God . Thank you very much . I want to tell you I 'm very happy to have a few mom\"| __truncated__ ...\n $ party : chr  \"rep\" \"rep\" \"rep\" \"rep\" ...\n $ fileid: chr  \"akeyes-95\" \"akeyes-98\" \"akeyes1-2-00\" \"akeyes1-6-96\" ...\n $ name  : chr  \"Alan_Keyes\" \"Alan_Keyes\" \"Alan_Keyes\" \"Alan_Keyes\" ...\n\n\nThe output tells us what we are dealing with: a object consisting of 3247 documents and 3 document variables. We also see the first six rows of the object, which shows that we are dealing with a two dimensional matrix or a table, which in the R context is also called a data frame. What may be surprising is that, although the object is described as having three document variables, the data frame has five columns.\nIf we look at the structure of the corpus, we can see that the file contains four columns, labeled party, fileid, name and text, respectively. The object rt to which we assigned the data created an additional column, labeled doc_id. In this variable, the row number of each row in the file is stored. If we were to load in a second document and append it to the rt object, it would be clear which row of data comes from which corpus. So, while it’s great to have this doc_id, the output also makes it clear that it is not meaningful information relevant to the text we are interested in.\nThe text source files come from the textual component of the files, and the document-level metadata (“docvars”) come from either the file contents or filenames.\nSo the three document variables are the document-level metadata, which pertain to the texts we are interested in. Depending on the type of analysis or operation you are interested in performing on the data, this is not essential information. For document classification, however, as well as many other analyses, this metadata is essential."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#tokenization",
    "href": "tutorials/atap_docclass/atap_docclass.html#tokenization",
    "title": "Classifying American Speeches",
    "section": "Tokenization",
    "text": "Tokenization\nThe first step here is to transform the text in the corpus object into individual tokens. Right now, each text cell in the fulltext object contains one whole presidential campaign speech. By tokenizing the corpus, each text entry in fulltext is transformed into a list of words. In order to do this, we define a new variable, toks, and use the perfectly descriptive command tokens() to tokenize the corpus and assign it to the new variable toks:\n\ntoks &lt;- tokens(fulltext, remove_punct = TRUE)\n\nSneakily, we also use a flag here to remove punctuation. Let’s look at how this step changed the content of the corpus. Having removed the punctuation, it makes sense to pick a speech which contained punctuation in the beginning, so we choose speech four. We can look at this specific speech by entering the name of the object, toks, and adding the row number of choice in square brackets, thus:\n\ntoks[4]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.4 :\n [1] \"Thank\"  \"you\"    \"Thank\"  \"you\"    \"Praise\" \"God\"    \"Thank\"  \"you\"   \n [9] \"very\"   \"much\"   \"I\"      \"want\"  \n[ ... and 2,530 more ]"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#lowercasing",
    "href": "tutorials/atap_docclass/atap_docclass.html#lowercasing",
    "title": "Classifying American Speeches",
    "section": "Lowercasing",
    "text": "Lowercasing\nThis next step is fairly obvious: the heading says it, the command says it, and it’s precisely what we are going to do. We’re going to replace all capital letters by their lowercase equivalent in the corpus. This is mainly to reduce the variation that arises from words that stand at the beginning of a sentence. To do it, we use the command tokens_tolower(), as demonstrated here:\n\ntoks &lt;- tokens_tolower(toks)\n\nThe command is self explanatory, but we should add: we define the object toks as input, replace capital letters with lowercase ones and assign the outcome again to the object toks, thus overwriting what was there before. If we now look at row four again, we see the result of this transformation:\n\ntoks[4]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.4 :\n [1] \"thank\"  \"you\"    \"thank\"  \"you\"    \"praise\" \"god\"    \"thank\"  \"you\"   \n [9] \"very\"   \"much\"   \"i\"      \"want\"  \n[ ... and 2,530 more ]\n\n\nWhere before there was Thank and I, we now have thank and i. Not much more to see here, so let’s keep moving."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#stemming",
    "href": "tutorials/atap_docclass/atap_docclass.html#stemming",
    "title": "Classifying American Speeches",
    "section": "Stemming",
    "text": "Stemming\nThis time round, the transformation is a bit more drastic: we are going to reduce each word in the corpus to its word stem. Take a quick look at the first item of the toks object:\n\ntoks[1]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.1 :\n [1] \"we\"          \"who\"         \"are\"         \"christians\"  \"usually\"    \n [6] \"think\"       \"about\"       \"christ\"      \"in\"          \"terms\"      \n[11] \"of\"          \"obligations\"\n[ ... and 6,212 more ]\n\n\nWe have here, among other words, christians, usually and obligations. Keep those in mind as you reduce the corpus to wordstems, again using a fairly intuitive command:\n\ntoks &lt;- tokens_wordstem(toks)\n\nUsing the same procedure as with lowercasing, we overwrite the contents of the toks object with its latest transformation. We can see how this plays out by again looking at the first item.\n\ntoks[1]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.1 :\n [1] \"we\"        \"who\"       \"are\"       \"christian\" \"usual\"     \"think\"    \n [7] \"about\"     \"christ\"    \"in\"        \"term\"      \"of\"        \"oblig\"    \n[ ... and 6,212 more ]\n\n\nEverything is shortened compared to before: christian, usually, and oblig. Compared to the preceding steps, a bit more informational value is lost, but this is often a trade-off worth taking, seeing as it typically enhances the performance of the document classification quite substantially."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#remove-stopwords",
    "href": "tutorials/atap_docclass/atap_docclass.html#remove-stopwords",
    "title": "Classifying American Speeches",
    "section": "Remove Stopwords",
    "text": "Remove Stopwords\nThe final intrusion into the linguistic source material is the removal of stopwords. These are words which are very common, to a degree that one can assume that they will, on average, be equally distributed between the two classes. They are typically also words that have no meaning on their own, for example the, in, etc. As they rather regulate the relations between words, they are also called function words (as opposed to content words, which we want to keep). There is no universal list of stopwords - and some methods make do without removing stopwords - but for our purposes, it makes sense to just work with a list that is readily available, which is the one specified in the flag. The command is as follows:\n\ntoks &lt;- tokens_remove(toks, pattern = stopwords::stopwords(language = \"en\", source = \"snowball\"))\n\nAgain, the command is very explicit in its function, the input is the tokenized, punctuation-less, lowercased and stemmed version of the toks object, which we overwrite to create a version that is all of the above and additionally does not contain stopwords. We see in row one how that looks:\n\ntoks[1]\n\nTokens consisting of 1 document and 3 docvars.\nSEL_perparty_v2.csv.1 :\n [1] \"christian\" \"usual\"     \"think\"     \"christ\"    \"term\"      \"oblig\"    \n [7] \"chariti\"   \"faith\"     \"god\"       \"forth\"     \"don\"       \"t\"        \n[ ... and 2,751 more ]\n\n\nThis looks quite a bit different: a lot of words are gone, such as we, who and are. For the most part, what remains are the stems of nouns and verbs, words which carry semantic meaning."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#imperfections",
    "href": "tutorials/atap_docclass/atap_docclass.html#imperfections",
    "title": "Classifying American Speeches",
    "section": "Imperfections",
    "text": "Imperfections\nClearly, this approach to pre-processing is not without flaws. We still see, for instance, in the last two positions of the output, that don’t was split into two tokens, don and t. Don would probably also appear as the appropriate word for various garments a lot in a Victorian fashion weekly, which is probably not what Alan Keyes was referring to in his presidential campaign speech, while not should probably be part of the stopword list. Or, to return to the purged stopwards mentioned above, who could have been accidentally removed: once the World Health Organization’s abbreviation is lowercased, it is indistinguishable from the pronoun. So you can see that there are some pitfalls to pre-processing, which are usually quite harmless compared to the benefits (and improved model). However, it can lead to problematic omissions in certain contexts. Ultimately, you need to be aware of the linguistic context you’re working in and make the trade-off on whether finer-grained pre-processing is worth the effort."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#document-feature-matrix",
    "href": "tutorials/atap_docclass/atap_docclass.html#document-feature-matrix",
    "title": "Classifying American Speeches",
    "section": "Document-Feature Matrix",
    "text": "Document-Feature Matrix\nWith the linguistic pre-processing done, we continue to the next step: creating a document-feature matrix – simply a table which says how often which word occurs in each document. This is the format we need to run the document classification. We’ll get into its structure in a second. But first, we turn the pre-processed corpus into a document-feature matrix. Once more, the command, dfm, is fairly self-explanatory:\n\ndtm &lt;- dfm(toks)\n\nThis time round, we create a new object with the name dtm, and the input to this operation is the latest version of the toks object. In case you are not familiar with this particular type of matrix, you might get a better sense for what we are dealing with is by looking at the object directly:\n\ndtm\n\nDocument-feature matrix of: 3,247 documents, 31,939 features (98.41% sparse) and 3 docvars.\n                       features\ndocs                    christian usual think christ term oblig chariti faith\n  SEL_perparty_v2.csv.1        17     3    28     12    4     1       1    22\n  SEL_perparty_v2.csv.2         1     1    14      3    0     1       0     4\n  SEL_perparty_v2.csv.3         0     0    12      0    0     0       0     7\n  SEL_perparty_v2.csv.4         1     0    15      0    0     0       0     0\n  SEL_perparty_v2.csv.5         6     1    16     11    1     0       0     3\n  SEL_perparty_v2.csv.6         3     0    26     10    4     3       0     7\n                       features\ndocs                    god forth\n  SEL_perparty_v2.csv.1  82     1\n  SEL_perparty_v2.csv.2  21     2\n  SEL_perparty_v2.csv.3   8     0\n  SEL_perparty_v2.csv.4   4     1\n  SEL_perparty_v2.csv.5  48     5\n  SEL_perparty_v2.csv.6   6     5\n[ reached max_ndoc ... 3,241 more documents, reached max_nfeat ... 31,929 more features ]\n\n\nAt first glance, you can see that our variable dtm is an object similar to the earlier fulltext, in that only the first six rows of the matrix are pasted in the output, and that there is a summary of the object. And now we can also see the structure of this document-feature matrix. Turns out, the name is quite the give-away: the document feature matrix allows us to see how often each feature appears in each document.\nIn this matrix, we can access both the columns and the rows, the way we usually do with data frames in R. To do this, we append a square bracket to the object. We can access rows thusly:\n\ndtm[3, ]\n\nDocument-feature matrix of: 1 document, 31,939 features (98.61% sparse) and 3 docvars.\n                       features\ndocs                    christian usual think christ term oblig chariti faith\n  SEL_perparty_v2.csv.3         0     0    12      0    0     0       0     7\n                       features\ndocs                    god forth\n  SEL_perparty_v2.csv.3   8     0\n[ reached max_nfeat ... 31,929 more features ]\n\n\nHere, we see that think features twelve times in the third speech in the corpus. Or, accounting for what the pre-processing did to the text, we can say that some version of think appears in this speech.\nAccessing the columns works analogously: we simply place the number after the comma:\n\ndtm[, 3]\n\nDocument-feature matrix of: 3,247 documents, 1 feature (19.00% sparse) and 3 docvars.\n                       features\ndocs                    think\n  SEL_perparty_v2.csv.1    28\n  SEL_perparty_v2.csv.2    14\n  SEL_perparty_v2.csv.3    12\n  SEL_perparty_v2.csv.4    15\n  SEL_perparty_v2.csv.5    16\n  SEL_perparty_v2.csv.6    26\n[ reached max_ndoc ... 3,241 more documents ]\n\n\nWe see here that think is a frequent feature, at least in the first six speeches in the corpus. By looking at the metadata, or, as we call them here, the document variables, we can see that the first six speeches were all given by the same speaker. To access the document variables and solve the riveting riddle of who this famous thinker is, we can use another one of these intuitively named commands:\n\nhead(docvars(dtm))\n\n  party       fileid       name\n1   rep    akeyes-95 Alan_Keyes\n2   rep    akeyes-98 Alan_Keyes\n3   rep akeyes1-2-00 Alan_Keyes\n4   rep akeyes1-6-96 Alan_Keyes\n5   rep akeyes1-6-98 Alan_Keyes\n6   rep  akeyes10-96 Alan_Keyes\n\n\nWith the docvars command, we get a list of the document variables for each document, that is each row, in the dtm object. However, we nest this inside of the command head() - which is useful for taking a first look at any variable, since it only displays the first six rows of a variable. We do this here because the docvars are not subject to the same behavior protocol that the corpus object is. Your computer is unlikely to overload if you were to accidentally view the whole docvars without taking this head precaution. So let’s try it:\n\ndocvars(dtm)[1:10, 1:3]\n\n   party         fileid       name\n1    rep      akeyes-95 Alan_Keyes\n2    rep      akeyes-98 Alan_Keyes\n3    rep   akeyes1-2-00 Alan_Keyes\n4    rep   akeyes1-6-96 Alan_Keyes\n5    rep   akeyes1-6-98 Alan_Keyes\n6    rep    akeyes10-96 Alan_Keyes\n7    rep  akeyes11-5-00 Alan_Keyes\n8    rep  akeyes11-9-95 Alan_Keyes\n9    rep akeyes12-10-96 Alan_Keyes\n10   rep  akeyes12-5-00 Alan_Keyes\n\n\nEven though it becomes clear enough which document variable stands in which column, it requires a lot of scrolling to get to the top where the precise name of each document variable is pasted at the top of each column. The very top of the list is also where we find the answer to the mystery of who uses think so gratuitously. If the name Alan Keyes does not ring a bell, you are in the fortunate position to only learn today of a person whose claims to fame include having run for president three times, being appointed ambassador to the UN’s Economic and Social Council by Ronald Reagan and filing a lawsuit against Barack Obama - requesting documentation proving that Barack Obama is a natural born citizen of the US. The things you learn in computational linguistics never cease to amaze, eh?"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#document-frequency-of-the-features",
    "href": "tutorials/atap_docclass/atap_docclass.html#document-frequency-of-the-features",
    "title": "Classifying American Speeches",
    "section": "Document Frequency of the Features",
    "text": "Document Frequency of the Features\nThe next step in our journey toward document classification requires us to count how often each feature appears in the corpus, for reasons which will become apparent shortly. Our trusty friend, the Quanteda package, anticipates this need and furnishes us with the appropriate command, named docfreq. We use it to create a new variable, doc_freq:\n\ndoc_freq &lt;- docfreq(dtm)\n\nThe docfreq command takes our pre-processed corpus as an argument, in the form of the dtm variable. The new variable, doc_freq is now a long list of all the features and their frequencies. We can confirm this by looking at the first twenty entries, i.e. words:\n\nhead(doc_freq, n = 20)\n\n  christian       usual       think      christ        term       oblig \n        143         278        2630          30         954         559 \n    chariti       faith         god       forth         don           t \n        158         827        1739         294        2341        2831 \n      alway       jesus       great      wisdom citizenship  understood \n       1825          32        2812         285         137         277 \n       said        fact \n       2572        1558 \n\n\nThe output shows us each feature and the number of occurrences. It also shows us here that the features are not ranked by frequency yet, but by order of appearance in the corpus. The first few features are the ones we encounter in the first speech in the corpus.\nTo see which features occur most and least frequently in the corpus, we can sort the variable doc_freq. To do this, we use the intuitively named sort() function. In order to save ourselves the trouble of scrolling, we nest the sort() function inside the head function, which then looks thusly:\n\nhead(sort(doc_freq, decreasing = T), n = 15)\n\n       s    thank    peopl      can       us      one     want     year \n    3165     3121     3091     3076     3041     3040     3030     3009 \n countri     time     make     know     work      now american \n    3009     2978     2977     2975     2955     2925     2908 \n\n\nThe outermost layer of the line is the head() command. Inside of this, we nest the sort() function, which takes as an argument the variable of interest, doc_freq and the logical argument decreasing = TRUE. After closing the brackets on the sort function, we add the specification that we want to see the first fifteen entries of the head.\nIn the output, we see many features that we would expect to appear in presidential campaign speeches. The contracted ’s is the most common feature, which makes sense considering that it can be used both in the sense of let’s build a better future together or as a possessive. Contraction are also generally used frequently in spoken language. We are clearly reminded that we are dealing with a corpus of political speeches, in because many words are associated with shaping something: can, want, make, work indicating the activities and countri pointing to the object to be shaped. There are also flavors of urgency coming through, with year, time and now referring to the temporality of the campaign. Finally, we are reminded that we are dealing with American politicians. So, we have a pretty good interpretive turn-out already here.1\nFor some insightful contrast, let’s look at the last fifteen features in this ranking. To do this, we use almost exactly the same command as before, but instead of nesting everything under head(), we do it under the tail() function, like so:\n\ntail(sort(doc_freq, decreasing = T), n = 15)\n\n           umphf           brasim t-worry-be-happi        5-percent \n               1                1                1                1 \nafrica-caribbean              koz            4x400         meteorit \n               1                1                1                1 \n     micro-organ        somebody-            nowak          pantoja \n               1                1                1                1 \n          aspira          rosello        hillcrest \n               1                1                1 \n\n\nIt would be possible to go and look at the contexts of these features, but in the interest of your sanity and ours, we graciously skip this step.\nInstead, we return the earlier promise and explain why we created the doc_freq variable. As we can still see in the output, the features in the tail are on the one hand extremely rare, and on the other hand not very useful. The latter reason would not be a criteria for exclusion, but the fact that they are extremely rare could muddle up the document classification model we are interested in, the model could learn corpus coincidences, which are often referred to as noise, thus blurring the signal of frequent words. So, we remove all features from the corpus which occur less frequently than five times. We do this using the following command:\n\ndtm &lt;- dtm[, doc_freq &gt;= 5]\n\nAs you can see, we are once again overwriting the dtm variable with a specific iteration of the variable. This time, we choose to retain all the features which occur more than five times in the corpus, with the result that the features which occur less frequently drop out. Let’s see how well this worked, using the same set of commands as above. First, we overwrite the doc_freq variable with the document frequencies of the latest version, then we check out the tail:\n\ndoc_freq &lt;- docfreq(dtm)\ntail(sort(doc_freq, decreasing = T), n = 15)\n\n     salina        soto   job-train  oldfashion    kennelli   underwood \n          5           5           5           5           5           5 \n    telecom        huey   gutenberg      hilari kaleidoscop  multi-raci \n          5           5           5           5           5           5 \n  sweepstak      bifida        1910 \n          5           5           5 \n\n\nWe now find a new set of random seeming words at the tail of the doc_freq variable, but at least they all occur at least five times, which means they won’t distort the model too badly. And just to make sure we didn’t accidentally mess up the frequent features, we can again take a look at the head:\n\nhead(sort(doc_freq, decreasing = T), n = 15)\n\n       s    thank    peopl      can       us      one     want     year \n    3165     3121     3091     3076     3041     3040     3030     3009 \n countri     time     make     know     work      now american \n    3009     2978     2977     2975     2955     2925     2908 \n\n\nOnly to find, lo and behold, that they are identical to what we had above. Awesome! But it makes sense to check if our data develops in the expected way. Such checks, also called sanity checking, detect early on if one is on the right track – instead of getting error messages in one’s last processing step. On to a final step of preparation."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#tfidf",
    "href": "tutorials/atap_docclass/atap_docclass.html#tfidf",
    "title": "Classifying American Speeches",
    "section": "TFIDF",
    "text": "TFIDF\nBefore we continue to the actual document classification (never fear: we are actually going to get there), we need to weight the features in the corpus object dtm by their tfidf. This beauty of an abbreviation stands for term frequency-inverse document frequency, and does exactly what it says: the number of occurrences of the word in the current document is divided by the number of documents in which the feature occurs. By this division, we account for the fact that some words are generally very frequent, and thus would receive an overproporational amount of significance despite being rather pedestrian and carrying little specific semantic content if we relied on frequency alone. An example we’ve already discussed above is the contracted ’s, which is the most frequent feature in the corpus, on account of just being a very common word in the English language.\nBefore we apply the tfidf weights, let’s load the first document in the dtm object:\n\ndtm[1, ]\n\nDocument-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.\n                       features\ndocs                    christian usual think christ term oblig chariti faith\n  SEL_perparty_v2.csv.1        17     3    28     12    4     1       1    22\n                       features\ndocs                    god forth\n  SEL_perparty_v2.csv.1  82     1\n[ reached max_nfeat ... 11,770 more features ]\n\n\nThis will save us a lot of scrolling time when we compare the pre- and post-tfidf weights.\nOnce again, quanteda actually furnishes us with a command to easily transform our dtm variable to adapt to the tfidf weights. The command is self-explanatorily labeled dfm_tfidf(), and we use it to overwrite the current version of the dtm variable:\n\ndtm &lt;- dfm_tfidf(dtm, force = TRUE)\n\nYou can see that the command takes as its argument first the object to be transformed. The second argument, the flag force=TRUE is usually not needed. As a protection to makes sure that one does not tfidf-weight twice, the dfm_tfidf function refuses to accept matrices that are already weighted. Let’s take a look at the first entry of the dtm object now, we can see that the weights of the features have changed substantially:\n\ndtm[1, ]\n\nDocument-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.\n                       features\ndocs                    christian    usual    think   christ     term     oblig\n  SEL_perparty_v2.csv.1  23.05449 3.202312 2.562743 24.41233 2.127736 0.7640705\n                       features\ndocs                     chariti    faith      god    forth\n  SEL_perparty_v2.csv.1 1.312825 13.06749 22.23698 1.043135\n[ reached max_nfeat ... 11,770 more features ]\n\n\nWhere before we had a simple count of occurrences, we now have the tfidf scores. Some of the salient changes can be seen in the features christian and think. In terms of raw counts, Alan Keyes’ speech contains christian 17 times. With the tfidf weights, christian actually becomes more important, with a weight of 23.06. This indicates that christian is used comparatively rarely in other speeches, making Keyes’ usage here more impactful. By contrast, although think occurs 28 times in Keyes’ speech, it only receives a tfidf score of 2.56, indicating that it also features heavily in many other speeches in the corpus. Turns out, Keyes is not a lone thinker among these politicians. What a shocker.\nLet’s stick with the feature think for a second, and look at its tfidf score in the first few speeches. Remember that, since dtm is an object containing data and a behavior, we can simply enter the third column of dtm and receive six manageable lines of output:\n\ndtm[, 3]\n\nDocument-feature matrix of: 3,247 documents, 1 feature (19.00% sparse) and 3 docvars.\n                       features\ndocs                       think\n  SEL_perparty_v2.csv.1 2.562743\n  SEL_perparty_v2.csv.2 1.281372\n  SEL_perparty_v2.csv.3 1.098318\n  SEL_perparty_v2.csv.4 1.372898\n  SEL_perparty_v2.csv.5 1.464425\n  SEL_perparty_v2.csv.6 2.379690\n[ reached max_ndoc ... 3,241 more documents ]\n\n\nWe see that think generally has a relatively low score, but the scores differ from speech to speech. The reason for this is that each speech contains a different number of think features, but quite a high number of speeches contain a relatively high number. Thus, the count in each of these first six speeches is divided by the total number of speeches containing think, yielding a different but consistently rather low tfidf weight in each row.\nAs an aside, tfidf can be used directly to see which words are most characteristic of a document, i.e. its keywords. We could see the keywords of a document, for example document 1, by considering the words with the highest tfidf values. The simplest way to do so is to sort the tfidf values in decreasing order, from most to least frequent, and the inspect the top of the list.\n\nsdtm_tfidf &lt;- dfm_sort(dtm[1, ], decreasing = T, margin = \"features\")\nhead(sdtm_tfidf)\n\nDocument-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.\n                       features\ndocs                      caesar   christ christian      god    roman    fetus\n  SEL_perparty_v2.csv.1 35.48031 24.41233  23.05449 22.23698 18.86858 18.66469\n                       features\ndocs                    citizenship     coin     imag    faith\n  SEL_perparty_v2.csv.1     17.8719 15.84003 15.52798 13.06749\n[ reached max_nfeat ... 11,770 more features ]"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#the-training-data",
    "href": "tutorials/atap_docclass/atap_docclass.html#the-training-data",
    "title": "Classifying American Speeches",
    "section": "The Training Data",
    "text": "The Training Data\nIn broad brushstrokes, it works the way it usually does: if you want to generate an output, you need an input. This is what we spent the last ten pages setting up. From the removal of punctuation, via lowercasing and the removal of the features with less than five occurrences to the weighting using the tfidf score, each step is a little boost that helps our model achieve better results. The final (promise) step before creating the actual model is choosing the input.\nThe situation, of course, is as follows: we have a corpus with 3,247 speeches, all of which have already been classified as being either Republican or Democrat. If we were to simply throw the full corpus into the model, we would run into a problem: we couldn’t test how good the model is generally, because every scrap of data available to us right now would be informing the model. Our results would not be robust. Instead, what we want to do is sample a large chunk of the speeches available to us, use that to train the model, and then test how good our model is on the remaining speeches.\nOnce again, quanteda holds ready for us a command with the descriptive name dfm_sample(). We use it as follows:\n\ntrain_dtm &lt;- dfm_sample(dtm, size = 2800)\n\nOr, to put it into words: we create a new object called train_dtm, since we are going to train the model with this. The train_dtm object contains a sample of 2,800 speeches from the pre-processed and weighted dtm corpus. For this tutorial, we have just taken a random sample with a size corresponding to roughly 85% of the corpus. There are no hard and fast rules as to which percentage of the data should be used for training and which for testing, but if you’re somewhere in the 90% ballpark for training data you should be doing fine.\nLet’s take a superficial look at our training data:\n\ntrain_dtm\n\nDocument-feature matrix of: 2,800 documents, 11,780 features (95.77% sparse) and 3 docvars.\n                          features\ndocs                       christian usual      think christ      term\n  SEL_perparty_v2.csv.2470         0     0 0.36610616      0 0.5319339\n  SEL_perparty_v2.csv.2304         0     0 0.18305308      0 0        \n  SEL_perparty_v2.csv.1150         0     0 0.82373886      0 0        \n  SEL_perparty_v2.csv.2291         0     0 0.09152654      0 0        \n  SEL_perparty_v2.csv.2497         0     0 2.28816350      0 0.5319339\n  SEL_perparty_v2.csv.1687         0     0 1.28137156      0 1.0638678\n                          features\ndocs                           oblig  chariti     faith       god    forth\n  SEL_perparty_v2.csv.2470 0         0        0         0.2711827 0       \n  SEL_perparty_v2.csv.2304 0         0        0.5939768 0.2711827 0       \n  SEL_perparty_v2.csv.1150 0         0        0         0.2711827 1.043135\n  SEL_perparty_v2.csv.2291 0         1.312825 0         0.5423654 0       \n  SEL_perparty_v2.csv.2497 0         0        0         0.8135481 0       \n  SEL_perparty_v2.csv.1687 0.7640705 0        0         0         0       \n[ reached max_ndoc ... 2,794 more documents, reached max_nfeat ... 11,770 more features ]\n\n\nWe see in the first row of the output that the object is a document-feature matrix consisting of 2,800 documents, 11,785 features and 3 document variables. We also see in the first column that the speeches included do not form an interpretable sequence, indicating that they are random. All is as it should be."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#the-testing-data",
    "href": "tutorials/atap_docclass/atap_docclass.html#the-testing-data",
    "title": "Classifying American Speeches",
    "section": "The Testing Data",
    "text": "The Testing Data\nNow, let’s create the necessary counterpart, the testing data. We create a new variable, which we’ll call test_dtm, and which contains those speeches which are not part of the training data. The way to do this is by taking the dtm object and choosing those documents which constitute the difference between the full dtm corpus and the train_dtm object. The appropriate function for this is setdiff(), which takes as arguments the documents which constitute the full dtm corpus and removes the documents named in the train_dtm variable. The function setdiff is actually used here as an index of the dtm. Indices cannot only contain numbers, but also variables and entire functions – Python programmers refer to this as list comprehension. Observe that dtm is a two-dimensional object. We constrain the first dimension (the rows, i.e. the documents) to those that are not part of the training set, but the second dimension (the column, i.e. the word features) is just kept. This explains the last letters of the following code, the comma followed by the square bracket.\n\ntest_dtm &lt;- dtm[setdiff(docnames(dtm), docnames(train_dtm)), ]\ntest_dtm\n\nDocument-feature matrix of: 447 documents, 11,780 features (95.81% sparse) and 3 docvars.\n                        features\ndocs                     christian    usual     think   christ      term\n  SEL_perparty_v2.csv.5   8.136878 1.067437 1.4644246 22.37797 0.5319339\n  SEL_perparty_v2.csv.11  0        1.067437 0.6406858  0       0        \n  SEL_perparty_v2.csv.26  0        0        0.4576327  0       0        \n  SEL_perparty_v2.csv.29  0        0        1.4644246  0       3.7235374\n  SEL_perparty_v2.csv.36  8.136878 0        2.1966370  0       1.5958017\n  SEL_perparty_v2.csv.37  0        0        1.0067919  0       0.5319339\n                        features\ndocs                         oblig chariti     faith       god    forth\n  SEL_perparty_v2.csv.5  0               0 1.7819303 13.016770 5.215675\n  SEL_perparty_v2.csv.11 0               0 0          2.169462 0       \n  SEL_perparty_v2.csv.26 0.7640705       0 0          1.084731 2.086270\n  SEL_perparty_v2.csv.29 0               0 0.5939768  1.084731 4.172540\n  SEL_perparty_v2.csv.36 0               0 4.7518142 12.474405 6.258810\n  SEL_perparty_v2.csv.37 0               0 0.5939768  1.898279 0       \n[ reached max_ndoc ... 441 more documents, reached max_nfeat ... 11,770 more features ]\n\n\nLooking at the new test_dtm object, we see that it contains the 447 speeches not included in the training data, but with the same number of features and document variables. We can also see that, this time round, the object is not a random sample: the documents included are ordered numerically. In other words: all is as it should be. And with that, onwards."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#naive-bayes",
    "href": "tutorials/atap_docclass/atap_docclass.html#naive-bayes",
    "title": "Classifying American Speeches",
    "section": "Naive Bayes",
    "text": "Naive Bayes\nThe first document classification model we are going to train is of a type called Naive Bayes. We will not get into the statistical theory behind Naive Bayes too much, only pointing out the distinctive feature of this model. The name derives from the fact that the model applies Bayes’ theorem with a strong, i.e. naive, independence assumption. Basically, the model assumes that the class of any feature is independent of the class of every other feature in the set. While this is pretty simplistic, or even naive, it works well enough in practice. Let’s have a look.\nTraining the model is simple enough, requiring just a single command, aptly titled textmodel_nb():\n\nnb_model &lt;- textmodel_nb(train_dtm, y = docvars(train_dtm, \"party\"))\n\nYou see that we once again created a new variable, this time one called nb_model. We do so with the aforementioned textmodel_nb command, which takes as first argument, or input, the train_dtm object which we sampled earlier. The second argument might be a bit more confusing.\nOne way to think about it is this: we have a two dimensional object. On the x-axis, we have the training data. On the y-axis we have the variable we want to predict, in this case the party of the speaker. With Naive Bayes, training the model is basically a question of one simple calculation: given that we see a feature on the x-axis, what is the probability that the corresponding position on the y-axis is Republican. Each feature is thus assigned a probability between 0 and 1, meaning it is associated either more with the Democratic or more with the Republican party. The dividing line for the result suggested is 0.5. Simple as that, one might think. However, looking at the output of the model, you may feel a bit underwhelmed:\n\nnb_model\n\n\nCall:\ntextmodel_nb.dfm(x = train_dtm, y = docvars(train_dtm, \"party\"))\n\n Distribution: multinomial ; priors: 0.5 0.5 ; smoothing value: 1 ; 2800 training documents;  fitted features. \n\n\nWe get some information regarding the model’s distribution, priors, smoothing value, training documents and fitted features, but nothing about how well it performs."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#prediction",
    "href": "tutorials/atap_docclass/atap_docclass.html#prediction",
    "title": "Classifying American Speeches",
    "section": "Prediction",
    "text": "Prediction\nIn order to see how good our model is, we draw on the data we set aside earlier, the data for testing. In this step, we use the nb_model we trained a minute ago to predict the party to which the politicians delivering the speeches in the test data belong. The command for this is the predict function, which works as follows:\n\npred_nb &lt;- predict(nb_model, newdata = test_dtm)\n\nThe first argument is the model, on the basis of which the prediction is to be made. In this instance, we are obviously using our Naive Bayes model, stored in the nb_model object. The second argument is where we give the new data, which we set aside earlier in the test_dtm variable. With that we define a new variable, pred_nb which contains the predictions our model generated. Let’s have a look at the first twenty positions:\n\nhead(pred_nb, n = 20)\n\n  SEL_perparty_v2.csv.5  SEL_perparty_v2.csv.11  SEL_perparty_v2.csv.26 \n                    rep                     rep                     rep \n SEL_perparty_v2.csv.29  SEL_perparty_v2.csv.36  SEL_perparty_v2.csv.37 \n                    rep                     rep                     rep \n SEL_perparty_v2.csv.42  SEL_perparty_v2.csv.52  SEL_perparty_v2.csv.56 \n                    rep                     dem                     dem \n SEL_perparty_v2.csv.58  SEL_perparty_v2.csv.62  SEL_perparty_v2.csv.70 \n                    dem                     dem                     dem \n SEL_perparty_v2.csv.74  SEL_perparty_v2.csv.81  SEL_perparty_v2.csv.85 \n                    dem                     dem                     rep \nSEL_perparty_v2.csv.101 SEL_perparty_v2.csv.104 SEL_perparty_v2.csv.109 \n                    rep                     dem                     dem \nSEL_perparty_v2.csv.113 SEL_perparty_v2.csv.119 \n                    dem                     dem \nLevels: dem rep\n\n\nWhat we get in the output here is exactly what we asked for: the document title and the corresponding prediction. This is great, insofar that we can essentially check the box on classify documents. But, and this is where we’re really getting into the thick of it: how can we figure out how well our model performs?"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#confusion-matrix",
    "href": "tutorials/atap_docclass/atap_docclass.html#confusion-matrix",
    "title": "Classifying American Speeches",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe first step for this is constructing the so-called confusion matrix. The confusion matrix is a two-dimensional table which shows the actual party associated with each speech on one axis, and the predictions on the other. We can construct this using the table() function:\n\nconfmat_nb &lt;- table(prediction = pred_nb, party = docvars(test_dtm, \"party\"))\n\nCreating a new object called confmat_nb, our confusion matrix for the Naive Bayes model, we assign to it a table. The table() function takes as its arguments first the content of the rows, here the values we predicted for the pred_nb variable. Then it takes the content of the columns, here the document variable party from the test data. As you will see as soon as you look at the confmat_nb object, the labels prediction and party to which we assign the data are exactly that: the labels for the vertical and the horizontal axis. Let’s take a look:\n\nconfmat_nb\n\n          party\nprediction dem rep\n       dem 254  14\n       rep  11 168\n\n\nIf everything worked out alright, you should see two pretty high numbers in the top left and the bottom right. This is the diagonal on which the actual label and the prediction correspond. As I’m running this, I have 232 speeches by Democrats that were correctly predicted to be by Democrats, and I have 187 speeches by Republicans that were correctly predicted to be by Republicans (due to the fact that we used a random sample, your numbers may be a bit different).\nOn the other diagonal, from bottom left to top right, we have the wrong predictions. In my case here, that is 10 Democratic speeches that were classified as Republican and 18 Republican speeches that were classified as Democratic. With this representation of correct and incorrect predictions, we can already see that the model isn’t too bad. But to get an even better sense for how well it does, let’s put a number on it."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#accuracy",
    "href": "tutorials/atap_docclass/atap_docclass.html#accuracy",
    "title": "Classifying American Speeches",
    "section": "Accuracy",
    "text": "Accuracy\nGetting from the confusion matrix to a numeric representation of the accuracy is pretty straightforward. First, we take the sum of all correct predictions, then we divide it through the total sum of predictions. One line of code to save this percentage value, then we look at it, easy-peasy:\n\naccuracy_nb &lt;- (confmat_nb[1, 1] + confmat_nb[2, 2]) / sum(confmat_nb)\naccuracy_nb\n\n[1] 0.9440716\n\n\nIn my case, I get an accuracy of 94%, which is not at all shabby. You will have received a slightly different number, which is due to the fact that you have, in all conceivable likelihood, sampled a different set of speeches to train your model. More on which in a minute. But first, let’s dive a bit deeper into the Naive Bayes model and take a look at the features which are representative for each party."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#feature-table",
    "href": "tutorials/atap_docclass/atap_docclass.html#feature-table",
    "title": "Classifying American Speeches",
    "section": "Feature Table",
    "text": "Feature Table\nLet’s start with an obvious one: God. At least, it’s an obvious feature in the American political context. One party is known for heavily pandering to their god-fearing subjects, the other is known for mildly pandering to their more or less god-fearing subjects. You get the idea. Let’s see, then, how God features in our model as a predictor for the two parties. We can see that by accessing the parameters of the nb_model and specifying the parameter god:\n\nnb_model$param[, \"god\"]\n\n         dem          rep \n0.0003746412 0.0007005099 \n\n\nAgain, there will be slight differences in the output on account of us using different samples, but your output should also indicate that the feature god has roughly twice the weight for Republicans than it does for Democrats. With this example, we are obviously being facetious, but we think it’s useful to see\n\nwhat the quote-unquote insides of the model look like, and\nhow the same feature is differently weighted for the two parties.\n\nBut let’s shift our perspective somewhat: instead of assuming that we know how a feature will be differently weighted for the different parties, let’s look at the features which have the strongest weights, to find out what our data says is most typical for the Republican or the Democratic party. To do so, we are again going to use the sort() function, this time on the model parameters for the Republican party, and nest it inside the head() function to show the top thirty Republican features:\n\nhead(sort(nb_model$param[\"rep\", ], decreasing = T), n = 30)\n\n         tax    terrorist         bush         iraq        kerri       terror \n0.0027238942 0.0024983932 0.0024108645 0.0022281015 0.0021000742 0.0018203199 \n       senat      freedom         vote        enemi        iraqi          war \n0.0014107432 0.0014056640 0.0013904485 0.0013635712 0.0013563735 0.0013297102 \n afghanistan       attack     militari           re         must         peac \n0.0013009036 0.0012680546 0.0012232347 0.0011247557 0.0011068329 0.0010957369 \n      saddam       weapon       govern        georg        secur        women \n0.0010828526 0.0010556663 0.0010397783 0.0010328137 0.0010267315 0.0010240252 \n       spend        oppon        money        feder           ve          got \n0.0009987781 0.0009944085 0.0009849022 0.0009781785 0.0009770452 0.0009674753 \n\n\nThis is where it becomes interesting on a content level: we can see that many of the strongest Republican features directly reflect aspects of Republican policy priorities. There are strong flavors of national security in the top thirty features, among them terrorist, enemi, war, attack, militari, and several more. There are features pointing to fiscal policy, with tax and money; former Republican presidents, with georg and bush; and we can be pretty certain that the feature 11 refers to 9/11. Clearly, there is more to unpack here, but we’ll leave that to you, and carry on by looking at the top features in Democratic speeches:\n\nhead(sort(nb_model$param[\"dem\", ], decreasing = T), n = 30)\n\n      health        insur          kid       school       colleg        crime \n0.0017171539 0.0015274245 0.0015070969 0.0013879450 0.0012918369 0.0012412544 \n        educ         care          got           re        thing    communiti \n0.0012195919 0.0011354195 0.0011346835 0.0011338757 0.0011307551 0.0011038649 \n       parti      compani     children         bill      economi       invest \n0.0010782351 0.0010674145 0.0010626101 0.0010535669 0.0010267896 0.0010111958 \n     student          tax          lot           mr    everybodi         loan \n0.0010062883 0.0010002669 0.0009987632 0.0009961280 0.0009719973 0.0009580849 \n      welfar       budget      tonight      problem         rate        chang \n0.0009565111 0.0009430977 0.0009400048 0.0009389148 0.0009370438 0.0009370141 \n\n\nThe output again is not surprising, in the sense that the policy focus of the Democratic party also becomes visible here. There are features pointing to Medicare, with health, insur, care, likely also bill (although it will probably also be a reference to Bill Clinton, which would look exactly the same as a result of our pre-processing); education is a big theme as well, with school, colleg, educ and student featuring prominently; and there is also a sense of inclusion, with features like communiti, everybodi and tonight.\nWe should state here clearly that these feature tables are great to get a high-level view, firstly on whether the model managed to roughly identify the key features, and secondly on what the content of a corpus is, like the party programs here, but obviously no conclusions should be drawn merely on the basis of these keywords. Just in case anyone needed reminding.\nThat being said, there are many ways to mine these feature tables for insight, and we’ll discuss some of them right now."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#weighted-feature-tables",
    "href": "tutorials/atap_docclass/atap_docclass.html#weighted-feature-tables",
    "title": "Classifying American Speeches",
    "section": "Weighted Feature Tables",
    "text": "Weighted Feature Tables\nThe first weighting we do is very simple: we look at features that are relatively more important for Republicans than for Democrats. They way we do this is by dividing the the weight each feature has for Republicans by the weight the same feature has for the Democrats, like this:\n\nrel_weight_rep &lt;- (nb_model$param[\"rep\", ] / nb_model$param[\"dem\", ])\n\nBefore looking at the results, a quick sanity check. We saw above that all of the weights are rather small decimal numbers. What happens when you divide a fraction by an even smaller fraction? You get a higher number. So, if a feature has a high weight for Republicans, but a small weight for Democrats, the result of the operation above yields a relatively high number, one that is probably bigger than 1. We save the results of the operation in a new variable, rel_weight_rep, a step you’re very familiar with by now. Let’s see how this plays out, by looking at the top 30 entries:\n\nhead(sort(rel_weight_rep, decreasing = T), n = 30)\n\n   nuisanc sandinista     casper up-or-down        sdi       womb     frivol \n 375.22346  296.08089  232.92708  224.48433  220.37016  211.73288  182.42785 \n    riyadh       bork     allawi    baldrig        lsu     embryo        241 \n 155.73124  154.42994  135.28771  133.94929  126.40909  123.35388  120.12115 \n   ghadafi      paula    zarqawi     malais nicaraguan  murkowski        inf \n 116.48799  113.77908  110.04492  107.60813  105.28078  104.49887  103.63398 \n decontrol  bioshield    moammar       goos      cruis    indecis     khobar \n 103.63398  102.90982   95.32471   95.23751   93.03586   92.99762   91.32243 \n    khalid   deadlier \n  89.85255   89.68113 \n\n\nThis looks pretty different than the top 30 unweighted Republican features, right? Most obviously, there are a lot of rare words in there - sandinista or bork, anyone? - which indicates that there is a sparse data problem. This is to say that, because some features only occur a handful of times (in this case, thanks to our pre-processing, a minimum of five times), they can end up overly prominent. Our results will differ slightly from yours again, but we’ll just take this as read in the following discussion.\nNot everything we get in our output is readily interpretable, but there are some features that are insightful and point to certain hot topics. For instance, we have womb and embryo which are plausibly connected with the Republican’s stance and debates on abortion; we also find moammar and ghadafi, who was the dictator of Libya at the time these speeches were made; another feature of interest during these plague times is bioshield, which refers to a US government project aimed at setting up an early warning system for anthrax and other pathogens.\nLet’s use the same procedure to generate the equivalent list for the Democrats:\n\nrel_weight_dem &lt;- (nb_model$param[\"dem\", ] / nb_model$param[\"rep\", ])\nhead(sort(rel_weight_dem, decreasing = T), n = 30)\n\n          obes         tipper       preexist microenterpris       kathleen \n     246.11797      175.81475      170.72065      142.01188      118.25409 \n           arn      sotomayor            dlc   superhighway          felon \n     111.61178      104.39121      101.03600       96.17538       92.78167 \n       shalala        downsiz         calori           jare             bp \n      92.45516       89.88485       88.51446       84.64651       82.71321 \n          1835        aristid         turbin          romer          sonia \n      82.35565       81.89667       81.16402       79.36510       74.91888 \n     underserv        bailout     work-studi            dnc        nutriti \n      73.15704       72.40512       72.05574       69.99354       67.27705 \n        natoma        automak          autom       refinanc           ngos \n      65.28060       64.91219       64.41153       64.40296       64.05631 \n\n\nAs above, we get a bit of a mixed bag of features here, with some features being somewhat unplaceable, but others tying in to relevant policy topics. We get, for instance, obes, calori and nutriti, which all relate to diet - which ties in with public health policy; we also see the features refinanc and bailout, which are reasonably important in the context of the global financial crisis of the late 2000s.\nOkay, so while there are some insights to be gleaned here that point us to single events or referents, we are also left with a bunch of features from which we cannot readily make heads nor tails. But we can get more insightful feature tables: In order to detect broader trends, we can add biases that favor features that occur with a high frequency.\nFor this next step, take almost exactly the same measure as above, except this time round, we take the Republican weights for the features to the power of two, like this:\n\nrel_weight_rep2 &lt;- (nb_model$param[\"rep\", ]^2 / nb_model$param[\"dem\", ])\n\nSo, what are we doing here? Essentially, we are entering the exponential realm, where the changes to numbers are in reference to their original value. What this means is that the effect of an operation is bigger on the weights which had higher value starting out, than to those which had lower values.\nPerhaps this becomes clearer with a numeric example. If we take the second power of 0.1, the result is further away from the original number than if we take the second power of 0.01. We can see this by subtracting the second power:\n\n0.1 - (0.1^2)\n\n[1] 0.09\n\n0.01 - (0.01^2)\n\n[1] 0.0099\n\n\nWhy does this matter? Because the change to the rare words we saw above, which came out on top of the list because they had higher weights, is bigger than the change to the more frequent words, which did not make the top pick because they had lower weights. Since we are keeping the Democrat weights the way they were, we are basically evening the playing field in our variable rel_weight_rep2. Let’s see how that affects the top 30 feature table:\n\nhead(sort(rel_weight_rep2, decreasing = T), n = 30)\n\n       nuisanc     sandinista         frivol         casper     up-or-down \n    0.15116820     0.09412418     0.07736537     0.05825329     0.05410689 \n           sdi           womb          kerri          index      terrorist \n    0.05214180     0.04813457     0.03329440     0.03088833     0.02622593 \n        riyadh           bork         liabil    bush-cheney      nicaragua \n    0.02603946     0.02560610     0.02475248     0.02051168     0.02005364 \n          bush          libya         allawi        baldrig        lawsuit \n    0.01997794     0.01985878     0.01965156     0.01926466     0.01772052 \n           lsu         embryo          iraqi            241          regim \n    0.01715683     0.01633752     0.01589467     0.01549243     0.01547855 \ntwo-and-a-half           wyom        ghadafi         terror          paula \n    0.01488313     0.01481652     0.01456944     0.01429368     0.01389970 \n\n\nAfter all the conceptual trouble we went through for these results, you may be a bit underwhelmed to find a lot of the same words cropping up. In our example, we find some shifts that indicate broader trends, and you’ll likely see one or two more mainstream words in your top 30 feature table us well, if you look closely.\nIn our case, the relevant new features are regim, iraqi and libya. As mentioned earlier, Libya and former dictator Muammar Gahadafi were big topics at the time, and we think it’s telling that libya and regim only shows up in the feature table after this weighting step, since it’s quite likely that the Democratic candidates would also have discussed the Libyan situation frequently during this period. The same, of course, goes for the iraqi feature.\nOkay, now let’s see how things stand for the Democrats:\n\nrel_weight_dem2 &lt;- (nb_model$param[\"dem\", ]^2 / nb_model$param[\"dem\", ])\nhead(sort(rel_weight_dem2, decreasing = T), n = 30)\n\n      health        insur          kid       school       colleg        crime \n0.0017171539 0.0015274245 0.0015070969 0.0013879450 0.0012918369 0.0012412544 \n        educ         care          got           re        thing    communiti \n0.0012195919 0.0011354195 0.0011346835 0.0011338757 0.0011307551 0.0011038649 \n       parti      compani     children         bill      economi       invest \n0.0010782351 0.0010674145 0.0010626101 0.0010535669 0.0010267896 0.0010111958 \n     student          tax          lot           mr    everybodi         loan \n0.0010062883 0.0010002669 0.0009987632 0.0009961280 0.0009719973 0.0009580849 \n      welfar       budget      tonight      problem         rate        chang \n0.0009565111 0.0009430977 0.0009400048 0.0009389148 0.0009370438 0.0009370141 \n\n\nIn our results, we see a rather pronounced change in the output once we apply this weighting to the Democrat features. The features showing up in the top thirty here are fairly similar to the unweighted features, and point to broadly recognizable policy issues: health, insur, care and bill are likely in reference to Medicare for All; kid, school, educ, student and college point to the education platform which a lot of Democratic candidates highlight.\nLet’s look at two more ways of biasing the feature tables in favor of frequent words. The next one is a lot more intuitive: we take the weights, and multiply them by the document frequencies for each feature. The more frequent a feature, the higher the result, and words mentioned particularly frequently, for instance because they were hot topics for longer periods of time, are boosted. Easy peasy:\n\nrel_weight_rep3 &lt;- (nb_model$param[\"rep\", ] * doc_freq)\nhead(sort(rel_weight_rep3, decreasing = T), n = 30)\n\n      tax        re     senat        ve   freedom    govern    presid   america \n 3.968714  2.960357  2.710038  2.579399  2.440233  2.289592  2.221753  2.190247 \n      war      must    nation      vote         t     secur       got      bush \n 2.154130  2.079739  2.055121  2.043959  2.011945  1.986725  1.981389  1.899761 \n      job     world terrorist      unit         m        ll      busi    terror \n 1.803758  1.784602  1.763866  1.746628  1.733413  1.658925  1.631931  1.629186 \n    women      need  congress      peac     state       get \n 1.609768  1.607389  1.574204  1.566904  1.557564  1.555780 \n\n\nWe are really entering mainstream topics now, and the features start to tie in with the Republican policy themes on a major level: we get tax, freedom, secur, job and busi, pointing strongly to Republican domestic policy focal points; we get foreign policy for a turbulent world with war, terror, forc, iraq; and we get aspects of US gouvernance, with state, presid, nation, vote and bush.\nLet’s also take a look at the top Democratic features under this weighting scheme:\n\nrel_weight_dem3 &lt;- (nb_model$param[\"dem\", ] * doc_freq)\nhead(sort(rel_weight_dem3, decreasing = T), n = 30)\n\n       re     thing         t    school       got    health        ve     think \n 2.984361  2.676497  2.650152  2.441395  2.323832  2.323309  2.293171  2.192446 \n       go       get      care  children       job       don      educ     world \n 2.158660  2.127033  2.110745  2.109281  2.074797  2.059541  2.045256  1.916136 \n      say communiti   economi       lot    famili      give      just    becaus \n 1.900879  1.898648  1.896480  1.879672  1.768832  1.746722  1.744852  1.736892 \n   believ       new     right       let      need     chang \n 1.716312  1.701343  1.698826  1.698398  1.682909  1.674444 \n\n\nHere, we also get a rather distinct set of new words in the table: we still have aspects of care and health, as well as educ and school, but on top of that we get some value-based features like right and just. This weighting also shows a bit of how Democrats argue: there is a flavor of causality with becaus and need, and there are calls for the new and chang.\nWhat cropped up in the top features for both parties are contractions: re, ve, don, t, and m. These are words that are generally frequent, and our frequency weighting now has the side effect that entries for frequent words overlap.\nOne could imagine a research question here: how well can document classification distinguish between American and British politicians on the basis of speeches?\nLet’s look at a final weighting scheme. We use almost the same approach as above, but this time round we multiply the feature weights with the logarithms of the document frequency. This is different compared to the other approaches in that it still somewhat favors frequent words, but the gap between the more and less frequent words becomes relatively small. The danger that generally frequent words, e.g. contractions, move up to the top gets smaller. In case you need convincing of this, just take a quick look at the first few document frequencies and their logarithms:\n\nhead(doc_freq)\n\nchristian     usual     think    christ      term     oblig \n      143       278      2630        30       954       559 \n\nlog(head(doc_freq))\n\nchristian     usual     think    christ      term     oblig \n 4.962845  5.627621  7.874739  3.401197  6.860664  6.326149 \n\n\nSo far, so good. Now let’s take a look at how this impacts the top thirty features:\n\nrel_weight_rep4 &lt;- (nb_model$param[\"rep\", ] * log(doc_freq))\nhead(sort(rel_weight_rep4, decreasing = T), n = 30)\n\n        tax   terrorist        bush        iraq      terror       kerri \n0.019841213 0.016388498 0.016079256 0.014557945 0.012372394 0.011178716 \n      senat     freedom        vote         war       enemi          re \n0.010666067 0.010485324 0.010140565 0.009826799 0.008897342 0.008858013 \n   militari      attack afghanistan        must       iraqi      govern \n0.008631359 0.008604496 0.008551724 0.008343854 0.008140160 0.008003300 \n       peac       secur          ve       women         got       spend \n0.007961000 0.007770163 0.007697684 0.007536932 0.007376630 0.007268355 \n      money      weapon       feder         men        forc       georg \n0.007211957 0.007211124 0.007104833 0.007052529 0.007015802 0.006860451 \n\n\nThe changes compared to the earlier weightings are relatively slight here, but there are some different features which offer a new policy flavor. While tax appears in most of the Republican top features, regardless of the approach to weighting, here we get the features money and spend, which chimes well with Republican fiscal policy.\nFinally, let’s see what this weighting yields for the Democrats:\n\nrel_weight_dem4 &lt;- (nb_model$param[\"dem\", ] * log(doc_freq))\nhead(sort(rel_weight_dem4, decreasing = T), n = 30)\n\n     health      school         kid       insur        educ          re \n0.012380816 0.010371420 0.010276779 0.009818446 0.009055179 0.008929837 \n     colleg       thing         got        care   communiti       crime \n0.008862859 0.008785265 0.008651529 0.008547204 0.008223881 0.008169968 \n   children       parti        bill     economi         lot           t \n0.008068796 0.007859177 0.007738153 0.007722811 0.007530765 0.007440632 \n        tax      invest     compani          mr       chang   everybodi \n0.007286079 0.007191956 0.007106857 0.007016758 0.007016636 0.006959568 \n    problem          ve         job         don     student       money \n0.006931770 0.006843494 0.006843016 0.006825546 0.006808739 0.006796932 \n\n\nAgain, we see many familiar features, but we get a few features which bring in something new. We see problem, which is a good springboard for any policy platform; we also get percent, which may point to a mode of argumentation; and everybodi, augmenting the message of inclusion which has been part of previous feature tables in the form of communiti.\nHaving looked at all these different weightings and brought our perspective as politically interested researchers to them, we ought to issue a caveat: Obviously, we would have to go and look at the speeches in detail to draw meaningful conclusions about what the policy programs and interests of the Republican and Democratic parties are. What we wanted to demonstrate here is that the feature tables and the different weightings offer a strong flavor as to the most important topics, but this should not be confused with actual political demands.\nAfter this extensive trawl through the feature tables, two topics remain for us to cover together here. The first of these are alternatives to the Naive Bayes model. Second, and ultimately, we are going to point out some steps which could be summarized aptly under Best Practices."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#logistic-regression-model",
    "href": "tutorials/atap_docclass/atap_docclass.html#logistic-regression-model",
    "title": "Classifying American Speeches",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\nAll classification algorithms have their pros and cons, usually resulting in a trade-off the researcher has to make between the amount of computational capacity required, the accuracy of the model, the interpretability etc. With the Naive Bayes model, we came down on the side of the less computationally intensive and less accurate end. With the logistic regression model, we ramp up both the computational requirements as well as the accuracy, while still remaining easily interpretable. You’ll see what we mean in a second.\nA lot of what we are doing here should look very familiar. First, we create a new variable, called lr_model. We assign to it a text model based on the logistic regression algorithm, which we create using one of these aptly named quanteda functions: textmodel_lr. The arguments are identical to the arguments in the Naive Bayes approach. First, we throw in the training data, then we specify the document variable we want to train on. We run this command, and take a peek at the lr_model:\n\nlr_model &lt;- textmodel_lr(train_dtm, y = docvars(train_dtm, \"party\"))\nlr_model\n\n\nCall:\ntextmodel_lr.dfm(x = train_dtm, y = docvars(train_dtm, \"party\"))\n\n2,800 training documents; 11,780 fitted features.\nMethod: binomial logistic regression\n\n\nAs with the nb_model, just looking at the lr_model does not yield terrific insight, beyond the fact that the model looks the way it ought. We can tell this from the description: we input 2,800 training documents and have 11,785 features, as always in our data. Most importantly for the current purpose, we receive confirmation that the method is that of the binomial logistic regression. Perfect!\nTo see how well the logistic regression model does, we take exactly the same steps as we did for the Naive Bayes model: first, we use the model to predict the party of the speeches in the test data. Then, we create a confusion matrix. We also already draw up the confusion matrix of the nb_model, so we can easily compare the two.\n\npred_lr &lt;- predict(lr_model, newdata = test_dtm)\nconfmat_lr &lt;- table(prediction = pred_lr, PARTY = docvars(test_dtm, \"party\"))\nconfmat_lr\n\n          PARTY\nprediction dem rep\n       dem 263  14\n       rep   2 168\n\nconfmat_nb\n\n          party\nprediction dem rep\n       dem 254  14\n       rep  11 168\n\n\nAs above, we will not get exactly the same numbers here, but unless something went terribly wrong, you should see that the logistic regression model is somewhat more successful at correctly predicting the party.\nLet’s go a step further and put the proverbial and actual number on it. We discussed earlier how we can calculate the accuracy from the confusion matrix. So, we sum again the number of correct predictions and divide them by the total number of predictions. And for easier comparison, we’ll draw up the accuracy of the Naive Bayes model as well:\n\naccuracy_lr &lt;- (confmat_lr[1, 1] + confmat_lr[2, 2]) / sum(confmat_lr)\naccuracy_lr\n\n[1] 0.9642058\n\naccuracy_nb\n\n[1] 0.9440716\n\n\nWhat we see in the confusion matrix is, of course, mirrored in the accuracy of the models. In my case, the logistic regression model is approximately 2.5 percentage points more accurate than the Naive Bayes model. Does this roughly match the difference you see between your two models?"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#differences-explained",
    "href": "tutorials/atap_docclass/atap_docclass.html#differences-explained",
    "title": "Classifying American Speeches",
    "section": "Differences Explained",
    "text": "Differences Explained\nThat we get different results when creating models using different algorithms will not surprise you whatsoever. But you might have noticed a different theme running through the last few steps: we are not able to say confidently that the numbers we get are exactly the ones you are seeing. The reason, as alluded to briefly above, is that chances are really high that you are training your model on a different sample of speeches than we are.\nAnd you can see for yourself how this works. We are going to repeat the steps from the data sampling to the calculation of the accuracy. The only thing that changes is the set of training data that is sampled initially. You can either just let the code do its thing and check out the results; or you can quickly try to recall for yourself what each line does (and sneak a peek at the descriptions above, in case you can’t reconstruct why we are doing what). For didactic purposes, we obviously recommend the latter, but we can’t hold a grudge against you if you just want to see the results:\n\ntrain_dtm2 &lt;- dfm_sample(dtm, size = 2800)\ntest_dtm2 &lt;- dtm[setdiff(docnames(dtm), docnames(train_dtm2)), ]\nlr_model2 &lt;- textmodel_lr(train_dtm2, y = docvars(train_dtm2, \"party\"))\npred_lr2 &lt;- predict(lr_model2, newdata = test_dtm)\nconfmat_lr2 &lt;- table(prediction = pred_lr2, PARTY = docvars(test_dtm, \"party\"))\nconfmat_lr2\n\n          PARTY\nprediction dem rep\n       dem 265   2\n       rep   0 180\n\nconfmat_lr\n\n          PARTY\nprediction dem rep\n       dem 263  14\n       rep   2 168\n\n\nYou’ll find that the confusion matrices show a visible difference between the two logistic regression models, trained on two different samples. Constituted by the fact of how we calculate it, this difference is obviously also reflected in the accuracy:\n\naccuracy_lr2 &lt;- (confmat_lr2[1, 1] + confmat_lr2[2, 2]) / sum(confmat_lr2)\naccuracy_lr2\n\n[1] 0.9955257\n\naccuracy_lr\n\n[1] 0.9642058\n\n\nThe difference we see here in the confusion matrix and the accuracy have a pretty detrimental implication for any scientific application of document classification: the results are not robustly replicable in the face of random sampling. To prevent you from turning in naive results (pardon the pun), we’ll discuss next what you can do in the face of this issue."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#best-practices",
    "href": "tutorials/atap_docclass/atap_docclass.html#best-practices",
    "title": "Classifying American Speeches",
    "section": "Best Practices",
    "text": "Best Practices\nIn any field, whether empirical or theoretical, part of what constitutes good academic practice is to be transparent about a) what the steps you have taken to arrive from your starting point to your conclusion and b) what the limitations and affordances of your methodology are.\nAs far as the steps go, we have given you a detailed blow-by-blow instruction about how we get from the input data to meaningful results, both in terms of document classification accuracy and salient features. If you made it this far into this notebook you must also be quite convinced of the affordances of document classification. What we are going to discuss next is probably the best way to address the limitations of document classification which arise from sampling."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#loops",
    "href": "tutorials/atap_docclass/atap_docclass.html#loops",
    "title": "Classifying American Speeches",
    "section": "Loops",
    "text": "Loops\nSince it’s a bit tedious to run through the whole sequence of commands from sampling to calculating the accuracy, we can use a loop to go through this process any given number of times (although here, as with the choice of algorithm, there is a small trade-off between the number of times we can and want to do this and the benefits to robustness). Let’s take a look at how this loop thing works.\nFirst, we construct a new variable. Let’s call it accuracy. We’ll be basing our loop on this, so we will make accuracy a vector, a list of numbers, whose length corresponds to the number of times we want to run our loop. We can do this as follows:\n\naccuracy &lt;- 1:100\n\nIf you look at accuarcy, you can see that it’s just a vector containing the integers 1 through 100:\n\naccuracy\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nNow for the loop. If you’ve never come across this before, it might seem like a bit of an unwieldy construction - in which case, we recommend focusing first on the six lines of code inside the { curly brackets }:\n\nfor (i in accuracy) {\n    train_dtm &lt;- dfm_sample(dtm, size = 2800)\n    test_dtm &lt;- dtm[setdiff(docnames(dtm), docnames(train_dtm)), ]\n    lr_model &lt;- quanteda.textmodels::textmodel_nb(train_dtm, docvars(train_dtm, \"party\"))\n    pred_lr &lt;- predict(lr_model, newdata = test_dtm)\n    confmat_lr &lt;- table(prediction = pred_lr, PARTY = docvars(test_dtm, \"party\"))\n    accuracy[i] &lt;- (confmat_lr[1, 1] + confmat_lr[2, 2]) / sum(confmat_lr)\n}\n\nWhat you find inside the curly brackets should, for the most part, be familiar. In the first line, we have the sampling of the training data. This is the essential step, which allows us to identify how robust our results are. The second line defines the test data in relation to the training data. In the third line, we train our linear regression model. The fourth line uses the model to predict the parties of the speeches in the test data. The fifth line creates the by now familiar confusion matrix. And finally, in the sixth line, we calculate the accuracy, and save it to our variable, accuracy.\nThe one unfamiliar element occurs in this sixth line, where we indicate that the accuracy is to be saved to the index position [i] in the vector accuracy. This starts to make sense if we look at what is outside the curly brackets. The line which we actually start the command with is for (i in accuracy), followed by what is in the brackets.\nThis basically states that for each index, each entry, in the vector accuracy, the sequence of commands in the curly brackets should be executed. With the sixth line inside the curly bracket, we make use of the fact that we can overwrite any single value contained within the vector accuracy, without altering the length of the vector.\nWhen we execute the loop, the six lines of code inside the curly brackets run twenty times. The first time, the accuracy that we calculate is saved in the first position of the vector, overwriting the number 1 that was there before. The second time, the calculated accuracy overwrites the second position of the vector, where the number 2 was before. And so on, until each of the twenty integers has been replaced with the accuracy of twenty models trained on twenty different sets of training data.\nWe can see that this works by looking at the variable accuracy once we’ve run the loop:\n\naccuracy\n\n  [1] 0.9328859 0.9261745 0.9328859 0.9306488 0.9485459 0.9552573 0.9485459\n  [8] 0.9373602 0.9418345 0.9485459 0.9328859 0.9284116 0.9261745 0.9507830\n [15] 0.9574944 0.9239374 0.9463087 0.9395973 0.9485459 0.9395973 0.9351230\n [22] 0.9642058 0.9574944 0.9395973 0.9328859 0.9351230 0.9373602 0.9373602\n [29] 0.9530201 0.9395973 0.9194631 0.9418345 0.9373602 0.9485459 0.9351230\n [36] 0.9463087 0.9507830 0.9574944 0.9463087 0.9373602 0.9440716 0.9552573\n [43] 0.9395973 0.9284116 0.9306488 0.9328859 0.9507830 0.9574944 0.9418345\n [50] 0.9463087 0.9194631 0.9440716 0.9664430 0.9463087 0.9418345 0.9284116\n [57] 0.9306488 0.9194631 0.9373602 0.9306488 0.9373602 0.9440716 0.9418345\n [64] 0.9485459 0.9217002 0.9418345 0.9373602 0.9261745 0.9418345 0.9373602\n [71] 0.9328859 0.9507830 0.9328859 0.9507830 0.9440716 0.9664430 0.9507830\n [78] 0.9440716 0.9060403 0.9306488 0.9418345 0.9395973 0.9351230 0.9373602\n [85] 0.9373602 0.9351230 0.9306488 0.9395973 0.9194631 0.9597315 0.9328859\n [92] 0.9239374 0.9485459 0.9507830 0.9194631 0.9463087 0.9597315 0.9328859\n [99] 0.9217002 0.9239374\n\n\nWhere before we had integers, we now have values somewhere in the ballpark of 92-98% (also depending on whether we use Naive Bayes or the more performant Logistic Regression). On the one hand this shows us that there is some observable variation in the accuracy of our models which is predicated on the data sample the model is trained on. On the other hand, it shows us that, regardless of the training data, the models all reach a decent degree of accuracy somewhere above 90% also in a worst-case scenario."
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#demonstrating-robustness",
    "href": "tutorials/atap_docclass/atap_docclass.html#demonstrating-robustness",
    "title": "Classifying American Speeches",
    "section": "Demonstrating Robustness",
    "text": "Demonstrating Robustness\nWhile it’s easy enough to take in the results of the loop if we’re working with a fairly low number of iterations, it becomes a bit more challenging if we consider running this loop hundreds of times. In those cases, we can summarize our results in some key statistics and show them graphically in the form of a histogram. We aren’t going for any shiny visuals here, but you can find many a good instruction into how to make those. We’ll just show you the bare-bones approaches here.\nOne of the easiest ways to get a summary of a numerical vector is the summary() command, which is part of basic R functionality. Let’s take a look:\n\nsummary(accuracy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9060  0.9329  0.9396  0.9396  0.9485  0.9664 \n\n\nWe get several statistics here. On either pole of the output we see the minimum and maximum values contained in the vector. In my case, the worst model achieved an accuracy of 91% while the best model achieved an accuracy of 97%. In between these two poles we have the median and the arithmetic mean, the colloquial (sometimes even pedestrian) average. In my case, both the medium and the mean landed on 94.2%, indicating both that the module robustly achieves an accuracy of 94% and that the accuracies are indeed normally distributed.\nWe can also visualize this normal distribution in a histogram, using the hist() command. The first argument here is the data, our accuarcy vector, and the second argument makes the whole graphic more visually palatable (try it without this specification to see what we’re talking about):\n\naccuracy %&gt;%\n    as.data.frame() %&gt;%\n    dplyr::rename(accuracy = 1) %&gt;%\n    ggplot(aes(x = accuracy)) +\n    geom_histogram() +\n    theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWhat you see in the bottom right quadrant should roughly resemble a normal distribution - although we will add the disclaimer that there is a chance that it doesn’t, on account of the fact that we don’t have that many data points. Once we reach somewhere around a thousand runs, we should more or less reliably arrive at a normal distribution. But be warned: if you do that, it’s probably worth starting the loop and then going to fix yourself a snack. Unless you have a really good computer, this could take a while.\nWhat you’ll also notice about the graph we just created is that it’s centered around the mean of our distribution. This is great when you want to see what the distribution looks like. If you’ve arrived at the point where you want to brag about how good your results are, we recommend plotting the histogram showing all of the x-axis between 0 and 1. We can do this with the additional specification of xlim=0:1. Like so:\n\naccuracy %&gt;%\n    as.data.frame() %&gt;%\n    dplyr::rename(accuracy = 1) %&gt;%\n    ggplot(aes(x = accuracy)) +\n    geom_histogram() +\n    coord_cartesian(xlim = c(0, 1)) +\n    theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNow you can see that the results cluster fairly heavily in the far right of the graph, as expected. The final metric we want to draw your attention to, in case it hasn’t sprung to mind yet, is the standard deviation. There is a technical definition that you can look up at your leisure; but in short it describes how widely the data is scattered around the mean. The heuristic is that, the higher the standard deviation, the less robust the results. Let’s take a look at the standard deviations for our accuracies:\n\nsd(accuracy)\n\n[1] 0.01152176\n\n\nIn my case, I get a standard deviation of 1.2 percentage points. This translates to the following: roughly 66% of all models achieve an accuracy of 94.2% (the mean), plus and minus 1.2 percentage points. So two thirds of the models make predictions that are 93% to 95.4% accurate.\nThere are other methods to measure robustness, a popular one is n-fold cross validation, in which we train n times, with a specific 1-1/n section used for training and 1/n for testing, in such a way that every section is used once for testing. You could program n-fold cross validation as an advanced exercise. Using a loop with random sampling is a simple way to explore robustness, albeit less systematic than cross-validation.\nIf you always want to get the same result, you can also force the sampling to always return the same documents, by setting a seed for the random calculation with the set.seed() function. While this ensures reproducibility, it may hide robustness problems, and this is why we chose not to use it.\nFor a spam filter, the performance of our system would clearly be insufficient (we can see at least one of us biting into his laptop if every 20th spam mail reached our inbox). But as you have seen, there are various ways in which a higher accuracy can be achieved, from the specific pre-processing steps to the choice of algorithms. While there are dozens, if not hundreds, of other parameters that can be tweaked, after working your way through this introduction you should be ready to start your own document classification experiments and do cool stuff! Let’s get on it!"
  },
  {
    "objectID": "tutorials/atap_docclass/atap_docclass.html#footnotes",
    "href": "tutorials/atap_docclass/atap_docclass.html#footnotes",
    "title": "Classifying American Speeches",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs an aside, we once again see some of the potential harm done by pre-processing: we have the feature us ranking really highly here, but we cannot be sure whether this is a plural, inclusive pronoun, or whether this is a botched version of the United States. Depending on what you are working towards, you might be interested in keeping abbreviations like this intact.↩︎"
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#wordclouds",
    "href": "tutorials/textanalysis/textanalysis.html#wordclouds",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Wordclouds",
    "text": "Wordclouds\nAlternatively, word frequency lists can be visually represented as word clouds, though they provide less detailed information. Word clouds are visual representations where words appear larger based on their frequency, offering a quick visual summary of word importance in a dataset.\n\n# create a word cloud visualization\ntext %&gt;%\n  # Convert text data to a quanteda corpus\n  quanteda::corpus() %&gt;%\n  # tokenize the corpus, removing punctuation\n  quanteda::tokens(remove_punct = TRUE) %&gt;%\n  # remove English stopwords\n  quanteda::tokens_remove(stopwords(\"english\")) %&gt;%\n  # create a document-feature matrix (DFM)\n  quanteda::dfm() %&gt;%\n  # generate a word cloud using textplot_wordcloud\n  quanteda.textplots::textplot_wordcloud(\n    # maximum words to display in the word cloud\n    max_words = 150,\n    # determine the maximum size of words\n    max_size = 10,\n    # determine the minimum size of words\n    min_size = 1.5,\n    # Define a color palette for the word cloud\n    color = scales::viridis_pal(option = \"A\")(10)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nThe textplot_wordcloud function creates a word cloud visualization of text data in R. Its main arguments are x (a Document-Feature Matrix or DFM), max_words (maximum words to display), and color (color palette for the word cloud).\n\n\n\n\n\n\nAnother form of word clouds, known as comparison clouds, is helpful in discerning disparities between texts. For instance, we can load various texts and assess how they vary in terms of word frequencies. To illustrate this, we’ll load Herman Melville’s Moby Dick, George Orwell’s 1984, and Charles Darwin’s Origin.\nFirst, we’ll load these texts and combine them into single documents.\n\n# load data\norwell_sep &lt;- base::readRDS(\"tutorials/textanalysis/data/orwell.rda\", \"rb\")\norwell &lt;- orwell_sep %&gt;%\n  paste0(collapse = \" \")\nmelville_sep &lt;- base::readRDS(\"tutorials/textanalysis/data/melville.rda\", \"rb\")\nmelville &lt;- melville_sep %&gt;%\n  paste0(collapse = \" \")\ndarwin_sep &lt;- base::readRDS(\"tutorials/textanalysis/data/darwin.rda\", \"rb\")\ndarwin &lt;- darwin_sep %&gt;%\n  paste0(collapse = \" \")\n\nNow, we generate a corpus object from these texts and create a variable with the author name.\n\ncorp_dom &lt;- quanteda::corpus(c(darwin, melville, orwell))\nattr(corp_dom, \"docvars\")$Author &lt;- c(\"Darwin\", \"Melville\", \"Orwell\")\n\nNow, we can remove so-called stopwords (non-lexical function words) and punctuation and generate the comparison cloud.\n\n# create a comparison word cloud for a corpus\ncorp_dom %&gt;%\n  # tokenize the corpus, removing punctuation, symbols, and numbers\n  quanteda::tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE,\n    remove_numbers = TRUE\n  ) %&gt;%\n  # remove English stopwords\n  quanteda::tokens_remove(stopwords(\"english\")) %&gt;%\n  # create a Document-Feature Matrix (DFM)\n  quanteda::dfm() %&gt;%\n  # group the DFM by the 'Author' column from 'corp_dom'\n  quanteda::dfm_group(groups = corp_dom$Author) %&gt;%\n  # trim the DFM, keeping terms that occur at least 10 times\n  quanteda::dfm_trim(min_termfreq = 10, verbose = FALSE) %&gt;%\n  # generate a comparison word cloud\n  quanteda.textplots::textplot_wordcloud(\n    # create a comparison word cloud\n    comparison = TRUE,\n    # set colors for different groups\n    color = c(\"darkgray\", \"orange\", \"purple\"),\n    # define the maximum number of words to display in the word cloud\n    max_words = 150\n  )"
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#frequency-changes",
    "href": "tutorials/textanalysis/textanalysis.html#frequency-changes",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Frequency changes",
    "text": "Frequency changes\nWe can also explore how the term alice is used throughout the chapters of our example text. To begin, let’s extract the word count for each chapter.\n\n# extract the number of words per chapter\nWords &lt;- text_chapters %&gt;%\n  # split each chapter into words based on spaces\n  stringr::str_split(\" \") %&gt;%\n  # measure the length (number of words) in each chapter\n  lengths()\n# display the resulting data, which contains the word counts per chapter\nWords\n\n [1]    8 2364 2125 1765 2616 2339 2609 2307 2487 2272 2058 1886 2153\n\n\nNext, we extract the number of matches in each chapter.\n\n# extract the number of matches of \"alice\" per chapter\nMatches &lt;- text_chapters %&gt;%\n  # count the number of times \"alice\" appears in each chapter\n  stringr::str_count(\"alice\")\n# display the resulting data, which shows the number of matches of \"alice\" per chapter\nMatches\n\n [1]  1 28 26 23 31 35 43 51 39 52 30 16 23\n\n\nNow, we extract the names of the chapters and create a table with the chapter names and the relative frequency of matches per 1,000 words.\n\n# extract chapters\nChapters &lt;- paste0(\"chapter\", 0:(length(text_chapters) - 1))\nChapters\n\n [1] \"chapter0\"  \"chapter1\"  \"chapter2\"  \"chapter3\"  \"chapter4\"  \"chapter5\" \n [7] \"chapter6\"  \"chapter7\"  \"chapter8\"  \"chapter9\"  \"chapter10\" \"chapter11\"\n[13] \"chapter12\"\n\n\nNext, we combine the information in a single data frame and add a column containing the relative frequency of alice in each chapter.\n\n# create table of results\ntb &lt;- data.frame(Chapters, Matches, Words) %&gt;%\n  # create new variable with the relative frequency\n  dplyr::mutate(Frequency = round(Matches / Words * 1000, 2)) %&gt;%\n  # reorder chapters\n  dplyr::mutate(Chapters = factor(Chapters, levels = c(paste0(\"chapter\", 0:12))))\n\n\n\nChaptersMatchesWordsFrequencychapter018125.00chapter1282,36411.84chapter2262,12512.24chapter3231,76513.03chapter4312,61611.85chapter5352,33914.96chapter6432,60916.48chapter7512,30722.11chapter8392,48715.68chapter9522,27222.89chapter10302,05814.58chapter11161,8868.48chapter12232,15310.68\n\n\nNow, let’s visualize the relative frequencies of our search term in each chapter.\n\n# create a plot using ggplot\nggplot(tb, aes(x = Chapters, y = Frequency, group = 1)) +\n  # add a smoothed line (trendline) in purple color\n  geom_smooth(color = \"purple\") +\n  # add a line plot in dark gray color\n  geom_line(color = \"darkgray\") +\n  # remove fill from the legend\n  guides(color = guide_legend(override.aes = list(fill = NA))) +\n  # set a white and black theme\n  theme_bw() +\n  # rotate x-axis text by 45 degrees and adjust alignment\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  # customize the y-axis label\n  scale_y_continuous(name = \"Relative Frequency (per 1,000 words)\")"
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#dispersion-plots",
    "href": "tutorials/textanalysis/textanalysis.html#dispersion-plots",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Dispersion plots",
    "text": "Dispersion plots\nTo show when in a text or in a collection of texts certain terms occur, we can use dispersion plots. The quanteda package offers a very easy-to-use function textplot_xray to generate dispersion plots.\n\n# add chapter names\nnames(text_chapters) &lt;- Chapters\n# generate corpus from chapters\n\n# Assuming text_chapters is your character vector\n# text_corpus &lt;- quanteda::corpus(text_chapters)\n\n# Create a tokens object\ntext_corpus &lt;- quanteda::tokens(text_chapters)\n\n# generate dispersion plots\nquanteda.textplots::textplot_xray(kwic(text_corpus, pattern = \"alice\"),\n  kwic(text_corpus, pattern = \"hatter\"),\n  sort = T\n)\n\n\n\n\n\n\n\n\nWe can modify the plot by saving it into an object and then use ggplot to modify it appearance.\n\n# generate and save dispersion plots\ndp &lt;- quanteda.textplots::textplot_xray(\n  kwic(text_corpus, pattern = \"alice\"),\n  kwic(text_corpus, pattern = \"cat\")\n)\n# modify plot\ndp + aes(color = keyword) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#over--and-underuse",
    "href": "tutorials/textanalysis/textanalysis.html#over--and-underuse",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Over- and underuse",
    "text": "Over- and underuse\nFrequency data serves as a valuable lens through which we can explore the essence of a text. For instance, when we examine private dialogues, we often encounter higher occurrences of second-person pronouns compared to more formal text types like scripted monologues or speeches. This insight holds the potential to aid in text classification and assessing text formality.\nTo illustrate, consider the following statistics: the counts of second-person pronouns, you and your, as well as the total word count excluding these pronouns in private dialogues versus scripted monologues within the Irish segment of the International Corpus of English (ICE). Additionally, the tables provide the percentage of second-person pronouns in both text types, enabling us to discern whether private dialogues indeed contain more of these pronouns compared to scripted monologues, such as speeches.\n\n\n.Private dialoguesScripted monologuesyou, your6761659Other words259625105295Percent2.600.63\n\n\nThis straightforward example highlights that second-person pronouns constitute 2.6 percent of all words in private dialogues, yet they represent only 0.63 percent in scripted speeches. To vividly illustrate such variations, we can employ association and mosaic plots, which offer effective visual presentations.\n\n# create a matrix 'd' with the specified values and dimensions\nd &lt;- matrix(c(6761, 659, 259625, 105295), nrow = 2, byrow = TRUE)\n# assign column names to the matrix\ncolnames(d) &lt;- c(\"D\", \"M\")\n# assign row names to the matrix\nrownames(d) &lt;- c(\"you, your\", \"Other words\")\n# generate an association plot using 'assocplot' function\nassocplot(d)\n\n\n\n\n\n\n\n\nIn an association plot, bars above the dashed line signify relative overuse, while bars below indicate relative underuse. Accordingly, the plot reveals that in monologues, there’s an underuse of you and your and an overuse of other words. Conversely, in dialogues, the opposite patterns emerge: an overuse of you and your and an underuse of other words. This visual representation helps us grasp the distinctive word usage patterns between these text types."
  },
  {
    "objectID": "tutorials/textanalysis/textanalysis.html#footnotes",
    "href": "tutorials/textanalysis/textanalysis.html#footnotes",
    "title": "Practical Overview of Selected Text Analytics Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [bibliography file](/assets/bibliography.bib and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "tutorials/reinfnlp/reinfnlp.html#preparation-and-session-set-up",
    "href": "tutorials/reinfnlp/reinfnlp.html#preparation-and-session-set-up",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts in this tutorial are executed without errors. Before continuing, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\nFor this tutorial we will be primarily requiring four packages: tidytext for text manipulations, tidyverse for general tasks, textrank for the implementation of the TextRank algorithm and rvest to scrape through an article to use as an example. For this analysis an article for Time has been selected.\n\n# set options\noptions(stringsAsFactors = F)\n# install libraries\ninstall.packages(c(\"tidytext\", \"tidyverse\", \"textrank\", \"rvest\", \"ggplot2\"))\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow that we have installed the packages, we activate them as shown below.\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 12) # suppress math annotation\n# activate packages\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(textrank)\nlibrary(rvest)\nlibrary(ggplot2)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R and RStudio and also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/reinfnlp/reinfnlp.html#text-summarisation",
    "href": "tutorials/reinfnlp/reinfnlp.html#text-summarisation",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Text Summarisation",
    "text": "Text Summarisation\nA deep reinforced model for text summarisation involves sequence of input tokens x={x1,x2,…,xn} and produces a sequence of output (summary) tokens. A schematic presentation of the process is shown below:\n\nFor the article summarisation objective the deep RL has the following components:\n\nAction which involves a function ut which copies and generates summary output yt\nState it encapsulates the hidden states of encoder and previous outputs\nReward which generates a rough score determining the performance of the summarisation\n\nText summarisation (see Mihalcea and Tarau) is highly critical in extracting important information from large texts.\nIn case of text summarisation there are broadly two categories:\n\nExtractive Summarisation\nAbstractive Summarisation\n\nIn case of Extractive Summarisation words and sentences are scored according to a specific metric and then utilizing that information for summarizing based copying or pasting the most informative parts of the text.\nOn the other hand Abstractive Summarisation involves building a semantic representation of the text and then incorporating natural language generation techniques to generate text highlighting the informative parts of the parent text document.\nHere, we will be focusing on an extractive summarisation method called TextRank which is hinged upon the PageRank algorithm which was developed by Google to rank websites based on their importance.\nThe TextRank Algorithm\nTextRank is a graph-based ranking algorithm for NLP. Graph-based ranking algorithms evaluate the importance of a vertex within a graph, based on global information extracted recursively from the entire graph. When one vertex is associated with another it is actually casting a vote for that vertex. The higher the number of votes cast for a vertex, the higher importance of that vertex.\nIn the NLP case it is necessary to define vertices and edges. In this tutorial we will be using sentences as vertices and words as edges. Thus sentences with words present in many other sentences will have higher priority\n\n# define url\nurl &lt;- \"http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\"\n# read in data\nsite &lt;- read_html(url)\narticle &lt;- html_text(html_nodes(site, \"p,h1,h2,h3\"))\n# inspect\narticle\n\n[1] \"Fitbit’s Newest Fitness Tracker Is Just for Kids\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n[2] \"Fitbit is launching a new fitness tracker designed for children called the Fitbit Ace, which will go on sale for $99.95 in the second quarter of this year.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n[3] \"The [tempo-ecommerce src=”http://www.amazon.com/Fitbit-Activity-Tracker-Purple-Stainless/dp/B07B9DX4WB&#8221; title=”Fitbit Ace” context=”body”] looks a lot like the company’s Alta tracker, but with a few child-friendly tweaks. The most important of which is Fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the Children’s Online Privacy Protection Act, or COPPA. Parents must approve who their child can connect with via the Fitbit app and can view their kid’s activity progress and sleep trends, the latter of which can help them manage their children’s bedtimes.\"                                                              \n[4] \"Like many of Fitbit’s other products, the Fitbit Ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long. But while Fitbit’s default move goal is 30 minutes for adult users, the Ace’s will be 60 minutes, in line with the World Health Organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day. Fitbit says the tracker is designed for children eight years old and up.\"                                                                                                                                                                                                                  \n[5] \"Fitbit will also be introducing a Family Faceoff feature that lets kids compete in a five-day step challenge against the other members of their family account. The app also will reward children with in-app badges for achieving their health goals. Fitbit’s new child-friendly fitness band will be available in blue and purple, is showerproof, and should last for five days on a single charge.\"                                                                                                                                                                                                                                                                                                                         \n[6] \"The Ace launch is part of Fitbit’s broader goal of branching out to new audiences. The company also announced a new smartwatch on Tuesday called the Versa, which is being positioned as an everyday smartwatch rather than a fitness-only device or sports watch, like some of the company’s other products.\"                                                                                                                                                                                                                                                                                                                                                                                                                   \n[7] \"Above all else, the Ace is an effort to get children up and moving. The Centers for Disease Control and Prevention report that the percentage of children and adolescents affected by obesity has more than tripled since the 1970’s. But parents who want to encourage their children to move already have several less expensive options to choose from. Garmin’s $79.99 Vivofit Jr. 2, for example, comes in themed skins like these Minnie Mouse and Star Wars versions, while the wristband entices kids to move by reflecting their fitness achievements in an accompanying smartphone game. The $39.99 Nabi Compete, meanwhile, is sold in pairs so that family members can work together to achieve movement milestones.\"\n[8] \"More Must-Reads from TIME\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n[9] \"Contact us at letters@time.com\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n\n\nNext the article is loaded into a tibble. Then tokenisation is implemented according to sentences. Although this tokenisation is fully perfect it has a lower number of dependencies and is suitable for this case. Finally we add column for sentence number and switch the order of the columns.\n\ntibble::tibble(text = article) %&gt;%\n    tidytext::unnest_tokens(sentence, text, token = \"sentences\") %&gt;%\n    dplyr::mutate(sentence_id = row_number()) %&gt;%\n    dplyr::select(sentence_id, sentence) -&gt; article_sentences\n# inspect\narticle_sentences\n\n# A tibble: 20 × 2\n   sentence_id sentence                                                         \n         &lt;int&gt; &lt;chr&gt;                                                            \n 1           1 fitbit’s newest fitness tracker is just for kids                 \n 2           2 fitbit is launching a new fitness tracker designed for children …\n 3           3 the [tempo-ecommerce src=”http://www.amazon.com/fitbit-activity-…\n 4           4 the most important of which is fitbit’s new family account optio…\n 5           5 parents must approve who their child can connect with via the fi…\n 6           6 like many of fitbit’s other products, the fitbit ace can automat…\n 7           7 but while fitbit’s default move goal is 30 minutes for adult use…\n 8           8 fitbit says the tracker is designed for children eight years old…\n 9           9 fitbit will also be introducing a family faceoff feature that le…\n10          10 the app also will reward children with in-app badges for achievi…\n11          11 fitbit’s new child-friendly fitness band will be available in bl…\n12          12 the ace launch is part of fitbit’s broader goal of branching out…\n13          13 the company also announced a new smartwatch on tuesday called th…\n14          14 above all else, the ace is an effort to get children up and movi…\n15          15 the centers for disease control and prevention report that the p…\n16          16 but parents who want to encourage their children to move already…\n17          17 garmin’s $79.99 vivofit jr. 2, for example, comes in themed skin…\n18          18 the $39.99 nabi compete, meanwhile, is sold in pairs so that fam…\n19          19 more must-reads from time                                        \n20          20 contact us at letters@time.com                                   \n\n\nNext we will tokenize based on words.\n\narticle_words &lt;- article_sentences %&gt;%\n    tidytext::unnest_tokens(word, sentence)\narticle_words\n\n# A tibble: 466 × 2\n   sentence_id word    \n         &lt;int&gt; &lt;chr&gt;   \n 1           1 fitbit’s\n 2           1 newest  \n 3           1 fitness \n 4           1 tracker \n 5           1 is      \n 6           1 just    \n 7           1 for     \n 8           1 kids    \n 9           2 fitbit  \n10           2 is      \n# ℹ 456 more rows\n\n\nWe have one last step left is to remove the stop words in article_words as they are prone to result in redundancy.\n\narticle_words &lt;- article_words %&gt;%\n    dplyr::anti_join(stop_words, by = \"word\")\narticle_words\n\n# A tibble: 225 × 2\n   sentence_id word     \n         &lt;int&gt; &lt;chr&gt;    \n 1           1 fitbit’s \n 2           1 fitness  \n 3           1 tracker  \n 4           1 kids     \n 5           2 fitbit   \n 6           2 launching\n 7           2 fitness  \n 8           2 tracker  \n 9           2 designed \n10           2 children \n# ℹ 215 more rows\n\n\nUsing the textrank package it is really easy to implement the TextRank algorithm. The textrank_sentences function requires only 2 inputs:\n\nA data frame with sentences\nA data frame with tokens which are part of each sentence\n\n\narticle_summary &lt;- textrank_sentences(\n    data = article_sentences,\n    terminology = article_words\n)\n# inspect the summary\narticle_summary\n\nTextrank on sentences, showing top 5 most important sentences found:\n  1. fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.\n  2. fitbit says the tracker is designed for children eight years old and up.\n  3. fitbit’s newest fitness tracker is just for kids\n  4. like many of fitbit’s other products, the fitbit ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long.\n  5. the most important of which is fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the children’s online privacy protection act, or coppa.\n\n\nLets have a look where these important sentences appear in the article:\n\nlibrary(ggplot2)\narticle_summary[[\"sentences\"]] %&gt;%\n    ggplot(aes(textrank_id, textrank, fill = textrank_id)) +\n    geom_col() +\n    theme_minimal() +\n    scale_fill_viridis_c() +\n    guides(fill = \"none\") +\n    labs(\n        x = \"Sentence\",\n        y = \"TextRank score\",\n        title = \"Most informative sentences appear within first half of sentences\",\n        subtitle = 'In article \"Fitbits Newest Fitness Tracker Is Just for Kids\"',\n        caption = \"Source: http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\"\n    )\n\n\n\n\nPosition of Important Sentences in the Article"
  },
  {
    "objectID": "tutorials/reinfnlp/reinfnlp.html#other-applications-of-rl",
    "href": "tutorials/reinfnlp/reinfnlp.html#other-applications-of-rl",
    "title": "Reinforcement Learning and Text Summarization in R",
    "section": "Other Applications of RL",
    "text": "Other Applications of RL\n\nDialogue Generation\nIn today’s digital world dialogue generation is a widely used application especially in chatbots. One widely used model in this regard is the Long Short Term Memory (LSTM) sequence-to-sequence (SEQ2SEQ) model. It is a neural generative model that maximizes the probability of generating a response given the previous dialogue. However SEQ2SEQ model has some constraints:\n\nThey tend to generate highly generic responses\nOften they are stuck in an infinite loop of repetitive responses\n\nThis is where deep RL is much more efficient as it can integrate developer-defined rewards which efficiently mimics the true goal of chatbot development. In case of dialogue generation the component:\n\nAction which involves a function that generates sequences of arbitrary lengths\nState it comprises of previous 2 dialogue turns [pi,qi]\nReward which determines the ease of answering, information flow and semantic coherence\n\nThe schematic diagram highlighting the dialogue simulation between 2 agents using deep RL is shown below:\n\n\n\nNeural Machine Translation\nMost of Neural Machine Translation (NMT) models are based encoder-decoder framework with attention mechanism. The encoder initially maps a source sentence x={x1,x2,…,xn} to a set of continuous representations z={z1,z2,…,zn} . Given z the decoder then generates a target sentence y={y1,y2,…,ym} of word tokens one by one. RL is used to bridge the gap between training and inference of of NMT by directly optimizing the loss function at training time. In this scenario the NMT model acts as the agent which interacts with the environment which in this case are the previous words and the context vector z available at each step t. This is a a policy based RL and in place of a state a policy will be assigned in every iteration. The critical components of the RL for NMT are discussed below:\n\nPolicy which is a conditional probability defined by the parameters of the agent\nAction is decided by the agent based on the policy and it will pick up a candidate word from the vocabulary\nReward is evaluated once the agent generates a complete sequence which in case of machine translation is Bilingual Evaluation Understudy (BLEU).BLEU is defined by comparing the generated sequence with the ground truth sequence.\n\nThe schematic of the overall process is depicted below:"
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#preparation-and-session-set-up",
    "href": "tutorials/basicstatz/basicstatz.html#preparation-and-session-set-up",
    "title": "Basic Inferential Statistics using R",
    "section": "Preparation and session set up",
    "text": "Preparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# install packages\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"flextable\")\ninstall.packages(\"e1071\")\ninstall.packages(\"lawstat\")\ninstall.packages(\"fGarch\")\ninstall.packages(\"gridExtra\")\ninstall.packages(\"cfa\")\ninstall.packages(\"effectsize\")\ninstall.packages(\"report\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNow, we load the packages that we will need in this tutorial.\n\n# load packages\nlibrary(dplyr) # for data processing\nlibrary(ggplot2) # for data vis\nlibrary(tidyr) # for data transformation\nlibrary(flextable) # for creating tables\nlibrary(e1071) # for checking assumptions\nlibrary(lawstat) # for statistical tests\nlibrary(fGarch) # for statistical tests\nlibrary(gridExtra) # for plotting\nlibrary(cfa) # for stats\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nAlso we load some sample data sets that we will use in this tutorial.\n\n# data for indep. t-test\nitdata &lt;- base::readRDS(\"tutorials/basicstatz/data/itdata.rda\", \"rb\")\n# data for paired t-test\nptdata &lt;- base::readRDS(\"tutorials/basicstatz/data/ptdata.rda\", \"rb\")\n# data for fishers exact test\nfedata &lt;- base::readRDS(\"tutorials/basicstatz/data/fedata.rda\", \"rb\")\n# data for mann-whitney u-test\nmwudata &lt;- base::readRDS(\"tutorials/basicstatz/data/mwudata.rda\", \"rb\")\n# data for wilcox test\nuhmdata &lt;- base::readRDS(\"tutorials/basicstatz/data/uhmdata.rda\", \"rb\")\n# data for friedmann-test\nfrdata &lt;- base::readRDS(\"tutorials/basicstatz/data/frdata.rda\", \"rb\")\n# data for x2-test\nx2data &lt;- base::readRDS(\"tutorials/basicstatz/data/x2data.rda\", \"rb\")\n# data for extensions of x2-test\nx2edata &lt;- base::readRDS(\"tutorials/basicstatz/data/x2edata.rda\", \"rb\")\n# multi purpose data\nmdata &lt;- base::readRDS(\"tutorials/basicstatz/data/mdata.rda\", \"rb\")\n\nOnce you have installed R, RStudio, and once you have also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#visual-inspection-of-normality",
    "href": "tutorials/basicstatz/basicstatz.html#visual-inspection-of-normality",
    "title": "Basic Inferential Statistics using R",
    "section": "Visual inspection of normality",
    "text": "Visual inspection of normality\nTo check if data are distributed normally, we extract a sample of words uttered my 100 men and 100 women from a sample corpus (the details of the sample corpus are represented by the mdata data set).\n\nndata &lt;- mdata %&gt;%\n  dplyr::rename(\n    Gender = sex,\n    Words = word.count\n  ) %&gt;%\n  dplyr::select(Gender, Words) %&gt;%\n  dplyr::filter(\n    !is.na(Words),\n    !is.na(Gender)\n  ) %&gt;%\n  dplyr::group_by(Gender) %&gt;%\n  dplyr::sample_n(100)\n\nThe table below shows the first ten lines of the women sample.\n\n\nGenderWordsfemale118female595female494female367female172female589female1,019female590female330female5\n\n\nWe can now go ahead and visualize the data to check for normality.\n\nHistograms\nThe first type of visualization we are going to use are histograms (with densities) as histograms will give us an impression about the distribution of the values. If the histograms and density functions follow a symmetric bell-shaped course, then the data are approximately normally distributed.\n\nggplot(ndata, aes(x = Words)) +\n  facet_grid(~Gender) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(aes(y = ..density..), color = \"red\") +\n  theme_bw() +\n  labs(title = \"Histograms with denisty of words uttered by men and women in a sample corpus\")\n\n\n\n\n\n\n\n\nThe histograms shows that the words uttered by men and women in the sample corpus are non-normal (this means that the errors will also be distributed non-normally).\n\n\nQuantile-Quantile Plots\nQuantile-Quantile Plots (or QQ-Plots) compare the quantiles between two distributions - typically the observed distribution and an assumed distribution like the normal distribution. If the points fall on the diagonal line, then the distributions match. If the points differ from the diagonal line, then the distributions differ.\nA very clear and concise explanation of how to manually create QQ-plots is shown is this StatQuest video by Josh Starmer (I can highly recommend all of his really informative, overall fantastic, and entertaining videos!) and explained in this TowardsDataScience post.\n\nggplot(ndata, aes(sample = Words)) +\n  facet_grid(~Gender) +\n  geom_qq() +\n  geom_qq_line(color = \"red\") +\n  theme_bw() +\n  labs(title = \"QQ-plot of words uttered by men and women in a sample corpus\", x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nThe deviation of the points from the line show that the data differs substantively from a normal distribution."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#statistical-measures",
    "href": "tutorials/basicstatz/basicstatz.html#statistical-measures",
    "title": "Basic Inferential Statistics using R",
    "section": "Statistical measures",
    "text": "Statistical measures\nAnother way to test if data are distributed normally is to calculate certain parameter which tell us about the skewness (whether the distribution is asymmetrical) or the kurtosis (is the data is too spiky or too flat) of the data.\n\nSkewness\nSkewed distributions are asymmetrical and they can be positively or negatively skewed. The tail of a negatively skewed distribution points towards negative values (to the left) which means that the distribution leans towards the right (towards positive values) while the tail of a positively skewed distribution points towards positive values (to the right) which means that the distribution leans towards the left (towards negative values).\nAnother characteristic of skewed distributions is that the mean and median of a distribution differ. If the median is greater than the mean, the distribution is negatively skewed (the long tail points towards negative values). If the median is smaller than the mean, the distribution is positively skewed (the long tail points towards positive values).\nNegatively skewed\n\nTail points towards negative values (to the left)\nMedian is greater than the mean\n\nPositively skewed\n\nTail points towards positive values (to the right)\nMedian is lower than the mean\n\n\n\n\n\n\n\n\n\n\nAs we need to test skewness within groups, we start by extracting the word counts of only women and then test if the distribution of the women’s word counts are normal.\n\n# extract word counts for one group\nwords_women &lt;- ndata %&gt;%\n  dplyr::filter(Gender == \"female\") %&gt;%\n  dplyr::pull(Words)\n# inspect\nhead(words_women)\n\n[1] 118 595 494 367 172 589\n\n\nTo see if a distribution is skewed, we can use the summary function to check if the mean and the median differ.\n\nsummary(words_women)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   125.8   438.5   545.6   776.2  2211.0 \n\n\nIn our example, the mean is larger than the median which suggests that the data are positively skewed.\nWe apply the skewness function from the e1071 package to the sample scores to calculated skewness. The skewness function allows to calculate skewness in three different ways:\nType 1 (This is the typical definition used in many older textbooks): \\[\\begin{equation}\ng_1 = m_3 / m_2^{(3/2)}.\n\\end{equation}\\]\nType 2 (Used in SAS and SPSS): \\[\\begin{equation}\nG_1 = g_1 * sqrt(n(n-1)) / (n-2)\n\\end{equation}\\]\nType 3 (Used in MINITAB and BMDP): \\[\\begin{equation}\nb_1 = m_3 / s^3 = g_1 ((n-1)/n)^{(3/2)}\n\\end{equation}\\]\nAll three formulas have in common that the more negative values are, the more strongly positively skewed are the data (distribution leans to the left) and the more positive the values are, the more strongly negatively skewed are the data (distribution leans to the right). Here we use the second formula (by setting type = 2) that is also used in SPSS.\n\nskewness(words_women, type = 2)\n\n[1] 1.292685\n\n\nIf the reported skewness value is negative, then the distribution is positively skewed. If the value is positive, then the distribution is negatively skewed. If the value is lower than -1 or greater than +1, then the distribution can be assumed to be substantively skewed (Hair et al.).\n\n\nKurtosis\nAnother way in which distributions can differ from the normal distribution relates to the thickness of the tails and the spikiness of the distribution. If distributions are bell-shaped like the normal distribution, they are called mesokurtic. If distributions are symmetrical but they are more spiky than the normal distribution, they are called leptokurtic. If symmetrical distributions are flatter and have bigger tails than the normal distribution, the distributions are called platykurtic.\n\n\n\n\n\n\n\n\n\nTo show how we can calculate kurtosis (or if a distribution suffers from kurtosis), we apply the kurtosis function from the e1071 package to the sample scores we generated above /when we calculated skewness).\n\nkurtosis(words_women)\n\n[1] 1.450228\n\n\nAs the kurtosis value is positive, the distribution is leptokurtic (if it were negative, the distribution would be platykurtic). As a rule of thumb, values greater than +1 indicate that the distribution is too peaked while values lower than –1 indicate that the distribution is substantively platykurtic (Hair et al., pp61).\nThe kurtosis score can thus be interpreted as follows:\n\nA values of 0 means that the distribution is perfectly mesokurtic with Values between -0.5 and 0.5 suggesting that the distribution is approximately mesokurtic\nValues between -0.5 and -1 mean that the distribution is moderately platykurtic with values smaller than -1 indicating that the distribution is platykurtic.\nValues between 0.5 and 1 mean that the distribution is moderately leptokurtic with values greater than 1 indicating that the distribution is leptokurtic"
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#statistical-test-of-assumptions",
    "href": "tutorials/basicstatz/basicstatz.html#statistical-test-of-assumptions",
    "title": "Basic Inferential Statistics using R",
    "section": "Statistical test of assumptions",
    "text": "Statistical test of assumptions\nThe two most common tests to check assumptions are the Shapiro-Wilk test, which tests if the data differ significantly from a normal distribution, and the Levene’s test which tests if the variances are of two groups are approximately equal.\n\nShapiro-Wilk test\nThe Shapiro-Wilk test (cf. Shapiro and Wilk) can be used to check if the data differs significantly from a normal distribution (within groups). However, it has been shown to be too lenient when dealing with small sample sizes (below 50 to 100 cases per variable level), but too strict when dealing with larger sample sizes (500 or more cases per variable level). A such, the Shapiro-Wilk test should only be used in combination with visual inspection on the data.\nIn this example, we will only test if the words uttered by women differ significantly from a normal distribution (we use the words_women vector created above). Once we have a a vector of word counts of one group only (in this case the women’s word counts), we can continue by performing the Shapiro-Wilk test.\n\nshapiro.test(words_women)\n\n\n    Shapiro-Wilk normality test\n\ndata:  words_women\nW = 0.88027, p-value = 1.864e-07\n\n\nIf the p-value of the Shapiro-Wilk test is greater than .05, the data do not support the hypothesis that they differ from normality. In other words, if the p-value is greater than .05, we can assume that the data are approximately normally distributed.\nThe output of the Shapiro-Wilk test shown above thus indicates that our data differs significantly from normal (W = 0.7925, p &lt; .001***). For more information about the implementation of the Shapiro-Wilk test in R, type ?shapiro.test into the console.\n\n\nLevene’s test\nThe Levene’s test (cf. Levene) evaluates if the variances of two groups is approximately equal - this is referred to as homoskedasticity. This is important, because unequal variances - or heteroskedasticity - strongly suggest that there is another factor, a confound, that is not included in the model but that significantly affects the dependent variable which renders results of an analysis unreliable.\nTo implement a Levene’s test in R, we need to install and load thelawstat package.\n\nlevene.test(mdata$word.count, mdata$sex)\n\n\n    Modified robust Brown-Forsythe Levene-type test based on the absolute\n    deviations from the median\n\ndata:  mdata$word.count\nTest Statistic = 0.0050084, p-value = 0.9436\n\n\nIf the p-values of the Levene’s test is greater than .05, we can assume that the variances are approximately equal. Thus, the output of the Levene’s test shown above thus indicates that the variances of men and women in our data are approximately equal (W = 0.005, p = .9436). For more information about the implementation of the Levene’s test in R, type ?levene.test into the console."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#students-t-test",
    "href": "tutorials/basicstatz/basicstatz.html#students-t-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Student’s t-test",
    "text": "Student’s t-test\nThere are two basic types of t-tests: the dependent or paired t-test and the independent t-test. Paired t-test are used when the data points are not independent, for example, because they come form the same subjects in a pre-post test design. In contrast, Independent t-tests are used when the data points are independent and come from two different groups (e.g., from learners and native speakers or from men and women).\nThe assumptions of the Student’s t-test are that\n\nthe dependent variable is a continuous, numeric variable;\nthe independent variable is a nominal variable (two levels / groups)\nthe variances within each group are approximately normal;\nthe errors within each group are approximately normal (this implies that the distributions of the scores of each group are approximately normal).\n\nIf the variances are not normal, then this indicates that another important variable is confounding the results. In such cases, you should go back to the data and check what other variable could cause the unequal variances. If you decide to proceed with the analysis, you can switch to a Welch t-test which does not assume equal variances within each group.\n\nPaired t-test\nPaired t-tests take into account that the scores (or values) come from the same individuals in two conditions (e.g. before and after a treatment).There are two equations for the paired t-test that are used.\n\\[\\begin{equation}\nt = \\frac{\\sum D}{\\sqrt{\\frac{N \\sum D^2 - (\\sum D)^2}{N-1}}}\n\\end{equation}\\]\nor\n\\[\\begin{equation}\nt = \\frac{\\bar D}{\\frac{s_D}{\\sqrt{N}}}\n\\end{equation}\\]\nTo show how a paired t-tests works, we will test if a treatment (a teaching method) reduces the number of spelling errors in a long essay of 6 students. In a first step, we generate some data representing the errors in two essays of the same length written before and after the teaching method was used for 8 weeks.\n\nPretest &lt;- c(78, 65, 71, 68, 76, 59)\nPosttest &lt;- c(71, 62, 70, 60, 66, 48)\nptd &lt;- data.frame(Pretest, Posttest)\n\nThe data look like as shown in the table below.\n\n\nPretestPosttest787165627170686076665948\n\n\nTo perform a paired t-test in R, we use the t.test function and specify the argument paired as TRUE.\n\nt.test(ptd$Pretest,\n  ptd$Posttest,\n  paired = TRUE,\n  conf.level = 0.95\n)\n\n\n    Paired t-test\n\ndata:  ptd$Pretest and ptd$Posttest\nt = 4.1523, df = 5, p-value = 0.00889\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  2.539479 10.793854\nsample estimates:\nmean difference \n       6.666667 \n\n\nIn addition to testing if the groups differ significantly, we also want to calculate the effect size of the difference. We can use the effectsize package to extract Cohen’s \\(d\\) which is the standard effect size measure for t-tests.\n\neffectsize::cohens_d(\n  x = ptd$Pretest,\n  y = ptd$Posttest,\n  paired = TRUE\n)\n\nCohen's d |       95% CI\n------------------------\n1.70      | [0.37, 2.96]\n\n\nTo check if the effect is small or big - that is if a Cohen’s \\(d\\) value can be interpreted as being small or big, we can use the following overview.\n\n\nEffectSizedReferenceVery small0.01Sawilowsky (2009)Small0.20Cohen (1988)Medium0.50Cohen (1988)Large0.80Cohen (1988)Very large1.20Sawilowsky (2009)Huge2.00Sawilowsky (2009)\n\n\nThe classification combines Sawilowsky and Cohen. The analysis can be summarized using the reports package (Makowski et al. 2021) as follows.\n\nreport::report(t.test(ptd$Pretest, ptd$Posttest, paired = TRUE, conf.level = 0.95))\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Paired t-test testing the difference between ptd$Pretest and ptd$Posttest\n(mean difference = 6.67) suggests that the effect is positive, statistically\nsignificant, and large (difference = 6.67, 95% CI [2.54, 10.79], t(5) = 4.15, p\n= 0.009; Cohen's d = 1.70, 95% CI [0.37, 2.96])\n\n\nWe can use this output to write up a final report:\nA paired t-test test was applied to the data and it confirmed that the number of spelling errors after the 8 weeks of using the new teaching method significantly decreased (t5: 4.1523, p = .009**). The treatment had a very large, statistically significant, positive effect (Cohen’s \\(d\\) = 1.70 [CIs: 0.41, 3.25]) (cf. Sawilowsky).\n\n\nIndependent t-tests\nIndependent t-tests are used very widely and they determine if the means of two groups are significantly different. As such, t-tests are used when we have a normally distributed (or parametric), numeric dependent variable and a nominal predictor variable.\n\\[\\begin{equation}\nt = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{\\frac{s^2_p}{N_1} + \\frac{s^2_p}{N_2}}}\n\\end{equation}\\]\nwhere\n\\[\\begin{equation}\ns^2_p = \\frac{(N_1 - 1)s^2_1 + (N_2 - 1)s^2_2}{N_1 + N_2 - 2}\n\\end{equation}\\]\nWe now load some data that we can apply a t-test to. The data represents scores on a proficiency test of native speakers and learners of English. We want to use a t-test to determine if the native speakers and learners differ in their proficiency.\n\n# load data\ntdata &lt;- base::readRDS(\"tutorials/basicstatz/data/d03.rda\", \"rb\") %&gt;%\n  dplyr::rename(\n    NativeSpeakers = 1,\n    Learners = 2\n  ) %&gt;%\n  tidyr::gather(Group, Score, NativeSpeakers:Learners) %&gt;%\n  dplyr::mutate(Group = factor(Group))\n\n\n\nGroupScoreNativeSpeakers6NativeSpeakers65NativeSpeakers12NativeSpeakers56NativeSpeakers45NativeSpeakers84NativeSpeakers38NativeSpeakers46NativeSpeakers64NativeSpeakers24\n\n\nWe now apply an independent t-test to the data.\n\nt.test(Score ~ Group,\n  var.equal = T,\n  data = tdata\n)\n\n\n    Two Sample t-test\n\ndata:  Score by Group\nt = -0.054589, df = 18, p-value = 0.9571\nalternative hypothesis: true difference in means between group Learners and group NativeSpeakers is not equal to 0\n95 percent confidence interval:\n -19.74317  18.74317\nsample estimates:\n      mean in group Learners mean in group NativeSpeakers \n                        43.5                         44.0 \n\n\nAs the p-value is higher than .05, we cannot reject the H-0- and we thus have to conclude that our evidence does not suffice to say that learners and Native Speakers differ in their proficiency. However, we still extract the effect size, again using Cohen’s \\(d\\). In contract to the extraction of the effect size for paired t-tests, however, we will set the argument paired to FALSE (in fact, we could simply leave it out as the paired = FALSE is the default).\n\neffectsize::cohens_d(tdata$Score ~ tdata$Group,\n  paired = FALSE\n)\n\nCohen's d |        95% CI\n-------------------------\n-0.02     | [-0.90, 0.85]\n\n- Estimated using pooled SD.\n\n\nThe analysis can be summarized using the reports package (Makowski et al. 2021) as follows.\n\nreport::report(t.test(Score ~ Group, var.equal = T, data = tdata))\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Two Sample t-test testing the difference of Score by Group (mean in group\nLearners = 43.50, mean in group NativeSpeakers = 44.00) suggests that the\neffect is negative, statistically not significant, and very small (difference =\n-0.50, 95% CI [-19.74, 18.74], t(18) = -0.05, p = 0.957; Cohen's d = -0.03, 95%\nCI [-0.95, 0.90])\n\n\nWe can use this output to write up a final report:\nAn independent t-test test was applied to the data and it reported that the scores between the two groups did not differ significantly (t18: -0.0546, p = .9571). In addition to not differing significantly, the effect size of the difference between the groups was also very small (Cohen’s \\(d\\) = -0.03 [CIs: -0.95, 0.90]) (cf. Sawilowsky)."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#fishers-exact-test",
    "href": "tutorials/basicstatz/basicstatz.html#fishers-exact-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\nFisher’s Exact test is very useful because it does not rely on distributional assumptions relying on normality. Instead, Fisher’s Exact Test calculates the probabilities of all possible outcomes and uses these to determine significance. To understand how a Fisher’s Exact test, we will use a very simple example.\nImagine you are interested in adjective modification and you want to find out if very and truly differ in their collocational preferences. So you extract all instances of cool, all instances of very, and all instances of truly from a corpus. Now that you have gathered this data, you want to test if truly and very differ with respect to their preference to co-occur with cool. Accordingly, you tabulate the results and get the following table.\n\n\nAdverbwith coolwith other adjectivestruly540very1741\n\n\nTo perform a Fisher’s Exact test, we first create a table with these results and then use the fisher.test function to perform the Fisher’s Exact Test to see if very and truly differ in their preference to co-occur with cool (as shown below). The null hypothesis is that there is no difference between the adverbs.\n\n# create table\ncoolmx &lt;- matrix(\n  c(5, 17, 40, 41),\n  nrow = 2, # number of rows of the table\n  # def. dimension names\n  dimnames = list(\n    Adverbs = c(\"truly\", \"very\"),\n    Adjectives = c(\"cool\", \"other adjective\")\n  )\n)\n# perform test\nfisher.test(coolmx)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  coolmx\np-value = 0.03024\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.08015294 0.96759831\nsample estimates:\nodds ratio \n 0.3048159 \n\n\nThe results of the Fisher’s Exact test show that the p-value is lower than .05, which means we reject the null hypothesis, and we are therefore justified in assuming that very and truly differ in their collocational preferences to co-occur with cool.\nThe analysis can be summarized as follows:\nA Fisher’s Exact test was applied to the data to determine if there was a significant difference in the modification of adjective cool. Specifically, we tested if the preference of cool to be modified by very and by truly differed significantly. The results of the Fisher’s Exact test confirmed that there was a statistically significant difference in the modification preference of cool(p: .030*). However, the effect size of the preference is small (Odds Ratio: 0.305)."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#mann-whitney-u-test",
    "href": "tutorials/basicstatz/basicstatz.html#mann-whitney-u-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Mann-Whitney U-Test",
    "text": "Mann-Whitney U-Test\nIt is actually quite common that numeric depend variables need to be transformed or converted into ranks, i.e. ordinal variables, because the distribution of residuals does not allow the application of parametric tests such as t-tests or linear regression. In such cases, as we are dealing with rank (ordinal) data, the application of a chi-square test is unwarranted and we need to use another test. There are different alternatives depending on whether the data are paired (coming from the same individuals) or if all observations are independent.\nThe non-parametric alternative for independent t-tests, i.e. for data where we are dealing with two separate groups and a numeric dependent variable that violates parametric assumptions (or an ordinal dependent variable), is the Mann-Whitney U-test. In contrast, if the groups under investigation represent identical participants that are tested under two conditions, the appropriate alternative is a Wilcoxon Signed Rank test (which is thus the alternative for paired t-test).\nImagine we wanted to determine if two language families differed with respect to the size of their phoneme inventories. You have already ranked the inventory sizes and would now like to now if language family correlates with inventory size. As such, we are dealing with two independent groups and we want to implement a non-parametric alternative of a t-test. To answer this question, you create the table shown below.\n\n# create table\nRank &lt;- c(1, 3, 5, 6, 8, 9, 10, 11, 17, 19, 2, 4, 7, 12, 13, 14, 15, 16, 18, 20)\nLanguageFamily &lt;- c(rep(\"Kovati\", 10), rep(\"Urudi\", 10))\nlftb &lt;- data.frame(LanguageFamily, Rank)\n\n\n\nLanguageFamilyRankKovati1Kovati3Kovati5Kovati6Kovati8Kovati9Kovati10Kovati11Kovati17Kovati19Urudi2Urudi4Urudi7Urudi12Urudi13Urudi14Urudi15Urudi16Urudi18Urudi20\n\n\nWe will also briefly inspect the data visually using a box plot.\n\nggplot(lftb, aes(x = LanguageFamily, y = Rank, fill = LanguageFamily)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"orange\", \"darkgrey\")) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nTo use the Mann-Whitney U test, the dependent variable (Rank) must be ordinal and independent variable (Group) must be a binary factor. We briefly check this by inspecting the structure of the data.\n\n# inspect structure\nstr(lftb)\n\n'data.frame':   20 obs. of  2 variables:\n $ LanguageFamily: chr  \"Kovati\" \"Kovati\" \"Kovati\" \"Kovati\" ...\n $ Rank          : num  1 3 5 6 8 9 10 11 17 19 ...\n\n\nAs the variables are what we need them to be, we can now perform the Mann-Whitney U test on the table. The null hypothesis is that there is no difference between the 2 groups.\n\n# perform test\nwilcox.test(lftb$Rank ~ lftb$LanguageFamily)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  lftb$Rank by lftb$LanguageFamily\nW = 34, p-value = 0.2475\nalternative hypothesis: true location shift is not equal to 0\n\n\nSince the p-value is greater than 0.05, we fail to reject the null hypothesis. The results of the Mann-Whitney U test tell us that the two language families do not differ significantly with respect to their phoneme inventory size.\nThe analysis can be summarized using the reports package (Makowski et al. 2021) as follows.\n\nreport::report(wilcox.test(lftb$Rank ~ lftb$LanguageFamily))\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum exact test testing the difference in ranks between\nlftb$Rank and lftb$LanguageFamily suggests that the effect is negative,\nstatistically not significant, and large (W = 34.00, p = 0.247; r (rank\nbiserial) = -0.32, 95% CI [-0.69, 0.18])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is no statistically significant relationship between the size of phoneme inventories and being member of selected language families. Despite being statistically insignificant, the effect may be large (W = 34.00, p = 0.247; r (rank biserial) = -0.32, 95% CI [-0.69, 0.18]).\nMann-Whitney U tests with continuity correction\nThe Mann-Whitney U test can also be used with continuity correction. A continuity correction is necessary when both variables represent numeric values that are non-normal. In the following example, we want to test if the reaction time for identifying a word as real is correlated with its token frequency.\nFor this example, we generate data is deliberately non-normal.\n\n\nFrequencyNormalizedReactionReaction20,0002,114.24310,0001,725.21086,6671,642.51955,0001,504.63524,0002,235.83703,3331,922.33362,8571,805.56582,5002,413.83122,2221,671.04142,0002,466.8700\n\n\nWhen we plot the data, we see that both the frequency of words (Frequency) and the reaction times that it took subjects to recognize the token as a word (Reaction) are non-normal (in this case, the distributions are negative skewed).\n\n\n\n\n\n\n\n\n\nBoth variables are negatively skewed (non-normally distributed) but we can use the wilcox.test function to perform the Mann-Whitney U test with continuity correction which takes the skewness into account. The null hypothesis is that there is no difference between the 2 groups. Although the output states that the test that was performed is a Wilcoxon rank sum test with continuity correction, we have actually performed a Mann-Whitney U test - this is because the nomenclature for the tests is not unanimous.\n\n# perform test\nwilcox.test(wxdata$Reaction, wxdata$Frequency)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  wxdata$Reaction and wxdata$Frequency\nW = 7701, p-value = 4.157e-11\nalternative hypothesis: true location shift is not equal to 0\n\n\nAgain, we use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(wilcox.test(wxdata$Reaction, wxdata$Frequency))\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum test with continuity correction testing the difference in\nranks between wxdata$Reaction and wxdata$Frequency suggests that the effect is\npositive, statistically significant, and very large (W = 7701.00, p &lt; .001; r\n(rank biserial) = 0.54, 95% CI [0.42, 0.64])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is a strong, positive, statistically significant relationship between the reaction time for identifying a word as real and its token frequency (W = 7613.50, p &lt; .001; r (rank biserial) = 0.52, 95% CI [0.40, 0.63])."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#wilcoxon-rank-sum-test",
    "href": "tutorials/basicstatz/basicstatz.html#wilcoxon-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Wilcoxon rank sum test",
    "text": "Wilcoxon rank sum test\nThe Wilcoxon rank sum test is the non-parametric alternative to a paired or dependent t-test. Thus, a Wilcoxon rank sum test is used when the data represent the same individuals that were tested under two condition. To tell R that we are dealing with paired data, we have to set the argument paired to TRUE while we can still use the wilcox.test function (as we did before when using the Mann-Whitney U test which is applied to independent groups).\nIn this example, the same individuals had to read tongue twisters when they were sober and when they were intoxicated. A Wilcoxon signed rank test with continuity correction is used to test if the number of errors that occur when reading tongue twisters correlates with being sober/intoxicated. Again, we create fictitious data.\n\n# create data\nsober &lt;- sample(0:9, 15, replace = T)\nintoxicated &lt;- sample(3:12, 15, replace = T)\n# tabulate data\nintoxtb &lt;- data.frame(sober, intoxicated)\n\n\n\nsoberintoxicated0329698455931106536278841151051103\n\n\nNow, we briefly plot the data.\n\nintoxtb2 &lt;- data.frame(\n  c(\n    rep(\"sober\", nrow(intoxtb)),\n    rep(\"intoxicated\", nrow(intoxtb))\n  ),\n  c(intoxtb$sober, intoxtb$intoxicated)\n) %&gt;%\n  dplyr::rename(\n    State = 1,\n    Errors = 2\n  )\nggplot(intoxtb2, aes(State, Errors)) +\n  geom_boxplot(fill = c(\"orange\", \"darkgrey\"), width = 0.5) +\n  labs(y = \"Number of errors\", x = \"State\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe boxes indicate a significant difference. Finally, we perform the Wilcoxon signed rank test with continuity correction. The null hypothesis is that the two groups are the same.\n\n# perform test\nwilcox.test(intoxtb$intoxicated, intoxtb$sober, paired = T)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  intoxtb$intoxicated and intoxtb$sober\nV = 74.5, p-value = 0.04553\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe p-value is lower than 0.05 (rejecting the null hypothesis) which means that the number of errors when reading tongue twisters is affected by one’s state (sober/intoxicated) - at least in this fictitious example.\nAgain, we use the reports package (Makowski et al. 2021) to summarize the analysis.\n\nreport::report(wilcox.test(intoxtb$intoxicated, intoxtb$sober, paired = T))\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon signed rank test with continuity correction testing the difference\nin ranks between intoxtb$intoxicated and intoxtb$sober suggests that the effect\nis positive, statistically significant, and very large (W = 74.50, p = 0.046; r\n(rank biserial) = 0.64, 95% CI [0.18, 0.87])\n\n\nWe can use this output to write up a final report:\nThe Wilcoxon rank sum exact test suggests that there is a very large, positive, statistically significant relationship between the number of errors produced in tongue twisters and being intoxicated (W = 6.50, p = 0.003; r (rank biserial) = -0.89, 95% CI [-0.97, -0.64])."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#kruskal-wallis-rank-sum-test",
    "href": "tutorials/basicstatz/basicstatz.html#kruskal-wallis-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "Kruskal-Wallis Rank Sum Test",
    "text": "Kruskal-Wallis Rank Sum Test\nThe Kruskal-Wallis rank sum test is a type of ANOVA (Analysis of Variance). For this reason, the Kruskal Wallis Test is also referred to as a one-way Anova by ranks which can handle numeric and ordinal data.\nIn the example below, uhm represents the number of filled pauses in a short 5 minute interview while speaker represents whether the speaker was a native speaker or a learner of English. As before, the data is generated and thus artificial.\n\n# create data\nuhms &lt;- c(15, 13, 10, 8, 37, 23, 31, 52, 11, 17)\nSpeaker &lt;- c(rep(\"Learner\", 5), rep(\"NativeSpeaker\", 5))\n# create table\nuhmtb &lt;- data.frame(Speaker, uhms)\n\n\n\nSpeakeruhmsLearner15Learner13Learner10Learner8Learner37NativeSpeaker23NativeSpeaker31NativeSpeaker52NativeSpeaker11NativeSpeaker17\n\n\nNow, we briefly plot the data.\n\nggplot(uhmtb, aes(Speaker, uhms)) +\n  geom_boxplot(fill = c(\"orange\", \"darkgrey\")) +\n  theme_bw() +\n  labs(x = \"Speaker type\", y = \"Errors\")\n\n\n\n\n\n\n\n\nNow, we test for statistical significance. The null hypothesis is that there is no difference between the groups.\n\nkruskal.test(uhmtb$Speaker ~ uhmtb$uhms)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  uhmtb$Speaker by uhmtb$uhms\nKruskal-Wallis chi-squared = 9, df = 9, p-value = 0.4373\n\n\nThe p-value is greater than 0.05, therefore we fail to reject the null hypothesis. The Kruskal-Wallis test does not report a significant difference for the number of uhms produced by native speakers and learners of English in the fictitious data."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#the-friedman-rank-sum-test",
    "href": "tutorials/basicstatz/basicstatz.html#the-friedman-rank-sum-test",
    "title": "Basic Inferential Statistics using R",
    "section": "The Friedman Rank Sum Test",
    "text": "The Friedman Rank Sum Test\nThe Friedman rank sum test is also called a randomized block design and it is used when the correlation between a numeric dependent variable, a grouping factor and a blocking factor is tested. The Friedman rank sum test assumes that each combination of the grouping factor (Gender) and the blocking factor (Age) occur only once. Thus, imagine that the values of uhms represent the means of the respective groups.\n\n# create data\nuhms &lt;- c(7.2, 9.1, 14.6, 13.8)\nGender &lt;- c(\"Female\", \"Male\", \"Female\", \"Male\")\nAge &lt;- c(\"Young\", \"Young\", \"Old\", \"Old\")\n# create table\nuhmtb2 &lt;- data.frame(Gender, Age, uhms)\n\n\n\nGenderAgeuhmsFemaleYoung7.2MaleYoung9.1FemaleOld14.6MaleOld13.8\n\n\nWe now perform the Friedman rank sum test.\n\nfriedman.test(uhms ~ Age | Gender, data = uhmtb2)\n\n\n    Friedman rank sum test\n\ndata:  uhms and Age and Gender\nFriedman chi-squared = 2, df = 1, p-value = 0.1573\n\n\nIn our example, age does not affect the use of filled pauses even if we control for gender as the p-value is higher than .05."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#pearsonss-chi-square-test",
    "href": "tutorials/basicstatz/basicstatz.html#pearsonss-chi-square-test",
    "title": "Basic Inferential Statistics using R",
    "section": "(Pearsons’s) Chi-Square Test",
    "text": "(Pearsons’s) Chi-Square Test\nOne of the most frequently used statistical test in linguistics is the \\(\\chi\\)2 test (or Pearsons’s chi-square test, chi-squared test, or chi-square test). We will use a simple, practical example to explore how this test works. In this example, we will test whether speakers of American English (AmE) and speakers of British English (BrE) differ in their use of the near-synonyms sort of and kind of as in “He’s sort of stupid” and “He’s kind of stupid”. As a first step, we formulate the hypothesis that we want to test (H1) and its null hypothesis (H0). The alternative- or test hypothesis reads:\nH1: Speakers of AmE and BrE differ with respect to their preference for sort of and kind of.\nwhile the Null Hypothesis (H0) states\nH0: Speakers of AmE and BrE do not differ with respect to their preference for sort of and kind of.\nThe H0 claims the non-existence of something (which is the more conservative position) and in our example the non-existence of a correlation between variety of English and the use of sort of and kind of. The question now arises what has to be the case in order to reject the H0 in favor of the H1.\nTo answer this question, we require information about the probability of error, i.e. the probability that the H0 does indeed hold for the entire population. Before performing the chi-square test, we follow the convention that the required significance level is 5 percent. In other words, we will reject the H0 if the likelihood for the H\\(_{0}\\) being true is less than 5 percent given the distribution of the data. In that case, i.e. in case that the likelihood for the H0 being true is less than 5 percent, we consider the result of the chi-square test as statistically significant. This means that the observed distribution makes it very unlikely that there is no correlation between the variety of English and the use of sort of and kind of.\nLet us now assume that we have performed a search for sort of and kind of in two corpora representing American and British English and that we have obtained the following frequencies:\n\n\nHedgeBrEAmEkindof181655sortof17767\n\n\nIn a first step, we now have to calculate the row and column sums of our table.\nHedgeBrEAmETotalkindof181655836sortof17767244Total3587221,080\nNext, we calculate, the values that would have expected if there was no correlation between variety of English and the use of sort of and kind of. In order to get these expected frequencies, we apply the equation below to all cells in our table.\n\\[\\begin{equation}\n\\frac{Column total*Row total}{Overall total}\n\\end{equation}\\]\nIn our example this means that for the cell with [+]BrE [+]kindof we get:\n\\[\\begin{equation}\n\\frac{836*358}{1080} = \\frac{299288}{1080} = 277.1185\n\\end{equation}\\]\nFor the entire table this means we get the following expected values:\nHedgeBrEAmETotalkindof277.11850558.8815836sortof80.88148163.1185244Total358.00000722.00001,080\nIn a next step, we calculate the contribution of each cell to the overall \\(\\chi\\)2 value (\\(\\chi\\)2 contribution). To get \\(\\chi\\)2 contribution for each cell, we apply the equation below to each cell.\n\\[\\begin{equation}\n\\frac{(observed – expected)^{2}}{expected}\n\\end{equation}\\]\nIn our example this means that for the cell with [+]BrE [+]kindof we get:\n\\[\\begin{equation}\n\\frac{(181 – 277.1185)^{2}}{277.1185} = \\frac{-96.1185^{2}}{277.1185} = \\frac{9238.766}{277.1185} = 33.33868\n\\end{equation}\\]\nFor the entire table this means we get the following \\(\\chi^{2}\\) values:\nHedgeBrEAmETotalkindof33.3386916.5308249.86951sortof114.2260256.63839170.86440Total147.5647073.16921220.73390\nThe sum of \\(\\chi\\)2 contributions in our example is 220.7339. To see if this value is statistically significant, we need to calculate the degrees of freedom because the \\(\\chi\\) distribution differs across degrees of freedom. Degrees of freedom are calculated according to the equation below.\n\\[\\begin{equation}\nDF = (rows -1) * (columns – 1) = (2-1) * (2-1) = 1 * 1 = 1\n\\end{equation}\\]\nIn a last step, we check whether the \\(\\chi\\)2 value that we have calculated is higher than a critical value (in which case the correlation in our table is significant). Degrees of freedom are relevant here because the critical values are dependent upon the degrees of freedom: the more degrees of freedom, the higher the critical value, i.e. the harder it is to breach the level of significance.\nSince there is only 1 degree of freedom in our case, we need to consider only the first column in the table of critical values below.\nDFp&lt;.05p&lt;.01p&lt;.00113.846.6410.8325.999.2113.8237.8211.3516.2749.4913.2818.47511.0715.0920.52\nSince the \\(\\chi\\)2 value that we have calculated is much higher than the critical value provided for p&lt;.05, we can reject the H0 and may now claim that speakers of AmE and BrE differ with respect to their preference for sort of and kind of.\nBefore we summarize the results, we will calculate the effect size which is a measure for how strong the correlations are.\n\nEffect Sizes in Chi-Square\nEffect sizes are important because they correlations may be highly significant but the effect between variables can be extremely weak. The effect size is therefore a measure how strong the correlation or the explanatory and predictive power between variables is.\nThe effect size measure for \\(\\chi\\)2 tests can be either the \\(\\phi\\)-coefficient (phi-coefficient) or Cramer’s \\(\\phi\\) (Cramer’s phi). The \\(\\phi\\)-coefficient is used when dealing with 2x2 tables while Cramer’s \\(\\phi\\) is used when dealing with tables with more than 4 cells. The \\(\\phi\\) coefficient can be calculated by using the equation below (N = overall sample size).\n\\[\\begin{equation}\n\\phi = \\sqrt{\\frac{\\chi^{2}}{N}}\n\\end{equation}\\]\nIn our case, this means:\n\\[\\begin{equation}\n\\phi = \\sqrt{\\frac{220.7339}{1080}} = \\sqrt{0.2043832} = 0.4520876\n\\end{equation}\\]\nThe \\(\\phi\\) coefficient varies between 0 (no effect) and 1 (perfect correlation). For the division into weak, moderate and strong effects one can follow the division for \\(\\omega\\) (small omega), so that with values beginning with .1 represent weak, values between 0.3 and .5 represent moderate and values above .5 represent strong effects (Bühner and Ziegler, 266). So, in this example we are dealing with a medium-sized effect/correlation.\n\n\nChi-Square in R\nBefore we summarize the results, we will see how to perform a chi-square test in R. In addition to what we have done above, we will also visualize the data. To begin with, we will have a look at the data set (which is the same data we have used above).\n\n# inspect data\nchidata %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(\"Hedge\") %&gt;%\n  flextable() %&gt;%\n  flextable::set_table_properties(width = .75, layout = \"autofit\") %&gt;%\n  flextable::theme_zebra() %&gt;%\n  flextable::fontsize(size = 12) %&gt;%\n  flextable::fontsize(size = 12, part = \"header\") %&gt;%\n  flextable::align_text_col(align = \"center\") %&gt;%\n  flextable::border_outer()\n\nHedgeBrEAmEkindof181655sortof17767\n\n\nWe will now visualize the data with an association. Bars above the dashed line indicate that a feature combination occurs more frequently than expected by chance. The width of the bars indicates the frequency of the feature combination.\n\n\n\n\n\n\n\n\n\nThe fact that the bars are distributed complimentary (top left red and below bar; top right black above bar; bottom left black above bar; bottom right red below bar) indicates that the use of sort of and kind of differs across AmE and BrE. We will check whether the mosaic plot confirms this impression.\n\nmosaicplot(chidata, shade = TRUE, type = \"pearson\", main = \"\") # mosaic plot\n\n\n\n\n\n\n\n\nThe color contrasts in the mosaic plot substantiate the impression that the two varieties of English differ significantly. To ascertain whether the differences are statistically significant, we can now apply the chi-square test.\n\n# perform chi square test without Yate's correction\nchisq.test(chidata, corr = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  chidata\nX-squared = 220.73, df = 1, p-value &lt; 2.2e-16\n\n\nThe results reported by R are identical to the results we derived by hand and confirm that BrE and AmE differ significantly in their use of sort of and kind of.\n\n\n\n\nNOTE\n\nWhen conducting a \\(\\chi\\)2 test of independence and in order to interpret the Pearson’s \\(\\chi\\)2 statistic, by default, most of the statistical packages, including R, assume that the observed frequencies in a contingency table can be approximated by the continuous \\(\\chi\\)2 distribution.  To avoid-reduce the error that the approximation introduces, the chisq.test function in base R includes the correct argument that, by default, is set to TRUE. This argument integrates the Frank Yate’s adjustment that aims at compensating for deviations from the (smooth) theoretical chi-squared distribution and it is considered especially useful if the frequency in each cell is less than a small number. Some statisticians set this number to 5 and others to 10. Although this case scenario is relevant to linguistic data, this is not always the case. Moreover, there is a strong tendency in other fields to avoid the Yates’ continuity correction altogether due to the overestimated amount of adjustment that it introduces. That is why, it is helpful to know how to avoid the Yates’ continuity correction. To do that, you simply set the correct argument to FALSE, as follows (see also above):\n\n# X2-test without Yate's correction\nchisq.test(chidata, correct = FALSE)\n\nAlso, in case you would like to deactivate the scientific notation used for displaying the p-value, as it is displayed in the output of the chisq.test function, you can do the following:\n\nformat(chisq.test(chidata)$p.value, scientific = FALSE)\n\n\n\n\n\n\n\nIn a next step, we calculate the effect size.\n\n# calculate effect size\nsqrt(chisq.test(chidata, corr = F)$statistic / sum(chidata) * (min(dim(chidata)) - 1))\n\nX-squared \n0.4520877 \n\n\nThe \\(\\phi\\) coefficient of .45 shows that variety of English correlates moderately with the use of sort of and kind of. We will now summarize the results.\n\n\nSummarizing Chi-Square Results\nThe results of our analysis can be summarized as follows: A \\(\\chi\\)2-test confirms a highly significant correlation of moderate size between the variety of English and the use of the near-synonymous hedges sort of and kind of (\\(\\chi\\)2 = 220.73, df = 1, p &lt; .001***, \\(\\phi\\) = .452).\n\n\nRequirements of Chi-Square\nChi-square tests depend on certain requirements that, if violated, negatively affect the reliability of the results of the test. To provide reliable results, 80 percent of cells in a table to which the chi-square test is applied have to have expected values of 5 or higher and at most 20 percent of expected values can be smaller than 5 (see Bortz, Lienert, and Boehnke, 98). In addition, none of the expected values can be smaller than 1 (see Bortz, Lienert, and Boehnke, 136) because then, the estimation, which relies on the \\(\\chi\\)2-distribution, becomes too imprecise to allow meaningful inferences (Cochran).\nIf these requirements are violated, then the Fisher’s Exact Test is more reliable and offers the additional advantage that these tests can also be applied to data that represent very small sample sizes. When applying the Fisher’s Exact Test, the probabilities for all possible outcomes are calculated and the summed probability for the observed or more extreme results are determined. If this sum of probabilities exceeds five percent, then the result is deemed statistically significant.\n\n\n\n\nEXERCISE TIME!\n\n\n\n\n\n\n`\n\nImagine you are interested in whether older or younger speakers tend to refer to themselves linguistically. The underlying hypothesis is that - contrary to common belief - older people are more narcissistic compared with younger people. Given this research question, perform a chi-square test and summarize the results on the data below.\n\n\n\nAge1SGPNPN without 1SGTotalYoung6143104Old423678Total10379182\n\n\n\n\nAnswer\n\n\n    # generate data\n    ex1dat &lt;- matrix(c(61, 43, 42, 36), ncol = 2, byrow = T)\n    # add column names\n    colnames(ex1dat) &lt;- c(\"1SGPN\", \"PN without 1SG\")\n    # perform x2-test\n    x2_ex1 &lt;- chisq.test(ex1dat)\n    # inspect results\n    x2_ex1\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  ex1dat\nX-squared = 0.2465, df = 1, p-value = 0.6195\n\n\nGiven the data, we cannot reject the H0 according to which old people are more narcissistic compared to young people measured by their use of 1st person pronouns in conversation (\\(\\chi\\)~{1}: 0.2465026, p: 0.6195485).\n\n\nImagine you are interested in whether young men or young women exhibit a preference for the word whatever because you have made the unsystematic, anecdotal observation that young men use this word more frequently than young women. Given this research question, perform a chi-square test and summarize the results on the data below.\n\n\n\nItemYoungMalesYoungFemalesTotalwhatever175571other words3451289165521261680Total3451459166071261752\n\n\n\n\nAnswer\n\n\n    # generate data\n    ex2dat &lt;- matrix(c(17, 55, 345128, 916552), ncol = 2, byrow = T)\n    # add column names\n    colnames(ex2dat) &lt;- c(\"YoungMales\", \"YoungFemales\")\n    # perform x2-test\n    x2_ex2 &lt;- chisq.test(ex2dat)\n    # inspect results\n    x2_ex2\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  ex2dat\nX-squared = 0.33682, df = 1, p-value = 0.5617\n\n\nGiven the data, we cannot reject the H0 according to which there is no difference between young men and young women in their use of the word whatever (\\(\\chi\\)~{1}: 0.3368202, p: 0.5616705).\n\n\nFind a partner and discuss the relationship between significance and effect size. Then, go and find another partner and discuss problems that may arise when testing the frequency of certain words compared with the overall frequency of words in a corpus."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#extensions-of-chi-square",
    "href": "tutorials/basicstatz/basicstatz.html#extensions-of-chi-square",
    "title": "Basic Inferential Statistics using R",
    "section": "Extensions of Chi-Square",
    "text": "Extensions of Chi-Square\nIn the following, we will have a look at tests and methods that can be used if the requirements for ordinary (Pearson’s) chi-square tests are violated and their use would be inappropriate\n\nThe Yates-Correction\nIf all requirements for ordinary chi-square tests are acceptable and only the sample size is the issue, then applying a so-called Yates-correction may be appropriate. This type of correction is applied in cases where the overall sample size lies in-between 60 and 15 cases (Bortz, Lienert, and Boehnke, 91). The difference between the ordinary chi-square and a Yates-corrected chi-square lies in the fact that the Yates-corrected chi-square is calculated according to the equation below.\n\\[\\begin{equation}\n\\frac{(|observed – expected|-0.5)^{2}}{expected}\n\\end{equation}\\]\nAccording to this formula, we would get the values shown below rather than the values tabulated above. It is important to note here that this is only a demonstration because a Yates-Correction would actually be inappropriate as our sample size exceeds 60 cases.\n\n\nVariantBrEAmETotalkind of32.9927113.0407146.0335sort of16.359356.050772.41Total49.352169.0914218.4434\n\n\nIf the Yates-correction were applied, then this results in a slightly lower \\(\\chi\\)2-value and thus in more conservative results compared with the traditional test according to Pearson.\n\n\nChi-Square within 2-by-k tables\nAlthough the \\(\\chi\\)2-test is widely used, it is often used inappropriately. This is especially the case when chi-square tests are applied to data representing tables with more than two rows and more than two columns. It is important to note that applying the common Pearson’s’ chi-square test to sub-tables of a larger table is inappropriate because, in such cases, a modified variant of Pearson’s’ chi-square test is warranted. We will go through two examples that represent different scenarios where we are dealing with sub-samples of larger tables and a modified version of the \\(\\chi\\)2-test should be used rather than Pearson’s’ chi-square.\nIn this first example, we are dealing with a table consisting of two columns and multiple rows, a so-called 2*k table (two-by-k table). In order to test if a feature combination, that is represented by a row in the 2*k table, is significantly more common compared with other feature combinations, we need to implement the \\(\\chi\\)2-equation from (Bortz, Lienert, and Boehnke, 126–27).\nIn this example, we want to find out whether soft and hard X-rays differ in their effect on grasshopper larva. The question is whether the larva reach or do not reach a certain life cycle depending on whether they are exposed to soft X-rays, hard X-rays, light, or beta rays. The data for this example is provided below.\nTreatmentMitosis not reachedMitosis reachedTotalX-ray soft211435X-ray hard181331Beta-rays241236Light133043Total7669145\nIf we would apply an ordinary chi-square test, we would ignore that all data were collected together and using only a sub-sample would ignore the data set of which the sub-sample is part of. In other words, the sub-sample is not independent from the other data (as it represents a subsection of the whole data set). However, for exemplary reasons, we will apply an ordinary chi-square test first and then compare its results to results provided by the correct version of the chi-square test. In a first step, we create a table with all the data.\n\n# create data\nwholetable &lt;- matrix(c(21, 14, 18, 13, 24, 12, 13, 30), byrow = T, nrow = 4)\ncolnames(wholetable) &lt;- c(\"reached\", \"notreached\") # add column names\nrownames(wholetable) &lt;- c(\"rsoft\", \"rhard\", \"beta\", \"light\") # add row names\n\n\n\nTreatmentreachednotreachedrsoft2114rhard1813beta2412light1330\n\n\nNow, we extract the sub-sample from the data.\n\nsubtable &lt;- wholetable[1:2, ] # extract subtable\n\n\n\nTreatmentreachednotreachedrsoft2114rhard1813\n\n\nNext, we apply the ordinary chi-square test to the sub-sample.\n\n# simple x2-test\nchisq.test(subtable, corr = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  subtable\nX-squared = 0.025476, df = 1, p-value = 0.8732\n\n\nFinally, we perform the correct chi-square test.\n\n# load function for correct chi-square\nsource(\"rscripts/x2.2k.r\")\nx2.2k(wholetable, 1, 2)\n\n$Description\n[1] \"rsoft  against  rhard  by  reached  vs  notreached\"\n\n$`Chi-Squared`\n[1] 0.025\n\n$df\n[1] 1\n\n$`p-value`\n[1] 0.8744\n\n$Phi\n[1] 0.013\n\n$Report\n[1] \"Conclusion: the null hypothesis cannot be rejected! Results are not significant!\"\n\n\nBelow is a table comparing the results of the two chi-square tests.\n\n\nStatisticchi-squarechi-square in 2*k-tableschi-squared0.02550.025p-value0.87320.8744\n\n\nThe comparison shows that, in this example, the results of the two tests are very similar but this may not always be the case.\n\n\nChi-Square within z-by-k tables\nAnother application in which the \\(\\chi\\)2 test is often applied incorrectly is when ordinary Parsons’s \\(\\chi\\)2 tests are used to test portions of tables with more than two rows and more than two columns, that is z*k tables (z: row, k: column). An example is discussed by Gries who also wrote the R Script for the correct version of the \\(\\chi\\)2 test.\nLet’s first load the data discussed in the example of Gries 9. The example deals with metaphors across registers. Based on a larger table, a \\(\\chi\\)2 confirmed that registers differ with respect to the frequency of EMOTION metaphors. The more refined question is whether the use of the metaphors EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE differs between spoken conversation and fiction.\n\n# create table\nwholetable &lt;- matrix(c(8, 31, 44, 36, 5, 14, 25, 38, 4, 22, 17, 12, 8, 11, 16, 24), ncol = 4)\nattr(wholetable, \"dimnames\") &lt;- list(\n  Register = c(\"acad\", \"spoken\", \"fiction\", \"new\"),\n  Metaphor = c(\"Heated fluid\", \"Light\", \"NatForce\", \"Other\")\n)\n\nBased on the table above, we can extract the following sub-table.\nRegisterHeated fluidLightNatForceOtheracad8548spoken31142211fiction44251716new36381224\nIf we used an ordinary Pearson’s \\(\\chi\\)2 test (the use of which would be inappropriate here), it would reveal that spoken conversations do not differ significantly from fiction in their use of EMOTION IS LIGHT and EMOTION IS A FORCE OF NATURE (\\(\\chi\\)2=3.3016, df=1, p=.069, \\(\\phi\\)=.2057).\n\n# create table\nsubtable &lt;- matrix(c(14, 25, 22, 17), ncol = 2)\nchisq.results &lt;- chisq.test(subtable, correct = FALSE) # WRONG!\nphi.coefficient &lt;- sqrt(chisq.results$statistic / sum(subtable) * (min(dim(subtable)) - 1))\nchisq.results\n\n\n    Pearson's Chi-squared test\n\ndata:  subtable\nX-squared = 3.3016, df = 1, p-value = 0.06921\n\nphi.coefficient\n\nX-squared \n0.2057378 \n\n\nThe correct analysis takes into account that it is a sub-table that is not independent of the overall table. This means that the correct analysis should take into account the total number of cases, as well as the row and column totals (cf. Bortz, Lienert, and Boehnke, 144–48).\nIn order to perform the correct analysis, we must either implement the equation proposed in Bortz, Lienert, and Boehnke or read in the function written by Gries and apply it to the sub-table.\n\n# load function for chi square test for subtables\nsource(\"rscripts/sub.table.r\")\n# apply test\nresults &lt;- sub.table(wholetable, 2:3, 2:3, out = \"short\")\n# inspect results\nresults\n\n$`Whole table`\n         Metaphor\nRegister  Heated fluid Light NatForce Other Sum\n  acad               8     5        4     8  25\n  spoken            31    14       22    11  78\n  fiction           44    25       17    16 102\n  new               36    38       12    24 110\n  Sum              119    82       55    59 315\n\n$`Sub-table`\n         Metaphor\nRegister  Light NatForce Sum\n  spoken     14       22  36\n  fiction    25       17  42\n  Sum        39       39  78\n\n$`Chi-square tests`\n                                  Chi-square Df    p-value\nCells of sub-table to whole table  7.2682190  3 0.06382273\nRows (within sub-table)            0.2526975  1 0.61518204\nColumns (within sub-table)         3.1519956  1 0.07583417\nContingency (within sub-table)     3.8635259  1 0.04934652\n\n\nThe results show that the difference is, in fact, statistically significant at an \\(\\alpha\\)-level of .05 (\\(\\chi^{2}\\)=3.864, df=1, p&lt;.05*)."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#configural-frequency-analysis-cfa",
    "href": "tutorials/basicstatz/basicstatz.html#configural-frequency-analysis-cfa",
    "title": "Basic Inferential Statistics using R",
    "section": "Configural Frequency Analysis (CFA)",
    "text": "Configural Frequency Analysis (CFA)\nConfigural Frequency Analysis (CFA) is a multivariate extension of the \\(\\chi^{2}\\)-test. If we perform a \\(\\chi^{2}\\)-test on a table with more than 2 rows and 2 columns, the test tells us that somewhere in that table there is at least one cell tat differs significantly from the expected value. However, we do not know which cell or cells this is. To determine which of the cells differ significantly from the expected value, we use the CFA.\nTo perform a CFA in R, we need to load the cfa package and load some data (as shown below).\n\n# load package\nlibrary(cfa)\n# load data\ncfadata &lt;- base::readRDS(\"tutorials/basicstatz/data/cfd.rda\", \"rb\")\n\n\n\nVarietyAgeGenderClassFrequencyAmericanOldManMiddle7BritishOldWomanMiddle9BritishOldManMiddle6AmericanOldWomanWorking2AmericanOldManMiddle5BritishOldManMiddle8AmericanOldManMiddle8BritishOldManMiddle6BritishOldManMiddle3AmericanOldWomanMiddle8\n\n\nIn a next step, we define the configurations and separate them from the counts. The configurations are the independent variables in this design and the counts represent the dependent variable.\n\n# define configurations\nconfigs &lt;- cfadata %&gt;%\n  dplyr::select(Variety, Age, Gender, Class)\n# define counts\ncounts &lt;- cfadata$Frequency\n\nNow that configurations and counts are separated, we can perform the configural frequency analysis.\n\n# perform cfa\ncfa(configs, counts)\n\n\n*** Analysis of configuration frequencies (CFA) ***\n\n                         label   n   expected            Q       chisq\n1     American Old Man Working   9  17.269530 0.0074991397 3.959871781\n2    American Young Man Middle  20  13.322419 0.0060338993 3.346996519\n3    British Old Woman Working  33  24.277715 0.0079603059 3.133665860\n4   British Young Woman Middle  12  18.728819 0.0061100471 2.417504403\n5  American Young Woman Middle  10   6.362422 0.0032663933 2.079707490\n6      British Old Man Working  59  50.835658 0.0076361897 1.311214959\n7     British Young Man Middle  44  39.216698 0.0044257736 0.583424432\n8    American Old Woman Middle  81  76.497023 0.0043152503 0.265066491\n9     British Old Woman Middle 218 225.181379 0.0080255135 0.229025170\n10     American Old Man Middle 156 160.178850 0.0043537801 0.109020569\n11  American Old Woman Working   8   8.247454 0.0002225797 0.007424506\n12      British Old Man Middle 470 471.512390 0.0023321805 0.004851037\n      p.chisq sig.chisq          z        p.z sig.z\n1  0.04659725     FALSE -2.1267203 0.98327834 FALSE\n2  0.06732776     FALSE  1.7026500 0.04431680 FALSE\n3  0.07669111     FALSE  1.6871254 0.04578962 FALSE\n4  0.11998594     FALSE -1.6845116 0.95395858 FALSE\n5  0.14926878     FALSE  1.2474422 0.10611771 FALSE\n6  0.25217480     FALSE  1.1002146 0.13561931 FALSE\n7  0.44497317     FALSE  0.6962784 0.24312726 FALSE\n8  0.60666058     FALSE  0.4741578 0.31769368 FALSE\n9  0.63224759     FALSE -0.5726832 0.71657040 FALSE\n10 0.74126197     FALSE -0.3993470 0.65518123 FALSE\n11 0.93133480     FALSE -0.2612337 0.60304386 FALSE\n12 0.94447273     FALSE -0.1217934 0.54846869 FALSE\n\n\nSummary statistics:\n\nTotal Chi squared         =  17.44777 \nTotal degrees of freedom  =  11 \np                         =  2.9531e-05 \nSum of counts             =  1120 \n\nLevels:\n\nVariety     Age  Gender   Class \n      2       2       2       2 \n\n\nThe output table contains the configurations (here called lables), then the observed frequencies (the counts) and the expected frequencies, the Q and the \\(\\chi^{2}\\) statistic as well as the p-value associated with the \\(\\chi^{2}\\) value. In addition, the table provides the z-transformed \\(\\chi^{2}\\)-values and their p-value.\nIf a p-value is below the level of significance - typically below .05 - the configuration occurs with a frequency that differs significantly from the expected frequency. If the observed value is higher than the expected value, then the configuration occurs significantly more frequently than would be expected by chance. If the observed value is lower than the expected value, then the configuration occurs significantly less frequently than would be expected by chance."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#hierarchical-configural-frequency-analysis-hcfa",
    "href": "tutorials/basicstatz/basicstatz.html#hierarchical-configural-frequency-analysis-hcfa",
    "title": "Basic Inferential Statistics using R",
    "section": "Hierarchical Configural Frequency Analysis (HCFA)",
    "text": "Hierarchical Configural Frequency Analysis (HCFA)\nA hierarchical alternative to CFA is Hierarchical Configural Frequency Analysis (HCFA). In contrast to CFA, in HCFA, the data is assumed to be nested! We begin by defining the configurations and separate them from the counts.\n\n# define configurations\nconfigs &lt;- cfadata %&gt;%\n  dplyr::select(Variety, Age, Gender, Class)\n# define counts\ncounts &lt;- cfadata$Frequency\n\nNow that configurations and counts are separated, we can perform the hierarchical configural frequency analysis.\n\n# perform cfa\nhcfa(configs, counts)\n\n\n*** Hierarchical CFA ***\n\n                     Overall chi squared df          p order\nVariety Age Class              12.218696  4 0.01579696     3\nVariety Gender Class            8.773578  4 0.06701496     3\nVariety Age Gender              7.974102  4 0.09253149     3\nVariety Class                   6.078225  1 0.01368582     2\nVariety Class                   6.078225  1 0.01368582     2\nAge Gender Class                5.164357  4 0.27084537     3\nVariety Age                     4.466643  1 0.03456284     2\nVariety Age                     4.466643  1 0.03456284     2\nAge Gender                      1.934543  1 0.16426233     2\nAge Gender                      1.934543  1 0.16426233     2\nAge Class                       1.673538  1 0.19578534     2\nAge Class                       1.673538  1 0.19578534     2\nGender Class                    1.546666  1 0.21362833     2\nGender Class                    1.546666  1 0.21362833     2\nVariety Gender                  1.120155  1 0.28988518     2\nVariety Gender                  1.120155  1 0.28988518     2\n\n\nAccording to the HCFA, only a single configuration (Variety : Age : Class) is significant (X2 = 12.21, p = .016)."
  },
  {
    "objectID": "tutorials/basicstatz/basicstatz.html#footnotes",
    "href": "tutorials/basicstatz/basicstatz.html#footnotes",
    "title": "Basic Inferential Statistics using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the bibliography file and store it in the same folder where you store the Rmd file.↩︎"
  },
  {
    "objectID": "resources.html#atap",
    "href": "resources.html#atap",
    "title": "RESOURCES",
    "section": "ATAP",
    "text": "ATAP\nLADAL is part of the Australian Text Analytics Platform (ATAP). The aim of ATAP is to provide researchers with a Notebook environment – in other words a tool set - that is more powerful and customisable than standard packages, while being accessible to a large number of researchers who do not have strong coding skills."
  },
  {
    "objectID": "resources.html#tools",
    "href": "resources.html#tools",
    "title": "RESOURCES",
    "section": "Tools",
    "text": "Tools\n\n\nAntConc (and other Ant tools)\nAntConc is a freeware corpus analysis toolkit for concordancing and text analysis developed by Laurence Anthony. In addition to AntConc, Laurence Anthony’s AntLab contains a conglomeration of extremely useful and very user-friendly software tools, that help and facilitate the analysis of textual data. Laurence has really developed an impressive, very user-friendly selection of tools that assist anyone interested in working with language data.\n\n\n\nSMARTool\nSMARTool is a corpus-based language learning and analysis tool for for English-speaking learners of Russian. It is linguist-built and thus informed by modern linguistic theory. SMARTool assists learners with coping with the rich Russian morphology and has user-friendly, corpus-based information and help for learning the most frequent forms of 3,000 basic vocabulary items."
  },
  {
    "objectID": "resources.html#courses",
    "href": "resources.html#courses",
    "title": "RESOURCES",
    "section": "Courses",
    "text": "Courses\n\n\nApplied Language Technology\nApplied Language Technology is a website hosting learning materials for two courses taught at the University of Helsinki: Working with Text in Python and Natural Language Processing for Linguists. Together, these two courses provide an introduction to applied language technology for audiences who are unfamiliar with language technology and programming. The learning materials assume no previous knowledge of the Python programming language.\n\n\n\nCultural Analytics with Python\nIntroduction to Cultural Analytics & Python is a website established by Melanie Walsh that hosts an online textbook which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences.\n\n\nGLAM Workbench\nThe GLAM Workbench is a collection of tools, tutorials, examples, and hacks to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time.\nThe resources in the GLAM Workbench are created and shared as Jupyter notebooks. Jupyter lets you combine narrative text and live code in an environment that encourages you to learn and explore. Jupyter notebooks run in your browser, and you can get started without installing any software!\nOne really great advantage of the GLAM workbench is that it is interactive: if you click on one of the links that says Run live on Binder, this will open the notebook, ready to use, in a customised computing environment using the Binder service.\n\n\nProgramming Historian\nThe Programming Historian is a collaborative blog that contains lessons on various topics associated with computational humanities. The blog was founded in 2008 by William J. Turkel and Alan MacEachern. It focused heavily on the Python programming language and was published open access as a Network in Canadian History & Environment (NiCHE) Digital Infrastructure project. In 2012, Programming Historian expanded its editorial team and launched as an open access peer reviewed scholarly journal of methodology for digital historians.\n\n\nTAPoR 3\nTAPoR 3, the Text Analysis Portal for Research contains a unique, well-curated list of a wide variety of tools commonly used or widely respected groups of tools used by leading scholars in the various fields of Digital Humanities. TAPoR 3 was developed with support from the Text Mining the Novel Project. The tools mentioned in the list provided by TAPoR 3 represent both tried and trusted tools used by DH scholars, or new advancements that offer exciting new possibilities in their field. Curated lists are excellent places to start exploring TAPoR from, and can help with deciding what to include in your own lists.\n\n\nQuick-R\nQuick-R by DataCamp and maintained by Rob Kabacoff that contains tutorial and R code on R, data management, statistics, and data visualization. The site is extremely helpful and recommendable for everyone that looks for code snippets that can be adapted for your own use.\n\n\nSTHDA\nStatistical Tools For High-Throughput Data Analysis (STHDA) is a website containing various tutorials on the implementation of statistical methods in R. It is particularly useful due to the wide range of topics and procedures it covers."
  },
  {
    "objectID": "resources.html#centers-labs",
    "href": "resources.html#centers-labs",
    "title": "RESOURCES",
    "section": "Centers | Labs",
    "text": "Centers | Labs\n\n\nText Crunching Centre\nAt the Text Crunching Centre, a team of experts in Natural Language Processing supports your text technology needs. The TCC is part of the Department of Computational Linguistics at the University of Zurich and it is a service offered to all departments of the University of Zurich as well as to external partners or customers.\n\n\n\nVARIENG\nVARIENG stands for the Research Unit for the Study of Variation, Contacts and Change in English. It also stands for innovative thinking and team work in English corpus linguistics and the study of language variation and change. VARIENG members study the English language, its uses and users, both today and in the past.\n\n\n\nAcqVA Aurora Lab\nThe AcqVA Aurora Lab, is the lab of the UiT Aurora Center for Language Acquisition, Variation & Attrition at The Arctic University of Norway in Tromsø. Together with the Psycholinguistics of Language Representation (PoLaR) lab, the AcqVA Lab provides methodological support for researchers at the AcqVA Aurora Center.\n\n\n\nSydney Corpus Lab\nThe Sydney Corpus Lab aims to promote corpus linguistics in Australia. It’s a virtual, rather than a physical lab, and is an online platform for connecting computer-based linguists across the University of Sydney and beyond. Its mission is to build research capacity in corpus linguistics at the University of Sydney, to connect Australian corpus linguists, and to promote the method in Australia, both in linguistics and in other disciplines. We have strong links with the Sydney Centre for Language Research (Computational Approaches to Language node) and the Sydney Digital Humanities Research Group.\n\n\n\nAntLab\nLaurence Anthony’s AntLab contains a conglomeration of extremely useful and very user-friendly software tools, such as AntConc that help and facilitate the analysis of textual data.\n\n\n\n\nMedia Research Methods Lab\nThe Media Research Methods Lab (MRML) at the Leibniz Institute for Media Research │ Hans Bredow Institute (HBI) is designed as a method-oriented lab, which focuses on linking established social science methods (surveys, observations, content analysis, experiments) with new digital methods from the field of computational social science (e.g. automated content analysis, network analysis, log data analysis, experience sampling) across topics and disciplines.\n\n\nNational Centre for Text Mining\nThe National Centre for Text Mining (NaCTeM), located in the UK, is the first publicly-funded text mining centre in the world. We provide text mining services in response to the requirements of the UK academic community. On the NaCTeM website, you can find pointers to sources of information about text mining such as links to text mining services provided by NaCTeM, software tools (both those developed by the NaCTeM team and by other text mining groups), seminars, general events, conferences and workshops, tutorials and demonstrations, and text mining publications."
  },
  {
    "objectID": "resources.html#blogs",
    "href": "resources.html#blogs",
    "title": "RESOURCES",
    "section": "Blogs",
    "text": "Blogs\n\nHypotheses\nHypotheses is a research blog by [Guillaume Desagulier](http://www2.univ-paris8.fr/desagulier/home/. In this blog, entitled Around the word, A corpus linguist’s notebook, Guillaume has recorded reflections and experiments on his practice as a usage-based corpus linguist and code for analyzing language using R.\n\n\nAneesha Bakharia\nAneesha Bakharia is a blog by Aneesha Bakharia about Data Science, Topic Modeling, Deep Learning, Algorithm Usability and Interpretation, Learning Analytics, and Electronics. Aneesha began her career as an electronics engineer but quickly transitioned to an educational software developer. Aneesha has worked in the higher education and vocational educational sectors in a variety of technical, innovation and project management roles. In her most recent role before commencing at UQ, Aneesha was a project manager for a large OLT Teaching and Learning grant on Learning Analytics at QUT. Aneesha’s primary responsibilities at ITaLI include directing the design, development and implementation of learning analytics initiatives (including the Course Insights teacher facing dashboard) at UQ.\n\n\nLinguistics with a Corpus\nLinguistics with a Corpus is a companion blog to the Cambridge Element book Doing Linguistics with a Corpus. Methodological Considerations for the Everyday User (Egbert, Larsson, and Biber, n.d.). The blog is intended to be a forum for methodological discussion in corpus linguistics, and Jesse, Tove, and Doug very much welcome comments and thoughts by visitors and readers!\n\n\nDigital Observatory Blog\nThe blog of the Digital Observatory at the Queensland University of Technology (QUT) contains updates on resources, meetings, (open) office hours, workshops, and developments at the Digital Observatory which provides services to researchers such as retrieving and processing social media data or offering workshops on data processing, data visualization, and anything related to gathering and working with social media data."
  },
  {
    "objectID": "resources.html#other-resources",
    "href": "resources.html#other-resources",
    "title": "RESOURCES",
    "section": "Other Resources",
    "text": "Other Resources\n\nBookNLP\nBookNLP is a natural language processing pipeline that scales to books and other long documents (in English), including:\n\nPart-of-speech tagging\n\nDependency parsing\n\nEntity recognition\n\nCharacter name clustering (e.g., “Tom”, “Tom Sawyer”, “Mr. Sawyer”, “Thomas Sawyer” -&gt; TOM_SAWYER) and conference resolution\n\nQuotation speaker identification\n\nSupersense tagging (e.g., “animal”, “artifact”, “body”, “cognition”, etc.)\n\nEvent tagging\n\nReferential gender inference (TOM_SAWYER -&gt; he/him/his)\n\nBookNLP ships with two models, both with identical architectures but different underlying BERT sizes. The larger and more accurate big model is fit for GPUs and multi-core computers; the faster small model is more appropriate for personal computers.\n\n\nDigital Humanities Awards\nThe Digital Humanities Awards page contains a list of great DH resources for the following categories:\n\nBest Exploration Of DH Failure/Limitations\n\nBest DH Data Visualization\n\nBest Use Of DH For Fun\n\nBest DH Dataset\n\nBest DH Short Publication\n\nBest DH Tool Or Suite Of Tools\n\nBest DH Training Materials\n\nSpecial Category: Best DH Response To COVID-19\n\n\n\nText Analysis in Python for Social Scientists\nThe book Text Analysis in Python for Social Scientists. Prediction and Classification contains a wealth of information about about a wide variety of sociocultural constructs. Automated prediction methods can infer these quantities (sentiment analysis is probably the most well-known application). However, there is virtually no limit to the kinds of things we can predict from text: power, trust, misogyny, are all signaled in language. These algorithms easily scale to corpus sizes unfeasible for manual analysis. Prediction algorithms have become steadily more powerful, especially with the advent of neural network methods. However, applying these techniques usually requires profound programming knowledge and machine learning expertise. As a result, many social scientists do not apply them. This Element provides the working social scientist with an overview of the most common methods for text classification, an intuition of their applicability, and Python code to execute them. It covers both the ethical foundations of such work as well as the emerging potential of neural network methods.\n\n\nData Measurements Tool\nThis blog entry introduces the Data Measurements Tool. The Data Measurements Tool is a Interactive Tool for Looking at Datasets. The Data Measurements Tool is a open-source Python library and no-code interface called the Data Measurements Tool, using our Dataset and Spaces Hubs paired with the great Streamlit tool. This can be used to help understand, build, curate, and compare datasets.\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "resources.html#introduction",
    "href": "resources.html#introduction",
    "title": "RESOURCES",
    "section": "Introduction",
    "text": "Introduction\nIf you want to contribute to LADAL and become an author, you can use this stylesheet for your LADAL tutorial. The sections below provide additional information on the style and format of your LADAL tutorial. The R Markdown document that you can use as a template can be downloaded here.\nYou will also have to download the bibliography file from https://slcladal.github.io/content/bibliography.bib for the tutorial to be knitted correctly. Although the knitted (or rendered) html file will look different from the LADAL design (because we have modified the theme for the LADAL page), it will be just like a proper LADAL tutorial once we have knitted the Rmd file on our machines and integrated your tutorial into the LADAL website.\nPreparation and session set up\nThis tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R here. For this tutorials, we need to install certain packages from an R library so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).\n\n# set options\noptions(stringsAsFactors = F) # no automatic data transformation\noptions(\"scipen\" = 100, \"digits\" = 4) # suppress math annotation\n# install packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flextable\")\n# install klippy for copy-to-clipboard button in code chunks\ninstall.packages(\"remotes\")\nremotes::install_github(\"rlesur/klippy\")\n\nNext, we activate the packages. Please include klippy in the installation and loading chunks to allow easy copy&pasting of code.\n\n# activate packages\nlibrary(tidyverse)\nlibrary(flextable)\n# activate klippy for copy-to-clipboard button\nklippy::klippy()\n\n\n\n\nOnce you have installed R, RStudio, and have also initiated the session by executing the code shown above, you are good to go."
  },
  {
    "objectID": "resources.html#tutorial-content",
    "href": "resources.html#tutorial-content",
    "title": "RESOURCES",
    "section": "Tutorial content",
    "text": "Tutorial content\nLoad some data and show what you want to show.\n\n# load data\ndata &lt;- base::readRDS(url(\"https://slcladal.github.io/data/sld.rda\", \"rb\"))\n\nUse flextable for displaying tabular data as shown below.\n\ndata %&gt;%\n    as.data.frame() %&gt;%\n    head(10) %&gt;%\n    flextable() %&gt;%\n    flextable::set_table_properties(width = .5, layout = \"autofit\") %&gt;%\n    flextable::theme_zebra() %&gt;%\n    flextable::fontsize(size = 12) %&gt;%\n    flextable::fontsize(size = 12, part = \"header\") %&gt;%\n    flextable::align_text_col(align = \"center\") %&gt;%\n    flextable::set_caption(caption = \"\") %&gt;%\n    flextable::border_outer()\n\nDateGenreTextPrepositionsRegion1,736Sciencealbin166.01North1,711Educationanon139.86North1,808PrivateLetterausten130.78North1,878Educationbain151.29North1,743Educationbarclay145.72North1,908Educationbenson120.77North1,906Diarybenson119.17North1,897Philosophyboethja132.96North1,785Philosophyboethri130.49North1,776Diaryboswell135.94North\n\n\nBelow is the code chunk for exercises.\n\n\n\nEXERCISE TIME!\n\n\n\n\nThis is an example question.\n\n\n\nAnswer\n\n\n# this is some code\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "resources.html#remarks-on-type-setting",
    "href": "resources.html#remarks-on-type-setting",
    "title": "RESOURCES",
    "section": "Remarks on type setting",
    "text": "Remarks on type setting\nLevel 1 headers with numbers,, lower level headers without numbering (add {-} at the end of the header to suppress numbering).\nFunction and package names in package style (different from normal text).\nUse italics for emphasis rather than bold type."
  },
  {
    "objectID": "resources.html#citation-session-info",
    "href": "resources.html#citation-session-info",
    "title": "RESOURCES",
    "section": "Citation & Session Info",
    "text": "Citation & Session Info\nYour last name, your first name. 2025. The title of your tutorial. Your location: your affiliation (in case you have one). url: https://slcladal.github.io/shorttitleofyourtutorial.html (Version 2025.01.13).\n@manual{yourlastname2025net,\n  author = {YourLastName, YourFirstName},\n  title = {The title of your tutorials},\n  note = {https://slcladal.github.io/shorttitleofyourtutorial.html},\n  year = {2025},\n  organization = {Your affiliation},\n  address = {Your location},\n  edition = {2025.01.13}\n}\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Australia/Brisbane\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] flextable_0.9.7 lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1  \n [5] dplyr_1.1.4     purrr_1.0.2     readr_2.1.5     tidyr_1.3.1    \n [9] tibble_3.2.1    ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4              generics_0.1.3          fontLiberation_0.1.0   \n [4] xml2_1.3.6              stringi_1.8.4           hms_1.1.3              \n [7] digest_0.6.37           magrittr_2.0.3          evaluate_1.0.1         \n[10] grid_4.4.1              timechange_0.3.0        fastmap_1.2.0          \n[13] jsonlite_1.8.9          zip_2.3.1               fansi_1.0.6            \n[16] scales_1.3.0            fontBitstreamVera_0.1.1 codetools_0.2-20       \n[19] klippy_0.0.0.9500       textshaping_0.4.0       cli_3.6.3              \n[22] rlang_1.1.4             fontquiver_0.2.1        munsell_0.5.1          \n[25] withr_3.0.2             yaml_2.3.10             gdtools_0.4.0          \n[28] tools_4.4.1             officer_0.6.7           uuid_1.2-1             \n[31] tzdb_0.4.0              colorspace_2.1-1        assertthat_0.2.1       \n[34] vctrs_0.6.5             R6_2.5.1                lifecycle_1.0.4        \n[37] htmlwidgets_1.6.4       ragg_1.3.3              pkgconfig_2.0.3        \n[40] pillar_1.9.0            gtable_0.3.6            glue_1.8.0             \n[43] data.table_1.16.2       Rcpp_1.0.13             systemfonts_1.1.0      \n[46] xfun_0.49               tidyselect_1.2.1        knitr_1.48             \n[49] htmltools_0.5.8.1       rmarkdown_2.28          compiler_4.4.1         \n[52] askpass_1.2.1           openssl_2.2.2          \n\n\n\nBack to top\nBack to HOME"
  },
  {
    "objectID": "notebooks/regex_cb.html",
    "href": "notebooks/regex_cb.html",
    "title": "Getting started with Regular Expressions",
    "section": "",
    "text": "An interactive LADAL notebook\nThis tutorial is the interactive Jupyter notebook accompanying the Language Technology and Data Analysis Laboratory (LADAL) tutorial Regular Expressions in R.\nPreparation and session set up\nWe set up our session by activating the packages we need for this tutorial.\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(htmlwidgets)\nOnce you have initiated the session by executing the code shown above, you are good to go.\nIf you are using this notebook on your own computer and you have not already installed the R packages listed above, you need to install them. You can install them by replacing the library command with install.packages and putting the name of the package into quotation marks like this: install.packages(\"dplyr\"). Then, you simply run this command and R will install the package you specified."
  },
  {
    "objectID": "notebooks/regex_cb.html#using-your-own-data",
    "href": "notebooks/regex_cb.html#using-your-own-data",
    "title": "Getting started with Regular Expressions",
    "section": "Using your own data",
    "text": "Using your own data\nWhile the tutorial uses data from the LADAL website, you can also use your own data. You can see below what you need to do to upload and use your own data.\nThe code chunk below allows you to upload two files from your own computer. To be able to load your own data, you need to click on the folder symbol to the left of the screen:\n\n\n\nBinder Folder Symbol\n\n\nThen, when the menu has unfolded, click on the smaller folder symbol (encircled in red in the picture below).\n\n\n\nSmall Binder Folder Symbol\n\n\nNow, you are in the main menu and can click on the ‘MyData’ folder.\n\n\n\nMyData Folder Symbol\n\n\nNow, that you are in the MyData folder, you can click on the upload symbol.\n\n\n\nBinder Upload Symbol\n\n\nSelect and upload the files you want to analyze (IMPORTANT: here, we assume that you upload some form of text data - not tabular data! You can upload only txt and docx files!). When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.\n\nmyfiles &lt;- list.files(here::here(\"MyData\"), # path to the corpus data\n    # full paths - not just the names of the files\n    full.names = T\n)\n# load colt files\nmytext &lt;- sapply(myfiles, function(x) {\n    x &lt;- scan(x,\n        what = \"char\",\n        sep = \"\",\n        quote = \"\",\n        quiet = T,\n        skipNul = T\n    )\n    x &lt;- paste0(x, sep = \" \", collapse = \" \")\n    x &lt;- stringr::str_squish(x)\n})\n# inspect\nstr(mytext)\n\n Named list()\n\n\nKeep in mind though that you need to adapt the names of the texts in the code chunks below so that the code below work on your own texts!\n\nBefore we delve into using regular expressions, we will have a look at the regular expressions that can be used in R and also check what they stand for.\nThere are three basic types of regular expressions:\n\nregular expressions that stand for individual symbols and determine frequencies\nregular expressions that stand for classes of symbols\nregular expressions that stand for structural properties\n\nThe regular expressions below show the first type of regular expressions, i.e. regular expressions that stand for individual symbols and determine frequencies.\n\n\n\nRegular expressions that stand for individual symbols and determine frequencies.\n\n\nThe regular expressions below show the second type of regular expressions, i.e. regular expressions that stand for classes of symbols.\n\n\n\nRegular expressions that stand for classes of symbols.\n\n\nThe regular expressions that denote classes of symbols are enclosed in [] and :. The last type of regular expressions, i.e. regular expressions that stand for structural properties are shown below.\n\n\n\nRegular expressions that stand for structural properties."
  },
  {
    "objectID": "tools/amtool/amtool.html",
    "href": "tools/amtool/amtool.html",
    "title": "Association-Measure Tool",
    "section": "",
    "text": "An interactive LADAL notebook"
  },
  {
    "objectID": "tools/amtool/amtool.html#using-your-own-data",
    "href": "tools/amtool/amtool.html#using-your-own-data",
    "title": "Association-Measure Tool",
    "section": "Using your own data",
    "text": "Using your own data\n\n\n\nTo use your own data, click on the folder called MyTexts (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.You can upload only txt-files (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! \n\n\n\n\n\n\n\n# load function that helps loading texts\nsource(\"tools/amtool/rscripts/loadtxts.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nhere() starts at /Users/laurenceanthony/Documents/projects/LADALQ\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nLoading required package: proxyC\n\n\n\nAttaching package: 'proxyC'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\n\n\nAttaching package: 'seededlda'\n\n\nThe following object is masked from 'package:quanteda':\n\n    info_tbb\n\n\nThe following object is masked from 'package:stats':\n\n    terms\n\n# load texts\ntext &lt;- loadtxts(\"notebooks/MyTexts\")\n# inspect the structure of the text object\nstr(text)\n\n Named chr [1:7] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"linguistics01.txt\" \"linguistics02.txt\" \"linguistics03.txt\" \"linguistics04.txt\" ...\n\n\nWe now prepare the table so that we can extract the association measures.\n\n# load function that extract association measures\nsource(\"tools/amtool/rscripts/prepam.R\")\n\nLoading required package: tokenizers\n\n\nLoading required package: tidytext\n\n# load texts\ncolldf &lt;- prepam(text)\n# inspect first 10 rows of the data\nhead(colldf, 10)\n\n# A tibble: 10 × 11\n# Groups:   w2 [10]\n   w1          w2           O11     N    R1   O12    R2    C1   O21    C2   O22\n   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 linguistics is             4 26132   149   145 25983   368   364 25764 25619\n 2 linguistics the           10 26132   149   139 25983  1839  1829 24293 24154\n 3 linguistics scientific     1 26132   149   148 25983     6     5 26126 25978\n 4 linguistics study          1 26132   149   148 25983   388   387 25744 25596\n 5 linguistics of            11 26132   149   138 25983  1777  1766 24355 24217\n 6 linguistics language       5 26132   149   144 25983   977   972 25155 25011\n 7 linguistics and            7 26132   149   142 25983  1629  1622 24503 24361\n 8 linguistics in             2 26132   149   147 25983   600   598 25532 25385\n 9 linguistics context        1 26132   149   148 25983    54    53 26078 25930\n10 linguistics century        1 26132   149   148 25983    41    40 26091 25943\n\n\nNext, we select the term that we want to extract collocates of.\n\n\n\nIMPORTANT: only execute the following code chunk if if you are interested in the collocates of a specific term!  Do NOT execute the following code chunk if you are interested in ALL collocations in the text (this would be unusual though). \n\n\n\n\n\n\nReplace linguistics with the term you are intersted in.\n\ncolldf %&gt;%\n  dplyr::filter(w1 == \"linguistics\") -&gt; colldf\n# inspect\nhead(colldf)\n\n# A tibble: 6 × 11\n# Groups:   w2 [6]\n  w1          w2           O11     N    R1   O12    R2    C1   O21    C2   O22\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 linguistics is             4 26132   149   145 25983   368   364 25764 25619\n2 linguistics the           10 26132   149   139 25983  1839  1829 24293 24154\n3 linguistics scientific     1 26132   149   148 25983     6     5 26126 25978\n4 linguistics study          1 26132   149   148 25983   388   387 25744 25596\n5 linguistics of            11 26132   149   138 25983  1777  1766 24355 24217\n6 linguistics language       5 26132   149   144 25983   977   972 25155 25011\n\n\nNow, we extract the association measures (if the table has many rows, this may take a few minutes).\n\n# load function that extract association measures\nsource(\"tools/amtool/rscripts/assocstats.R\")\n# calculate collocation statistics\nassocs &lt;- assocstats(colldf)\n# inspect first 10 rows of the data\nhead(assocs, 10)\n\n# A tibble: 10 × 21\n# Rowwise:  w2\n   w1        w2      O11     N       p     AM    X2     phi   Dice LogDice    MI\n   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 linguist… based     4 26132 0.00417 0.0351 17.4  0.0258  0.0304   -3.49 2.62 \n 2 linguist… on        4 26132 0.0137  0.0268 10.5  0.0200  0.0258   -3.66 2.12 \n 3 linguist… thro…     4 26132 0.0620  0.0268  4.34 0.0129  0.0196   -3.93 1.43 \n 4 linguist… with      3 26132 0.0798  0.0201  4.02 0.0124  0.0185   -3.99 1.58 \n 5 linguist… is        4 26132 0.159   0.0268  1.76 0.00820 0.0155   -4.17 0.931\n 6 linguist… comp…     2 26132 0.0152  0.0606 17.6  0.0259  0.0220   -3.82 3.41 \n 7 linguist… data      2 26132 0.0653  0.0274  6.08 0.0153  0.0180   -4.02 2.26 \n 8 linguist… corp…     2 26132 0.0653  0.0274  6.08 0.0153  0.0180   -4.02 2.26 \n 9 linguist… hist…     2 26132 0.148   0.0168  2.60 0.00998 0.0149   -4.20 1.56 \n10 linguist… scie…     1 26132 0.0337  0.167  27.4  0.0324  0.0129   -4.35 4.87 \n# ℹ 10 more variables: MS &lt;dbl&gt;, t.score &lt;dbl&gt;, z.score &lt;dbl&gt;, PMI &lt;dbl&gt;,\n#   DeltaP12 &lt;dbl&gt;, DeltaP21 &lt;dbl&gt;, DP &lt;dbl&gt;, LogOddsRatio &lt;dbl&gt;, G2 &lt;dbl&gt;,\n#   Sig_corrected &lt;chr&gt;"
  },
  {
    "objectID": "tools/amtool/amtool.html#exporting-the-association-table",
    "href": "tools/amtool/amtool.html#exporting-the-association-table",
    "title": "Association-Measure Tool",
    "section": "Exporting the association table",
    "text": "Exporting the association table\nTo export the table with the association measures as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory.\n\n# save data for MyOutput folder\nwrite_xlsx(assocs, here::here(\"notebooks/MyOutput/assocs.xlsx\"))\n\n\n\n\nYou will find the generated MS Excel spreadsheet named assocs.xlsx in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the assocs.xlsx file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/amtool/amtool.html#visualising-collocations",
    "href": "tools/amtool/amtool.html#visualising-collocations",
    "title": "Association-Measure Tool",
    "section": "Visualising collocations",
    "text": "Visualising collocations\nHere we will use a simple bar plot to visualize the 20 top collocations. The bar plot uses the DeltaP12 value but you are welcome to visualize another association measure (simply replace DeltaP12 with the measure you want to use).\n\n# sort the assocs data frame in descending order based on the 'DeltaP12' column\nassocs %&gt;%\n  dplyr::arrange(-DeltaP12) %&gt;%\n  # select the top 20 rows after sorting\n  head(20) %&gt;%\n  # create a ggplot with 'token' on the x-axis (reordered by 'DeltaP12') and 'DeltaP12' on the y-axis\n  ggplot(aes(x = reorder(w2, DeltaP12, mean), y = DeltaP12, label = DeltaP12)) +\n  # add a bar plot using the 'DeltaP12' values\n  geom_bar(stat = \"identity\") +\n  # add text labels above the bars with rounded 'DeltaP12' values\n  geom_text(aes(y = DeltaP12 - 0.005, label = round(DeltaP12, 3)), color = \"white\", size = 3) +\n  # flip the coordinates to have horizontal bars\n  coord_flip() +\n  # set the theme to a basic white and black theme\n  theme_bw() +\n  # add axes titles\n  labs(x = \"Collocate\", y = \"Association strength\")"
  },
  {
    "objectID": "tools/amtool/amtool.html#exporting-images",
    "href": "tools/amtool/amtool.html#exporting-images",
    "title": "Association-Measure Tool",
    "section": "Exporting images",
    "text": "Exporting images\nTo export network graph as an png-file, we use ggsave. Be aware that we use the here function to save the file in the MyOutput folder.\nThe ggsave function has the following main arguments:\n\nfilename: File name to create on disk.\n\ndevice: Device to use. Can either be a device function (e.g. png), or one of “eps”, “ps”, “tex” (pictex), “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg” or “wmf” (windows only). If NULL (default), the device is guessed based on the filename extension\n\npath: Path of the directory to save plot to: path and filename are combined to create the fully qualified file name. Defaults to the working directory.\n\nwidth, height: Plot size in units expressed by the units argument. If not supplied, uses the size of the current graphics device.\n\nunits: One of the following units in which the width and height arguments are expressed: “in”, “cm”, “mm” or “px”.\n\ndpi: Plot resolution. Also accepts a string input: “retina” (320), “print” (300), or “screen” (72). Applies only to raster output types.\n\nbg: Background colour. If NULL, uses the plot.background fill value from the plot theme.\n\n\n# save network graph for MyOutput folder\nggsave(here::here(\"notebooks/MyOutput/image_01.png\"), bg = \"white\")\n\n\n\n\nYou will find the image-file named image_01.png in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the image_01.png file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/keytool/keytool.html",
    "href": "tools/keytool/keytool.html",
    "title": "Keyword Tool",
    "section": "",
    "text": "An interactive LADAL notebook"
  },
  {
    "objectID": "tools/keytool/keytool.html#using-your-own-data",
    "href": "tools/keytool/keytool.html#using-your-own-data",
    "title": "Keyword Tool",
    "section": "Using your own data",
    "text": "Using your own data\n\n\n\n You need to upload file(s) into both the Target and the Reference folders.  Click on the folder called Target (it is in the menu to the left of the screen) and then simply drag and drop the txt-file(s) containing your target data into the folder.  click on the folder called Reference (it is in the menu to the left of the screen) and then simply drag and drop the txt-file(s) containing your reference data into the folder.   When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.You can upload only txt-files (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! \n\n\n\n\n\n\n\n# load function that helps loading texts\nsource(\"tools/keytool/rscripts/loadkeytxts.R\")\n\nLoading required package: here\n\n\nhere() starts at /Users/laurenceanthony/Documents/projects/LADALQ\n\n\nLoading required package: stringr\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2\n\n\nLoading required package: writexl\n\n# load texts\ntext &lt;- loadkeytxts(\"notebooks/Target\", \"notebooks/Reference\")\n# inspect the structure of the text object\nstr(text)\n\n chr [1:2] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ ...\n\n\nAfter loading the target and reference, we prepare the data for analysis.\n\n# load function that helps loading texts\nsource(\"tools/keytool/rscripts/prepkeydat.R\")\n# prepare texts\nkeystb &lt;- prepkeydat(text)\n# inspect the structure of the text object\nhead(keystb)\n\n# A tibble: 6 × 3\n  token      text1 text2\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt;\n1 a              4    12\n2 aadhyayi       1     0\n3 abstract       1     0\n4 according      2     0\n5 acoustic       1     0\n6 activities     1     0\n\n\nNow, we extract the keyness measures (if the table has many rows, this may take a few minutes).\n\n# load function that extract keyness measures (default ordering by G2)\nsource(\"tools/keytool/rscripts/keystats.R\")\n# load texts\nkeys &lt;- keystats(keystb)\n# inspect first 10 rows of the data\nhead(keys, 10)\n\n# A tibble: 10 × 27\n# Rowwise: \n   token       type  Sig_corrected   O11   O12 ptw_target ptw_ref    G2     RDF\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 grammar     type  p &lt; .01**         3     0       9.09    0     3.25 0.00909\n 2 his         type  p &lt; .01**         3     0       9.09    0     3.25 0.00909\n 3 langue      type  p &lt; .01**         3     0       9.09    0     3.25 0.00909\n 4 sound       type  p &lt; .01**         3     0       9.09    0     3.25 0.00909\n 5 meaning     type  p &lt; .01**         6     1      18.2     4.22  2.53 0.0140 \n 6 according   type  p &lt; .01**         2     0       6.06    0     2.16 0.00606\n 7 chomsky     type  p &lt; .01**         2     0       6.06    0     2.16 0.00606\n 8 competence  type  p &lt; .01**         2     0       6.06    0     2.16 0.00606\n 9 composition type  p &lt; .01**         2     0       6.06    0     2.16 0.00606\n10 description type  p &lt; .01**         2     0       6.06    0     2.16 0.00606\n# ℹ 18 more variables: RateRatio &lt;dbl&gt;, RateDifference &lt;dbl&gt;,\n#   DifferenceCoefficient &lt;dbl&gt;, LLR &lt;dbl&gt;, SignedDKL &lt;dbl&gt;, PDiff &lt;dbl&gt;,\n#   LogOddsRatio &lt;dbl&gt;, MI &lt;dbl&gt;, PMI &lt;dbl&gt;, phi &lt;dbl&gt;, X2 &lt;dbl&gt;,\n#   OddsRatio &lt;dbl&gt;, DeltaP &lt;dbl&gt;, p &lt;dbl&gt;, E11 &lt;dbl&gt;, O21 &lt;dbl&gt;, O22 &lt;dbl&gt;,\n#   N &lt;dbl&gt;\n\n\nFor information about the different keyness statsitics (what they mean and how they are calculated), please visit the LADAL tutorial on Keyword Analysis."
  },
  {
    "objectID": "tools/keytool/keytool.html#exporting-the-keyword-table",
    "href": "tools/keytool/keytool.html#exporting-the-keyword-table",
    "title": "Keyword Tool",
    "section": "Exporting the keyword table",
    "text": "Exporting the keyword table\nTo export the table with the keyness measures as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory.\n\n# save data for MyOutput folder\nwrite_xlsx(keys, here::here(\"notebooks/MyOutput/keys.xlsx\"))\n\n\n\n\nYou will find the generated MS Excel spreadsheet named keys.xlsx in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the keys.xlsx file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/keytool/keytool.html#visualising-keywords",
    "href": "tools/keytool/keytool.html#visualising-keywords",
    "title": "Keyword Tool",
    "section": "Visualising keywords",
    "text": "Visualising keywords\nHere we will use a simple bar plot to visualize the 10 top keywords of both texts. The bar plot uses the G2 value but you are welcome to visualize another keyness measure (simply replace G2 with the measure you want to use).\n\n# get top 10 keywords for text 1\ntop &lt;- keys %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_head(n = 10)\n# get top 10 keywords for text 2\nbot &lt;- keys %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_tail(n = 10)\n# combine into table\nrbind(top, bot) %&gt;%\n  # create a ggplot\n  ggplot(aes(x = reorder(token, G2, mean), y = G2, label = G2, fill = type)) +\n  # add a bar plot using the 'phi' values\n  geom_bar(stat = \"identity\") +\n  # add text labels above the bars with rounded 'phi' values\n  geom_text(aes(y = ifelse(G2 &gt; 0, G2 - 1, G2 + 1), \n                label = round(G2, 1)), color = \"white\", size = 3) + \n  # flip the coordinates to have horizontal bars\n  coord_flip() +\n  # set the theme to a basic white and black theme\n  theme_bw() +\n  # remove legend\n  theme(legend.position = \"none\") +\n    # define colors\n  scale_fill_manual(values = c(\"mediumblue\", \"orange\")) +\n  # set the x-axis label to \"Token\" and y-axis label to \"Keyness statistic\"\n  labs(title = \"Top 10 keywords for target and reference\", x = \"Keyword\", y = \"Keyness statistic\")"
  },
  {
    "objectID": "tools/keytool/keytool.html#exporting-images",
    "href": "tools/keytool/keytool.html#exporting-images",
    "title": "Keyword Tool",
    "section": "Exporting images",
    "text": "Exporting images\nTo export network graph as an png-file, we use ggsave. Be aware that we use the here function to save the file in the MyOutput folder.\nThe ggsave function has the following main arguments:\n\nfilename: File name to create on disk.\n\ndevice: Device to use. Can either be a device function (e.g. png), or one of “eps”, “ps”, “tex” (pictex), “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg” or “wmf” (windows only). If NULL (default), the device is guessed based on the filename extension\n\npath: Path of the directory to save plot to: path and filename are combined to create the fully qualified file name. Defaults to the working directory.\n\nwidth, height: Plot size in units expressed by the units argument. If not supplied, uses the size of the current graphics device.\n\nunits: One of the following units in which the width and height arguments are expressed: “in”, “cm”, “mm” or “px”.\n\ndpi: Plot resolution. Also accepts a string input: “retina” (320), “print” (300), or “screen” (72). Applies only to raster output types.\n\nbg: Background colour. If NULL, uses the plot.background fill value from the plot theme.\n\n\n# save network graph for MyOutput folder\nggsave(here::here(\"notebooks/MyOutput/image_01.png\"), bg = \"white\")\n\n\n\n\nYou will find the image-file named image_01.png in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the image_01.png file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/kwictool/kwictool.html",
    "href": "tools/kwictool/kwictool.html",
    "title": "Concordancing Tool",
    "section": "",
    "text": "An interactive LADAL notebook"
  },
  {
    "objectID": "tools/kwictool/kwictool.html#using-your-own-data",
    "href": "tools/kwictool/kwictool.html#using-your-own-data",
    "title": "Concordancing Tool",
    "section": "Using your own data",
    "text": "Using your own data\n\n\n\nTo use your own data, click on the folder called MyTexts (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.You can upload only txt-files (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! \n\n\n\n\n\n\n\n# load function that helps loading texts\nsource(\"tools/kwictool/rscripts/loadtxts.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nhere() starts at /Users/laurenceanthony/Documents/projects/LADALQ\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nLoading required package: proxyC\n\n\n\nAttaching package: 'proxyC'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\n\n\nAttaching package: 'seededlda'\n\n\nThe following object is masked from 'package:quanteda':\n\n    info_tbb\n\n\nThe following object is masked from 'package:stats':\n\n    terms\n\n# load texts\ntext &lt;- loadtxts(\"notebooks/MyTexts\")\n# inspect the structure of the text object\nstr(text)\n\n Named chr [1:7] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"linguistics01.txt\" \"linguistics02.txt\" \"linguistics03.txt\" \"linguistics04.txt\" ..."
  },
  {
    "objectID": "tools/kwictool/kwictool.html#creating-simple-concordances",
    "href": "tools/kwictool/kwictool.html#creating-simple-concordances",
    "title": "Concordancing Tool",
    "section": "Creating simple concordances",
    "text": "Creating simple concordances\nNow we can extract concordances using the kwic function from the quanteda package. This function has the following arguments:\n\nx: a text or collection of texts. The text needs to be tokenised, i.e. split it into individual words, which is why we use the text in the tokens() function.\npattern: a keyword defined by a search pattern\n\nwindow: the size of the context window (how many word before and after)\n\nvaluetype: the type of pattern matching\n\n“glob” for “glob”-style wildcard expressions;\n\n“regex” for regular expressions; or\n\n“fixed” for exact matching\n\n\nseparator: a character to separate words in the output\n\ncase_insensitive: logical; if TRUE, ignore case when matching a pattern or dictionary values\n\n\n\n\nYou can easily change and adapt the concordance. For instance, you can search for a different word or phrase by substituting the with the word or phrase as the pattern. Additionally, if you wish to widen the context window, just replace the ‘5’ with ‘10’. This adjustment will extend the context around the keyword by 5 additional words in both the preceding and following context. \n\n\n\n\n\n\n\nmykwic &lt;- kwic(\n  # tokenise and define text\n  tokens(text), \n  # define target word (this is called the \"search pattern\")\n  pattern = phrase(\"the\"),\n  # 5 words before and after\n  window = 5,\n  # no regex\n  valuetype = \"regex\",\n  # words separated by whitespace\n  separator = \" \",\n  # search should be case insensitive\n  case_insensitive = TRUE)\n\n# inspect resulting kwic\nmykwic %&gt;%\n  # convert into a data frame\n  as.data.frame() %&gt;%\n  # show only first 10 results\n  head(10)\n\n             docname from to                                    pre keyword\n1  linguistics01.txt    3  3                         Linguistics is     the\n2  linguistics01.txt   21 21              and language in context .     The\n3  linguistics01.txt   25 25           . The earliest activities in     the\n4  linguistics01.txt   35 35       language have been attributed to     the\n5  linguistics01.txt   48 48          wrote a formal description of     the\n6  linguistics01.txt   74 74             and meaning . Phonetics is     the\n7  linguistics01.txt   84 84      non-speech sounds and delves into   their\n8  linguistics01.txt   90 90 acoustic and articulatory properties .     The\n9  linguistics01.txt   96 96           study of language meaning on     the\n10 linguistics01.txt   97 97             of language meaning on the   other\n                                        post pattern\n1             scientific study of language .     the\n2   earliest activities in the documentation     the\n3  documentation and description of language     the\n4       th-century-BC Indian grammarian Pa ?     the\n5                 Sanskrit language in his A     the\n6             study of speech and non-speech     the\n7     acoustic and articulatory properties .     the\n8               study of language meaning on     the\n9                  other hand deals with how     the\n10             hand deals with how languages     the"
  },
  {
    "objectID": "tools/kwictool/kwictool.html#exporting-concordances",
    "href": "tools/kwictool/kwictool.html#exporting-concordances",
    "title": "Concordancing Tool",
    "section": "Exporting concordances",
    "text": "Exporting concordances\nTo export a concordance table as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory.\n\n# save data for MyOutput folder\nwrite_xlsx(mykwic, here::here(\"notebooks/MyOutput/mykwic.xlsx\"))\n\n\n\n\nYou will find the generated MS Excel spreadsheet named mykwic.xlsx in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the mykwic.xlsx file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/nettool/nettool.html",
    "href": "tools/nettool/nettool.html",
    "title": "Network-Analysis Tool",
    "section": "",
    "text": "An interactive LADAL notebook"
  },
  {
    "objectID": "tools/nettool/nettool.html#using-your-own-data",
    "href": "tools/nettool/nettool.html#using-your-own-data",
    "title": "Network-Analysis Tool",
    "section": "Using your own data",
    "text": "Using your own data\nThe data should be an MS excel spread sheet with three columns called\n\nfrom (start of the edge)\n\nto (end of the edge)\n\ns (width / strength of the edge)\n\nIf you do not want to use this column, simply insert 1 for all cells in that column.\nThe data should have the same structure as the example below:\n\n\n\nData format required for this notebook"
  },
  {
    "objectID": "tools/nettool/nettool.html#using-your-own-data-1",
    "href": "tools/nettool/nettool.html#using-your-own-data-1",
    "title": "Network-Analysis Tool",
    "section": "Using your own data",
    "text": "Using your own data\n\n\n\nTo use your own data, click on the folder called MyTables (it is in the menu to the left of the screen) and then simply drag and drop your xlsx-file into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.You can upload only xlsx-files! ! \n\n\n\n\n\n\n\n# load function that loads xlsx data\nsource(\"tools/nettool/rscripts/loadnetdata.R\")\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2\n\n\nLoading required package: ggraph\n\n\nLoading required package: igraph\n\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n\nLoading required package: here\n\n\nhere() starts at /Users/laurenceanthony/Documents/projects/LADALQ\n\n\nLoading required package: openxlsx\n\n\nLoading required package: readxl\n\n\nLoading required package: stringr\n\n\nLoading required package: tidyr\n\n\n\nAttaching package: 'tidyr'\n\n\nThe following object is masked from 'package:igraph':\n\n    crossing\n\n\nLoading required package: tidytext\n\n\nLoading required package: tidygraph\n\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:igraph':\n\n    groups\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n# load texts\ned &lt;- loadnetdata(\"notebooks/MyTables\")\n# inspect the structure of the data\ned\n\n# A tibble: 4 × 2\n  from  to   \n  &lt;chr&gt; &lt;chr&gt;\n1 a     b    \n2 b     c    \n3 c     a    \n4 c     b    \n\n\nWe generate the network with the netplot function. This function has the following arguments:\n\nx: a data frame with the columns from, to, ands`\n\nedge_col: color of the edges (default = gray80),\n\nedge_curv: curvature of the edges that ranges from 0 (no curvature) to 1 (extreme curvature) (default = 0.2),\n\nnode_tran: node transparency that ranges from from 0 (absolute transparency) to 1 (no transparency) (default = 0.2),\n\nn_min: minimum label size (default = 5),\n\nn_max: maximum label size (default = 10),\n\nn_col: node color (default = gray10),\n\n\n# load function that generates a network graph\nsource(\"tools/nettool/rscripts/netplot.R\")\n# plot the network\nnetplot(x = ed, edge_col = \"gray80\", edge_curv= .2, node_tran = .2, n_min = 5, n_max = 10, n_col = \"gray10\")"
  },
  {
    "objectID": "tools/nettool/nettool.html#exporting-images",
    "href": "tools/nettool/nettool.html#exporting-images",
    "title": "Network-Analysis Tool",
    "section": "Exporting images",
    "text": "Exporting images\nTo export network graph as an png-file, we use ggsave. Be aware that we use the here function to save the file in the MyOutput folder.\nThe ggsave function has the following main arguments:\n\nfilename: File name to create on disk.\n\ndevice: Device to use. Can either be a device function (e.g. png), or one of “eps”, “ps”, “tex” (pictex), “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg” or “wmf” (windows only). If NULL (default), the device is guessed based on the filename extension\n\npath: Path of the directory to save plot to: path and filename are combined to create the fully qualified file name. Defaults to the working directory.\n\nwidth, height: Plot size in units expressed by the units argument. If not supplied, uses the size of the current graphics device.\n\nunits: One of the following units in which the width and height arguments are expressed: “in”, “cm”, “mm” or “px”.\n\ndpi: Plot resolution. Also accepts a string input: “retina” (320), “print” (300), or “screen” (72). Applies only to raster output types.\n\nbg: Background colour. If NULL, uses the plot.background fill value from the plot theme.\n\n\n# save network graph for MyOutput folder\nggsave(here::here(\"notebooks/MyOutput/image_01.png\"), bg = \"white\")\n\n\n\n\nYou will find the image-file named image_01.png in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the image_01.png file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/postool/postool.html",
    "href": "tools/postool/postool.html",
    "title": "Pos-Tagging Tool",
    "section": "",
    "text": "An interactive LADAL notebook"
  },
  {
    "objectID": "tools/postool/postool.html#using-your-own-data",
    "href": "tools/postool/postool.html#using-your-own-data",
    "title": "Pos-Tagging Tool",
    "section": "Using your own data",
    "text": "Using your own data\n\n\n\nTo use your own data, click on the folder called MyTexts (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.You can upload only txt-files (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! \n\n\n\n\n\n\n\n# load function that helps loading texts\nsource(\"tools/postool/rscripts/loadtxts.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nhere() starts at /Users/laurenceanthony/Documents/projects/LADALQ\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nLoading required package: proxyC\n\n\n\nAttaching package: 'proxyC'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\n\n\nAttaching package: 'seededlda'\n\n\nThe following object is masked from 'package:quanteda':\n\n    info_tbb\n\n\nThe following object is masked from 'package:stats':\n\n    terms\n\n# load texts\ntext &lt;- loadtxts(\"notebooks/MyTexts\")\n# inspect the structure of the text object\nstr(text)\n\n Named chr [1:7] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"linguistics01.txt\" \"linguistics02.txt\" \"linguistics03.txt\" \"linguistics04.txt\" ..."
  },
  {
    "objectID": "tools/postool/postool.html#pos-tagging",
    "href": "tools/postool/postool.html#pos-tagging",
    "title": "Pos-Tagging Tool",
    "section": "Pos tagging",
    "text": "Pos tagging\nNow we can pos-tag the text using the postag function. This function has the following arguments:\n\nx: a text or collection of texts.\n\nlanguage: the language model used for pos-tagging (these are the availabel pre-trained models models: “afrikaans-afribooms”, “ancient_greek-perseus”, “ancient_greek-proiel”, “arabic-padt”, “armenian-armtdp”, “basque-bdt”, “belarusian-hse”, “bulgarian-btb”, “buryat-bdt”, “catalan-ancora”, “chinese-gsd”, “chinese-gsdsimp”, “classical_chinese-kyoto”, “coptic-scriptorium”, “croatian-set”, “czech-cac”, “czech-cltt”, “czech-fictree”, “czech-pdt”, “danish-ddt”, “dutch-alpino”, “dutch-lassysmall”, “english-ewt”, “english-gum”, “english-lines”, “english-partut”, “estonian-edt”, “estonian-ewt”, “finnish-ftb”, “finnish-tdt”, “french-gsd”, “french-partut”, “french-sequoia”, “french-spoken”, “galician-ctg”, “galician-treegal”, “german-gsd”, “german-hdt”, “gothic-proiel”, “greek-gdt”, “hebrew-htb”, “hindi-hdtb”, “hungarian-szeged”, “indonesian-gsd”, “irish-idt”, “italian-isdt”, “italian-partut”, “italian-postwita”, “italian-twittiro”, “italian-vit”, “japanese-gsd”, “kazakh-ktb”, “korean-gsd”, “korean-kaist”, “kurmanji-mg”, “latin-ittb”, “latin-perseus”, “latin-proiel”, “latvian-lvtb”, “lithuanian-alksnis”, “lithuanian-hse”, “maltese-mudt”, “marathi-ufal”, “north_sami-giella”, “norwegian-bokmaal”, “norwegian-nynorsk”, “norwegian-nynorsklia”, “old_church_slavonic-proiel”, “old_french-srcmf”, “old_russian-torot”, “persian-seraji”, “polish-lfg”, “polish-pdb”, “polish-sz”, “portuguese-bosque”, “portuguese-br”, “portuguese-gsd”, “romanian-nonstandard”, “romanian-rrt”, “russian-gsd”, “russian-syntagrus”, “russian-taiga”, “sanskrit-ufal”, “scottish_gaelic-arcosg”, “serbian-set”, “slovak-snk”, “slovenian-ssj”,\n“slovenian-sst”, “spanish-ancora”, “spanish-gsd”, “swedish-lines”, “swedish-talbanken”, “tamil-ttb”, “telugu-mtg”, “turkish-imst”, “ukrainian-iu”, “upper_sorbian-ufal”, “urdu-udtb”, “uyghur-udt”, “vietnamese-vtb”, “wolof-wtb”))\n\n\n\n\nThe code chunk below assumes that you want to pos-tag English text(s).  If you want to pos-tag text in another language, replace “english-ext” with another pre-trained language model, e.g. “german-gsd” for pos-tagging a German text (see above for available models). \n\n\n\n\n\n\n\n# load function that pos-tags text\nsource(\"tools/postool/rscripts/postag.R\")\n\n# perform pos-tagging\npostagged &lt;- postag(text, language = \"english-ewt\")\n\n# inspect result\nstr(postagged)\n\n Named chr [1:7] \"Linguistics/NNS is/VBZ the/DT scientific/JJ study/NN of/IN language/NN ./. It/PRP involves/VBZ analysing/VBG la\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"linguistics01.txt\" \"linguistics02.txt\" \"linguistics03.txt\" \"linguistics04.txt\" ..."
  },
  {
    "objectID": "tools/postool/postool.html#saving-to-myoutput",
    "href": "tools/postool/postool.html#saving-to-myoutput",
    "title": "Pos-Tagging Tool",
    "section": "Saving to MyOutput",
    "text": "Saving to MyOutput\nAs a concluding step, we save the outcomes – the three files housing our cleaned texts – in the ‘MyOutput’ folder, conveniently visible on the left side of the screen.\n\n# load function that helps loading texts\nsource(\"tools/postool/rscripts/savetxts.R\")\nsavetxts(postagged)\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n[[6]]\nNULL\n\n[[7]]\nNULL\n\n\n\n\n\nYou will find the txt-files in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then highlight the files, and choose Download from the dropdown menu to download the files."
  },
  {
    "objectID": "tools/sentool/sentool.html",
    "href": "tools/sentool/sentool.html",
    "title": "Sentiment-Analysis Tool",
    "section": "",
    "text": "An interactive LADAL notebook"
  },
  {
    "objectID": "tools/sentool/sentool.html#using-your-own-data",
    "href": "tools/sentool/sentool.html#using-your-own-data",
    "title": "Sentiment-Analysis Tool",
    "section": "Using your own data",
    "text": "Using your own data\n\n\n\nTo use your own data, click on the folder called MyTexts (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.You can upload only txt-files (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! \n\n\n\n\n\n\n\n# load function that helps loading texts\nsource(\"tools/sentool/rscripts/loadtxts.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nhere() starts at /Users/laurenceanthony/Documents/projects/LADALQ\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nLoading required package: proxyC\n\n\n\nAttaching package: 'proxyC'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\n\n\nAttaching package: 'seededlda'\n\n\nThe following object is masked from 'package:quanteda':\n\n    info_tbb\n\n\nThe following object is masked from 'package:stats':\n\n    terms\n\n# load texts\ntext &lt;- loadtxts(\"notebooks/MyTexts\")\n# inspect the structure of the text object\nstr(text)\n\n Named chr [1:7] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"linguistics01.txt\" \"linguistics02.txt\" \"linguistics03.txt\" \"linguistics04.txt\" ..."
  },
  {
    "objectID": "tools/sentool/sentool.html#sentiment-annotation",
    "href": "tools/sentool/sentool.html#sentiment-annotation",
    "title": "Sentiment-Analysis Tool",
    "section": "Sentiment Annotation",
    "text": "Sentiment Annotation\nNow, we reformat the data and combine it with the Word-Emotion Association Lexicon (Mohammad and Turney 2013). As this step uses this external dictionary, please cite the dictionary in addition to the tool (the reference for the dictionary is provided below at the end of this notebook).\nThis step produces a table where each word in the data is annotated with an emotion value or sentiment IF that word occurs in the Word-Emotion Association Lexicon.\n\n# load function that helps prepare the texts for a sentiment analysis\nsource(\"tools/sentool/rscripts/prepsenti.R\")\n# prepare texts\nsenti &lt;- prepsenti(text)\n# inspect\nhead(senti)\n\n# A tibble: 6 × 4\n# Groups:   file [1]\n  file          word        words sentiment\n  &lt;fct&gt;         &lt;chr&gt;       &lt;int&gt; &lt;fct&gt;    \n1 linguistics01 linguistics   143 &lt;NA&gt;     \n2 linguistics01 is            143 &lt;NA&gt;     \n3 linguistics01 the           143 &lt;NA&gt;     \n4 linguistics01 scientific    143 positive \n5 linguistics01 scientific    143 trust    \n6 linguistics01 study         143 positive"
  },
  {
    "objectID": "tools/sentool/sentool.html#exporting-the-results",
    "href": "tools/sentool/sentool.html#exporting-the-results",
    "title": "Sentiment-Analysis Tool",
    "section": "Exporting the results",
    "text": "Exporting the results\nTo export the table with the results as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory.\n\n# save data for MyOutput folder\nwrite_xlsx(senti, here::here(\"notebooks/MyOutput/senti.xlsx\"))\n\n\n\n\nYou will find the generated MS Excel spreadsheet named senti.xlsx in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the senti.xlsx file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/sentool/sentool.html#section",
    "href": "tools/sentool/sentool.html#section",
    "title": "Sentiment-Analysis Tool",
    "section": "",
    "text": "Next, we generate an overview table. You can choose to download the table shown or the table produced below depending on what you need.\n\n# load function that helps summarise the sentiment analysis\nsource(\"tools/sentool/rscripts/sumsenti.R\")\n# summarise the results\nsenti_sum &lt;- sumsenti(senti)\n\n`summarise()` has grouped output by 'file'. You can override using the\n`.groups` argument.\n\n# inspect\nhead(senti_sum)\n\n# A tibble: 6 × 5\n# Groups:   file [2]\n  file          sentiment    sentiment_freq words percentage\n  &lt;fct&gt;         &lt;fct&gt;                 &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 linguistics01 anticipation              1   143        0.7\n2 linguistics01 positive                 10   143        7  \n3 linguistics01 trust                     3   143        2.1\n4 linguistics02 anticipation              2    81        2.5\n5 linguistics02 positive                  4    81        4.9\n6 linguistics02 trust                     5    81        6.2"
  },
  {
    "objectID": "tools/sentool/sentool.html#exporting-the-results-1",
    "href": "tools/sentool/sentool.html#exporting-the-results-1",
    "title": "Sentiment-Analysis Tool",
    "section": "Exporting the results",
    "text": "Exporting the results\nTo export the table with the results as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory.\n\n# save data for MyOutput folder\nwrite_xlsx(senti_sum, here::here(\"notebooks/MyOutput/senti_sum.xlsx\"))\n\n\n\n\nYou will find the generated MS Excel spreadsheet named senti_sum.xlsx in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the senti_sum.xlsx file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/stringtool/stringtool.html",
    "href": "tools/stringtool/stringtool.html",
    "title": "Corpus-Cleaning Tool",
    "section": "",
    "text": "An interactive LADAL notebook"
  },
  {
    "objectID": "tools/stringtool/stringtool.html#using-your-own-data",
    "href": "tools/stringtool/stringtool.html#using-your-own-data",
    "title": "Corpus-Cleaning Tool",
    "section": "Using your own data",
    "text": "Using your own data\n\n\n\nTo use your own data, click on the folder called MyTexts (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.You can upload only txt-files (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! \n\n\n\n\n\n\n\n# load function that helps loading texts\nsource(\"tools/stringtool/rscripts/loadtxts.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nhere() starts at /Users/laurenceanthony/Documents/projects/LADALQ\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nLoading required package: proxyC\n\n\n\nAttaching package: 'proxyC'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\n\n\nAttaching package: 'seededlda'\n\n\nThe following object is masked from 'package:quanteda':\n\n    info_tbb\n\n\nThe following object is masked from 'package:stats':\n\n    terms\n\n# load texts\ntext &lt;- loadtxts(\"notebooks/MyTexts\")\n# inspect the structure of the text object\nstr(text)\n\n Named chr [1:7] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"linguistics01.txt\" \"linguistics02.txt\" \"linguistics03.txt\" \"linguistics04.txt\" ..."
  },
  {
    "objectID": "tools/stringtool/stringtool.html#reformatting",
    "href": "tools/stringtool/stringtool.html#reformatting",
    "title": "Corpus-Cleaning Tool",
    "section": "Reformatting",
    "text": "Reformatting\nNow, we convert the text(s) into a data frame.\n\n# load function that helps loading texts\nsource(\"tools/stringtool/rscripts/formattext.R\")\n# load texts\ntext_df &lt;- formattext(text)\n# inspect the structure of the text object\nstr(text_df)\n\n'data.frame':   7 obs. of  2 variables:\n $ file: chr  \"linguistics01\" \"linguistics02\" \"linguistics03\" \"linguistics04\" ...\n $ text: chr  \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ \"Grammar is a system of rules which governs the production and use of utterances in a given language. These rule\"| __truncated__ \"In the early 20th century, Ferdinand de Saussure distinguished between the notions of langue and parole in his \"| __truncated__ \"The study of parole (which manifests through cultural discourses and dialects) is the domain of sociolinguistic\"| __truncated__ ..."
  },
  {
    "objectID": "tools/stringtool/stringtool.html#cleaning",
    "href": "tools/stringtool/stringtool.html#cleaning",
    "title": "Corpus-Cleaning Tool",
    "section": "Cleaning",
    "text": "Cleaning\nNow that we’ve organized the data into a tabular format, the cleaning process becomes straightforward. We work with the data frame, employing str_remove_all and str_replace_all to eliminate undesired text sequences from the column contents. The distinction lies in their usage:\n\nstr_remove_all requires specifying the column to clean and indicating what to remove.\nstr_replace_all additionally needs information on the replacement pattern for the specified pattern.\n\nTo further clean your data, simply copy the commands and paste them into the code (. Pay attention to the column you are working on! If you want to clean the clean-text column, then do not define text as the column you want to clean (assuming that the clean-text column has been created).\n\n# create a data frame with 'file' and 'text' columns\ntext_df %&gt;%\n  \n  # create a new column called 'text_clean'\n  # clean 'text' column by removing \"&lt;.*?&gt;\"\n  dplyr::mutate(text_clean = stringr::str_remove_all(text, \"&lt;.*?&gt;\"),\n                \n                # replace strings of white spaces with a single white space\n                text_clean = stringr::str_replace_all(text_clean, \" {2,}\", \" \"),\n                \n                # PASTE YOUR CLEANING INSTRUCTIONS BELOW \n                # USE ONE OF THE TWO FORMATS: \n                # text_clean = stringr::str_replace_all(text_clean, \"TO BE REPLACED\", \"REPLACEMENT\"),\n                # OR\n                # text_clean = stringr::str_remove_all(text_clean, \"TO BE REMOVED\"),\n                \n                \n                # convert clean text to lower case)\n                text_clean = tolower(text_clean)) -&gt; clean_df  # assign the result to 'clean_df'\n  \n# inspect the first 10 rows of the cleaned data frame\nstr(clean_df)\n\n'data.frame':   7 obs. of  3 variables:\n $ file      : chr  \"linguistics01\" \"linguistics02\" \"linguistics03\" \"linguistics04\" ...\n $ text      : chr  \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ \"Grammar is a system of rules which governs the production and use of utterances in a given language. These rule\"| __truncated__ \"In the early 20th century, Ferdinand de Saussure distinguished between the notions of langue and parole in his \"| __truncated__ \"The study of parole (which manifests through cultural discourses and dialects) is the domain of sociolinguistic\"| __truncated__ ...\n $ text_clean: chr  \"linguistics is the scientific study of language. it involves analysing language form language meaning and langu\"| __truncated__ \"grammar is a system of rules which governs the production and use of utterances in a given language. these rule\"| __truncated__ \"in the early 20th century, ferdinand de saussure distinguished between the notions of langue and parole in his \"| __truncated__ \"the study of parole (which manifests through cultural discourses and dialects) is the domain of sociolinguistic\"| __truncated__ ..."
  },
  {
    "objectID": "tools/stringtool/stringtool.html#extract-text",
    "href": "tools/stringtool/stringtool.html#extract-text",
    "title": "Corpus-Cleaning Tool",
    "section": "Extract text",
    "text": "Extract text\nNow, we aggregate the cleaned texts from the ‘text_clean’ column by file, ensuring we obtain a single consolidated clean text for each file and save the result as ctext.\n\n# load function that helps loading texts\nsource(\"tools/stringtool/rscripts/extracttext.R\")\n# load texts\nctext &lt;- extracttext(clean_df)\n# inspect the structure of the text object\nstr(ctext)\n\n Named chr [1:7] \"linguistics is the scientific study of language. it involves analysing language form language meaning and langu\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"linguistics01.txt\" \"linguistics02.txt\" \"linguistics03.txt\" \"linguistics04.txt\" ..."
  },
  {
    "objectID": "tools/stringtool/stringtool.html#saving-to-myoutput",
    "href": "tools/stringtool/stringtool.html#saving-to-myoutput",
    "title": "Corpus-Cleaning Tool",
    "section": "Saving to MyOutput",
    "text": "Saving to MyOutput\nAs a concluding step, we save the outcomes – the three files housing our cleaned texts – in the ‘MyOutput’ folder, conveniently visible on the left side of the screen.\n\n# load function that helps loading texts\nsource(\"tools/stringtool/rscripts/savetxts.R\")\nsavetxts(ctext)\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n[[6]]\nNULL\n\n[[7]]\nNULL\n\n\n\n\n\nYou will find the txt-files in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then highlight the files, and choose Download from the dropdown menu to download the files."
  },
  {
    "objectID": "tools/topictool/topictool.html",
    "href": "tools/topictool/topictool.html",
    "title": "Topic-Model Tool",
    "section": "",
    "text": "An interactive LADAL notebook"
  },
  {
    "objectID": "tools/topictool/topictool.html#using-your-own-data",
    "href": "tools/topictool/topictool.html#using-your-own-data",
    "title": "Topic-Model Tool",
    "section": "Using your own data",
    "text": "Using your own data\n\n\n\nTo use your own data, click on the folder called MyTexts (it is in the menu to the left of the screen) and then simply drag and drop your txt-files into the folder. When you then execute the code chunk below, you will upload your own data and you can then use it in this notebook.You can upload only txt-files (simple unformatted files created in or saved by a text editor)! The notebook assumes that you upload some form of text data - not tabular data! IMPORTANT: topic modeling works best if the texts are neither too short nor too long. Typically paragraphs work best (texts above sentence level and below article length). \n\n\n\n\n\n\n\n# load function that helps loading texts\nsource(\"tools/topictool/rscripts/loadtxts.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nhere() starts at /Users/laurenceanthony/Documents/projects/LADALQ\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nLoading required package: proxyC\n\n\n\nAttaching package: 'proxyC'\n\n\nThe following object is masked from 'package:stats':\n\n    dist\n\n\n\nAttaching package: 'seededlda'\n\n\nThe following object is masked from 'package:quanteda':\n\n    info_tbb\n\n\nThe following object is masked from 'package:stats':\n\n    terms\n\n# load texts\ncorpus &lt;- loadtxts(\"notebooks/MyTexts\")\n# inspect the structure of the text object\nstr(corpus)\n\n Named chr [1:7] \"Linguistics is the scientific study of language. It involves analysing language form language meaning and langu\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:7] \"linguistics01.txt\" \"linguistics02.txt\" \"linguistics03.txt\" \"linguistics04.txt\" ..."
  },
  {
    "objectID": "tools/topictool/topictool.html#cleaning-and-tokenising",
    "href": "tools/topictool/topictool.html#cleaning-and-tokenising",
    "title": "Topic-Model Tool",
    "section": "Cleaning and tokenising",
    "text": "Cleaning and tokenising\nWe start by cleaning the corpus data (by removing tags, artefacts and non-alpha-numeric characters), then splitting the clean corpora into individual words, and creating a document-frequency matrix.\n\n# load function that helps loading texts\nsource(\"tools/topictool/rscripts/preptop.R\")\n\nLoading required package: tidytext\n\n\nLoading required package: topicmodels\n\n\n\nAttaching package: 'topicmodels'\n\n\nThe following objects are masked from 'package:seededlda':\n\n    perplexity, topics\n\n# clean corpus and convert to dfm\nclean_dfm &lt;- preptop(corpus)\n# inspect"
  },
  {
    "objectID": "tools/topictool/topictool.html#unsupervised-lda",
    "href": "tools/topictool/topictool.html#unsupervised-lda",
    "title": "Topic-Model Tool",
    "section": "Unsupervised LDA",
    "text": "Unsupervised LDA\nNow that we have cleaned the data, we can perform the topic modelling. This consists of two steps:\n\nFirst, we perform an unsupervised LDA. We do this to check what topics are in our corpus.\nThen, we perform a supervised LDA (based on the results of the unsupervised LDA) to identify meaningful topics in our data. For the supervised LDA, we define so-called seed terms that help in generating coherent topics.\n\n\n\n\nIn the code below, the default is to look for 5 topics but you will need to vary and adapt the number of topics (k) for your data and check what topics are in your data. \n\n\n\n\n\n# generate model: change k to different numbers, e.g. 10 or 15 and look for consistencies in the keywords for the topics below.\ntmlda &lt;- topicmodels::LDA(clean_dfm, k = 5, control = list(seed = 1234))\n\nNow that the topic model has been generated, we can inspect which terms correlate most strongly with each topic. The code chunk below will display the top 10 terms for each topic.\n\n# load function that tabulates top terms of topics\nsource(\"tools/topictool/rscripts/tabtop.R\")\n# inspect\ntabtop(tmlda, 10)\n\nWarning: Values from `term` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(top, topic)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\n# A tibble: 10 × 6\n   top    `topic 1` `topic 2` `topic 3` `topic 4` `topic 5`\n   &lt;fct&gt;  &lt;list&gt;    &lt;list&gt;    &lt;list&gt;    &lt;list&gt;    &lt;list&gt;   \n 1 top 1  &lt;chr [5]&gt; &lt;chr [6]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n 2 top 2  &lt;chr [5]&gt; &lt;chr [5]&gt; &lt;chr [7]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n 3 top 3  &lt;chr [5]&gt; &lt;chr [5]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [2]&gt;\n 4 top 4  &lt;chr [5]&gt; &lt;chr [5]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n 5 top 5  &lt;chr [5]&gt; &lt;chr [5]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n 6 top 6  &lt;chr [5]&gt; &lt;chr [5]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n 7 top 7  &lt;chr [5]&gt; &lt;chr [5]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n 8 top 8  &lt;chr [5]&gt; &lt;chr [5]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n 9 top 9  &lt;chr [4]&gt; &lt;chr [6]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n10 top 10 &lt;chr [4]&gt; &lt;chr [6]&gt; &lt;chr [6]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt;\n\n\nWe now extract a table with the probabilities of each term to occur in each of the topics.\n\ntermprobs_tmlda &lt;- tidytext::tidy(tmlda, matrix = \"beta\")\n# inspect\nhead(termprobs_tmlda)\n\n# A tibble: 6 × 3\n  topic term             beta\n  &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n1     1 linguistics 1.82e-  2\n2     2 linguistics 1.59e-  2\n3     3 linguistics 1.27e-  2\n4     4 linguistics 2.90e-  2\n5     5 linguistics 1.92e-216\n6     1 scientific  1.98e-223\n\n\n\nExporting data\nTo export a data frame as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory.\n\n# save data for MyOutput folder\nwrite_xlsx(termprobs_tmlda, here::here(\"notebooks/MyOutput/termprobs_tmlda.xlsx\"))\n\n\n\n\nYou will find the generated MS Excel spreadsheet named “termprobs_tmlda.xlsx” in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the “termprobs_tmlda.xlsx” file, and choose Download from the dropdown menu to download the file. \n\n\n\n\n\n\nBefore turning to the supervised LDA topic model, you can extract the probability of topics in each document. For this, we use the gamma statistic\n\ndocprobs_tmlda &lt;- tidy(tmlda, matrix = \"gamma\")\n# inspect\nhead(docprobs_tmlda)\n\n# A tibble: 6 × 3\n  document topic    gamma\n  &lt;chr&gt;    &lt;int&gt;    &lt;dbl&gt;\n1 text1        1 0.000196\n2 text2        1 0.000351\n3 text3        1 0.000245\n4 text4        1 0.999   \n5 text5        1 0.000209\n6 text6        1 0.000165\n\n\n\n\nExporting data\nTo export a data frame as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory.\n\n# save data for MyOutput folder\nwrite_xlsx(docprobs_tmlda, here::here(\"notebooks/MyOutput/docprobs_tmlda.xlsx\"))\n\n\n\n\nYou will find the generated MS Excel spreadsheet named “docprobs_tmlda.xlsx” in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the “docprobs_tmlda.xlsx” file, and choose Download from the dropdown menu to download the file."
  },
  {
    "objectID": "tools/topictool/topictool.html#supervised-lda",
    "href": "tools/topictool/topictool.html#supervised-lda",
    "title": "Topic-Model Tool",
    "section": "Supervised LDA",
    "text": "Supervised LDA\nNow, we perform a supervised LDA. Here we use the keywords extracted based on the unsupervised LDA as seed terms for topics to create coherent topics.\n\n\n\nIMPORTANT: you need to change and adapt the topics and keywords defined below  Simply replace the topics and seed terms with your own topics and seed terms (based on the results of the unsupervised LDA!). \n\n\n\n\n\n# semisupervised LDA\ndict &lt;- dictionary(list(Topic01 = c(\"computer\", \"information\", \"technology\"),\n                        Topic02 = c(\"students\", \"courses\", \"education\"),\n                        Topic03 = c(\"movie\", \"film\", \"watch\"),\n                        Topic04 = c(\"women\", \"feminist\", \"equality\"),\n                        Topic05 = c(\"money\", \"investment\", \"finance\")))\ntmod_slda &lt;- seededlda::textmodel_seededlda(clean_dfm, \n                                            dict, \n                                            residual = TRUE, \n                                            min_termfreq = 2)\n# inspect\ntmod_slda$theta\n\n          Topic01     Topic02     Topic03     Topic04    Topic05      other\ntext1 0.115853659 0.006097561 0.493902439 0.006097561 0.34756098 0.03048780\ntext2 0.053191489 0.010638298 0.351063830 0.308510638 0.26595745 0.01063830\ntext3 0.159090909 0.113636364 0.022727273 0.613636364 0.02272727 0.06818182\ntext4 0.474137931 0.094827586 0.060344828 0.215517241 0.04310345 0.11206897\ntext5 0.409090909 0.058441558 0.006493506 0.006493506 0.27922078 0.24025974\ntext6 0.005154639 0.469072165 0.036082474 0.005154639 0.29381443 0.19072165\ntext7 0.052083333 0.052083333 0.156250000 0.031250000 0.32291667 0.38541667\n\n\nNow, we extract files and create a data frame of topics and documents. This shows what topic is dominant in which file in tabular form.\n\nfiles &lt;- rownames(clean_dfm)\ncleancontent &lt;- corpus\ntopics &lt;- apply(tmod_slda$theta, 1, which.max)\n# generate data frame\ndf &lt;- data.frame(files, cleancontent, topics) %&gt;%\n  dplyr::mutate_if(is.character, factor)\n# inspect\nhead(df)\n\n                  files\nlinguistics01.txt text1\nlinguistics02.txt text2\nlinguistics03.txt text3\nlinguistics04.txt text4\nlinguistics05.txt text5\nlinguistics06.txt text6\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     cleancontent\nlinguistics01.txt                                                                                                                                                                                                                                Linguistics is the scientific study of language. It involves analysing language form language meaning and language in context. The earliest activities in the documentation and description of language have been attributed to the th-century-BC Indian grammarian Pa?ini who wrote a formal description of the Sanskrit language in his A??adhyayi. Linguists traditionally analyse human language by observing an interplay between sound and meaning. Phonetics is the study of speech and non-speech sounds and delves into their acoustic and articulatory properties. The study of language meaning on the other hand deals with how languages encode relations between entities properties and other aspects of the world to convey process and assign meaning as well as manage and resolve ambiguity. While the study of semantics typically concerns itself with truth conditions pragmatics deals with how situational context influences the production of meaning.\nlinguistics02.txt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.\nlinguistics03.txt                                                                                                                                                                                                                                                                                                                                                                                                                                  In the early 20th century, Ferdinand de Saussure distinguished between the notions of langue and parole in his formulation of structural linguistics. According to him, parole is the specific utterance of speech, whereas langue refers to an abstract phenomenon that theoretically defines the principles and system of rules that govern a language. This distinction resembles the one made by Noam Chomsky between competence and performance in his theory of transformative or generative grammar. According to Chomsky, competence is an individual's innate capacity and potential for language (like in Saussure's langue), while performance is the specific way in which it is used by individuals, groups, and communities (i.e., parole, in Saussurean terms).\nlinguistics04.txt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The study of parole (which manifests through cultural discourses and dialects) is the domain of sociolinguistics, the sub-discipline that comprises the study of a complex system of linguistic facets within a certain speech community (governed by its own set of grammatical rules and laws). Discourse analysis further examines the structure of texts and conversations emerging out of a speech community's usage of language. This is done through the collection of linguistic data, or through the formal discipline of corpus linguistics, which takes naturally occurring texts and studies the variation of grammatical and other features based on such corpora (or corpus data).\nlinguistics05.txt                                                                                                                                                                                                                                                                               Stylistics also involves the study of written, signed, or spoken discourse through varying speech communities, genres, and editorial or narrative formats in the mass media. In the 1960s, Jacques Derrida, for instance, further distinguished between speech and writing, by proposing that written language be studied as a linguistic medium of communication in itself. Palaeography is therefore the discipline that studies the evolution of written scripts (as signs and symbols) in language. The formal study of language also led to the growth of fields like psycholinguistics, which explores the representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; biolinguistics, which studies the biology and evolution of language; and language acquisition, which investigates how children and adults acquire the knowledge of one or more languages.\nlinguistics06.txt Linguistics also deals with the social, cultural, historical and political factors that influence language, through which linguistic and language-based context is often determined. Research on language through the sub-branches of historical and evolutionary linguistics also focus on how languages change and grow, particularly over an extended period of time. Language documentation combines anthropological inquiry (into the history and culture of language) with linguistic inquiry, in order to describe languages and their grammars. Lexicography involves the documentation of words that form a vocabulary. Such a documentation of a linguistic vocabulary from a particular language is usually compiled in a dictionary. Computational linguistics is concerned with the statistical or rule-based modeling of natural language from a computational perspective. Specific knowledge of language is applied by speakers during the act of translation and interpretation, as well as in language education � the teaching of a second or foreign language. Policy makers work with governments to implement new plans in education and teaching which are based on linguistic research.\n                  topics\nlinguistics01.txt      3\nlinguistics02.txt      3\nlinguistics03.txt      4\nlinguistics04.txt      1\nlinguistics05.txt      1\nlinguistics06.txt      2"
  },
  {
    "objectID": "tools/topictool/topictool.html#exporting-data-2",
    "href": "tools/topictool/topictool.html#exporting-data-2",
    "title": "Topic-Model Tool",
    "section": "Exporting data",
    "text": "Exporting data\nTo export a data frame as an MS Excel spreadsheet, we use write_xlsx. Be aware that we use the here function to save the file in the current working directory.\n\n# save data for MyOutput folder\nwrite_xlsx(df, here::here(\"notebooks/MyOutput/df.xlsx\"))\n\n\n\n\nYou will find the generated MS Excel spreadsheet named “df.xlsx” in the MyOutput folder (located on the left side of the screen). Simply double-click the MyOutput folder icon, then right-click on the “df.xlsx” file, and choose Download from the dropdown menu to download the file."
  }
]