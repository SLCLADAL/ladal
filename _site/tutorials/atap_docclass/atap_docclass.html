<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Gerold Schneider and Max Lauber">

<title>Classifying American Speeches – Language Technology and Data Analysis Laboratory (LADAL)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script>
    // Wait for the document to be fully loaded
    document.addEventListener("DOMContentLoaded", function () {
        // Create a new div element to hold the dynamic content
        var newContainer = document.createElement("div");
        newContainer.classList.add("custom-banner");  // You can assign a class to the new container for styling

        // You can also add content inside the new container
        newContainer.innerHTML = `
          <div class="banner">
              <img src="/images/ladal_icon_white.png" alt="Icon" class="banner-icon">
              <span class="banner-text">Language Technology and Data Analysis Laboratory</span>
          </div>
      `

        // Select the header element
        var header = document.getElementById("quarto-header");

        // Select the nav element inside the header to place the new container just above it
        var navbar = header.querySelector("nav");

        // Insert the new container before the navbar element
        header.insertBefore(newContainer, navbar);
    });
</script>


<link rel="stylesheet" href="../../css/styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Language Technology and Data Analysis Laboratory (LADAL)</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">HOME</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">ABOUT</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../events.html"> 
<span class="menu-text">EVENTS</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tutorials.html"> 
<span class="menu-text">TUTORIALS</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tools.html"> 
<span class="menu-text">TOOLS</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resources.html"> 
<span class="menu-text">RESOURCES</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../contact.html"> 
<span class="menu-text">CONTACT</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#american-speeches" id="toc-american-speeches" class="nav-link" data-scroll-target="#american-speeches">American Speeches</a>
  <ul class="collapse">
  <li><a href="#preparation-and-session-setup" id="toc-preparation-and-session-setup" class="nav-link" data-scroll-target="#preparation-and-session-setup">Preparation and Session Setup</a></li>
  <li><a href="#from-data-to-useable-objects" id="toc-from-data-to-useable-objects" class="nav-link" data-scroll-target="#from-data-to-useable-objects">From Data to Useable Objects</a></li>
  </ul></li>
  <li><a href="#from-text-to-corpus" id="toc-from-text-to-corpus" class="nav-link" data-scroll-target="#from-text-to-corpus">From Text to Corpus</a></li>
  <li><a href="#pre-processing" id="toc-pre-processing" class="nav-link" data-scroll-target="#pre-processing">Pre-Processing</a>
  <ul class="collapse">
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization">Tokenization</a></li>
  <li><a href="#lowercasing" id="toc-lowercasing" class="nav-link" data-scroll-target="#lowercasing">Lowercasing</a></li>
  <li><a href="#stemming" id="toc-stemming" class="nav-link" data-scroll-target="#stemming">Stemming</a></li>
  <li><a href="#remove-stopwords" id="toc-remove-stopwords" class="nav-link" data-scroll-target="#remove-stopwords">Remove Stopwords</a></li>
  <li><a href="#imperfections" id="toc-imperfections" class="nav-link" data-scroll-target="#imperfections">Imperfections</a></li>
  </ul></li>
  <li><a href="#signal-and-noise" id="toc-signal-and-noise" class="nav-link" data-scroll-target="#signal-and-noise">Signal and Noise</a>
  <ul class="collapse">
  <li><a href="#document-feature-matrix" id="toc-document-feature-matrix" class="nav-link" data-scroll-target="#document-feature-matrix">Document-Feature Matrix</a></li>
  <li><a href="#document-frequency-of-the-features" id="toc-document-frequency-of-the-features" class="nav-link" data-scroll-target="#document-frequency-of-the-features">Document Frequency of the Features</a></li>
  <li><a href="#tfidf" id="toc-tfidf" class="nav-link" data-scroll-target="#tfidf">TFIDF</a></li>
  </ul></li>
  <li><a href="#document-classification" id="toc-document-classification" class="nav-link" data-scroll-target="#document-classification">Document Classification</a>
  <ul class="collapse">
  <li><a href="#the-training-data" id="toc-the-training-data" class="nav-link" data-scroll-target="#the-training-data">The Training Data</a></li>
  <li><a href="#the-testing-data" id="toc-the-testing-data" class="nav-link" data-scroll-target="#the-testing-data">The Testing Data</a></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes">Naive Bayes</a></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a></li>
  </ul></li>
  <li><a href="#accuracy-and-interpretation" id="toc-accuracy-and-interpretation" class="nav-link" data-scroll-target="#accuracy-and-interpretation">Accuracy and Interpretation</a>
  <ul class="collapse">
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix">Confusion Matrix</a></li>
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy">Accuracy</a></li>
  <li><a href="#feature-table" id="toc-feature-table" class="nav-link" data-scroll-target="#feature-table">Feature Table</a></li>
  <li><a href="#weighted-feature-tables" id="toc-weighted-feature-tables" class="nav-link" data-scroll-target="#weighted-feature-tables">Weighted Feature Tables</a></li>
  </ul></li>
  <li><a href="#alternative-models" id="toc-alternative-models" class="nav-link" data-scroll-target="#alternative-models">Alternative Models</a>
  <ul class="collapse">
  <li><a href="#logistic-regression-model" id="toc-logistic-regression-model" class="nav-link" data-scroll-target="#logistic-regression-model">Logistic Regression Model</a></li>
  <li><a href="#differences-explained" id="toc-differences-explained" class="nav-link" data-scroll-target="#differences-explained">Differences Explained</a></li>
  <li><a href="#best-practices" id="toc-best-practices" class="nav-link" data-scroll-target="#best-practices">Best Practices</a></li>
  </ul></li>
  <li><a href="#sampling-and-re-sampling-our-way-to-robustness" id="toc-sampling-and-re-sampling-our-way-to-robustness" class="nav-link" data-scroll-target="#sampling-and-re-sampling-our-way-to-robustness">Sampling and Re-Sampling Our Way to Robustness</a>
  <ul class="collapse">
  <li><a href="#loops" id="toc-loops" class="nav-link" data-scroll-target="#loops">Loops</a></li>
  <li><a href="#demonstrating-robustness" id="toc-demonstrating-robustness" class="nav-link" data-scroll-target="#demonstrating-robustness">Demonstrating Robustness</a></li>
  </ul></li>
  <li><a href="#citation-session-info" id="toc-citation-session-info" class="nav-link" data-scroll-target="#citation-session-info">Citation &amp; Session Info</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Classifying American Speeches</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Gerold Schneider and Max Lauber </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>This tutorial shows how to perform document classification using R. The entire R markdown document for the tutorial can be downloaded <a href="&quot;https://slcladal.github.io/content/atap_docclass.Rmd&quot;">here</a>.</p>
<p>The tutorial requires you to install and load several packages to analyze linguistic data. To help you, we directly include the commands to do so in the script and walk you through it step by step. Let’s get on it!</p>
</section>
<section id="motivation" class="level1">
<h1>Motivation</h1>
<p>Document classification is a useful method of computational linguistics since it provides an effective way of handling large quantities of text. Think for a moment about spam mails. According to an analysis – or self-promotion, if you will – by the anti-virus company Kaspersky, more than half of the global email traffic is spam mail <span class="citation" data-cites="vergelis2019spam">(<a href="#ref-vergelis2019spam" role="doc-biblioref">Vergelis et al. 2019</a>)</span> (<a href="https://securelist.com/spam-and-phishing-in-q1-2019/90795/">see here</a>) . How do email providers manage to sift these quantities of data? They use document classification to separate the important email traffic from spam mail.</p>
<p>Emails are obviously a rather specific category of text, with one distinguishing feature: either you want them in your inbox, or you don’t. Of course, one person’s spam may be the other person’s key to retrieving a lost fortune, but for all intents and purposes, the global mail traffic is binary:</p>
<ol start="0" type="1">
<li><p>you don’t want it = spam</p></li>
<li><p>you want it = mails</p></li>
</ol>
<p>For binary classes of text such as this, document classification comes in really handy. We will point to sources that address how document classification can be used in contexts with more than two categories, but in this tutorial we eschew such complexity.. To further preserve our collective wellbeing, we now turn our backs on spam mail.</p>
</section>
<section id="american-speeches" class="level1">
<h1>American Speeches</h1>
<p>Instead, we dive into another category of text that is notoriously binary: American presidential speeches. Although all candidates claim to peddle policies, what they are giving us instead are words (at least during the campaign trails). And by throwing these words into the meat grinder of document classification, we can identify which linguistic features are typical of which party, and, in a slightly more extensive analysis, we could tell the party hard-liners from the mavericks - but for now, we’ll stick with the binary categories and with linguistic features.</p>
<p>To use the jargon of computational linguistics: we are using supervised document classification for political profiling. Political profiling should not require further explanation, but the document classification we are getting to in a minute is considered supervised because the corpus has been constructed to include only two categories on the basis of pre-existing knowledge.</p>
<p>We extend our thanks to Marco Guerini and his co-authors for the construction of the CORPS II corpus, which contains 8 million words from 3618 speeches by American presidential candidates <span class="citation" data-cites="guerini2010new">(<a href="#ref-guerini2010new" role="doc-biblioref">Guerini et al. 2010</a>)</span>. This CORPS II serves as our input. On the other end, our output will contain both a model that can predict a speaker’s party affiliation with a pleasingly high degree of accuracy and a list of keywords containing the linguistic features most typical for Republican candidates - and by extension, also those words least typical for Republicans, which in this beautifully binary political system means the words most typical of Democratic candidates.</p>
<section id="preparation-and-session-setup" class="level2">
<h2 class="anchored" data-anchor-id="preparation-and-session-setup">Preparation and Session Setup</h2>
<p>As mentioned at the outset, this is an introduction to document classification based on R. A rudimentary familiarity with R and RStudio are helpful for getting the most out of this. If you have yet to install R or are new to it, we can recommend <a href="https://slcladal.github.io/intror.html">this tutorial to R</a>, which walks you through the installation and shows a range of its functionalities. In the following, we assume that you have downloaded and installed both R and RStudio and take it from there.</p>
<p>Although R in its basic version already contains a lot of functionalities, we do need to install some packages for the code below to run without exploding into a bouquet of error messages. If you already have the packages <code>quanteda</code>, <code>quanteda.textmodels</code>, <code>stopwords</code> and <code>ggplot2</code> installed, just skip to the next heading. If you don’t have them yet, the first thing you want to do is run these first lines of code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("quanteda")</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("quanteda.textmodels")</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("stopwords")</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("ggplot2")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This may take a minute or three. In most cases, this will work without any hiccups. If you should get an error message, we recommend taking a moment to read what it says, and, if it does not make any sense to you, to google it. If an issue comes up for you, chances are that this has already happened to someone else - and, fortunately, the R community has a pretty good track record of responding to questions about technical issues. Generally, it is also a good idea to use a relatively new version of R. If you have last used R two years ago, do update it.</p>
<p>Once you have installed the packages, you’ll need to load them in the current session. This is done with the following lines of code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda.textmodels)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stopwords)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the packages loaded, the stage is set. Now we need to dress up the actors.</p>
</section>
<section id="from-data-to-useable-objects" class="level2">
<h2 class="anchored" data-anchor-id="from-data-to-useable-objects">From Data to Useable Objects</h2>
<p>Before we can do anything further, we need to load the data into R. As mentioned earlier, we are working with the CORPS II corpus compiled by <span class="citation" data-cites="guerini2010new">Guerini et al. (<a href="#ref-guerini2010new" role="doc-biblioref">2010</a>)</span>. There are many ways to get the corpus into R, but we are going to download the data from the data directory of the LADAL GitHub repository. By executing the following line of code, you tell R to download the file containing the corpus from github and store it in the session under the variable name <code>rt</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>rt <span class="ot">&lt;-</span> base<span class="sc">::</span><span class="fu">readRDS</span>(<span class="fu">url</span>(<span class="st">"https://ladal.edu.au/data/SEL_perparty_v2.rda"</span>, <span class="st">"rb"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the meantime, we want to draw your attention to the second parameter of the readtext command, namely the specification defining the text field. These specifications are called flags. These are often useful and sometimes necessary. In this case, the flag does exactly what it says: it tells R in which column the text is stored.</p>
<p>Let’s take a look at <code>rt</code>, the variable in which we stored the corpus. We can do this simply by <code>str</code> which returns the structure of the object:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(rt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Classes 'readtext' and 'data.frame':    3247 obs. of  5 variables:
 $ doc_id: chr  "SEL_perparty_v2.csv.1" "SEL_perparty_v2.csv.2" "SEL_perparty_v2.csv.3" "SEL_perparty_v2.csv.4" ...
 $ text  : chr  "We who are Christians usually think about Christ in terms of obligations of charity , and faith in God , and so"| __truncated__ "It is a very great honor and pleasure for me to be here and participate in a dinner this evening that aims at s"| __truncated__ "Thank you . Praise God . Thank you very much . Tonight , I would like you first of all to join me in rememberin"| __truncated__ "Thank you . Thank you . Praise God . Thank you very much . I want to tell you I 'm very happy to have a few mom"| __truncated__ ...
 $ party : chr  "rep" "rep" "rep" "rep" ...
 $ fileid: chr  "akeyes-95" "akeyes-98" "akeyes1-2-00" "akeyes1-6-96" ...
 $ name  : chr  "Alan_Keyes" "Alan_Keyes" "Alan_Keyes" "Alan_Keyes" ...</code></pre>
</div>
</div>
<p>The output tells us what we are dealing with: a object consisting of 3247 documents and 3 document variables. We also see the first six rows of the object, which shows that we are dealing with a two dimensional matrix or a table, which in the R context is also called a data frame. What may be surprising is that, although the object is described as having three document variables, the data frame has five columns.</p>
<p>If we look at the structure of the corpus, we can see that the file contains four columns, labeled <code>party</code>, <code>fileid</code>, <code>name</code> and <code>text</code>, respectively. The object <code>rt</code> to which we assigned the data created an additional column, labeled <code>doc_id</code>. In this variable, the row number of each row in the file is stored. If we were to load in a second document and append it to the <code>rt</code> object, it would be clear which row of data comes from which corpus. So, while it’s great to have this <code>doc_id</code>, the output also makes it clear that it is not meaningful information relevant to the text we are interested in.</p>
<p>The text source files come from the textual component of the files, and the document-level metadata (“docvars”) come from either the file contents or filenames.</p>
<p>So the three document variables are the document-level metadata, which pertain to the texts we are interested in. Depending on the type of analysis or operation you are interested in performing on the data, this is not essential information. For document classification, however, as well as many other analyses, this metadata is essential.</p>
</section>
</section>
<section id="from-text-to-corpus" class="level1">
<h1>From Text to Corpus</h1>
<p>Before we can proceed to the document classification, we need to process the data some more. As a first step, we turn the data frame <code>rt</code> into a corpus object. We can do this with one intuitive command:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>fulltext <span class="ot">&lt;-</span> <span class="fu">corpus</span>(rt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Thereby, we create a new corpus object called <code>fulltext</code>. Let’s take a look at it:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>fulltext</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Corpus consisting of 3,247 documents and 3 docvars.
SEL_perparty_v2.csv.1 :
"We who are Christians usually think about Christ in terms of..."

SEL_perparty_v2.csv.2 :
"It is a very great honor and pleasure for me to be here and ..."

SEL_perparty_v2.csv.3 :
"Thank you . Praise God . Thank you very much . Tonight , I w..."

SEL_perparty_v2.csv.4 :
"Thank you . Thank you . Praise God . Thank you very much . I..."

SEL_perparty_v2.csv.5 :
"Thank you very much , God bless you . Unaccustomed as I am t..."

SEL_perparty_v2.csv.6 :
"Thank you very much . You all make me feel so good with that..."

[ reached max_ndoc ... 3,241 more documents ]</code></pre>
</div>
</div>
<p>The output here contains the first six entries of the corpus, which are now only the text fields, the actual speeches. The metadata is not lost, however, as the first line of the output shows - telling us that the corpus consost of 3,247 documents and 3 document variables. This is precisely as it should be.</p>
<p>If you’re somewhat familiar with R, you may have noticed something slightly off-putting. Usually, when you enter a variable name, the output shows the whole variable. In this case, we only got the first six entries. This is one of the benefits of the corpus object: since corpora are often voluminous and take a surprising amount of computational power to be displayed in the console, there is a safety mechanism built into the corpus object. To prevent an accidental system overload, only the head of the object is displayed.</p>
<p>With this, we have entered the domain of object-oriented programming. We are not going to discuss this at length, but want to briefly introduce the term and the idea behind it. Object-oriented programming is based on the concept of <code>objects</code>, which have considerable internal complexity, and can contain both data and code. As far as the data goes, this is not surprising: we have something like a data frame or a vector that contains variables. This exists in most programming paradigms. What is different in object-oriented programming is the code, which is often called method or behavior. The behavior we encountered just now is that of only displaying the first six entries of the object, and only showing the text fields, which contain the data of interest in corpus objects. This is also where we turn our attention next.</p>
</section>
<section id="pre-processing" class="level1">
<h1>Pre-Processing</h1>
<p>Let’s look again at the output that we got looking at the corpus object <code>fulltext</code>. We see several features there which pose a hindrance to solid document classification. For instance, there is punctuation which will have to go. Similarly, the capital letters, both in the beginning of sentences and in words like <em>Christians</em> will have to be - for lack of a better word - exorcized. The point of removing these and other features is to reduce the variation in the language to such a degree that we can process the texts well statistically, while leaving as much of the semantic content as possible intact. With this in mind, there are several steps which are necessary in terms of pre-processing, and we will take these one at a time.</p>
<section id="tokenization" class="level2">
<h2 class="anchored" data-anchor-id="tokenization">Tokenization</h2>
<p>The first step here is to transform the text in the corpus object into individual tokens. Right now, each text cell in the <code>fulltext</code> object contains one whole presidential campaign speech. By tokenizing the corpus, each text entry in fulltext is transformed into a list of words. In order to do this, we define a new variable, <code>toks</code>, and use the perfectly descriptive command <code>tokens()</code> to tokenize the corpus and assign it to the new variable <code>toks</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>toks <span class="ot">&lt;-</span> <span class="fu">tokens</span>(fulltext, <span class="at">remove_punct =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Sneakily, we also use a flag here to remove punctuation. Let’s look at how this step changed the content of the corpus. Having removed the punctuation, it makes sense to pick a speech which contained punctuation in the beginning, so we choose speech four. We can look at this specific speech by entering the name of the object, <code>toks</code>, and adding the row number of choice in square brackets, thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>toks[<span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokens consisting of 1 document and 3 docvars.
SEL_perparty_v2.csv.4 :
 [1] "Thank"  "you"    "Thank"  "you"    "Praise" "God"    "Thank"  "you"   
 [9] "very"   "much"   "I"      "want"  
[ ... and 2,530 more ]</code></pre>
</div>
</div>
</section>
<section id="lowercasing" class="level2">
<h2 class="anchored" data-anchor-id="lowercasing">Lowercasing</h2>
<p>This next step is fairly obvious: the heading says it, the command says it, and it’s precisely what we are going to do. We’re going to replace all capital letters by their lowercase equivalent in the corpus. This is mainly to reduce the variation that arises from words that stand at the beginning of a sentence. To do it, we use the command <code>tokens_tolower()</code>, as demonstrated here:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>toks <span class="ot">&lt;-</span> <span class="fu">tokens_tolower</span>(toks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The command is self explanatory, but we should add: we define the object <code>toks</code> as input, replace capital letters with lowercase ones and assign the outcome again to the object <code>toks</code>, thus overwriting what was there before. If we now look at row four again, we see the result of this transformation:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>toks[<span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokens consisting of 1 document and 3 docvars.
SEL_perparty_v2.csv.4 :
 [1] "thank"  "you"    "thank"  "you"    "praise" "god"    "thank"  "you"   
 [9] "very"   "much"   "i"      "want"  
[ ... and 2,530 more ]</code></pre>
</div>
</div>
<p>Where before there was <em>Thank</em> and <em>I</em>, we now have <em>thank</em> and <em>i</em>. Not much more to see here, so let’s keep moving.</p>
</section>
<section id="stemming" class="level2">
<h2 class="anchored" data-anchor-id="stemming">Stemming</h2>
<p>This time round, the transformation is a bit more drastic: we are going to reduce each word in the corpus to its word stem. Take a quick look at the first item of the toks object:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>toks[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokens consisting of 1 document and 3 docvars.
SEL_perparty_v2.csv.1 :
 [1] "we"          "who"         "are"         "christians"  "usually"    
 [6] "think"       "about"       "christ"      "in"          "terms"      
[11] "of"          "obligations"
[ ... and 6,212 more ]</code></pre>
</div>
</div>
<p>We have here, among other words, <em>christians</em>, <em>usually</em> and <em>obligations</em>. Keep those in mind as you reduce the corpus to wordstems, again using a fairly intuitive command:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>toks <span class="ot">&lt;-</span> <span class="fu">tokens_wordstem</span>(toks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using the same procedure as with lowercasing, we overwrite the contents of the <code>toks</code> object with its latest transformation. We can see how this plays out by again looking at the first item.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>toks[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokens consisting of 1 document and 3 docvars.
SEL_perparty_v2.csv.1 :
 [1] "we"        "who"       "are"       "christian" "usual"     "think"    
 [7] "about"     "christ"    "in"        "term"      "of"        "oblig"    
[ ... and 6,212 more ]</code></pre>
</div>
</div>
<p>Everything is shortened compared to before: <em>christian</em>, <em>usually</em>, and <em>oblig</em>. Compared to the preceding steps, a bit more informational value is lost, but this is often a trade-off worth taking, seeing as it typically enhances the performance of the document classification quite substantially.</p>
</section>
<section id="remove-stopwords" class="level2">
<h2 class="anchored" data-anchor-id="remove-stopwords">Remove Stopwords</h2>
<p>The final intrusion into the linguistic source material is the removal of stopwords. These are words which are very common, to a degree that one can assume that they will, on average, be equally distributed between the two classes. They are typically also words that have no meaning on their own, for example <em>the</em>, <em>in</em>, etc. As they rather regulate the relations between words, they are also called function words (as opposed to content words, which we want to keep). There is no universal list of stopwords - and some methods make do without removing stopwords - but for our purposes, it makes sense to just work with a list that is readily available, which is the one specified in the flag. The command is as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>toks <span class="ot">&lt;-</span> <span class="fu">tokens_remove</span>(toks, <span class="at">pattern =</span> stopwords<span class="sc">::</span><span class="fu">stopwords</span>(<span class="at">language =</span> <span class="st">"en"</span>, <span class="at">source =</span> <span class="st">"snowball"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Again, the command is very explicit in its function, the input is the tokenized, punctuation-less, lowercased and stemmed version of the toks object, which we overwrite to create a version that is all of the above and additionally does not contain stopwords. We see in row one how that looks:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>toks[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokens consisting of 1 document and 3 docvars.
SEL_perparty_v2.csv.1 :
 [1] "christian" "usual"     "think"     "christ"    "term"      "oblig"    
 [7] "chariti"   "faith"     "god"       "forth"     "don"       "t"        
[ ... and 2,751 more ]</code></pre>
</div>
</div>
<p>This looks quite a bit different: a lot of words are gone, such as <em>we</em>, <em>who</em> and <em>are</em>. For the most part, what remains are the stems of nouns and verbs, words which carry semantic meaning.</p>
</section>
<section id="imperfections" class="level2">
<h2 class="anchored" data-anchor-id="imperfections">Imperfections</h2>
<p>Clearly, this approach to pre-processing is not without flaws. We still see, for instance, in the last two positions of the output, that <em>don’t</em> was split into two tokens, <em>don</em> and <em>t</em>. <em>Don</em> would probably also appear as the appropriate word for various garments a lot in a Victorian fashion weekly, which is probably not what Alan Keyes was referring to in his presidential campaign speech, while <em>not</em> should probably be part of the stopword list. Or, to return to the purged stopwards mentioned above, <em>who</em> could have been accidentally removed: once the World Health Organization’s abbreviation is lowercased, it is indistinguishable from the pronoun. So you can see that there are some pitfalls to pre-processing, which are usually quite harmless compared to the benefits (and improved model). However, it can lead to problematic omissions in certain contexts. Ultimately, you need to be aware of the linguistic context you’re working in and make the trade-off on whether finer-grained pre-processing is worth the effort.</p>
</section>
</section>
<section id="signal-and-noise" class="level1">
<h1>Signal and Noise</h1>
<section id="document-feature-matrix" class="level2">
<h2 class="anchored" data-anchor-id="document-feature-matrix">Document-Feature Matrix</h2>
<p>With the linguistic pre-processing done, we continue to the next step: creating a document-feature matrix – simply a table which says how often which word occurs in each document. This is the format we need to run the document classification. We’ll get into its structure in a second. But first, we turn the pre-processed corpus into a document-feature matrix. Once more, the command, <code>dfm</code>, is fairly self-explanatory:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> <span class="fu">dfm</span>(toks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This time round, we create a new object with the name <code>dtm</code>, and the input to this operation is the latest version of the <code>toks</code> object. In case you are not familiar with this particular type of matrix, you might get a better sense for what we are dealing with is by looking at the object directly:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>dtm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 3,247 documents, 31,939 features (98.41% sparse) and 3 docvars.
                       features
docs                    christian usual think christ term oblig chariti faith
  SEL_perparty_v2.csv.1        17     3    28     12    4     1       1    22
  SEL_perparty_v2.csv.2         1     1    14      3    0     1       0     4
  SEL_perparty_v2.csv.3         0     0    12      0    0     0       0     7
  SEL_perparty_v2.csv.4         1     0    15      0    0     0       0     0
  SEL_perparty_v2.csv.5         6     1    16     11    1     0       0     3
  SEL_perparty_v2.csv.6         3     0    26     10    4     3       0     7
                       features
docs                    god forth
  SEL_perparty_v2.csv.1  82     1
  SEL_perparty_v2.csv.2  21     2
  SEL_perparty_v2.csv.3   8     0
  SEL_perparty_v2.csv.4   4     1
  SEL_perparty_v2.csv.5  48     5
  SEL_perparty_v2.csv.6   6     5
[ reached max_ndoc ... 3,241 more documents, reached max_nfeat ... 31,929 more features ]</code></pre>
</div>
</div>
<p>At first glance, you can see that our variable <code>dtm</code> is an object similar to the earlier <code>fulltext</code>, in that only the first six rows of the matrix are pasted in the output, and that there is a summary of the object. And now we can also see the structure of this document-feature matrix. Turns out, the name is quite the give-away: the document feature matrix allows us to see how often each feature appears in each document.</p>
<p>In this matrix, we can access both the columns and the rows, the way we usually do with data frames in R. To do this, we append a square bracket to the object. We can access rows thusly:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>dtm[<span class="dv">3</span>, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 1 document, 31,939 features (98.61% sparse) and 3 docvars.
                       features
docs                    christian usual think christ term oblig chariti faith
  SEL_perparty_v2.csv.3         0     0    12      0    0     0       0     7
                       features
docs                    god forth
  SEL_perparty_v2.csv.3   8     0
[ reached max_nfeat ... 31,929 more features ]</code></pre>
</div>
</div>
<p>Here, we see that <em>think</em> features twelve times in the third speech in the corpus. Or, accounting for what the pre-processing did to the text, we can say that some version of <em>think</em> appears in this speech.</p>
<p>Accessing the columns works analogously: we simply place the number after the comma:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>dtm[, <span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 3,247 documents, 1 feature (19.00% sparse) and 3 docvars.
                       features
docs                    think
  SEL_perparty_v2.csv.1    28
  SEL_perparty_v2.csv.2    14
  SEL_perparty_v2.csv.3    12
  SEL_perparty_v2.csv.4    15
  SEL_perparty_v2.csv.5    16
  SEL_perparty_v2.csv.6    26
[ reached max_ndoc ... 3,241 more documents ]</code></pre>
</div>
</div>
<p>We see here that <em>think</em> is a frequent feature, at least in the first six speeches in the corpus. By looking at the metadata, or, as we call them here, the document variables, we can see that the first six speeches were all given by the same speaker. To access the document variables and solve the riveting riddle of who this famous thinker is, we can use another one of these intuitively named commands:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">docvars</span>(dtm))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  party       fileid       name
1   rep    akeyes-95 Alan_Keyes
2   rep    akeyes-98 Alan_Keyes
3   rep akeyes1-2-00 Alan_Keyes
4   rep akeyes1-6-96 Alan_Keyes
5   rep akeyes1-6-98 Alan_Keyes
6   rep  akeyes10-96 Alan_Keyes</code></pre>
</div>
</div>
<p>With the <code>docvars</code> command, we get a list of the document variables for each document, that is each row, in the <code>dtm</code> object. However, we nest this inside of the command <code>head()</code> - which is useful for taking a first look at any variable, since it only displays the first six rows of a variable. We do this here because the docvars are not subject to the same behavior protocol that the corpus object is. Your computer is unlikely to overload if you were to accidentally view the whole docvars without taking this <code>head</code> precaution. So let’s try it:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">docvars</span>(dtm)[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   party         fileid       name
1    rep      akeyes-95 Alan_Keyes
2    rep      akeyes-98 Alan_Keyes
3    rep   akeyes1-2-00 Alan_Keyes
4    rep   akeyes1-6-96 Alan_Keyes
5    rep   akeyes1-6-98 Alan_Keyes
6    rep    akeyes10-96 Alan_Keyes
7    rep  akeyes11-5-00 Alan_Keyes
8    rep  akeyes11-9-95 Alan_Keyes
9    rep akeyes12-10-96 Alan_Keyes
10   rep  akeyes12-5-00 Alan_Keyes</code></pre>
</div>
</div>
<p>Even though it becomes clear enough which document variable stands in which column, it requires a lot of scrolling to get to the top where the precise name of each document variable is pasted at the top of each column. The very top of the list is also where we find the answer to the mystery of who uses <em>think</em> so gratuitously. If the name Alan Keyes does not ring a bell, you are in the fortunate position to only learn today of a person whose claims to fame include having run for president three times, being appointed ambassador to the UN’s Economic and Social Council by Ronald Reagan and filing a lawsuit against Barack Obama - requesting documentation proving that Barack Obama is a natural born citizen of the US. The things you learn in computational linguistics never cease to amaze, eh?</p>
</section>
<section id="document-frequency-of-the-features" class="level2">
<h2 class="anchored" data-anchor-id="document-frequency-of-the-features">Document Frequency of the Features</h2>
<p>The next step in our journey toward document classification requires us to count how often each feature appears in the corpus, for reasons which will become apparent shortly. Our trusty friend, the Quanteda package, anticipates this need and furnishes us with the appropriate command, named <code>docfreq</code>. We use it to create a new variable, <code>doc_freq</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>doc_freq <span class="ot">&lt;-</span> <span class="fu">docfreq</span>(dtm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>docfreq</code> command takes our pre-processed corpus as an argument, in the form of the <code>dtm</code> variable. The new variable, <code>doc_freq</code> is now a long list of all the features and their frequencies. We can confirm this by looking at the first twenty entries, i.e.&nbsp;words:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(doc_freq, <span class="at">n =</span> <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  christian       usual       think      christ        term       oblig 
        143         278        2630          30         954         559 
    chariti       faith         god       forth         don           t 
        158         827        1739         294        2341        2831 
      alway       jesus       great      wisdom citizenship  understood 
       1825          32        2812         285         137         277 
       said        fact 
       2572        1558 </code></pre>
</div>
</div>
<p>The output shows us each feature and the number of occurrences. It also shows us here that the features are not ranked by frequency yet, but by order of appearance in the corpus. The first few features are the ones we encounter in the first speech in the corpus.</p>
<p>To see which features occur most and least frequently in the corpus, we can sort the variable <code>doc_freq</code>. To do this, we use the intuitively named <code>sort()</code> function. In order to save ourselves the trouble of scrolling, we nest the <code>sort()</code> function inside the head function, which then looks thusly:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(doc_freq, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       s    thank    peopl      can       us      one     want     year 
    3165     3121     3091     3076     3041     3040     3030     3009 
 countri     time     make     know     work      now american 
    3009     2978     2977     2975     2955     2925     2908 </code></pre>
</div>
</div>
<p>The outermost layer of the line is the <code>head()</code> command. Inside of this, we nest the <code>sort()</code> function, which takes as an argument the variable of interest, <code>doc_freq</code> and the logical argument <code>decreasing = TRUE</code>. After closing the brackets on the sort function, we add the specification that we want to see the first fifteen entries of the head.</p>
<p>In the output, we see many features that we would expect to appear in presidential campaign speeches. The contracted <em>’s</em> is the most common feature, which makes sense considering that it can be used both in the sense of <em>let’s build a better future together</em> or as a possessive. Contraction are also generally used frequently in spoken language. We are clearly reminded that we are dealing with a corpus of political speeches, in because many words are associated with shaping something: <em>can</em>, <em>want</em>, <em>make</em>, <em>work</em> indicating the activities and <em>countri</em> pointing to the object to be shaped. There are also flavors of urgency coming through, with <em>year</em>, <em>time</em> and <em>now</em> referring to the temporality of the campaign. Finally, we are reminded that we are dealing with <em>American</em> politicians. So, we have a pretty good interpretive turn-out already here.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>For some insightful contrast, let’s look at the last fifteen features in this ranking. To do this, we use almost exactly the same command as before, but instead of nesting everything under <code>head()</code>, we do it under the <code>tail()</code> function, like so:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(<span class="fu">sort</span>(doc_freq, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           umphf           brasim t-worry-be-happi        5-percent 
               1                1                1                1 
africa-caribbean              koz            4x400         meteorit 
               1                1                1                1 
     micro-organ        somebody-            nowak          pantoja 
               1                1                1                1 
          aspira          rosello        hillcrest 
               1                1                1 </code></pre>
</div>
</div>
<p>It would be possible to go and look at the contexts of these features, but in the interest of your sanity and ours, we graciously skip this step.</p>
<p>Instead, we return the earlier promise and explain why we created the <code>doc_freq</code> variable. As we can still see in the output, the features in the tail are on the one hand extremely rare, and on the other hand not very useful. The latter reason would not be a criteria for exclusion, but the fact that they are extremely rare could muddle up the document classification model we are interested in, the model could learn corpus coincidences, which are often referred to as <em>noise</em>, thus blurring the <em>signal</em> of frequent words. So, we remove all features from the corpus which occur less frequently than five times. We do this using the following command:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> dtm[, doc_freq <span class="sc">&gt;=</span> <span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you can see, we are once again overwriting the <code>dtm</code> variable with a specific iteration of the variable. This time, we choose to retain all the features which occur more than five times in the corpus, with the result that the features which occur less frequently drop out. Let’s see how well this worked, using the same set of commands as above. First, we overwrite the <code>doc_freq</code> variable with the document frequencies of the latest version, then we check out the <code>tail</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>doc_freq <span class="ot">&lt;-</span> <span class="fu">docfreq</span>(dtm)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(<span class="fu">sort</span>(doc_freq, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     salina        soto   job-train  oldfashion    kennelli   underwood 
          5           5           5           5           5           5 
    telecom        huey   gutenberg      hilari kaleidoscop  multi-raci 
          5           5           5           5           5           5 
  sweepstak      bifida        1910 
          5           5           5 </code></pre>
</div>
</div>
<p>We now find a new set of random seeming words at the tail of the <code>doc_freq</code> variable, but at least they all occur at least five times, which means they won’t distort the model too badly. And just to make sure we didn’t accidentally mess up the frequent features, we can again take a look at the <code>head</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(doc_freq, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       s    thank    peopl      can       us      one     want     year 
    3165     3121     3091     3076     3041     3040     3030     3009 
 countri     time     make     know     work      now american 
    3009     2978     2977     2975     2955     2925     2908 </code></pre>
</div>
</div>
<p>Only to find, lo and behold, that they are identical to what we had above. Awesome! But it makes sense to check if our data develops in the expected way. Such checks, also called sanity checking, detect early on if one is on the right track – instead of getting error messages in one’s last processing step. On to a final step of preparation.</p>
</section>
<section id="tfidf" class="level2">
<h2 class="anchored" data-anchor-id="tfidf">TFIDF</h2>
<p>Before we continue to the actual document classification (never fear: we are actually going to get there), we need to weight the features in the corpus object <code>dtm</code> by their tfidf. This beauty of an abbreviation stands for <code>term frequency-inverse document frequency</code>, and does exactly what it says: the number of occurrences of the word in the current document is divided by the number of documents in which the feature occurs. By this division, we account for the fact that some words are generally very frequent, and thus would receive an overproporational amount of significance despite being rather pedestrian and carrying little specific semantic content if we relied on frequency alone. An example we’ve already discussed above is the contracted <em>’s</em>, which is the most frequent feature in the corpus, on account of just being a very common word in the English language.</p>
<p>Before we apply the tfidf weights, let’s load the first document in the <code>dtm</code> object:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>dtm[<span class="dv">1</span>, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.
                       features
docs                    christian usual think christ term oblig chariti faith
  SEL_perparty_v2.csv.1        17     3    28     12    4     1       1    22
                       features
docs                    god forth
  SEL_perparty_v2.csv.1  82     1
[ reached max_nfeat ... 11,770 more features ]</code></pre>
</div>
</div>
<p>This will save us a lot of scrolling time when we compare the pre- and post-tfidf weights.</p>
<p>Once again, quanteda actually furnishes us with a command to easily transform our <code>dtm</code> variable to adapt to the tfidf weights. The command is self-explanatorily labeled <code>dfm_tfidf()</code>, and we use it to overwrite the current version of the <code>dtm</code> variable:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> <span class="fu">dfm_tfidf</span>(dtm, <span class="at">force =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can see that the command takes as its argument first the object to be transformed. The second argument, the flag force=TRUE is usually not needed. As a protection to makes sure that one does not tfidf-weight twice, the dfm_tfidf function refuses to accept matrices that are already weighted. Let’s take a look at the first entry of the <code>dtm</code> object now, we can see that the weights of the features have changed substantially:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>dtm[<span class="dv">1</span>, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.
                       features
docs                    christian    usual    think   christ     term     oblig
  SEL_perparty_v2.csv.1  23.05449 3.202312 2.562743 24.41233 2.127736 0.7640705
                       features
docs                     chariti    faith      god    forth
  SEL_perparty_v2.csv.1 1.312825 13.06749 22.23698 1.043135
[ reached max_nfeat ... 11,770 more features ]</code></pre>
</div>
</div>
<p>Where before we had a simple count of occurrences, we now have the tfidf scores. Some of the salient changes can be seen in the features <em>christian</em> and <em>think</em>. In terms of raw counts, Alan Keyes’ speech contains <em>christian</em> 17 times. With the tfidf weights, <em>christian</em> actually becomes more important, with a weight of 23.06. This indicates that <em>christian</em> is used comparatively rarely in other speeches, making Keyes’ usage here more impactful. By contrast, although <em>think</em> occurs 28 times in Keyes’ speech, it only receives a tfidf score of 2.56, indicating that it also features heavily in many other speeches in the corpus. Turns out, Keyes is not a lone thinker among these politicians. What a shocker.</p>
<p>Let’s stick with the feature <em>think</em> for a second, and look at its tfidf score in the first few speeches. Remember that, since <code>dtm</code> is an object containing data and a behavior, we can simply enter the third column of <code>dtm</code> and receive six manageable lines of output:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>dtm[, <span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 3,247 documents, 1 feature (19.00% sparse) and 3 docvars.
                       features
docs                       think
  SEL_perparty_v2.csv.1 2.562743
  SEL_perparty_v2.csv.2 1.281372
  SEL_perparty_v2.csv.3 1.098318
  SEL_perparty_v2.csv.4 1.372898
  SEL_perparty_v2.csv.5 1.464425
  SEL_perparty_v2.csv.6 2.379690
[ reached max_ndoc ... 3,241 more documents ]</code></pre>
</div>
</div>
<p>We see that <em>think</em> generally has a relatively low score, but the scores differ from speech to speech. The reason for this is that each speech contains a different number of <em>think</em> features, but quite a high number of speeches contain a relatively high number. Thus, the count in each of these first six speeches is divided by the total number of speeches containing <em>think</em>, yielding a different but consistently rather low tfidf weight in each row.</p>
<p>As an aside, tfidf can be used directly to see which words are most characteristic of a document, i.e.&nbsp;its keywords. We could see the keywords of a document, for example document 1, by considering the words with the highest tfidf values. The simplest way to do so is to sort the tfidf values in decreasing order, from most to least frequent, and the inspect the top of the list.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>sdtm_tfidf <span class="ot">&lt;-</span> <span class="fu">dfm_sort</span>(dtm[<span class="dv">1</span>, ], <span class="at">decreasing =</span> T, <span class="at">margin =</span> <span class="st">"features"</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(sdtm_tfidf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 1 document, 11,780 features (94.03% sparse) and 3 docvars.
                       features
docs                      caesar   christ christian      god    roman    fetus
  SEL_perparty_v2.csv.1 35.48031 24.41233  23.05449 22.23698 18.86858 18.66469
                       features
docs                    citizenship     coin     imag    faith
  SEL_perparty_v2.csv.1     17.8719 15.84003 15.52798 13.06749
[ reached max_nfeat ... 11,770 more features ]</code></pre>
</div>
</div>
</section>
</section>
<section id="document-classification" class="level1">
<h1>Document Classification</h1>
<p>Finally, after a substantial amount of fine-tuning and preparing, we get to the juicy bits: the document classification proper.</p>
<section id="the-training-data" class="level2">
<h2 class="anchored" data-anchor-id="the-training-data">The Training Data</h2>
<p>In broad brushstrokes, it works the way it usually does: if you want to generate an output, you need an input. This is what we spent the last ten pages setting up. From the removal of punctuation, via lowercasing and the removal of the features with less than five occurrences to the weighting using the tfidf score, each step is a little boost that helps our model achieve better results. The final (promise) step before creating the actual model is choosing the input.</p>
<p>The situation, of course, is as follows: we have a corpus with 3,247 speeches, all of which have already been classified as being either Republican or Democrat. If we were to simply throw the full corpus into the model, we would run into a problem: we couldn’t test how good the model is generally, because every scrap of data available to us right now would be informing the model. Our results would not be robust. Instead, what we want to do is sample a large chunk of the speeches available to us, use that to train the model, and then test how good our model is on the remaining speeches.</p>
<p>Once again, quanteda holds ready for us a command with the descriptive name <code>dfm_sample()</code>. We use it as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>train_dtm <span class="ot">&lt;-</span> <span class="fu">dfm_sample</span>(dtm, <span class="at">size =</span> <span class="dv">2800</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Or, to put it into words: we create a new object called <code>train_dtm</code>, since we are going to train the model with this. The <code>train_dtm</code> object contains a sample of 2,800 speeches from the pre-processed and weighted <code>dtm</code> corpus. For this tutorial, we have just taken a random sample with a size corresponding to roughly 85% of the corpus. There are no hard and fast rules as to which percentage of the data should be used for training and which for testing, but if you’re somewhere in the 90% ballpark for training data you should be doing fine.</p>
<p>Let’s take a superficial look at our training data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>train_dtm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 2,800 documents, 11,780 features (95.78% sparse) and 3 docvars.
                          features
docs                       christian    usual      think christ      term oblig
  SEL_perparty_v2.csv.1936  0        0        0.64068578      0 0             0
  SEL_perparty_v2.csv.2384  0        0        0.27457962      0 0             0
  SEL_perparty_v2.csv.3232  1.356146 0        2.28816350      0 0             0
  SEL_perparty_v2.csv.1058  0        0        0.18305308      0 0.5319339     0
  SEL_perparty_v2.csv.971   0        1.067437 0.09152654      0 0             0
  SEL_perparty_v2.csv.3120  0        0        0.54915924      0 1.5958017     0
                          features
docs                       chariti     faith       god forth
  SEL_perparty_v2.csv.1936       0 1.1879536 0             0
  SEL_perparty_v2.csv.2384       0 0         0.2711827     0
  SEL_perparty_v2.csv.3232       0 6.5337446 0.5423654     0
  SEL_perparty_v2.csv.1058       0 0         0.2711827     0
  SEL_perparty_v2.csv.971        0 0.5939768 0.5423654     0
  SEL_perparty_v2.csv.3120       0 0         0.8135481     0
[ reached max_ndoc ... 2,794 more documents, reached max_nfeat ... 11,770 more features ]</code></pre>
</div>
</div>
<p>We see in the first row of the output that the object is a document-feature matrix consisting of 2,800 documents, 11,785 features and 3 document variables. We also see in the first column that the speeches included do not form an interpretable sequence, indicating that they are random. All is as it should be.</p>
</section>
<section id="the-testing-data" class="level2">
<h2 class="anchored" data-anchor-id="the-testing-data">The Testing Data</h2>
<p>Now, let’s create the necessary counterpart, the testing data. We create a new variable, which we’ll call <code>test_dtm</code>, and which contains those speeches which are not part of the training data. The way to do this is by taking the <code>dtm</code> object and choosing those documents which constitute the difference between the full <code>dtm</code> corpus and the <code>train_dtm</code> object. The appropriate function for this is <code>setdiff()</code>, which takes as arguments the documents which constitute the full <code>dtm</code> corpus and removes the documents named in the <code>train_dtm</code> variable. The function setdiff is actually used here as an index of the dtm. Indices cannot only contain numbers, but also variables and entire functions – Python programmers refer to this as list comprehension. Observe that dtm is a two-dimensional object. We constrain the first dimension (the rows, i.e.&nbsp;the documents) to those that are not part of the training set, but the second dimension (the column, i.e.&nbsp;the word features) is just kept. This explains the last letters of the following code, the comma followed by the square bracket.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>test_dtm <span class="ot">&lt;-</span> dtm[<span class="fu">setdiff</span>(<span class="fu">docnames</span>(dtm), <span class="fu">docnames</span>(train_dtm)), ]</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>test_dtm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 447 documents, 11,780 features (95.77% sparse) and 3 docvars.
                        features
docs                     christian    usual     think christ     term     oblig
  SEL_perparty_v2.csv.7   1.356146 0        2.7457962      0 3.191603 0        
  SEL_perparty_v2.csv.10  0        0        2.4712166      0 0        0.7640705
  SEL_perparty_v2.csv.20  0        0        1.0983185      0 2.659670 3.8203524
  SEL_perparty_v2.csv.29  0        0        1.4644246      0 3.723537 0        
  SEL_perparty_v2.csv.34  0        1.067437 0.8237389      0 1.063868 0        
  SEL_perparty_v2.csv.46  0        0        3.9356412      0 1.595802 3.0562819
                        features
docs                      chariti     faith      god    forth
  SEL_perparty_v2.csv.7  0        0         1.898279 3.129405
  SEL_perparty_v2.csv.10 1.312825 0         1.627096 5.215675
  SEL_perparty_v2.csv.20 0        0.5939768 2.169462 3.129405
  SEL_perparty_v2.csv.29 0        0.5939768 1.084731 4.172540
  SEL_perparty_v2.csv.34 0        0         3.525375 1.043135
  SEL_perparty_v2.csv.46 0        0.5939768 3.254192 4.172540
[ reached max_ndoc ... 441 more documents, reached max_nfeat ... 11,770 more features ]</code></pre>
</div>
</div>
<p>Looking at the new <code>test_dtm</code> object, we see that it contains the 447 speeches not included in the training data, but with the same number of features and document variables. We can also see that, this time round, the object is not a random sample: the documents included are ordered numerically. In other words: all is as it should be. And with that, onwards.</p>
</section>
<section id="naive-bayes" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes">Naive Bayes</h2>
<p>The first document classification model we are going to train is of a type called Naive Bayes. We will not get into the statistical theory behind Naive Bayes too much, only pointing out the distinctive feature of this model. The name derives from the fact that the model applies Bayes’ theorem with a strong, i.e.&nbsp;naive, independence assumption. Basically, the model assumes that the class of any feature is independent of the class of every other feature in the set. While this is pretty simplistic, or even naive, it works well enough in practice. Let’s have a look.</p>
<p>Training the model is simple enough, requiring just a single command, aptly titled <code>textmodel_nb()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>nb_model <span class="ot">&lt;-</span> <span class="fu">textmodel_nb</span>(train_dtm, <span class="at">y =</span> <span class="fu">docvars</span>(train_dtm, <span class="st">"party"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You see that we once again created a new variable, this time one called <code>nb_model</code>. We do so with the aforementioned <code>textmodel_nb</code> command, which takes as first argument, or input, the <code>train_dtm</code> object which we sampled earlier. The second argument might be a bit more confusing.</p>
<p>One way to think about it is this: we have a two dimensional object. On the x-axis, we have the training data. On the y-axis we have the variable we want to predict, in this case the party of the speaker. With Naive Bayes, training the model is basically a question of one simple calculation: given that we see a feature on the x-axis, what is the probability that the corresponding position on the y-axis is Republican. Each feature is thus assigned a probability between 0 and 1, meaning it is associated either more with the Democratic or more with the Republican party. The dividing line for the result suggested is 0.5. Simple as that, one might think. However, looking at the output of the model, you may feel a bit underwhelmed:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>nb_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
textmodel_nb.dfm(x = train_dtm, y = docvars(train_dtm, "party"))

 Distribution: multinomial ; priors: 0.5 0.5 ; smoothing value: 1 ; 2800 training documents;  fitted features. </code></pre>
</div>
</div>
<p>We get some information regarding the model’s distribution, priors, smoothing value, training documents and fitted features, but nothing about how well it performs.</p>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>In order to see how good our model is, we draw on the data we set aside earlier, the data for testing. In this step, we use the <code>nb_model</code> we trained a minute ago to predict the party to which the politicians delivering the speeches in the test data belong. The command for this is the <code>predict</code> function, which works as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>pred_nb <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb_model, <span class="at">newdata =</span> test_dtm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first argument is the model, on the basis of which the prediction is to be made. In this instance, we are obviously using our Naive Bayes model, stored in the <code>nb_model</code> object. The second argument is where we give the new data, which we set aside earlier in the <code>test_dtm</code> variable. With that we define a new variable, <code>pred_nb</code> which contains the predictions our model generated. Let’s have a look at the first twenty positions:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pred_nb, <span class="at">n =</span> <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  SEL_perparty_v2.csv.7  SEL_perparty_v2.csv.10  SEL_perparty_v2.csv.20 
                    rep                     rep                     rep 
 SEL_perparty_v2.csv.29  SEL_perparty_v2.csv.34  SEL_perparty_v2.csv.46 
                    rep                     rep                     rep 
 SEL_perparty_v2.csv.52  SEL_perparty_v2.csv.64  SEL_perparty_v2.csv.88 
                    dem                     dem                     dem 
 SEL_perparty_v2.csv.90  SEL_perparty_v2.csv.95 SEL_perparty_v2.csv.110 
                    dem                     dem                     rep 
SEL_perparty_v2.csv.113 SEL_perparty_v2.csv.124 SEL_perparty_v2.csv.131 
                    dem                     dem                     dem 
SEL_perparty_v2.csv.136 SEL_perparty_v2.csv.153 SEL_perparty_v2.csv.158 
                    dem                     dem                     dem 
SEL_perparty_v2.csv.159 SEL_perparty_v2.csv.163 
                    dem                     dem 
Levels: dem rep</code></pre>
</div>
</div>
<p>What we get in the output here is exactly what we asked for: the document title and the corresponding prediction. This is great, insofar that we can essentially check the box on <code>classify documents</code>. But, and this is where we’re really getting into the thick of it: how can we figure out how well our model performs?</p>
</section>
</section>
<section id="accuracy-and-interpretation" class="level1">
<h1>Accuracy and Interpretation</h1>
<p>you thought this was going to be straightforward: we extend our sincere apologies. But we’re not done yet. Now that we have the predictions for the party associated with every speech in the test data, we can check how well the prediction performed. How? Well, we kept the test data aside because it is already labeled, which means that we can compare the the prediction to the actual (i.e.&nbsp;correct) label.</p>
<section id="confusion-matrix" class="level2">
<h2 class="anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h2>
<p>The first step for this is constructing the so-called confusion matrix. The confusion matrix is a two-dimensional table which shows the actual party associated with each speech on one axis, and the predictions on the other. We can construct this using the <code>table()</code> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>confmat_nb <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">prediction =</span> pred_nb, <span class="at">party =</span> <span class="fu">docvars</span>(test_dtm, <span class="st">"party"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Creating a new object called <code>confmat_nb</code>, our confusion matrix for the Naive Bayes model, we assign to it a table. The <code>table()</code> function takes as its arguments first the content of the rows, here the values we predicted for the <code>pred_nb</code> variable. Then it takes the content of the columns, here the document variable <code>party</code> from the test data. As you will see as soon as you look at the <code>confmat_nb</code> object, the labels <code>prediction</code> and <code>party</code> to which we assign the data are exactly that: the labels for the vertical and the horizontal axis. Let’s take a look:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>confmat_nb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          party
prediction dem rep
       dem 245  20
       rep  14 168</code></pre>
</div>
</div>
<p>If everything worked out alright, you should see two pretty high numbers in the top left and the bottom right. This is the diagonal on which the actual label and the prediction correspond. As I’m running this, I have 232 speeches by Democrats that were correctly predicted to be by Democrats, and I have 187 speeches by Republicans that were correctly predicted to be by Republicans (due to the fact that we used a random sample, your numbers may be a bit different).</p>
<p>On the other diagonal, from bottom left to top right, we have the wrong predictions. In my case here, that is 10 Democratic speeches that were classified as Republican and 18 Republican speeches that were classified as Democratic. With this representation of correct and incorrect predictions, we can already see that the model isn’t too bad. But to get an even better sense for how well it does, let’s put a number on it.</p>
</section>
<section id="accuracy" class="level2">
<h2 class="anchored" data-anchor-id="accuracy">Accuracy</h2>
<p>Getting from the confusion matrix to a numeric representation of the accuracy is pretty straightforward. First, we take the sum of all correct predictions, then we divide it through the total sum of predictions. One line of code to save this percentage value, then we look at it, easy-peasy:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>accuracy_nb <span class="ot">&lt;-</span> (confmat_nb[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> confmat_nb[<span class="dv">2</span>, <span class="dv">2</span>]) <span class="sc">/</span> <span class="fu">sum</span>(confmat_nb)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>accuracy_nb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9239374</code></pre>
</div>
</div>
<p>In my case, I get an accuracy of 94%, which is not at all shabby. You will have received a slightly different number, which is due to the fact that you have, in all conceivable likelihood, sampled a different set of speeches to train your model. More on which in a minute. But first, let’s dive a bit deeper into the Naive Bayes model and take a look at the features which are representative for each party.</p>
</section>
<section id="feature-table" class="level2">
<h2 class="anchored" data-anchor-id="feature-table">Feature Table</h2>
<p>Let’s start with an obvious one: God. At least, it’s an obvious feature in the American political context. One party is known for heavily pandering to their god-fearing subjects, the other is known for mildly pandering to their more or less god-fearing subjects. You get the idea. Let’s see, then, how God features in our model as a predictor for the two parties. We can see that by accessing the parameters of the <code>nb_model</code> and specifying the parameter <code>god</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>nb_model<span class="sc">$</span>param[, <span class="st">"god"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         dem          rep 
0.0003779813 0.0007297060 </code></pre>
</div>
</div>
<p>Again, there will be slight differences in the output on account of us using different samples, but your output should also indicate that the feature <code>god</code> has roughly twice the weight for Republicans than it does for Democrats. With this example, we are obviously being facetious, but we think it’s useful to see</p>
<ol type="a">
<li><p>what the quote-unquote <em>insides</em> of the model look like, and</p></li>
<li><p>how the same feature is differently weighted for the two parties.</p></li>
</ol>
<p>But let’s shift our perspective somewhat: instead of assuming that we know how a feature will be differently weighted for the different parties, let’s look at the features which have the strongest weights, to find out what our data says is most typical for the Republican or the Democratic party. To do so, we are again going to use the <code>sort()</code> function, this time on the model parameters for the Republican party, and nest it inside the <code>head()</code> function to show the top thirty Republican features:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(nb_model<span class="sc">$</span>param[<span class="st">"rep"</span>, ], <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   terrorist          tax         bush         iraq        kerri       terror 
0.0027263357 0.0025833085 0.0024335081 0.0022634099 0.0021382798 0.0018931958 
       enemi        iraqi        senat      freedom         vote  afghanistan 
0.0014407140 0.0014318273 0.0014307967 0.0014120269 0.0013972301 0.0013688277 
         war       attack     militari           re         must         peac 
0.0013649638 0.0013375806 0.0012460776 0.0011354051 0.0010909518 0.0010831288 
      saddam       weapon        secur       govern        women        georg 
0.0010679771 0.0010587423 0.0010486988 0.0010331179 0.0010324453 0.0010277067 
          11        oppon         forc           ve            9          men 
0.0010202837 0.0009950418 0.0009903109 0.0009785223 0.0009727465 0.0009671968 </code></pre>
</div>
</div>
<p>This is where it becomes interesting on a content level: we can see that many of the strongest Republican features directly reflect aspects of Republican policy priorities. There are strong flavors of national security in the top thirty features, among them <em>terrorist</em>, <em>enemi</em>, <em>war</em>, <em>attack</em>, <em>militari</em>, and several more. There are features pointing to fiscal policy, with <em>tax</em> and <em>money</em>; former Republican presidents, with <em>georg</em> and <em>bush</em>; and we can be pretty certain that the feature <em>11</em> refers to 9/11. Clearly, there is more to unpack here, but we’ll leave that to you, and carry on by looking at the top features in Democratic speeches:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(nb_model<span class="sc">$</span>param[<span class="st">"dem"</span>, ], <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      health        insur          kid       school       colleg        crime 
0.0017384387 0.0014966996 0.0014830241 0.0013742897 0.0012697023 0.0012523090 
        educ         care           re          got        thing        parti 
0.0011907901 0.0011545364 0.0011449606 0.0011446589 0.0011396034 0.0010852208 
   communiti         bill      compani      economi     children          tax 
0.0010817212 0.0010578212 0.0010378343 0.0010255794 0.0010237001 0.0010198765 
      invest          lot      student    everybodi           mr      tonight 
0.0010085849 0.0010046681 0.0009972725 0.0009866790 0.0009757271 0.0009701784 
       money        chang      problem      deficit         rate      centuri 
0.0009654147 0.0009642819 0.0009617746 0.0009580582 0.0009533844 0.0009505423 </code></pre>
</div>
</div>
<p>The output again is not surprising, in the sense that the policy focus of the Democratic party also becomes visible here. There are features pointing to Medicare, with <em>health</em>, <em>insur</em>, <em>care</em>, likely also <em>bill</em> (although it will probably also be a reference to Bill Clinton, which would look exactly the same as a result of our pre-processing); education is a big theme as well, with <em>school</em>, <em>colleg</em>, <em>educ</em> and <em>student</em> featuring prominently; and there is also a sense of inclusion, with features like <em>communiti</em>, <em>everybodi</em> and <em>tonight</em>.</p>
<p>We should state here clearly that these feature tables are great to get a high-level view, firstly on whether the model managed to roughly identify the key features, and secondly on what the content of a corpus is, like the party programs here, but obviously no conclusions should be drawn merely on the basis of these keywords. Just in case anyone needed reminding.</p>
<p>That being said, there are many ways to mine these feature tables for insight, and we’ll discuss some of them right now.</p>
</section>
<section id="weighted-feature-tables" class="level2">
<h2 class="anchored" data-anchor-id="weighted-feature-tables">Weighted Feature Tables</h2>
<p>The first weighting we do is very simple: we look at features that are relatively more important for Republicans than for Democrats. They way we do this is by dividing the the weight each feature has for Republicans by the weight the same feature has for the Democrats, like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>rel_weight_rep <span class="ot">&lt;-</span> (nb_model<span class="sc">$</span>param[<span class="st">"rep"</span>, ] <span class="sc">/</span> nb_model<span class="sc">$</span>param[<span class="st">"dem"</span>, ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before looking at the results, a quick sanity check. We saw above that all of the weights are rather small decimal numbers. What happens when you divide a fraction by an even smaller fraction? You get a higher number. So, if a feature has a high weight for Republicans, but a small weight for Democrats, the result of the operation above yields a relatively high number, one that is probably bigger than 1. We save the results of the operation in a new variable, <code>rel_weight_rep</code>, a step you’re very familiar with by now. Let’s see how this plays out, by looking at the top 30 entries:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(rel_weight_rep, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   baldrig       womb        sdi sandinista     riyadh     allawi    nuisanc 
 240.90668  201.61104  198.51968  178.78209  167.58407  141.95236  135.99654 
      bork        lsu     embryo      frist     frivol    zarqawi  decontrol 
 131.51267  128.52548  125.41913  123.88258  117.32835  114.19406  112.63911 
 murkowski    ghadafi     malais     casper        inf  bioshield nicaraguan 
 112.61713  112.04156  105.07997  103.54245  102.94571  100.40961   97.19428 
    khobar     gipper      saxbi      cruis   deadlier    midland       goos 
  97.11587   96.92068   93.52018   92.36902   91.18261   89.39415   88.85977 
   moammar up-or-down 
  88.21562   87.86447 </code></pre>
</div>
</div>
<p>This looks pretty different than the top 30 unweighted Republican features, right? Most obviously, there are a lot of rare words in there - <em>sandinista</em> or <em>bork</em>, anyone? - which indicates that there is a sparse data problem. This is to say that, because some features only occur a handful of times (in this case, thanks to our pre-processing, a minimum of five times), they can end up overly prominent. Our results will differ slightly from yours again, but we’ll just take this as read in the following discussion.</p>
<p>Not everything we get in our output is readily interpretable, but there are some features that are insightful and point to certain hot topics. For instance, we have <em>womb</em> and <em>embryo</em> which are plausibly connected with the Republican’s stance and debates on abortion; we also find <em>moammar</em> and <em>ghadafi</em>, who was the dictator of Libya at the time these speeches were made; another feature of interest during these plague times is <em>bioshield</em>, which refers to a US government project aimed at setting up an early warning system for anthrax and other pathogens.</p>
<p>Let’s use the same procedure to generate the equivalent list for the Democrats:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>rel_weight_dem <span class="ot">&lt;-</span> (nb_model<span class="sc">$</span>param[<span class="st">"dem"</span>, ] <span class="sc">/</span> nb_model<span class="sc">$</span>param[<span class="st">"rep"</span>, ])</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(rel_weight_dem, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        obes       tipper     preexist     kathleen          wto          arn 
   211.09946    171.62596    158.24533    109.89303    106.98350    106.20290 
superhighway    sotomayor       slater      downsiz      aristid       calori 
   102.03083     96.79835     95.53900     91.53139     88.13759     87.05692 
      turbin        felon   work-studi         jare      tobacco         1835 
    86.02117     85.42189     81.75980     80.96393     77.97834     77.91716 
     shalala           bp          dnc      bailout           gm        romer 
    77.58837     77.51820     75.63924     74.73056     74.64072     73.02344 
  youngstown    underserv     refinanc        sonia          g20         2020 
    72.50484     71.95238     70.09749     69.85221     67.26647     66.95123 </code></pre>
</div>
</div>
<p>As above, we get a bit of a mixed bag of features here, with some features being somewhat unplaceable, but others tying in to relevant policy topics. We get, for instance, <em>obes</em>, <em>calori</em> and <em>nutriti</em>, which all relate to diet - which ties in with public health policy; we also see the features <em>refinanc</em> and <em>bailout</em>, which are reasonably important in the context of the global financial crisis of the late 2000s.</p>
<p>Okay, so while there are some insights to be gleaned here that point us to single events or referents, we are also left with a bunch of features from which we cannot readily make heads nor tails. But we can get more insightful feature tables: In order to detect broader trends, we can add biases that favor features that occur with a high frequency.</p>
<p>For this next step, take almost exactly the same measure as above, except this time round, we take the Republican weights for the features to the power of two, like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>rel_weight_rep2 <span class="ot">&lt;-</span> (nb_model<span class="sc">$</span>param[<span class="st">"rep"</span>, ]<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> nb_model<span class="sc">$</span>param[<span class="st">"dem"</span>, ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So, what are we doing here? Essentially, we are entering the exponential realm, where the changes to numbers are in reference to their original value. What this means is that the effect of an operation is bigger on the weights which had higher value starting out, than to those which had lower values.</p>
<p>Perhaps this becomes clearer with a numeric example. If we take the second power of 0.1, the result is further away from the original number than if we take the second power of 0.01. We can see this by subtracting the second power:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.1</span> <span class="sc">-</span> (<span class="fl">0.1</span><span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.09</code></pre>
</div>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.01</span> <span class="sc">-</span> (<span class="fl">0.01</span><span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0099</code></pre>
</div>
</div>
<p>Why does this matter? Because the change to the rare words we saw above, which came out on top of the list because they had higher weights, is bigger than the change to the more frequent words, which did not make the top pick because they had lower weights. Since we are keeping the Democrat weights the way they were, we are basically evening the playing field in our variable <code>rel_weight_rep2</code>. Let’s see how that affects the top 30 feature table:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(rel_weight_rep2, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       baldrig        nuisanc         frivol           womb            sdi 
    0.06209630     0.05779213     0.04905146     0.04349072     0.04216724 
    sandinista          kerri         riyadh      terrorist         liabil 
    0.03419921     0.03365541     0.03004924     0.02990812     0.02811661 
        casper         allawi     up-or-down        lawsuit           bush 
    0.02769779     0.02156022     0.02145746     0.02037906     0.01857149 
          bork    bush-cheney            lsu          regim         embryo 
    0.01850560     0.01769796     0.01767447     0.01709440     0.01683045 
         frist          iraqi two-and-a-half         terror        zarqawi 
    0.01642058     0.01602498     0.01496621     0.01464207     0.01395260 
         index      decontrol      murkowski        ghadafi          enemi 
    0.01374865     0.01357521     0.01356991     0.01343156     0.01316258 </code></pre>
</div>
</div>
<p>After all the conceptual trouble we went through for these results, you may be a bit underwhelmed to find a lot of the same words cropping up. In our example, we find some shifts that indicate broader trends, and you’ll likely see one or two more <em>mainstream</em> words in your top 30 feature table us well, if you look closely.</p>
<p>In our case, the relevant new features are <em>regim</em>, <em>iraqi</em> and <em>libya</em>. As mentioned earlier, Libya and former dictator Muammar Gahadafi were big topics at the time, and we think it’s telling that <em>libya</em> and <em>regim</em> only shows up in the feature table after this weighting step, since it’s quite likely that the Democratic candidates would also have discussed the Libyan situation frequently during this period. The same, of course, goes for the <em>iraqi</em> feature.</p>
<p>Okay, now let’s see how things stand for the Democrats:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>rel_weight_dem2 <span class="ot">&lt;-</span> (nb_model<span class="sc">$</span>param[<span class="st">"dem"</span>, ]<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> nb_model<span class="sc">$</span>param[<span class="st">"dem"</span>, ])</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(rel_weight_dem2, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      health        insur          kid       school       colleg        crime 
0.0017384387 0.0014966996 0.0014830241 0.0013742897 0.0012697023 0.0012523090 
        educ         care           re          got        thing        parti 
0.0011907901 0.0011545364 0.0011449606 0.0011446589 0.0011396034 0.0010852208 
   communiti         bill      compani      economi     children          tax 
0.0010817212 0.0010578212 0.0010378343 0.0010255794 0.0010237001 0.0010198765 
      invest          lot      student    everybodi           mr      tonight 
0.0010085849 0.0010046681 0.0009972725 0.0009866790 0.0009757271 0.0009701784 
       money        chang      problem      deficit         rate      centuri 
0.0009654147 0.0009642819 0.0009617746 0.0009580582 0.0009533844 0.0009505423 </code></pre>
</div>
</div>
<p>In our results, we see a rather pronounced change in the output once we apply this weighting to the Democrat features. The features showing up in the top thirty here are fairly similar to the unweighted features, and point to broadly recognizable policy issues: <em>health</em>, <em>insur</em>, <em>care</em> and <em>bill</em> are likely in reference to Medicare for All; <em>kid</em>, <em>school</em>, <em>educ</em>, <em>student</em> and <em>college</em> point to the education platform which a lot of Democratic candidates highlight.</p>
<p>Let’s look at two more ways of biasing the feature tables in favor of frequent words. The next one is a lot more intuitive: we take the weights, and multiply them by the document frequencies for each feature. The more frequent a feature, the higher the result, and words mentioned particularly frequently, for instance because they were <em>hot</em> topics for longer periods of time, are boosted. Easy peasy:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>rel_weight_rep3 <span class="ot">&lt;-</span> (nb_model<span class="sc">$</span>param[<span class="st">"rep"</span>, ] <span class="sc">*</span> doc_freq)</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(rel_weight_rep3, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      tax        re     senat        ve   freedom    govern    presid       war 
 3.763880  2.988386  2.748560  2.583299  2.451279  2.274926  2.244644  2.211241 
  america    nation      vote      must     secur         t       got terrorist 
 2.189457  2.070588  2.053928  2.049898  2.029232  1.997125  1.971281  1.924793 
     bush     world      unit       job         m        ll    terror     women 
 1.917604  1.797263  1.774218  1.762006  1.724214  1.701073  1.694410  1.623004 
     need     state      busi      forc      iraq       get 
 1.590850  1.574099  1.572305  1.562711  1.557226  1.550382 </code></pre>
</div>
</div>
<p>We are really entering mainstream topics now, and the features start to tie in with the Republican policy themes on a major level: we get <em>tax</em>, <em>freedom</em>, <em>secur</em>, <em>job</em> and <em>busi</em>, pointing strongly to Republican domestic policy focal points; we get foreign policy for a turbulent world with <em>war</em>, <em>terror</em>, <em>forc</em>, <em>iraq</em>; and we get aspects of US gouvernance, with <em>state</em>, <em>presid</em>, <em>nation</em>, <em>vote</em> and <em>bush</em>.</p>
<p>Let’s also take a look at the top Democratic features under this weighting scheme:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>rel_weight_dem3 <span class="ot">&lt;-</span> (nb_model<span class="sc">$</span>param[<span class="st">"dem"</span>, ] <span class="sc">*</span> doc_freq)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(rel_weight_dem3, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       re     thing         t    school    health       got        ve     think 
 3.013536  2.697441  2.659516  2.417376  2.352108  2.344261  2.315918  2.185339 
       go      care       get       job       don  children      educ     world 
 2.174433  2.146283  2.132836  2.075324  2.068395  2.032045  1.996955  1.970602 
      say   economi       lot communiti    becaus      just    famili      give 
 1.901712  1.894245  1.890785  1.860560  1.766426  1.760574  1.756249  1.753537 
    chang       let       new    believ      need     right 
 1.723172  1.718571  1.711858  1.691188  1.688258  1.687635 </code></pre>
</div>
</div>
<p>Here, we also get a rather distinct set of new words in the table: we still have aspects of <em>care</em> and <em>health</em>, as well as <em>educ</em> and <em>school</em>, but on top of that we get some value-based features like <em>right</em> and <em>just</em>. This weighting also shows a bit of how Democrats argue: there is a flavor of causality with <em>becaus</em> and <em>need</em>, and there are calls for the <em>new</em> and <em>chang</em>.</p>
<p>What cropped up in the top features for both parties are contractions: <em>re</em>, <em>ve</em>, <em>don</em>, <em>t</em>, and <em>m</em>. These are words that are generally frequent, and our frequency weighting now has the side effect that entries for frequent words overlap.</p>
<p>One could imagine a research question here: how well can document classification distinguish between American and British politicians on the basis of speeches?</p>
<p>Let’s look at a final weighting scheme. We use almost the same approach as above, but this time round we multiply the feature weights with the logarithms of the document frequency. This is different compared to the other approaches in that it still somewhat favors frequent words, but the gap between the more and less frequent words becomes relatively small. The danger that generally frequent words, e.g.&nbsp;contractions, move up to the top gets smaller. In case you need convincing of this, just take a quick look at the first few document frequencies and their logarithms:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(doc_freq)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>christian     usual     think    christ      term     oblig 
      143       278      2630        30       954       559 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(<span class="fu">head</span>(doc_freq))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>christian     usual     think    christ      term     oblig 
 4.962845  5.627621  7.874739  3.401197  6.860664  6.326149 </code></pre>
</div>
</div>
<p>So far, so good. Now let’s take a look at how this impacts the top thirty features:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>rel_weight_rep4 <span class="ot">&lt;-</span> (nb_model<span class="sc">$</span>param[<span class="st">"rep"</span>, ] <span class="sc">*</span> <span class="fu">log</span>(doc_freq))</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(rel_weight_rep4, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        tax   terrorist        bush        iraq      terror       kerri 
0.018817167 0.017883713 0.016230278 0.014788642 0.012867718 0.011382085 
      senat     freedom        vote         war       enemi      attack 
0.010817683 0.010532787 0.010190024 0.010087330 0.009400702 0.009076270 
afghanistan          re    militari       iraqi        must      govern 
0.008998235 0.008941882 0.008792542 0.008592989 0.008224134 0.007952034 
      secur        peac          ve       women         got        forc 
0.007936409 0.007869396 0.007709321 0.007598905 0.007338997 0.007292564 
     weapon         men       spend       feder       georg       money 
0.007232136 0.007110637 0.006918580 0.006885096 0.006826528 0.006569237 </code></pre>
</div>
</div>
<p>The changes compared to the earlier weightings are relatively slight here, but there are some different features which offer a new policy flavor. While <em>tax</em> appears in most of the Republican top features, regardless of the approach to weighting, here we get the features <em>money</em> and <em>spend</em>, which chimes well with Republican fiscal policy.</p>
<p>Finally, let’s see what this weighting yields for the Democrats:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>rel_weight_dem4 <span class="ot">&lt;-</span> (nb_model<span class="sc">$</span>param[<span class="st">"dem"</span>, ] <span class="sc">*</span> <span class="fu">log</span>(doc_freq))</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(rel_weight_dem4, <span class="at">decreasing =</span> T), <span class="at">n =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     health      school         kid       insur          re       thing 
0.012534281 0.010269381 0.010112629 0.009620942 0.009017136 0.008854010 
       educ         got      colleg        care       crime   communiti 
0.008841332 0.008727588 0.008711000 0.008691112 0.008242729 0.008058909 
      parti    children        bill     economi         lot           t 
0.007910096 0.007773338 0.007769400 0.007713709 0.007575288 0.007466922 
        tax       chang      invest     problem       money   everybodi 
0.007428918 0.007220826 0.007173386 0.007100538 0.007069259 0.007064690 
         ve     compani        rate          mr     centuri         don 
0.006911379 0.006909912 0.006887967 0.006873053 0.006856292 0.006854891 </code></pre>
</div>
</div>
<p>Again, we see many familiar features, but we get a few features which bring in something new. We see <em>problem</em>, which is a good springboard for any policy platform; we also get <em>percent</em>, which may point to a mode of argumentation; and <em>everybodi</em>, augmenting the message of inclusion which has been part of previous feature tables in the form of <em>communiti</em>.</p>
<p>Having looked at all these different weightings and brought our perspective as politically interested researchers to them, we ought to issue a caveat: Obviously, we would have to go and look at the speeches in detail to draw meaningful conclusions about what the policy programs and interests of the Republican and Democratic parties are. What we wanted to demonstrate here is that the feature tables and the different weightings offer a strong flavor as to the most important topics, but this should not be confused with actual political demands.</p>
<p>After this extensive trawl through the feature tables, two topics remain for us to cover together here. The first of these are alternatives to the Naive Bayes model. Second, and ultimately, we are going to point out some steps which could be summarized aptly under <em>Best Practices</em>.</p>
</section>
</section>
<section id="alternative-models" class="level1">
<h1>Alternative Models</h1>
<p>Although we did not state so explicitly above, you won’t be terribly surprised at the fact that the Naive Bayes isn’t the only algorithm we can use to train a document classification model. There is a flourishing ecosystem of classification algorithms out there, from the <em>support vector machine</em> to the <em>random forest</em>. The one we are looking at here, though, is the logistic regression.</p>
<section id="logistic-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-model">Logistic Regression Model</h2>
<p>All classification algorithms have their pros and cons, usually resulting in a trade-off the researcher has to make between the amount of computational capacity required, the accuracy of the model, the interpretability etc. With the Naive Bayes model, we came down on the side of the less computationally intensive and less accurate end. With the logistic regression model, we ramp up both the computational requirements as well as the accuracy, while still remaining easily interpretable. You’ll see what we mean in a second.</p>
<p>A lot of what we are doing here should look very familiar. First, we create a new variable, called <code>lr_model</code>. We assign to it a text model based on the logistic regression algorithm, which we create using one of these aptly named quanteda functions: <code>textmodel_lr</code>. The arguments are identical to the arguments in the Naive Bayes approach. First, we throw in the training data, then we specify the document variable we want to train on. We run this command, and take a peek at the <code>lr_model</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>lr_model <span class="ot">&lt;-</span> <span class="fu">textmodel_lr</span>(train_dtm, <span class="at">y =</span> <span class="fu">docvars</span>(train_dtm, <span class="st">"party"</span>))</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>lr_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
textmodel_lr.dfm(x = train_dtm, y = docvars(train_dtm, "party"))

2,800 training documents; 11,780 fitted features.
Method: binomial logistic regression</code></pre>
</div>
</div>
<p>As with the <code>nb_model</code>, just looking at the <code>lr_model</code> does not yield terrific insight, beyond the fact that the model looks the way it ought. We can tell this from the description: we input 2,800 training documents and have 11,785 features, as always in our data. Most importantly for the current purpose, we receive confirmation that the method is that of the <em>binomial logistic regression</em>. Perfect!</p>
<p>To see how well the logistic regression model does, we take exactly the same steps as we did for the Naive Bayes model: first, we use the model to predict the party of the speeches in the test data. Then, we create a confusion matrix. We also already draw up the confusion matrix of the <code>nb_model</code>, so we can easily compare the two.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>pred_lr <span class="ot">&lt;-</span> <span class="fu">predict</span>(lr_model, <span class="at">newdata =</span> test_dtm)</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>confmat_lr <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">prediction =</span> pred_lr, <span class="at">PARTY =</span> <span class="fu">docvars</span>(test_dtm, <span class="st">"party"</span>))</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>confmat_lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          PARTY
prediction dem rep
       dem 250   8
       rep   9 180</code></pre>
</div>
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>confmat_nb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          party
prediction dem rep
       dem 245  20
       rep  14 168</code></pre>
</div>
</div>
<p>As above, we will not get exactly the same numbers here, but unless something went terribly wrong, you should see that the logistic regression model is somewhat more successful at correctly predicting the party.</p>
<p>Let’s go a step further and put the proverbial and actual number on it. We discussed earlier how we can calculate the accuracy from the confusion matrix. So, we sum again the number of correct predictions and divide them by the total number of predictions. And for easier comparison, we’ll draw up the accuracy of the Naive Bayes model as well:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>accuracy_lr <span class="ot">&lt;-</span> (confmat_lr[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> confmat_lr[<span class="dv">2</span>, <span class="dv">2</span>]) <span class="sc">/</span> <span class="fu">sum</span>(confmat_lr)</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>accuracy_lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9619687</code></pre>
</div>
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>accuracy_nb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9239374</code></pre>
</div>
</div>
<p>What we see in the confusion matrix is, of course, mirrored in the accuracy of the models. In my case, the logistic regression model is approximately 2.5 percentage points more accurate than the Naive Bayes model. Does this roughly match the difference you see between your two models?</p>
</section>
<section id="differences-explained" class="level2">
<h2 class="anchored" data-anchor-id="differences-explained">Differences Explained</h2>
<p>That we get different results when creating models using different algorithms will not surprise you whatsoever. But you might have noticed a different theme running through the last few steps: we are not able to say confidently that the numbers we get are exactly the ones you are seeing. The reason, as alluded to briefly above, is that chances are really high that you are training your model on a different sample of speeches than we are.</p>
<p>And you can see for yourself how this works. We are going to repeat the steps from the data sampling to the calculation of the accuracy. The only thing that changes is the set of training data that is sampled initially. You can either just let the code do its thing and check out the results; or you can quickly try to recall for yourself what each line does (and sneak a peek at the descriptions above, in case you can’t reconstruct why we are doing what). For didactic purposes, we obviously recommend the latter, but we can’t hold a grudge against you if you just want to see the results:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>train_dtm2 <span class="ot">&lt;-</span> <span class="fu">dfm_sample</span>(dtm, <span class="at">size =</span> <span class="dv">2800</span>)</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>test_dtm2 <span class="ot">&lt;-</span> dtm[<span class="fu">setdiff</span>(<span class="fu">docnames</span>(dtm), <span class="fu">docnames</span>(train_dtm2)), ]</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>lr_model2 <span class="ot">&lt;-</span> <span class="fu">textmodel_lr</span>(train_dtm2, <span class="at">y =</span> <span class="fu">docvars</span>(train_dtm2, <span class="st">"party"</span>))</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>pred_lr2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(lr_model2, <span class="at">newdata =</span> test_dtm)</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>confmat_lr2 <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">prediction =</span> pred_lr2, <span class="at">PARTY =</span> <span class="fu">docvars</span>(test_dtm, <span class="st">"party"</span>))</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>confmat_lr2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          PARTY
prediction dem rep
       dem 258   1
       rep   1 187</code></pre>
</div>
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>confmat_lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          PARTY
prediction dem rep
       dem 250   8
       rep   9 180</code></pre>
</div>
</div>
<p>You’ll find that the confusion matrices show a visible difference between the two logistic regression models, trained on two different samples. Constituted by the fact of how we calculate it, this difference is obviously also reflected in the accuracy:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>accuracy_lr2 <span class="ot">&lt;-</span> (confmat_lr2[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> confmat_lr2[<span class="dv">2</span>, <span class="dv">2</span>]) <span class="sc">/</span> <span class="fu">sum</span>(confmat_lr2)</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>accuracy_lr2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9955257</code></pre>
</div>
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>accuracy_lr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9619687</code></pre>
</div>
</div>
<p>The difference we see here in the confusion matrix and the accuracy have a pretty detrimental implication for any scientific application of document classification: the results are not robustly replicable in the face of random sampling. To prevent you from turning in naive results (pardon the pun), we’ll discuss next what you can do in the face of this issue.</p>
</section>
<section id="best-practices" class="level2">
<h2 class="anchored" data-anchor-id="best-practices">Best Practices</h2>
<p>In any field, whether empirical or theoretical, part of what constitutes good academic practice is to be transparent about a) what the steps you have taken to arrive from your starting point to your conclusion and b) what the limitations and affordances of your methodology are.</p>
<p>As far as the steps go, we have given you a detailed blow-by-blow instruction about how we get from the input data to meaningful results, both in terms of document classification accuracy and salient features. If you made it this far into this notebook you must also be quite convinced of the affordances of document classification. What we are going to discuss next is probably the best way to address the limitations of document classification which arise from sampling.</p>
</section>
</section>
<section id="sampling-and-re-sampling-our-way-to-robustness" class="level1">
<h1>Sampling and Re-Sampling Our Way to Robustness</h1>
<p>Seeing as you’re here, we take it there’s a decent chance that you’ve come across a normal distribution before. While we’re certain that you’re a lovely person, all of us sometimes need our memories jogged - in which case we recommend you refer to this excellent introduction to descriptive statistics or our very own introduction to inferential statistics, where we discuss and demonstrate in R how the normal distribution arises, using the example of coin tosses (pardon the shameless plug).</p>
<p>The reason we’re bringing this up is because you can use the mechanisms of the normal distribution to increase the robustness of your document classification results. While the data you’re training the model on determines the precise accuracy of the model, there is nothing much that can make it go too far afield. Consider: we are using ca. 90% of the speeches to train the data. As a result most of the data is used to train the models each time, meaning that there is only a limited level of variation that can arise between the models trained on different samples. The consequence of this is that if we train enough models on enough samples and calculate the accuracy of each model, we will end up with a normal distribution.</p>
<section id="loops" class="level2">
<h2 class="anchored" data-anchor-id="loops">Loops</h2>
<p>Since it’s a bit tedious to run through the whole sequence of commands from sampling to calculating the accuracy, we can use a loop to go through this process any given number of times (although here, as with the choice of algorithm, there is a small trade-off between the number of times we can and want to do this and the benefits to robustness). Let’s take a look at how this loop thing works.</p>
<p>First, we construct a new variable. Let’s call it <em>accuracy</em>. We’ll be basing our loop on this, so we will make <em>accuracy</em> a vector, a list of numbers, whose length corresponds to the number of times we want to run our loop. We can do this as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you look at <code>accuarcy</code>, you can see that it’s just a vector containing the integers 1 through 100:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
 [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
 [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
 [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
 [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
 [91]  91  92  93  94  95  96  97  98  99 100</code></pre>
</div>
</div>
<p>Now for the loop. If you’ve never come across this before, it might seem like a bit of an unwieldy construction - in which case, we recommend focusing first on the six lines of code inside the { curly brackets }:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> accuracy) {</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>    train_dtm <span class="ot">&lt;-</span> <span class="fu">dfm_sample</span>(dtm, <span class="at">size =</span> <span class="dv">2800</span>)</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>    test_dtm <span class="ot">&lt;-</span> dtm[<span class="fu">setdiff</span>(<span class="fu">docnames</span>(dtm), <span class="fu">docnames</span>(train_dtm)), ]</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>    lr_model <span class="ot">&lt;-</span> quanteda.textmodels<span class="sc">::</span><span class="fu">textmodel_nb</span>(train_dtm, <span class="fu">docvars</span>(train_dtm, <span class="st">"party"</span>))</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>    pred_lr <span class="ot">&lt;-</span> <span class="fu">predict</span>(lr_model, <span class="at">newdata =</span> test_dtm)</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>    confmat_lr <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">prediction =</span> pred_lr, <span class="at">PARTY =</span> <span class="fu">docvars</span>(test_dtm, <span class="st">"party"</span>))</span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a>    accuracy[i] <span class="ot">&lt;-</span> (confmat_lr[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> confmat_lr[<span class="dv">2</span>, <span class="dv">2</span>]) <span class="sc">/</span> <span class="fu">sum</span>(confmat_lr)</span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>What you find inside the curly brackets should, for the most part, be familiar. In the first line, we have the sampling of the training data. This is the essential step, which allows us to identify how robust our results are. The second line defines the test data in relation to the training data. In the third line, we train our linear regression model. The fourth line uses the model to predict the parties of the speeches in the test data. The fifth line creates the by now familiar confusion matrix. And finally, in the sixth line, we calculate the accuracy, and save it to our variable, <code>accuracy</code>.</p>
<p>The one unfamiliar element occurs in this sixth line, where we indicate that the accuracy is to be saved to the index position [i] in the vector <code>accuracy</code>. This starts to make sense if we look at what is outside the curly brackets. The line which we actually start the command with is <code>for (i in accuracy)</code>, followed by what is in the brackets.</p>
<p>This basically states that for each index, each entry, in the vector accuracy, the sequence of commands in the curly brackets should be executed. With the sixth line inside the curly bracket, we make use of the fact that we can overwrite any single value contained within the vector <code>accuracy</code>, without altering the length of the vector.</p>
<p>When we execute the loop, the six lines of code inside the curly brackets run twenty times. The first time, the accuracy that we calculate is saved in the first position of the vector, overwriting the number <code>1</code> that was there before. The second time, the calculated accuracy overwrites the second position of the vector, where the number <code>2</code> was before. And so on, until each of the twenty integers has been replaced with the accuracy of twenty models trained on twenty different sets of training data.</p>
<p>We can see that this works by looking at the variable accuracy once we’ve run the loop:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  [1] 0.9418345 0.9373602 0.9172260 0.9395973 0.9395973 0.9373602 0.9507830
  [8] 0.9395973 0.9395973 0.9440716 0.9149888 0.9373602 0.9395973 0.9418345
 [15] 0.9485459 0.9351230 0.9440716 0.9552573 0.9351230 0.9440716 0.9351230
 [22] 0.9440716 0.9597315 0.9440716 0.9619687 0.9261745 0.9306488 0.9440716
 [29] 0.9373602 0.9485459 0.9619687 0.9440716 0.9440716 0.9440716 0.9328859
 [36] 0.9328859 0.9328859 0.9395973 0.9530201 0.9351230 0.9194631 0.9619687
 [43] 0.9463087 0.9351230 0.9284116 0.9395973 0.9328859 0.9217002 0.9351230
 [50] 0.9373602 0.9485459 0.9552573 0.9306488 0.9284116 0.9373602 0.9507830
 [57] 0.9574944 0.9284116 0.9328859 0.9306488 0.9261745 0.9217002 0.9306488
 [64] 0.9597315 0.9395973 0.9574944 0.9619687 0.9261745 0.9485459 0.9351230
 [71] 0.9440716 0.9395973 0.9082774 0.9440716 0.9507830 0.9418345 0.9306488
 [78] 0.9194631 0.9507830 0.9463087 0.9328859 0.9373602 0.9418345 0.9284116
 [85] 0.9373602 0.9530201 0.9373602 0.9395973 0.9507830 0.9239374 0.9619687
 [92] 0.9172260 0.9395973 0.9463087 0.9440716 0.9328859 0.9328859 0.9172260
 [99] 0.9507830 0.9418345</code></pre>
</div>
</div>
<p>Where before we had integers, we now have values somewhere in the ballpark of 92-98% (also depending on whether we use Naive Bayes or the more performant Logistic Regression). On the one hand this shows us that there is some observable variation in the accuracy of our models which is predicated on the data sample the model is trained on. On the other hand, it shows us that, regardless of the training data, the models all reach a decent degree of accuracy somewhere above 90% also in a worst-case scenario.</p>
</section>
<section id="demonstrating-robustness" class="level2">
<h2 class="anchored" data-anchor-id="demonstrating-robustness">Demonstrating Robustness</h2>
<p>While it’s easy enough to take in the results of the loop if we’re working with a fairly low number of iterations, it becomes a bit more challenging if we consider running this loop hundreds of times. In those cases, we can summarize our results in some key statistics and show them graphically in the form of a histogram. We aren’t going for any shiny visuals here, but you can find many a good instruction into how to make those. We’ll just show you the bare-bones approaches here.</p>
<p>One of the easiest ways to get a summary of a numerical vector is the <code>summary()</code> command, which is part of basic R functionality. Let’s take a look:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(accuracy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.9083  0.9329  0.9396  0.9394  0.9463  0.9620 </code></pre>
</div>
</div>
<p>We get several statistics here. On either pole of the output we see the minimum and maximum values contained in the vector. In my case, the worst model achieved an accuracy of 91% while the best model achieved an accuracy of 97%. In between these two poles we have the median and the arithmetic mean, the colloquial (sometimes even pedestrian) average. In my case, both the medium and the mean landed on 94.2%, indicating both that the module robustly achieves an accuracy of 94% and that the accuracies are indeed normally distributed.</p>
<p>We can also visualize this normal distribution in a histogram, using the <code>hist()</code> command. The first argument here is the data, our <code>accuarcy</code> vector, and the second argument makes the whole graphic more visually palatable (try it without this specification to see what we’re talking about):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="sc">%&gt;%</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="at">accuracy =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> accuracy)) <span class="sc">+</span></span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="atap_docclass_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>What you see in the bottom right quadrant should roughly resemble a normal distribution - although we will add the disclaimer that there is a chance that it doesn’t, on account of the fact that we don’t have that many data points. Once we reach somewhere around a thousand runs, we should more or less reliably arrive at a normal distribution. But be warned: if you do that, it’s probably worth starting the loop and then going to fix yourself a snack. Unless you have a really good computer, this could take a while.</p>
<p>What you’ll also notice about the graph we just created is that it’s centered around the mean of our distribution. This is great when you want to see what the distribution looks like. If you’ve arrived at the point where you want to brag about how good your results are, we recommend plotting the histogram showing all of the x-axis between 0 and 1. We can do this with the additional specification of <code>xlim=0:1</code>. Like so:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>accuracy <span class="sc">%&gt;%</span></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="at">accuracy =</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> accuracy)) <span class="sc">+</span></span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="atap_docclass_files/figure-html/hist-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Now you can see that the results cluster fairly heavily in the far right of the graph, as expected. The final metric we want to draw your attention to, in case it hasn’t sprung to mind yet, is the standard deviation. There is a technical definition that you can look up at your leisure; but in short it describes how widely the data is scattered around the mean. The heuristic is that, the higher the standard deviation, the less robust the results. Let’s take a look at the standard deviations for our accuracies:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(accuracy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.01147901</code></pre>
</div>
</div>
<p>In my case, I get a standard deviation of 1.2 percentage points. This translates to the following: roughly 66% of all models achieve an accuracy of 94.2% (the mean), plus and minus 1.2 percentage points. So two thirds of the models make predictions that are 93% to 95.4% accurate.</p>
<p>There are other methods to measure robustness, a popular one is n-fold cross validation, in which we train n times, with a specific 1-1/n section used for training and 1/n for testing, in such a way that every section is used once for testing. You could program n-fold cross validation as an advanced exercise. Using a loop with random sampling is a simple way to explore robustness, albeit less systematic than cross-validation.</p>
<p>If you always want to get the same result, you can also force the sampling to always return the same documents, by setting a seed for the random calculation with the set.seed() function. While this ensures reproducibility, it may hide robustness problems, and this is why we chose not to use it.</p>
<p>For a spam filter, the performance of our system would clearly be insufficient (we can see at least one of us biting into his laptop if every 20th spam mail reached our inbox). But as you have seen, there are various ways in which a higher accuracy can be achieved, from the specific pre-processing steps to the choice of algorithms. While there are dozens, if not hundreds, of other parameters that can be tweaked, after working your way through this introduction you should be ready to start your own document classification experiments and do cool stuff! Let’s get on it!</p>
</section>
</section>
<section id="citation-session-info" class="level1 unnumbered">
<h1 class="unnumbered">Citation &amp; Session Info</h1>
<p>Schneider, Gerold and Max Lauber. 2022. <em>Classifying American Speeches</em>. Brisbane: The University of Queensland. url: https://ladal.edu.au/atap_docclass.html (Version 2022.11.15).</p>
<pre><code>@manual{schneider2022class,
  author = {Schneider, Gerold  and Lauber, Max},
  title = {Classifying American Speeches},
  note = {https://ladal.edu.au/ATAP_DocClass_Markdown.html},
  year = {2022},
  organization = "Australian Text Analytics Platform (ATAP)},
  address = {Brisbane},
  edition = {2011.11.15}
}</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R version 4.4.1 (2024-06-14)
Platform: aarch64-apple-darwin20
Running under: macOS Sonoma 14.6.1

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib 
LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

time zone: Australia/Brisbane
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] ggplot2_3.5.1             stopwords_2.3            
[3] quanteda.textmodels_0.9.9 quanteda_4.1.0           

loaded via a namespace (and not attached):
 [1] Matrix_1.7-1      glmnet_4.1-8      gtable_0.3.6      jsonlite_1.8.9   
 [5] dplyr_1.1.4       compiler_4.4.1    tidyselect_1.2.1  Rcpp_1.0.13      
 [9] splines_4.4.1     scales_1.3.0      fastmap_1.2.0     lattice_0.22-6   
[13] R6_2.5.1          labeling_0.4.3    SnowballC_0.7.1   generics_0.1.3   
[17] shape_1.4.6.1     knitr_1.48        iterators_1.0.14  htmlwidgets_1.6.4
[21] tibble_3.2.1      munsell_0.5.1     pillar_1.9.0      rlang_1.1.4      
[25] utf8_1.2.4        fastmatch_1.1-4   stringi_1.8.4     xfun_0.49        
[29] LiblineaR_2.10-24 cli_3.6.3         withr_3.0.2       magrittr_2.0.3   
[33] digest_0.6.37     foreach_1.5.2     grid_4.4.1        rstudioapi_0.17.1
[37] lifecycle_1.0.4   vctrs_0.6.5       SparseM_1.84-2    evaluate_1.0.1   
[41] glue_1.8.0        farver_2.1.2      codetools_0.2-20  survival_3.7-0   
[45] fansi_1.0.6       colorspace_2.1-1  rmarkdown_2.28    pkgconfig_2.0.3  
[49] tools_4.4.1       htmltools_0.5.8.1</code></pre>
</div>
</div>
<hr>
<p><a href="#introduction">Back to top</a></p>
<p><a href="https://ladal.edu.au">Back to LADAL home</a></p>
<hr>
</section>
<section id="references" class="level1 unnumbered">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-guerini2010new" class="csl-entry" role="listitem">
Guerini, Marco, Danilo Giampiccolo, Giovanni Moretti, Rachele Sprugnoli, and Carlo Strapparava. 2010. <span>“The New Release of CORPS: A Corpus of Political Speeches Annotated with Audience Reactions.”</span> In <em>International Workshop on Political Speech</em>, 86–98. Berlin: Springer.
</div>
<div id="ref-vergelis2019spam" class="csl-entry" role="listitem">
Vergelis, Maria, Tatyana Shcherbakova, Tatyana Sidorina, and T Kulikova. 2019. <span>“Spam and Phishing in Q1 2019.”</span> <em>Secure List</em>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>As an aside, we once again see some of the potential harm done by pre-processing: we have the feature <em>us</em> ranking really highly here, but we cannot be sure whether this is a plural, inclusive pronoun, or whether this is a botched version of the United States. Depending on what you are working towards, you might be interested in keeping abbreviations like this intact.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>